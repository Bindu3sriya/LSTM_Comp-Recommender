title,description,component
tool to mount ndfs on linux,tool to mount ndfs on linux. It depends on fuse and fuse-j.,fs
make Configuration an interface,"The Configuration class should become an interface, e.g.:  public interface Configuration {   String get(String nam);   String set(String name, String value);    int getInt(String name);   void setInt(String name, int value);   float getFloat(String name);   void setFloat(String name, float value);   //... other utility methods based on get(String) and set(String,String) ... }  An abstract class named ConfigurationBase should be implemented as follows:  public abstract class ConfigurationBase implements Configuration {   abstract public String get(String nam);   abstract public String set(String name, String value);    public  int getInt(String name) { ... implementation in terms of get(String) ... }   public void setInt(String name, int value) {... implementation in terms of set(String, String) ...}   public float getFloat(String name)  { ... implementation in terms of get(String) ... }   public void setFloat(String name, float value)  {... implementation in terms of set(String, String) ...}   //... other utility methods based on get(String) and set(String,String) ... }  A concrete, default implementation will be provided as follows:  public class ConfigurationImpl implements Writable extends ConfigurationBase {   private Properties properties;    // implement abstract methods from ConfigurationBase   public String get(String name) { ... implemented in terms of props ...}   public String set(String name, String value) { .. implemented in terms of props ... }    // Writable methods   public write(DataOutputStream out);   public readFields(DataInputStream in);    // permit chaining of configurations   public Configuration getDefaults();   public void setDefaults(Configuration defaults); }  Only code which creates configurations should need to be updated, so this shouldn't be a huge change.",conf
DF enhancement: performance and win XP support,"1. DF is called twice for each heartbeat, which happens each 3 seconds. There is a simple fix for that in the attached patch.  2. cygwin is required to run df program in windows environment. There is a class org.apache.commons.io.FileSystemUtils, which can return disk free space for different OSs, but it does not have means to get disk capacity. In general in windows there is no efficient and uniform way to calculate disk capacity using a shell command. The choices are 'chkdsk' and 'defrag -a', but both of them are too slow to be called every 3 seconds. WinXP and 2003 server have a new tool called fsutil, which provides all necessary info. I implemented a call to fsutil in case df fails, and the OS is right. Other win versions should still run cygwin. I tested this fetaure for linux, winXP and cygwin. See attached patch.",fs
Adding some uniformity/convenience to environment management,"Currently, ""slaves"" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves  Perhaps split slaves, having a different set for ""datanodes"" vs. ""tasktracker"" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption.  Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable.  These changes would probably be useful to the nutch project, too.",conf
"bufferSize argument is ignored in FileSystem.create(File, boolean, int)","org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize)  ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config.  MapReduce although is probably affected by that, see   org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done)  The attached patch would fix it.",fs
JAVA_OPTS for the TaskRunner Child,"Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process.  Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes.    Examples of why its useful being able to set options are the child include:  + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process.  Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.",conf
PositionCache decrements its position for reads at the end of file,"See  int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len)   if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained.  The attached patch would fix it.",fs
RPC exceptions should include remote stack trace,"Remote exceptions currently only report the exception string.  Instead they should report the entire remote stack trace, as a string, to facilitate debugging.",ipc
"SequenceFile should compress blocks, not individual entries","SequenceFile will optionally compress individual values.  But both compression and performance would be much better if sequences of keys and values are compressed together.  Sync marks should only be placed between blocks.  This will require some changes to MapFile too, so that all file positions stored there are the positions of blocks, not entries within blocks.  Probably this can be accomplished by adding a getBlockStartPosition() method to SequenceFile.Writer.",io
support generic command-line options,"Hadoop commands should all support some common options.  For example, it should be possible to specify the namenode, datanode, and, for that matter, any config option, in a generic way.  This could be implemented with code like:  public interface Tool extends Configurable {   void run(String[] args) throws Exception; }  public class ToolBase implements Tool extends Configured {   public final void main(String[] args) throws Exception {     Configuration conf = new Configuration();     ... parse config options from args into conf ...     this.configure(conf);     this.run();   } }  public MyTool extends ExcecutableBase {   public static void main(String[] args) throws Exception {     new MyTool().main(args);   } }  The general command line syntax could be:  bin/hadoop [generalOptions] command [commandOptions]  Where generalOptions are things that ToolBase handles, and only the commandOptions are passed to Tool.run().  The most important generalOption would be '-D', which would define name/value pairs that are set in the configuration.  This alone would permit folks to set the namenode, datanode, etc.",conf
Bashless Hadoop Start Script,This is for the people who want to try to get hadoop running without cygwin.  The code needs some work and could be improved. Attachment to follow.,scripts
can't get environment variables from HADOOP_CONF_DIR,The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.,conf
add a record I/O framework to hadoop,"Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.","io,ipc"
hadoop doesn't take advatage of distributed compiting in TestDFSIO,"TestDFSIO runs N map jobs, each either writing to or reading from a separate file of the same size,  and collects statistical information on its performance.  The reducer further calculates the overall statistics for all maps.  It outputs the following data: - read or write test - date and time the test finished    - number of files - total number of bytes processed - overall throughput in mb/sec - average IO rate in mb/sec per file  __Results__ I run 7 iterations of the test one after another on a cluster of ~200 nodes.  The file size is the same in all cases 320Mb.  The number of files tried is 1,2,4,8,16,32,64. The log file with statistics is attached. It looks like we don't have any distributed computing here at all. The total execution time increases proportionally to the total size of data both for writes and reads. Another thing is that the io ratio for read is higher than the write rate just gradually. For comparison I attach time measuring for the same ios performed on the same cluster but sequentially in a simple loop. This is the summary:  Files map/red time sequential time  1  49     34   2  86     69  4  158   131  8  299   266 16  569   532 32  1131 64  2218  This doesn't look good, unless there is something wrong with my test (attached) or the cluster settings. ",fs
rpc commands not buffered,Calls using Hadoop's RPC framework get sent across the network byte by byte.,ipc
binary key,"I needed a binary key type, so I extended BytesWritable to be comparable also.",io
SequenceFile performance degrades substantially compression is on and large values are encountered,"The code snippet in quesiton is:       if (deflateValues) {         deflateIn.reset();         val.write(deflateIn);         deflater.reset();         deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength());         deflater.finish();         while (!deflater.finished()) {           int count = deflater.deflate(deflateOut);           buffer.write(deflateOut, 0, count);         }       } else {    A couple of issues with this code:  1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem.  2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'.   Proposed fix:  1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size.  2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.   ",io
Configuration: separate client config from server config (and from other-server config),"servers = JobTracker, NameNode, TaskTracker, DataNode clients =  runs JobClient (to submit MapReduce jobs), or runs DFSShell (to browse )  Server machines are administered together. So it is OK to have all server config together (esp file paths and network ports). This is stored in hadoop-default.xml or hadoop-mycluster.xml  Client machines: there may be as many client machines as there are MapRed developers. the temp space for DFS needs to be writable by the active user. So it should be possible to select the client temp space directory for the machine and for the user. (The global /tmp is not an option as discussed elsewhere: partition may be full)  Current situation:  Both the server and the clients have a copy of the server config: hadoop-default.xml But the XML property  ""dfs.data.dir"" is being used as a LOCAL directory path  on both the server machines (Data nodes) and the client machines.  Effect: Exception in thread ""main"" java.io.IOException: No valid local directories in property: dfs.data.dir  at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:286)  at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:560)  ...  at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:267)   Current Workaround: On the client use hadoop-site.xml to override dfs.data.dir  One proposed solution:  For the purpose of JobClient operations, use a different property in place of dfs.data.dir. (Ex: dfs.client.data.dir)  On the client, set this property in hadoop-site.xml so that it will override hadoop-default.xml   Another proposed solution:  Handle the fact that the world is made of a federation of independant Hadoop systems. They can talk to each other (as peers) but they are administered separately. Each Hadoop system should have its own separate XML config file. Clients should be able to specify the Hadoop system they want to talk to. An advantage is that clients can then easily sync their local copy of a given Hadoop system config:  just pull its config file  In this view of the world, a Job client is also a kind of independant (serverless) Hadoop system In this case the client config file may have its own dfs.data.dir, which is  separate from the dfs.data.dir in the server config file.  ",conf
Reading an ArrayWriter does not work because valueClass does not get initialized,"If you have a Reducer whose value type is an ArrayWriter it gets enstreamed alright but at reconstruction type when ArrayWriter::readFields(DataInput in) runs on a DataInput that has a nonempty ArrayWriter , newInstance fails trying to instantiate the null class.",io
LocalFileSystem.makeAbsolute bug on Windows," LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop).  Problem:  if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails.  Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three:  (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file)  So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below.   The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem:      private File makeAbsolute(File f) {       if (f.isAbsolute()) {         return f;       } else {         return new File(workingDir, f.toString());       }     }  Im happy to explain if this explanation is confusing... ",fs
Unclear precedence of config files and property definitions,"The order in which configuration resources are read is not sufficiently documented, and also there are no mechanisms preventing harmful re-definition of certain properties, if they are put in wrong config files.  From reading the code in Hadoop Configuration.java, JobConf.java and Nutch NutchConfiguration.java I _think_ this is what's happening.  There are two groups of resources: default resources, loaded first, and final resources, loaded at the end. All properties (re)-defined in files loaded later will override any previous definitions:  * default resources: loaded in the order as they are added. The following files are added here, in order:      1. hadoop-default.xml (Configuration)     2. nutch-default.xml  (NutchConfiguration)     3. mapred-default.xml (JobConf)     4. job_xx_xxx.xml       (JobConf, in JobConf(File config))  * final resource: which always come after default resources, i.e. if any value is defined here it will always override those set in default resources (NOTE: including per job settings!!!). The following files are added here, in reversed order:      2. hadoop-site.xml (Configuration)     1. nutch-site.xml    (NutchConfiguration)  (i.e. hadoop-site.xml will take precedence over anything else defined in any other config file).  I would appreciate checking that this is indeed the case, and suggestions how to ensure that you cannot so easily shoot yourself in the foot if you define wrong properties in hadoop-site or nutch-site ...",conf
FileSystem should not name files with java.io.File,"In Hadoop's FileSystem API, files are currently named using java.io.File.  This is confusing, as many methods on that class are inappropriate to call on Hadoop paths.  For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file.  Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names.  For example, new File(""/foo"") is not absolute on Windows, and prints its path as ""\\foo"", which causes confusion.  To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI.  The advantage of URI is that it can also naturally include the namenode host and port.  The disadvantage is that URI does not support tree operations like getParent().  This change will cause a lot of incompatibility.  Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.",fs
Overlong UTF8's not handled well,"When we feed an overlong string to the UTF8 constructor, two suboptimal things happen.  First, we truncate to 0xffff/3 characters on the assumption that every character takes three bytes in UTF8.  This can truncate strings that don't need it, and it can be overoptimistic since there are characters that render as four bytes in UTF8.  Second, the code doesn't actually handle four-byte characters.  Third, there's a behavioral discontinuity.  If the string is ""discovered"" to be overlong by the arbitrary limit described above, we truncate with a log message, otherwise we signal a RuntimeException.  One feels that both forms of truncation should be treated alike.  However, this issue is concealed by the second issue; the exception will never be thrown because UTF8.utf8Length can't return more than three times the length of its input.  I would recommend changing UTF8.utf8Length to let its caller know how many characters of the input string will actually fit if there's an overflow [perhaps by returning the negative of that number] and doing the truncation accurately as needed.  -dk  ",io
Deadlock in LocalFileSystem lock/release,"LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens:  1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It  keeps LocalFileSystem instance ""locked"" as well.  3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is ""locked"" by second thread - both threads are waiting to each other.  ",fs
io.skip.checksum.errors property clashes with LocalFileSystem#reportChecksumFailure,"Below is from email to the dev list on Tue, 11 Apr 2006 14:46:09 -0700.  Checksum errors seem to be a fact of life given the hardware we use.  They'll often cause my jobs to fail so I have been trying to figure how to just skip the bad records and files.  At the end is a note where Stefan pointed me at 'io.skip.checksum.errors'.   This property, when set, triggers special handling of checksum errors inside SequenceFile$Reader: If a checksum, try to skip to next record.  Only, this behavior can conflict with another checksum handler that moves aside the problematic file whenever a checksum error is found.  Below is from a recent log.  060411 202203 task_r_22esh3  Moving bad file /2/hadoop/tmp/task_r_22esh3/task_m_e3chga.out to /2/bad_files/task_m_e3chga.out.1707416716 060411 202203 task_r_22esh3  Bad checksum at 3578152. Skipping entries. 060411 202203 task_r_22esh3  Error running child 060411 202203 task_r_22esh3 java.nio.channels.ClosedChannelException 060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:89) 060411 202203 task_r_22esh3     at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:276) 060411 202203 task_r_22esh3     at org.apache.hadoop.fs.LocalFileSystem$LocalFSFileInputStream.seek(LocalFileSystem.java:79) 060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Checker.seek(FSDataInputStream.java:67) 060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$PositionCache.seek(FSDataInputStream.java:164) 060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:193) 060411 202203 task_r_22esh3     at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:243) 060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.seek(SequenceFile.java:420) 060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.sync(SequenceFile.java:431) 060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:412) 060411 202203 task_r_22esh3     at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:389) 060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:209) 060411 202203 task_r_22esh3     at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:709)  (Ignore line numbers.  My code is a little different from main because I've other debugging code inside in SequenceFile.  Otherwise I'm running w/ head of hadoop).  The SequenceFile$Reader#handleChecksumException is trying to skip to next record but the file has been closed by the move-aside.   On the list there is some discussion on merit of moving aside file when bad checksum found.  I've trying to test what happens if we leave the file in place but haven't had a checksum error in a while.    Opening this issue so place to fill in experience as we go. ",io
RPC code has socket leak?,"In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below.  The comment above the declaration was a bit of giveaway:      //TODO mb@media-style.com: static client or non-static client?   private static Client CLIENT;     private static class Invoker implements InvocationHandler {     private InetSocketAddress address;      public Invoker(InetSocketAddress address, Configuration conf) {       this.address = address;       CLIENT = (Client) conf.getObject(Client.class.getName());       if(CLIENT == null) {           CLIENT = new Client(ObjectWritable.class, conf);           conf.setObject(Client.class.getName(), CLIENT);       }     }      public Object invoke(Object proxy, Method method, Object[] args)       throws Throwable {       ObjectWritable value = (ObjectWritable)         CLIENT.call(new Invocation(method, args), address);       return value.get();     }   }    /** Construct a client-side proxy object that implements the named protocol,    * talking to a server at the named address. */   public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) {     return Proxy.newProxyInstance(protocol.getClassLoader(),                                   new Class[] { protocol },                                   new Invoker(addr, conf));   }    /** Expert: Make multiple, parallel calls to a set of servers. */   public static Object[] call(Method method, Object[][] params,                               InetSocketAddress[] addrs, Configuration conf)     throws IOException {      Invocation[] invocations = new Invocation[params.length];     for (int i = 0; i < params.length; i++)       invocations[i] = new Invocation(method, params[i]);     CLIENT = (Client) conf.getObject(Client.class.getName());     if(CLIENT == null) {         CLIENT = new Client(ObjectWritable.class, conf);         conf.setObject(Client.class.getName(), CLIENT);     }     Writable[] wrappedValues = CLIENT.call(invocations, addrs);          if (method.getReturnType() == Void.TYPE) {       return null;     }      Object[] values =       (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length);     for (int i = 0; i < values.length; i++)       if (wrappedValues[i] != null)         values[i] = ((ObjectWritable)wrappedValues[i]).get();          return values;   }.  ",ipc
Add a conf dir parameter to the scripts,"We'd like a conf_dir parameter on the startup scripts (ie. ""-c <confdif>""). In particular, it would be nice if it propagated down from hadoop-daemons.sh to slaves.sh to hadoop-daemon.sh using the command line rather than using the ssh -o SendEnv=HADOOP_CONF_DIR, which is not supported in many environments.",conf
IPC is unable to invoke methods that use interfaces as parameter,Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature. ,ipc
reducing the number of Configuration & JobConf objects created,"Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.",conf
setReplication and related bug fixes,"Having variable replication (HADOOP-51) it is natural to be able to change replication for existing files. This patch introduces the functionality. Here is a detailed list of issues addressed by the patch.  1) setReplication() and getReplication() methods are implemented. 2) DFSShell prints file replication for any listed file. 3) Bug fix. FSDirectory.delete() logs delete operation even if it is not successful. 4) Bug fix. This is a distributed bug. Suppose that file replication is 3, and a client reduces it to 1. Two data nodes will be chosen to remove their copies, and will do that. After a while they will report to the name node that the copies have been actually deleted. Until they report the name node assumes the copies still exist. Now the client decides to increase replication back to 3 BEFORE the data nodes reported the copies are deleted. Then the name node can choose one of the data nodes, which it thinks have a block copy, to replicate the block to new data nodes. This setting is quite unusual but possible even without variable replications. 5) Logging for name and data nodes is improved in several cases. E.g. data nodes never logged that they deleted a block.  ",fs
rpc doesn't handle returning null for a String[],The job tracker gets errors in returning the result from pollForTaskWithClosedJob  060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable         at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95)         at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230) ,ipc
Utilities for reading SequenceFile and MapFile,"Most data in Hadoop is stored in SequenceFile-s and MapFile-s. Sometimes there is a need to examine such files, but no specialized utilities exist ro read them.  These two classes provide a functionality to examine individual records in such files, and also to dump the content of such files to a plain text output.",io
comparators of integral writable types are not transitive for inequalities,"Consider the following code from IntWritable.java :      public int compare(byte[] b1, int s1, int l1,                        byte[] b2, int s2, int l2) {       int thisValue = readInt(b1, s1);       int thatValue = readInt(b2, s2);       return thisValue - thatValue;     }  If a Java Runtime subtracts 20 from -(2^31 - 10) it gets a huge positive number, not the negative value that the comparator should return.  LongWritable does this right, of course.  That last line should be         return (thisValue<thatValue ? -1 : (thisValue==thatValue ? 0 : 1));  -dk ",io
DFS i/o benchmark.,"DFS i/o benchmark is a map-reduce based test that measures performance of the cluster for reads and writes. This is an evolved version of HADOOP-72, and HADOOP-95 test.  This test writes into or reads from a specified number of files. File size is specified as a parameter to the test. Each file is processed in a separate map task. The unique reducer then collects stats. Finally, the following information is displayed  # read or write test # date and time the test finished # number of files processed # total number of bytes processed # throughput in mb/sec (total number of bytes / sum of processing times) # average i/o rate in mb/sec per file # standard i/o rate deviation  I  included the test into the AllTestDriver.",fs
Fix buggy uselessness of Configuration( Configuration other) constructor,The constructor  public Configuration(Configuration other) ,conf
remove deprecated java.io.File methods,"Now that the 0.2 release is out, we should remove the deprecated FileSystem methods that use java.io.File, since using org.apache.hadoop.fs.Path is less error-prone.",fs
Need to tweak a few things in the metrics package to support the Simon plugin,"(1) added an extra metrics.jar target to the build.xml so that I can build a stand-alone library containing only the metrics package and its subpackages.  (2) added serialversionUIDs to a bunch of classes to make Eclipse happy  (3) made AbstractMetricsContext.createRecord final, and added a protected newRecord that subclasses can use to customize record creation without breaking the parent class.  (4) minor fix to how errors in callbacks are handled  (5) constructor in MetricsRecordImpl made protected rather than package private so that it can be subclassed  (6) extended Util.parse(String serverSpecs, int defaultPort) to handle the case of a null serverSpecs by defaulting to localhost  ",metrics
Add a program to recursively copy directories across file systems,"A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as:  hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir  ""cp"" command would invoke a map-reduce program to copy files recursively.  I willl attach a patch as soon as svn is up and running.",fs
SequenceFile#handleChecksumException NPE,"The SequenceFile#handleChecksumException assumes the conf data member has been set.  It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor.  The latter is used by ReduceTask Sorter:   java.lang.NullPointerException   org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407)   org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400)   org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837)   org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881)   org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766)   org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702)   org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528)   org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787) ",io
Add -dfs and -jt command-line parameters to specify namenode and jobtracker.,Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.,fs
hadoop cp should have a -config option,"hadoop cp should have a -config option to enable overriding of default parameters. it  would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job",fs
hadoop cp should generate a better number of map tasks,"hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.",fs
Standard set of Performance Metrics for Hadoop,"I am starting to use Hadoop's shiny new Metrics API to publish performance (and other) Metrics of running jobs and other daemons.  Which performance metrics are people interested in seeing ? If possible, please group them according to modules, such as map-reduce, dfs, general-cluster-related etc. I will follow this process:  1. collect this list 2. assess feasibility of obtaining metric 3. assign context/record/metrics names 4. seek approval for names 5. instrument the code. ",metrics
TestCopyFiles fails under cygwin due to incorrect path,Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path. ,fs
record io translator doesn't strip path names,"When I run the record translator with a pathname, the path name is not stripped. So for example:  % bin/rcc --language c++ foo/bar/bat.jr  generates:  foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__  the first was unexpected and the second is clearly wrong.",record
the record-io generated c++ has wrong comments,The comments on the namespaces on the closing come out backward:  } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test ,record
add versioning to RPC,"currently, any change to any rpc message breaks the protocol, with mysterious exceptions occurring at run time. a versioning sceme would have two benefits: - intelligent error messages, indicating that an upgrade is required - backwards compatibility could be supported.  the proposal is to add a ""const version"" for each protocol, and a method: int getVersion(int version) that sends the client's version and receives the server's version. This would be the first method invoked on connection. Both sides then either agree on the lowest version number, providing backwards compatibility support, or abort the connection as ""unsupported version"".",ipc
Client Calls are not cancelled after a call timeout,"In ipc/Client.java, if a call times out, a SocketTimeoutException is thrown but the Call object still exists on the queue.  What I found was that when transferring very large amounts of data, it's common for queued up calls to timeout. Yet even though the caller has is no longer waiting, the request is still serviced on the server and the data is sent to the client. The client after receiving the full response calls callComplete() which is a noop since nobody is waiting.  The problem is that the calls that timeout will retry and the system gets into a situation where data is being transferred around, but it's all data for timed out requests and no progress is ever made.  My quick solution to this was to add a ""boolean timedout"" to the Call object which I set to true whenever the queued caller times out. And then when the client starts to pull over the response data (in Connection::run) to first check if the Call is timedout and immediately close the connection.  I think a good fix for this is to queue requests on the client, and do a single sendParam only when there is no outstanding request. This will allow closing the connection when receiving a response for a request we no longer have pending, reopen the connection, and resend the next queued request. I can provide a patch for this, but I've seen a lot of recent activity in this area so I'd like to get some feedback first.",ipc
RPC doesn not handle exceptions correctly,"1. Examining HADOOP-264 bug I realized that not all rpc server exceptions are actually returned to the client. Particularly, if an exception happens in  org.apache.hadoop.ipc.Server.Connection.run() for example inside  param.readFields(in); then it logged but never returned back to the client. Client simply timeouts in this case, which is not exactly what one would expect.  2. On the way back  org.apache.hadoop.ipc.Client.call() rpc client always throws a RemoteException, rather than the exception wrapped into this RemoteException. ",ipc
add FAQ to Wiki,Hadoop should have an FAQ in the Wiki.  We can bootstrap this by reviewing the mailing list archives.,documentation
Speed up SequenceFile sort with memory reduction,I replaced the merge sort with a quick sort and it yielded approx 30% improvement in sort time. It also reduced the memory requirement for sorting because the sort is done in place.,io
Hadoop Log Archiver/Analyzer utility,"Overview of the log archiver/analyzer utility...  1. Input   The tool takes as input a list of directory URLs, each url could also we associated with a file-pattern to specify what pattern of files in that directory are to be used.   e.g. http://g1015:50030/logs/hadoop-sameer-jobtracker-*          file:///export/crawlspace/sanjay/hadoop/trunk/run/logs/haddop-sanjay-namenode-* (local disk on the machine on which the job was submitted)  2. The tool supports 2 main functions:    a) Archival     Archive the logs in the DFS in the following hierarchy:    /users/<username>/log-archive/YYYY/mm/dd/HHMMSS.log by default     Or a user-specified directory and then:     <input-dir>/YYYY/mm/dd/HHMMSS.log    b) Processing with simple sort/grep primitives     Archive the logs as above and then grep for lines with given pattern (e.g. INFO) and then sort with spec e.g. <logger><level><date>. (Note: This is proposed with current log4j based logging in mind... do we need anything more generic?). The sort/grep specs are user-provided; along with directory URLs.  3. Thoughts on implementation...    a) Archival     Current idea is to put a .jsp page (src/webapps) on each of the nodes; which then does a *copyFromLocal* of the log-file into the DFS. The jobtracker will fire n map-tasks which only hit the jsp page as per the directory URLs. The reduce-task is a no-op and only collects statistics on failures (if any).    b) Processing with sort/grep     Here, the tool first archives the files as above and then another set of map-reduce tasks will do the sort/grep on the files in DFS with given specs.                                                                                             - * - * -    Suggestions/corrections welcome...  thanks, Arun",util
nicer reports of progress for distcp,The unformatted number of bytes in distcp is difficult to read.,util
class Text (replacement for class UTF8) was: HADOOP-136,"Just to verify, which length-encoding scheme are we using for class Text (aka LargeUTF8)   a) The ""UTF-8/Lucene"" scheme? (highest bit of each byte is an extension bit, which I think is what Doug is describing in his last comment) or  b) the record-IO scheme in o.a.h.record.Utils.java:readInt   Either way, note that:   1. UTF8.java and its successor Text.java need to read the length in two ways:    1a. consume 1+ bytes from a DataInput and    1b. parse the length within a byte array at a given offset  (1.b is used for the ""WritableComparator optimized for UTF8 keys"" ).   o.a.h.record.Utils only supports the DataInput mode.  It is not clear to me what is the best way to extend this Utils code when you need to support both reading modes   2 Methods like UTF8's WritableComparator are to be low overhead, in partic. there should be no Object allocation.  For the byte array case, the varlen-reader utility needs to be extended to return both:   the decoded length and the length of the encoded length.   (so that the caller can do offset += encodedlength)       3. A String length does not need (small) negative integers.   4. One advantage of a) is that it is standard (or at least well-known and natural) and there are no magic constants (like -120, -121 -124) ",io
Additional constructor requested in BytesWritable," It would be grand if BytesWritable.java had an additional constructor as below. This allows me to use the BytesWritable class without doing a buffer copy, since we have a less-than-fully-utilized byte array holding our key.  Thanks!    /**    * Create a BytesWritable using the byte array as the initial value.    * @param bytes This array becomes the backing storage for the object.    */   public BytesWritable(byte[] bytes, int size) {     this.bytes = bytes;     this.size = size;   }    ",io
Connections should not be cached,"Servers and clients (client include datanodes, tasktrackers, DFSClients & tasks) should not cache connections or maybe cache them for very short periods of time. Clients should set up & tear down connections to the servers everytime they need to contact the servers (including the heartbeats). If connection is cached, then reuse the existing connection for a few subsequent transactions until the connection expires. The heartbeat interval should be more so that many more clients (order of  tens of thousands) can be accomodated within 1 heartbeat interval.",ipc
bobo Exception in TestRPC,"We are getting a bobo exceptions in TestRPC. It looks like the test should fail in this case after throwing bobo. By the way, does anybody know whether ""bobo"" means anything?  error: java.io.IOException: bobo java.io.IOException: bobo     at org.apache.hadoop.ipc.TestRPC$TestImpl.error(TestRPC.java:84)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)     at java.lang.reflect.Method.invoke(Method.java:585)     at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:243)     at org.apache.hadoop.ipc.Server$Handler.run(Server.java:467)",ipc
"""connection was forcibly closed"" Exception in RPC on Windows","I see a lot of exceptions caused by RPC in the nightly build on Windows. The most often is thrown by RPC on the namenode, saying  06/06/21 19:28:36 INFO ipc.Server: Server listener on port 7017: readAndProcess threw exception java.io.IOException: An existing connection was forcibly closed by the remote host. Count of bytes read: 0 java.io.IOException: An existing connection was forcibly closed by the remote host     at sun.nio.ch.SocketDispatcher.read0(Native Method)     at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)     at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)     at sun.nio.ch.IOUtil.read(IOUtil.java:200)     at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:207)     at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:374)     at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:289)     at org.apache.hadoop.ipc.Server$Listener.run(Server.java:210)  I am not sure how serious that is, since the tests do not fail, and the name node does not crash. Besides the RPC we should probably also check that the unit tests actually fail if they need to. ",ipc
"FileSystem ""close"" does not remove the closed fs from the fs map","The close methods of both DistributedFileSystem and LocalFileSystem do not remove the closed file system from the fs map in FileSystem. As a result, a subsequent call to FileSystem.getNamed may return the handle to the closed file system and receive errors if perform any operation on the fs handle.",fs
"IO Exception at LocalFileSystem.renameRaw, when running Nutch nightly builds (0.8-dev).","IO Exception at LocalFileSystem.renameRaw, when running Nutch nightly builds (0.8-dev). Please see the deatil descriptions in: http://issues.apache.org/jira/browse/NUTCH-266  Not knowing how to reclassify an existing bug, I am opening this new bug under Hadoop.  The version number is 0.3.3 but because I don't see it in the jira list, I chose the closest matching version.  The Nutch-with-GUI build was running with hadoop-0.2 but stopped running, exhibiting the same symptom with other nightly builds, when switched to use hadoop-0.3.3.  I checked ""fs"" as component but this bug could also be caused by the order in which jobs are scheduled, I suspect.  ",fs
ClassNotFoundException under jvm 1.6,"We have been having problems with classes that are returned by RPC methods not being loaded/initialized correctly. The work around has been to put in the servers, code of the form:  static { new FooBar(); }  // to resolve the ClassNotFoundException for class FooBar.  When I tried running under java 1.6, that stopped working because one of the classes had to be instantiated from a package that didn't have visibility to create an instance. So I tracked the problem down to how the classes were being loaded via reflection.",ipc
ToolBase calls System.exit,The new ToolBase class calls System.exit when the main routine finishes. That will break if the application uses threads that need to finish before the jvm exits. The normal semantics is that the program doesn't finish execution until all of the non-daemon threads exit (including the main one) and System.exit should never be called except for critical errors.,util
add a -i option to distcp to ignore read errors of the input files,"Add an option ""-i"" to ignore problems reading files and just copy what can be copied.",util
Using wildcards in config pathnames,"In our cluster there's machines with very different disk setups I've solved this by not rsyncing hadoop-site.xml, but as you probably understand this means new settings will not get copied properly.  I'd like to be able to use wildcards in the dfs.data.dir path for example: <property>   <name>dfs.data.dir</name>     <value>/home/hadoop/disk*/dfs/data</value> </property>  then every disk mounted in that directory would be used",conf
Enhance distcp to handle *http* as a 'source protocol'.,"Requirements:    Presently distcp recursively copies a directory from one dfs to another i.e. both source and destination of of the *dfs* protocol.   Enhance it to handle *http* as the source protocol i.e. support copying files from arbitrary http-based sources into the dfs.  Design:      Follow distcp's current design: one map task per file which needs to be copied.    Caveat: distcp handles *recursive* copying by listing sub-directories; this is not as feasible with a http-based source since things like 'fancy-indexing' might not be enabled on the web-server (for all sub-locations recursively too), and even if it is enabled it will mean tedious parsing of the html served to glean the sub-directories etc. Hence the idea is to support an input file (via a -f option) which contains a list of the http-based urls which represent multiple source files. ",util
Allow info server to be turned off/on by conf file,"Since I am using hadoop within my own servlet which is not Jetty, it would be nice to not have to need Jetty to run my servlet. As such I propose adding an if statement/donf setting at FSNamesystem:171 as such           if(conf.getBoolean(""dfs.info.active"",true)){             this.infoPort = conf.getInt(""dfs.info.port"", 50070);             this.infoServer = new StatusHttpServer(""dfs"", infoPort, false);             this.infoServer.start(); }",conf
Remove Jetty dependency,"Somewhat related to HADOOP-349, it would be nice to not have a Jetty dependency for those of us embedding Hadoop within our own web applications. In particular the Server object is using SocketChannelOutputStream from the Jetty code base. It seems to me that for an object like this it would be better to simply have a Hadoop version of a blocking output stream on an nio channel if necessary vs. using the Jetty version requiring a Hadoop user's to package yet another jar file (and all the complications that are associated with that).",ipc
NPE in Path.equals,An NPE is raised in Path.equals when testing the method with two unequal pathes and with the first one having no drive.  This is due to operator precedence: && has a higher priority level than ?:  See http://java.sun.com/docs/books/tutorial/java/nutsandbolts/expressions.html  See attached patch (just added some parenthesis and a testcase).,fs
rpc versioning broke out-of-order server launches,The change to check the RPCs broke the ability to bring up the datanodes before the namenode and the tasktracker before the jobtracker.,ipc
"DistributedFSCheck should cleanup, seek, and report missing files.",None,fs
ant tar should package contrib jars,"From Hadoop-356: >I note that the contrib packages are not included in distributions (the ""tar"" target).   >They probably should be.  Michel, would you like to modify ""tar"" to include the contrib code?   >This should be done in a separate bug.  OK. This packaging is done in target deploy-contrib. So I can just add a dependency to the top-level tar target: <!-- Make release tarball(s)                                               --> <target name=""tar"" depends=""package, deploy-contrib"">   ",build
too many files are distributed to slaves nodes,"currently all the svn tree is sync'ed to the slaves on startup, excluding only .svn files. that includes the example jar file, sources, and other files that are not required. It would  be better to pick and choose the files that get sync'ed. It will improve sync times on startup, and can prevent some name conflicts, since all the jar files are also included in the classpath by the hadoop script.",conf
improved error messages for file checksum errors,Improves the messages on a couple of the failures we've been seeing to try and get enough information to identifiy the problem.,fs
The validateUTF function of class Text throws MalformedInputException for valid UTF8 code containing ascii chars,The validateUTF function does not handle ascii chars right therefore causing MIE exception to be thrown.,io
c/c++ record io library does not use autoconf,"It would be more convenient, if the C/C++ portion of record io used autoconf to configure the location of the required libraries and generate make files.",record
replace class UTF8 with class Text,"Since class UTF8 is deprecated, all references of UTF8 in hadoop should be replaced with class Text if the change does not break the system. ",io
Accumulate bytes & records statistics at the job level via haoop.metrics.,"We'd like to accumulate the statistics (and get graphs) at the job level rather than the job level, because it will provide a lot more useful information. This likely implies that they need to be rolled up in the TaskStatus reported up to the JobTracker.",metrics
replace String in hadoop record io with the new Text class,The record io in Java is currently using String and should be using the new Text class.,record
SequenceFile should support 'custom compressors',"SequenceFiles should support 'custom compressors' which can be specified by the user on creation of the file.   Readily available packages for gzip and zip (java.util.zip) are among obvious choices to support. Of course there will be hooks so that other compressors can be added in future as long as there is a way to construct (input/output) streams on top of the compressor/decompressor.  The 'classname' of the 'custom compressor/decompressor' could be stored in the header of the SequenceFile which can then be used by SequenceFile.Reader to figure out the appropriate 'decompressor'. Thus I propose we add constructors to SequenceFile.Writer which take in the 'classname' of the compressor's input/output stream classes (e.g. DeflaterOutputStream/InflaterInputStream or GZIPOutputStream/GZIPInputStream), which acts as the hook for future compressors/decompressors. ",io
"slaves file should include an 'exclude' section, to prevent ""bad"" datanodes and tasktrackers from disrupting  a cluster","I recently had a few nodes go bad, such that they were inaccessible to ssh, but were still running their java processes. tasks that executed on them were failing, causing jobs to fail. I couldn't stop the java processes, because of the ssh issue, so I was helpless until I could actually power down these nodes. restarting the cluster doesn't help, even when removing the bad nodes from the slaves file - they just reconnect and are accepted. while we plan to avoid tasks from launching on the same nodes over and over, what I'd like is to be able to prevent rogue processes from connecting to the masters. Ideally, the slaves file will contain an 'exclude' section, which will list nodes that shouldn't be accessed, and should be ignored if they try to connect. That would also help in configuring the slaves file for a large cluster - I'd list the full range of machines in the cluster, then list the ones that are down in the 'exclude' section",conf
Text class should support the DEL character,"The DEL character (aka 0x7F) is valid UTF-8, but the Text class does not support it.",io
variable expansion in Configuration,"Add variable expansion to Configuration class. =================  This is necessary for shared, client-side configurations:  A Job submitter (an HDFS client) requires: <name>dfs.data.dir</name><value>/tmp/${user.name}/dfs</value>  A local-mode mapreduce requires: <name>mapred.temp.dir</name><value>/tmp/${user.name}/mapred/tmp</value>  Why this is necessary : =================  Currently we use shared directories like: <name>dfs.data.dir</name><value>/tmp/dfs</value> This superficially seems to work. After all, different JVM clients create their own private subdirectory map_xxxx., so they will not conflict.  What really happens:  1. /tmp/ is world-writable, as it's supposed to. 2. Hadoop will create missing subdirectories.  This is Java so that for ex. /tmp/system is created as writable only by the JVM process user 3. This is a shared client machine so next user's JVM will find /tmp/system owned by somebody else. Creating a directory within /tmp/system fails  Implementation of var expansion ============= in class Configuration,  The Properties really store things like put(""banner"", ""hello ${user.name}""); In public String get(String name): postprocess the returned value: Use a regexp to find the pattern ${xxxx} Lookup xxxx as a system property If found, replace ${xxxx} by the system property value. Else leave as-is. An unexpanded ${xxxx} is a hint that the variable name is invalid.   Other workarounds  =============== The other proposed workarounds are not as elegant as variable expansion.  Workaround 1:  have an installation script which does: mkdir /tmp/dfs chmod uga+rw /tmp/dfs repeat for ALL configured subdirectories at ANY nesting level keep the script in sync with changes to hadoop XML configuration files. Support the script on non-Unix platform Make sure the installtion script runs before Hadoop runs for the first time. If users change the permissions/delete any of the shared directories, it breaks again.  Workaround 2:  do the chmod operations from within the Hadoop code. In pure java 1.4, 1.5 this is not possible. It requires the Hadoop client process to have chmod privilege (rather than just mkdir privilege) It requires to special-case directory creation code.   ",conf
Troubleshooting message for RunJar (bin/hadoop jar),"Many users get this exception when using bin/hadoop There are various reasons: HADOOP_CONF_DIR or HADOOP_HOME misconfigured.  ""hadoop jar"" parameter incorrect  So the patch displays the path of the jar it is trying to open if this fails:   Problem while opening hadoop job jar: /home/build/hadoop-streaming.jar Exception in thread ""main"" java.util.zip.ZipException: error in opening zip file         at java.util.zip.ZipFile.open(Native Method)         at java.util.zip.ZipFile.<init>(ZipFile.java:203)         at java.util.jar.JarFile.<init>(JarFile.java:132)         at java.util.jar.JarFile.<init>(JarFile.java:70)         at org.apache.hadoop.util.RunJar.main(RunJar.java:80) ",util
Startup scripts will not start instances of Hadoop daemons w/different configs w/o setting separate PID directories,"Configuration directories can be specified by either setting HADOOP_CONF_DIR or using the --config command line option. However, the hadoop-daemon.sh script will not start the daemons unless the PID directory is separate for each configuration.  The issue is that the code for generating PID filenames is not dependent on the configuration directory. While the PID directory can be changed in hadoop-env.sh, it seems a little unnecessary to have this restriction. ",conf
Hadoop mapred metrics should include per job input/output statistics rather than per-task statistics,"Currently hadoop reports metrics such as input bytes, input records, etc on per-task basis. Accurate aggregation of these metrics is required at the job-level and reporting should be done on a per-job basis.",metrics
DataNodes and TaskTrackers should be able to report hostnames and ips relative to customizable network interfaces and nameservers,"This patch allows for network configuration parameters to be aded to the hadoop-site.xml file. These parameters specify a network interface name and an optional nameserver hostname which DataNodes and TaskTrackers consult to resolve  their hostnames from the IP bound to the specified network interface.  This is useful when machines that are part of different physical or logical network need to participate in hadoop clusters as client nodes. The hostname and IP reported by InetAddress.getLocalHost() are not necessarily the ones that will allow the JobTracker and NameNode to reach the clients, as well as not necessarily the ones through which the DFS clients can reach the DataNodes.  The configuration parameters are  - cluster.report.nif  - cluster.report.ns  nif: takes the name of a network interface, like en0, en1 (on macs), eth0, etc... ns: the host name of a DNS server to use when resolving the IP bound to the specified nif  These parameters are set by default to the value ""default"" which will replicate the current behavior of reporting InetAddress.getLocalHost().getHostName() and getHostAddress()  As part of the patch, a new library dnsjava was added along with its license information (BSD license). The list of affected files is:  src  org.apache.hadoop.dfs.DataNode   org.apache.hadoop.mapred.taskTracker   org.apache.hadoop.util.NetworkUtils conf  hadoop-default.xml lib  dnsjava-2.0.2.jar  dnsjava-2.0.2.LICENSE.txt ",util
Summer buffer overflow exception,"The extended error message with the offending values finally paid off and I was able to get the values that were causing the Summber buffer overflow exception.  java.lang.RuntimeException: Summer buffer overflow b.len=4096, off=0, summed=512, read=2880, bytesPerSum=1, inSum=512         at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:100)         at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:170)         at java.io.BufferedInputStream.read1(BufferedInputStream.java:254)         at java.io.BufferedInputStream.read(BufferedInputStream.java:313)         at java.io.DataInputStream.read(DataInputStream.java:80)         at org.apache.hadoop.util.CopyFiles$DFSCopyFilesMapper.copy(CopyFiles.java:190)         at org.apache.hadoop.util.CopyFiles$DFSCopyFilesMapper.map(CopyFiles.java:391)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:196)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1075) Caused by: java.lang.ArrayIndexOutOfBoundsException         at java.util.zip.CRC32.update(CRC32.java:43)         at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98)         ... 9 more  Tracking through the code, what happens is inside of FSDataInputStream.Checker.read() the verifySum gets an  EOF Exception and turns off the summing. Among other things this sets the bytesPerSum to 1. Unfortunately, that leads to the ArrayIndexOutOfBoundsException.  I think the problem is that the original EOF exception was logged and ignored. I propose that we allow the original EOF to propagate back to the caller. (So that file not found will still disable the checksum checking, but we will detect truncated checksum files.)",fs
Runtime exception in org.apache.hadoop.io.WritableFactories.newInstance when trying to startup namenode/datanode,"Here's the logs:  arun@neo ~/dev/java/latest-hadoop/trunk $ cat /home/arun/dev/java/hadoop-0.4.0/build/libhdfs/tests/logs/hadoop-arun-namenode-neo.out 2006-09-05 22:18:39,756 INFO  conf.Configuration (Configuration.java:loadResource(496)) - parsing file:/home/arun/dev/java/hadoop-0.4.0/src/c++/libhdfs/tests/conf/hadoop-default.xml 2006-09-05 22:18:39,804 INFO  conf.Configuration (Configuration.java:loadResource(496)) - parsing file:/home/arun/dev/java/hadoop-0.4.0/src/c++/libhdfs/tests/conf/hadoop-site.xml 2006-09-05 22:18:39,918 INFO  util.Credential (FileResource.java:<clinit>(60)) - Checking Resource aliases 2006-09-05 22:18:39,935 INFO  http.HttpServer (HttpServer.java:doStart(729)) - Version Jetty/5.1.4 2006-09-05 22:18:40,366 INFO  util.Container (Container.java:start(74)) - Started org.mortbay.jetty.servlet.WebApplicationHandler@1171b26 2006-09-05 22:18:40,478 INFO  util.Container (Container.java:start(74)) - Started WebApplicationContext[/,/] 2006-09-05 22:18:40,478 INFO  util.Container (Container.java:start(74)) - Started HttpContext[/logs,/logs] 2006-09-05 22:18:40,479 INFO  util.Container (Container.java:start(74)) - Started HttpContext[/static,/static] 2006-09-05 22:18:40,485 INFO  http.SocketListener (SocketListener.java:start(204)) - Started SocketListener on 0.0.0.0:50070 2006-09-05 22:18:40,487 INFO  util.Container (Container.java:start(74)) - Started org.mortbay.jetty.Server@1b09468 Exception in thread ""main"" java.lang.RuntimeException: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""         at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:49)         at org.apache.hadoop.io.ArrayWritable.readFields(ArrayWritable.java:81)         at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:134)         at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:157)         at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:317)         at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:199)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:132)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:123)         at org.apache.hadoop.dfs.NameNode.main(NameNode.java:543) Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableFactories can not access a member of class org.apache.hadoop.dfs.Block with modifiers ""public""         at sun.reflect.Reflection.ensureMemberAccess(Reflection.java:65)         at java.lang.Class.newInstance0(Class.java:344)         at java.lang.Class.newInstance(Class.java:303)         at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:45)         ... 8 more  Steps to reproduce: 1. Start namenode/datanode 2. Run hdfs_test program (part of libhdfs) 3. Stop namenode/datanode 4. goto step 1 ",util
classloader problem for clients,"HADOOP-419 bit again, after updating with hadoop-0.6.0. Although resolved, there was one instance left in io.ObjectWritable.java still using Thread.currentThread().getContextClassLoader() instead of conf.getClassByName() I got exceptions like:  java.lang.RuntimeException: readObject can't find class         at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:223)         at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:59)         at org.apache.hadoop.ipc.Client$Connection.run(Client.java:256) Caused by: java.lang.ClassNotFoundException: org/apache/hadoop/io/ObjectWritable$NullInstance         at java.lang.Class.forName0(Native Method)         at java.lang.Class.forName(Class.java:242)         at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:219)         ... 2 more ",io
MapFile should support block compression,"MapFile is layered on SequenceFile and permits random-access to sorted data files (typically reduce output) through a parallel index file.  This is used widely in Nutch (e.g. at search time for displaying cached pages, incoming links, etc).  Such sorted data should benefit from block compression, but the current MapFile API does not support specification of block compression.  Also, even if it did, the semantics of SequenceFile methods like seek() and getPosition() are changed under block compression so that MapFile may not work.",io
Contrib documentation does not appear in Javadoc,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,documentation
Need raw comparators for hadoop record types,"Raw comparators are not generated for types that are generated with the Hadoop record framework. This could have a substantial performance impact when using hadoop record generated types in Map/Reduce. The record i/o framework should auto-generate raw comparators for types.  Comparison for hadoop record i/o types is defined to be member wise comparison of objects. A possible implementation could only deserialize one member from each object at a time, compare them and either return or move on to the next member if the values are equal.",record
Writable underrun in sort example,"When running the sort benchmark, I get consistent failures of this sort:  java.lang.RuntimeException: java.io.IOException: org.apache.hadoop.io.BytesWritable@43d748ad read 2048 bytes, should read 2052 at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:150) at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:271) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1066) Caused by: java.io.IOException: org.apache.hadoop.io.BytesWritable@43d748ad read 2048 bytes, should read 2052 at org.apache.hadoop.io.SequenceFile$Reader.getCurrentValue(SequenceFile.java:1163) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1239) at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:181) at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:147) ... 3 more",io
back to back testing of codecs,"We should write some unit tests that use codecs back to back doing writing and then reading.  compressed block1, compressed block 2, compressed block3, ...  that will check that the compression codecs are consuming the entire block when they read.",io
Implement a nio's 'direct buffer' based wrapper over zlib to improve performance of java.util.zip.{De|In}flater as a 'custom codec',"There has been more than one instance where java.util.zip's {De|In}flater classes perform unreliably, a simple wrapper over zlib-1.2.3 (latest stable) using java.nio.ByteBuffer (i.e. direct buffers) should go a long way in alleviating these woes.",io
Unused parameter in hadoop-default.xml,"found a parameter in haddop-default.xml that seems not to be used anymore in the java code : mapred.task.tracker.output.port.  I've done a grep ""mapred.task.tracker.output.port"" **/*.java (thanks zsh) in the whole source tree and the only result was : src/test/org/apache/hadoop/mapred/MiniMRCluster.java: jc.setInt(""mapred.task.tracker.output.port"", taskTrackerPort++);  I think it should be removed from hadoop-default.xml ",conf
Adding versioning to Writable classes,"We currently don't have a way to support versioned Writables, which among other problems means that it is very difficult to change the serialization of any types. (A recent example is that we can't change any of the Writables that currently use UTF8.writeString to use Text.writeString with out breaking everything.) Just changing the file version doesn't work because you can't read the old versions.  Therefore, I propose adding a new interface:  public interface VersionMapWritable extends Writable {   /**    * Destructively read into this object from in based on the version map.    * @param versionMap a map from each class to its version in the DataInput (version 0 classes are omitted)    */   public void readFields(DataInput in, Map<Class<VersionMapWritable>, int> versionMap) throws IOException;    /**    * Classes with non-zero versions should register themselves in static blocks.    */   public static void registerCurrentVersion(Class<VersionMapWritable> class, int version) {...}    /**    * If a version map includes the parent type, always include the child type as well.    */   public static void addDependence(Class<VersionMapWritable> parent, Class<VersionMapWritable> child) { ... }    /**    * Build a version map for a given list of classes, including any dependent types.    */   public static Map<Class<VersionMapWritable>, int>                buildVersionMap(Set<Class<VersionMapWritable>> classes) {...}    /**    * Add the types in the parameter/result types to the list of classes.    * Recursively adds the field types for any new types that are added to the set.    */   public static void addMethodTypes(Set<Class<VersionMapWritable>> result,                       Class<VersionedProtocol> protocol) {...}    /**    * Add the non-transient fields to the list of classes.    */   public static void addFieldTypes(Set<Class<VersionMapWritable>> result, Class<Writable> writable) {...}    public static Map<Class<VersionMapWritable>, int> readVersionMap(DataInput in) throws IOException { ... }    public static void writeVersionMap(DataOutput out,                    Map<Class<VersionMapWritable> versionMap) throws IOException {...} }  VersionedWritable, which stored a version byte within each object, will be depriciated.",io
"hadoop-daemons.sh fails with ""no such file or directory"" when used from a relative symlinked path","The new shell scripts fail with a relative directory:  % current/bin/hadoop-daemons.sh start datanode node1: current/bin/..: No such file or directory node2: current/bin/..: No such file or directory  The problem is that HADOOP_HOME is set to a relative directory and hadoop-daemons does a cd, breaking the other scripts.",scripts
The build script should record the Hadoop version into the build,"It would be good to compile the Hadoop version, subversion revision, and compilation date into the hadoop.jar file.   The web/ui would display the version for each of the server home page.  I'd also add ""bin/hadoop version"" to print the version information.",util
Path should use URI syntax,"The following changes are proposed: 1. Add a factory/registry of FileSystem implementations.  Given a protocol, hostname and port, it should be possible to get a FileSystem implementation. 2. Path's constructor should accept URI-formatted strings & a configuration. 3. A new Path method should be added: FileSystem.getFileSystem().  This returns the filesystem named in the path or the default configured filesystem. 4. Most methods which currently take FileSystem and Path parameters can be changed to take only Path. 5. Many FileSystem operations (create, open, delete, list, etc.) can become convenience methods on Path. 6. A URLStreamHandler can be defined in terms of the FileSystem API, so that URLs for any protocol with a registered FileSystem implementation can be accessed with a java.net.URL, permitting FileSystem implementations to be used on the classpath, etc.  It is tempting to try to replace Path with java.net.URL, but URL's methods are insufficient for mapreduce.  We require directory listings, random access, location hints, etc., which are not supported by existing URLStreamHandler implementations.  But we can expose all FileSystem implementations for access with java.net.URL.  (From a brainstorm with Owen.)",fs
want FileSystem implementation for Amazon S3,"An S3-based Hadoop FileSystem would make a great addition to Hadoop.  It would facillitate use of Hadoop on Amazon's EC2 computing grid, as discussed here:  http://www.mail-archive.com/hadoop-user@lucene.apache.org/msg00318.html  This is related to HADOOP-571, which would make Hadoop's FileSystem considerably easier to extend.  ",fs
Calling shell scripts from build.xml discriminates Windows user minority.,"This is introduced by HADOOP-567. The problem is that now I cannot even build hadoop in Eclipse under Windows unless I run it under Cygwin.  This is in a way the same as calling make in build.xml, which was recently fixed HADOOP-537. I think we should not introducing more dependencies on Cygwin just in order to show something in Web UI. I also don't remember we claimed that Cygwin or anything else except for Ant is required for Hadoop builds.  Is there another way of solving this? build.xml defines ""version"" property, Ant has ""user.name"" property. URL is not changing very often. Or may be the web ui should obtain these properties in run-time. Or may be the Packaging is a better solution, as you guys discussed.",scripts
we need some rpc retry framework,"We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances. In particular, we often end up with calls to rpcs being wrapped with retry loops for timeouts. We should be able to make a retrying proxy that will call the rpc and retry in some circumstances.",ipc
SequenceFile.Sorter should have a merge method that returns an iterator,SequenceFile.Sorter should get a new merge method that returns an iterator over the keys/values.  The current merge method should become a simple method that gets the iterator and writes the records out to a file.,io
none of the rpc servers check the protcol name for validity,"All of the Hadoop RPC servers either ignore the protocol name or do:  if (protocol.equals(Prot1.class.getName()) {   return Prot1.versionId; } else {   return Prot2.versionId; }  A much better structure would be:  if (Prot1.class.getName().equals(protocol)) {   return Prot1.versionId; } else if (Prot2.class.getName().equals(protocol)) {   return Prot2.versionId; } else {   throw new VersionMismatchException(""Expected protocol Prot1 or Prot2 and received: "" + protocol); }",ipc
MapFile constructor should accept Progressible,"MapFile's constructor should accept a Progressible and pass it down to the underlying SequenceFiles.  This permits DFS to signal task progress while writing blocks, and can keep reduce tasks from timing out when block writes are slow.",io
ipc.Server has memory leak -- serious issue for namenode server,"In my environment (running a lot of batch processes each of which reads, creates, and deletes a lof of  files in dfs) the namenode server can run out of memory rather quickly (in a few hours on a 150 node cluster). The netbeans profiler shows an increasing number of direct byte buffers not garbage collected. The documentation on java.nio.ByteBuffer indicates that their allocation might (and obviously does) happen outside the normal gc-collected heap, and, therefore, it is required that direct byte buffers should only be used for long-lived objects.  ipc.Server seems to use a 4KB direct byte buffer for every connection, but, worse, for every RPC call. If I replace the latter ones with non-direct byte buffers, the memory footprint of the namenode server increases only slowly, but even then it is just a matter of time (since I started it 24 hours ago, it leaked by about 300-400MB). If the performance increase by using direct buffers is a requirement, I would suggest to use a static pool.  Although my environment abuses the namenode server in unusual manner, I would imagine that the memory footprint of the namenode server creeps up slowly everywhere",ipc
Hadoop Record csv serialization should not convert Text into String,None,record
Path configuration properties should not be comma separated,"A few configuration properties allow multiple directory paths separated by comma's (,).  Since comma is a valid character for a directory name, it should not be used as a path separator.  At a minimum, this applies to these properties:      mapred.local.dir      dfs.name.dir  [ I also wonder how robust the implementation is against paths that contain spaces. ]",fs
Hadoop records should provide comvenient APIs for serialization/deserialization,"I found the following APIs are very convenient to use and should be  part of the generated class from Jute IDL :  public BytesWritable serialize(String format);  public void deserialize(BytesWitable data, String format);  public static MyRecordJT deserialize(BytesWitable data, String format);  ",record
RPC should send a fixed header and version at the start of connection,"There have been problems with http clients connecting to the RPC servers, which causes the RPC to try and allocate huge buffers and get OutOfMemoryExceptions. I want to have a fixed prefix that is sent first that identifies it as an RPC client. To make the system compatible as much as possible, I'd make the servers accept both forms for a Hadoop release and then change the clients to send the prefix, and finally make the servers only accept the new form.",ipc
bin/hadoop.sh doesn't work for /bin/dash (eg ubuntu 6.10b),"bin/hadoop.sh has a conditional which doesn't work with /bin/dash which ubuntu 6.10b symlinks to /bin/sh.  here's a trivial patch that works for me:  Index: bin/hadoop-daemon.sh =================================================================== --- bin/hadoop-daemon.sh        (revision 468719) +++ bin/hadoop-daemon.sh        (working copy) @@ -56,7 +56,7 @@  pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid    # Set default scheduling priority -if [ ""$HADOOP_NICENESS"" == """" ]; then +if [ ""$HADOOP_NICENESS"" = """" ]; then      export HADOOP_NICENESS=0  fi ",scripts
it would be nice to be able to view log and output files w/in the browser,"it would be nice to be able to view the node logs and output files from w/in the browser vs saving the file and opening an external application.  one way to achieve this is to simply add "".txt"" as a suffix to the relevant files, eg:  Index: bin/hadoop-daemon.sh =================================================================== --- bin/hadoop-daemon.sh        (revision 468719) +++ bin/hadoop-daemon.sh        (working copy) @@ -50,9 +50,9 @@  fi    # some variables -export HADOOP_LOGFILE=hadoop-$HADOOP_IDENT_STRING-$command-`hostname`.log +export HADOOP_LOGFILE=hadoop-$HADOOP_IDENT_STRING-$command-`hostname`.log.txt  export HADOOP_ROOT_LOGGER=""INFO,DRFA"" -log=$HADOOP_LOG_DIR/hadoop-$HADOOP_IDENT_STRING-$command-`hostname`.out +log=$HADOOP_LOG_DIR/hadoop-$HADOOP_IDENT_STRING-$command-`hostname`.out.txt  pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid    # Set default scheduling priority  it has been suggested that perhaps the content type of the log files could be specified, thereby allowing for wider/richer support.",scripts
Upgrade to Jetty 6 does not patch bin/hadoop,"The upgrade to jetty 6 does not fix 'bin/hadoop' to include new jars. The directory 'lib/hadoop/jetty-ext' has been replaced by 'lib/hadoop/jsp-2.0' however 'bin/hadoop' does not include the jars in the new directory in CLASSPATH. As a result, tasktrackers and namenodes fail to launch.  ",scripts
hadoop should provide a common way to wrap instances with different types into one type,"When two sequence files, which have same Key type but different Value types, are mapped out to reduce, multiple Value types is not allowed. In this case, we need a way to wrap instances with different types into one class type to reduce.  In current code, ObjectWritable is a sole choice. but it costs too many space, because the class declaration will be appended into output file as a string for every Key-value pair.",io
TestTextInputFormat fails on some platforms due to non-determinism in format.getSplits(),"TestTextInputFormat depends on format.getSplits() returning splits in the order they were created. On a local filesystem getSplits() depends on File.list(), the order produced by which is non-deterministic. This causes the test to fail on some platforms.",test
bin/hadoop includes in classpath all jar files in HADOOP_HOME,"The hadoop script includes all jars from HADOOP_HOME in the classpath.  This means that all the daemons get the examples and tests included in their class paths.  If a change is made to an example or test, all the daemons have to be restarted to see the affect of the change.  This doesn't seem desirable.",scripts
test-libhdfs.sh does not properly capture and return error status,The BUILD_STATUS variable in test-libhdfs.sh incorrectly attempts to capture the return value of running hdfs_test.,test
TestSymLink fails when ant is not executed from HADOOP_HOME,When I exec 'ant -f somePath/build.xml test' org.apache.hadoop.streaming.TestSymLink fails with the following exception:  junit.framework.AssertionFailedError: java.io.FileNotFoundException: build/test/dfs/tmp/tmp/client-2526974511105483818 (No such file or directory)   java.io.FileOutputStream.open(Native Method)   java.io.FileOutputStream.<init>(FileOutputStream.java:179)   java.io.FileOutputStream.<init>(FileOutputStream.java:131)   org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:907)   org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:278)   org.apache.hadoop.dfs.DistributedFileSystem.createRaw(DistributedFileSystem.java:106)   org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:58)   org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:47)   org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:148)   org.apache.hadoop.fs.FileSystem.create(FileSystem.java:263)   org.apache.hadoop.fs.FileSystem.create(FileSystem.java:170)   org.apache.hadoop.streaming.TestSymLink.testSymLink(Unknown Source)    org.apache.hadoop.streaming.TestSymLink.failTrace(Unknown Source)   org.apache.hadoop.streaming.TestSymLink.testSymLink(Unknown Source),test
Record-IO XML serialization is broken for control characters,Record I/O does not serialize control characters in XML. Patch is forthcoming.,record
build.xml sets up wrong 'hadoop.log.dir' property for 'ant test',"build.xml - line nos. 329-331       <sysproperty key=""hadoop.log.dir"" value=""${hadoop.log.dir}""/>       <sysproperty key=""test.src.dir"" value=""${test.src.dir}""/>       <sysproperty key=""hadoop.log.dir"" value="".""/>  wrongly overrides 'hadoop.log.dir' causing it to be setup incorrectly.",test
Javadoc warning in SequenceFile.java,Javadoc gives warning for a particular line in SequenceFile.java,io
"Write a white paper on Hadoop File System Architecture, Design and Features","Write a white paper on Hadoop File System Architecture, Design and Features.",documentation
"bin/hadoop:111 uses java directly, it should use JAVA_HOME",JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} java org.apache.hadoop.util.PlatformName`  should use JAVA_HOME instead of java.,scripts
Hadoop should include a general purpose distributed lock manager,"Related to HADOOP-726. Currently, there is no good way for distributed apps using HDFS or Map/Reduce to synchronize their operations. HDFS locking doesn't work very well and lock management is an unnecessary burden on the namenode. In addition, it depending on the filesystem for locking could make applications less portable. Hadoop should implement a general purpose distributed lock management service.",ipc
packageNativeHadoop.sh has non-standard sh code,"packageNativeHadoop.sh uses the shell check ""-e"" which fails on Solaris; this caused a nightly build to fail.",scripts
Local file system uses copy to implement rename,"There is a variable LocalFileSystem.useCopyForRename that is set to true. When true, the local file system will implement rename as a copy followed by a delete. This is likely a performance problem. Is there a reason that useCopyForRename is set?",fs
SequenceFile's header should allow to store metadata in the form of key/value pairs," The sequence file currently stores a fixed list of metadata attributes, such as key/value class names,  compression method, etc.  To make sequence file more self descriptable, it should allow to store a list of key/value pairs.  One particular attribute of interest is to indicate whether the key/value classes are actually hadoop record classes,  if so, store the DDls for the records. This way, we may create tools to extract DDl from a sequence file and  then generate necessary classes. It also make it possible to provide an interpretive version of Hadoop record.  This way, even in the situation where Hadoop or the application does not have the necessary classes,  a sequence file of Hadoop records can be read and deserialized ""interpretively"".  ",io
"The underlying data structure, ByteArrayOutputStream,  for buffer type of Hadoop record is inappropriate","With ByteArrayOutputStream as the underlying data structure for a buffer, the user is forced to convert it into a byte [] object in order to do any operations other than sequence append on the buffer. The convertion will create a new copy of bytes. That will cause huge performance problem.   It seems BytesWritable is a better replacement. ",record
TestIPC occassionally fails with BindException,org.apache.hadoop.ipc.TestIPC.testParallel occassionally fails with  java.net.BindException: Address already in use   sun.nio.ch.Net.bind(Native Method)   sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)   sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)   org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:161)   org.apache.hadoop.ipc.Server.start(Server.java:607)   org.apache.hadoop.ipc.TestIPC.testParallel(TestIPC.java:174)   org.apache.hadoop.ipc.TestIPC.testParallel(TestIPC.java:164)  My guess is that port 1234 used by TestIPC.testSerial hasn't been cleaned up by the time TestIPC.testParallel tries to use it.  The failure rate of this test seems around 5%. ,test
org.apache.hadoop.ipc.Server join method doees not wait for its subthreads to die,"Is there an explanation about this ?  In fact, my problem is that I have to launch consecutively two MiniMRCluster in order to perform unit tests as following :   - Start first MiniMrCLuster  - Do some tests  - Stop first MiniMRCluster  - Start second MiniMRCluster  - Do some tests  - Stop second MiniMRCluster  When executing this, the second MiniMRcluster give me continually this log : 06/11/22 11:09:22 INFO ipc.RPC: Problem connecting to server: localhost/127.0.0.1:60030 Waiting for task tracker to start.  The only solution I have found now is to insert a Thread.sleep(2000) between first MiniMRCluster stop and the second one start to ensure that the first one is completely stopped.  Any idea, suggestions, thoughts ?  ",ipc
The site docs are not included in the release tar file,"The top-level site directory should be included somewhere in the release, perhaps inside the docs directory.",documentation
CRC computation and reading should move into a nested FileSystem,"Currently FileSystem provides both an interface and a mechanism for computing and checking crc files. I propose splitting the crc code into a nestable FileSystem that like the PhasedFileSystem has a backing FileSystem. Once the Paths are converted to URI, this is fairly natural to express. To use crc files, your uris will look like:  crc://hdfs:%2f%2fhost1:8020/ which is a crc FileSystem with an underlying file system of hdfs://host1:8020  This will allow users to use crc files where they make sense for their application/cluster and get rid of the ""raw"" methods.",fs
RecordIO compiler does not produce correct Java code when buffer is used as key or value in map,"When buffer type is used as a key type or value type in map, the record IO translator does not produce correct serialization of deserialization code. Patch forthcoming.",record
Unit tests should cleanup created files in /tmp. It causes tests to fail if more than one users run tests on same machine.,"TestMiniMRLocalFS  test cases creates /tmp/wc/input/, which is not cleaned up, any other user running test on same machine simultaneously or later will see test failures.  It should either use a temp directory in the build/test or use java's File.createTempFile method for temporary data and then clean it up. ",test
NameNode benchmark using mapred is insufficient,"The current namenode benchmark (org.apache.hadoop.examples.NNBench) uses map/reduce to distribute a load on the namenode.  For the purposes of loading the namenode, this model gives insufficient control over job start and failure recovery.  I propose the namenode benchmark be re-written to use slaves.sh directly to execute the namenode benchmark.  The benchmark should also give finer control over the operations executed and the timings reported.",test
Path.toString() should retain trailing '/' if passed in constructor,"Does it break something if Path retains trailing '/' if passed in constructor and returns it back in toString() ?  Looking at Path.toString() JobClient wont have a way of knowing which Paths were intended to be directories and not files, in the context of globbed regex ( HADOOP-619 ).  ",fs
Divide the server and client configurations,"The configuration system is easy to misconfigure and I think we need to strongly divide the server from client configs.   An example of the problem was a configuration where the task tracker has a hadoop-site.xml that set mapred.reduce.tasks to 1. Therefore, the job tracker had the right number of reduces, but the map task thought there was a single reduce. This lead to a hard to find diagnose failure.  Therefore, I propose separating out the configuration types as:  class Configuration; // reads site-default.xml, hadoop-default.xml  class ServerConf extends Configuration; // reads hadoop-server.xml, $super  class DfsServerConf extends ServerConf; // reads dfs-server.xml, $super  class MapRedServerConf extends ServerConf; // reads mapred-server.xml, $super  class ClientConf extends Configuration; // reads hadoop-client.xml, $super  class JobConf extends ClientConf; // reads job.xml, $super  Note in particular, that nothing corresponds to hadoop-site.xml, which overrides both client and server configs. Furthermore, the properties from the *-default.xml files should never be saved into the job.xml.",conf
SequenceFile.Reader does not set Configuration on DefaultCodec in init(),init() in SequenceFile.Reader should call setConf(conf) on the newly created DefaultCodec,io
mapred.speculative.execution description in hadoop-defauls.xml is not complete,"<property>   <name>mapred.speculative.execution</name>   <value>true</value>   <description>If true, then multiple instances of some map tasks may   be executed in parallel.</description> </property>  This property also controls speculative execution of reduces, however the description field doesn't indicate that.",conf
need documentation of native build requirements,"We should document the requirements for building the native libraries.  A link to a wiki page should be added to src/java/overview.html.  The wiki page should provide instructions and requirements for building the native code on various platforms.  Ideally Fedora/Redhat, Ubuntu, Windows and Mac instructions should be provided. ",documentation
conf not set for the default Codec when initializing a Reader for a record-compressed sequence file,"Because the conf field of a default Codec does not get set when initializing a reader for a record-compressed sequence file, a NullPointerException is thrown when attempting to create an input stream for the default Codec as shown in the following stack trace:  java.lang.NullPointerException   org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:59)   org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1004)   org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:927)   org.apache.hadoop.io.SequenceFile$Reader.(SequenceFile.java:918)   org.apache.hadoop.mapred.SequenceFileRecordReader.(SequenceFileRecordReader.java:39)   org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:55)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:180)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1388) ",io
RunJar should unpack jar files into hadoop.tmp.dir,RunJar currently unpacks a jar file into the default system temp directory. RunJar sometimes can not proceed because the disk space is running out. It would be better to unpack the jar file into a configurable temp directory like hadoop.tmp.dir.,util
Improvements to GenericWritable,GenericWritable has a few easily addressed problems.,io
native hadoop libraries don't build properly with 64-bit OS and a 32-bit jvm,src/native/lib/Makefile.am doesn't pass along the appropriate -m32/-m64 flag while using libtool to create the shared-object which results in this problem.  Simple fix: -AM_LDFLAGS = @JNI_LDFLAGS@ +AM_LDFLAGS = @JNI_LDFLAGS@ -m$(JVM_DATA_MODEL) ,build
Metrics messages are sent on a fixed-delay schedule instead of a fixed-rate schedule,"This potentially affects all metrics implementation packages which use a server that expects data to be sent at a particular rate.  It means that with counter metrics you will see a spurious periodic dip in the metric, because the rate at which metrics are sent is not  quite keeping up with what the server expects.  The fix is trivial.  In org.apache.hadoop.metrics.spi.AbstractMetricsContext, the call to Timer.schedule should be changes to Timer.scheduleAtFixedRate (which takes the same arguments).  ",metrics
Add Writable implementations for variable-length integer types.,"Currently Hadoop supports only three basic integer-like types: ByteWritable, IntWritable and LongWritable. They provide a fixed tradeoff between their value range and on-disk space consumption. But it is sometimes useful to be able to store integer values with broader allowed range, but less space consumption when possible.  This is especially useful when storing very long series of  values, combined with delta encoding.  Lucene already implements variable-length encoding for positive int and long. I propose to add similar Writable implementations, which use the same encoding methods.",io
Implement the LzoCodec with support for the lzo compression algorithms,lzo is clearly one the best compression libraries out there: ... http://compression.ca/act/act-summary.html  It should be a good value-add for hadoop... ,io
want ant task for record definitions,"An ant task which invoked the record compiler would be good to have.  Then, in build.xml, we can use this task to scan the src tree and generate java files for all defined records.  Generated classes should also be included in the javadoc.",record
Move site directories to docs directories,"The site documentation needs to be included in the release (HADOOP-371) as a top-level docs directory.  Before doing so, we should move, in Subversion, the top-level site directory and src/site to a top-level docs directory and src/docs, respectively.",documentation
IOException when running map reduce on S3 filesystem,Setting fs.default.name to be an S3 path causes the following exception:  java.io.IOException: Cannot create file /tmp/hadoop-tom/mapred/system/submit_habo0/job.jar since parent directory does not exist.         at org.apache.hadoop.fs.s3.S3FileSystem.createRaw(S3FileSystem.java:150)         at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:58)         at org.apache.hadoop.fs.FSDataOutputStream$Summer.<init>(FSDataOutputStream.java:47)         at org.apache.hadoop.fs.FSDataOutputStream.<init>(FSDataOutputStream.java:148)         at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:369)         at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:276)         at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:94)         at org.apache.hadoop.fs.s3.S3FileSystem.copyFromLocalFile(S3FileSystem.java:290)         at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:294)         at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:371)         at org.apache.hadoop.examples.Grep.main(Grep.java:69)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:585)         at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)         at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:143)         at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:41)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:585)         at org.apache.hadoop.util.RunJar.main(RunJar.java:151) ,fs
clean up smallJobsBenchmark and move to src/test/org/apache/hadoop/mapred,"Finish off the missing piece of HADOOP-371 (move src/contrib/smallJobsBenchmark to src/test/org/apache/hadoop/mapred) and cleanup or consolidate documentation, scripts and dead code. ",build
start-all.sh fails on Ubuntu 6.10,"I'm using Hadoop on Ubuntu 6.10.  I ran into:  $ start-all.sh starting namenode, logging to /usr/local/hadoop-install/hadoop/bin/../logs/hadoop-jj-namenode-jjinuxland.out /usr/local/hadoop-install/hadoop/bin/slaves.sh: 36: Syntax error: Bad substitution starting jobtracker, logging to /usr/local/hadoop-install/hadoop/bin/../logs/hadoop-jj-jobtracker-jjinuxland.out /usr/local/hadoop-install/hadoop/bin/slaves.sh: 36: Syntax error: Bad substitution  Ubuntu 6.10 switched away from using bash for the default /bin/sh. The workaround is simple:  (cd /bin && ln -sf bash sh)  Nonetheless, it might be nice to update the script so that it works by default on Ubuntu since it is pretty popular.  I'm guessing that the change would be simple.  The offending line is:  ssh $HADOOP_SSH_OPTS $slave $""${@// /\\ }"" \ ",scripts
Add handling of s3 to CopyFile tool,CopyFile is a useful tool for doing bulk copies.  It doesn't have handling for the recently added s3 filesystem.,util
Files written to S3 but never closed can't be deleted,"I've been playing with the S3 integration. My first attempts to use it are actually as a drop-in replacement for a backup job, streaming data offsite by piping the backup job output to a ""hadoop dfs -put - targetfile"".  If enough errors occur posting to S3 (this happened easily last Thursday, during an S3 growth issue), the write can eventually fail. At that point, there are both blocks and a partial INode written into S3. Doing a ""hadoop dfs -ls filename"" shows the file, it has a non-zero size, etc. However, trying to ""hadoop dfs -rm filename"" a failed-written file results in the response ""rm: No such file or directory.""",fs
dfs -get should remove existing crc file if -crc is not specified,"When we added -crc option to dfs -get (aka dfs -copyToLocal) the absence of this command-line option implies not copying the crc file associated with hdfs file. However, if the checksum file already exists, it will not correspond with the newly copied data file, and opening it would cause checksum failure. The solution is to remove any existing crc file corresponding to the data file if -crc option is not given for -get. Patch forthcoming.",fs
native libraries aren't loaded unless the user specifies the java.library.path in the child jvm options,"The TaskRunner adds the setting of the java.library.path after the class name, which makes it an argument to the main of the TaskTracker.Child. It should be added to the command line before the class name.",util
merge code is really slow,"I had a case where the map output buffer size (io.sort.mb) was set too low and caused a spill and merge. Fixing the configuration caused it to not spill until it was finished. With the spill it took 9.5 minutes per a map. Without the spill it took 45 seconds. Therefore, I assume it was taking ~9 minutes to do the 2 file merge. That is really slow. The input files to the merge were two 25 mb sequence files (default codec (java), block compressed) ",io
we should automate checks of the output of the sort example program,"Since we are using the sort example program to do smoke tests on new versions of Hadoop, it would be nice to have some checks of the output. The checks that I've considered:   1. count the number of records on input & output   2. compute the md5 of each key/value and xor across all of the rows   3. use a map/reduce job to merge the input and output directories and make sure that each key/value appears on both input and output",test
Recursive delete for an S3 directory does not actually delete files or subdirectories,"Here is the bug report from Michael Stack:  Here I'm listing a BUCKET directory that was copied up using 'hadoop fs', then rmr'ing it and then listing again:  stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs s3://ID:SECRET@BUCKET -ls /fromfile Found 2 items /fromfile/diff.txt      <r 1>   591 /fromfile/x.js  <r 1>   2477 stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs s3://ID:SECRET@BUCKET -rmr /fromfile Deleted /fromfile stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs s3://ID:SECRET@BUCKET -ls /fromfile Found 0 items  The '0 items' is odd because, now, listing my BUCKET using a tool other than 'hadoop fs' (i.e. hanzo webs python scripts):  stack@bregeon:~/checkouts/hadoop.trunk$ s3ls BUCKET %2F %2Ffromfile%2F.diff.txt.crc %2Ffromfile%2F.x.js.crc %2Ffromfile%2Fdiff.txt %2Ffromfile%2Fx.js block_-5013142890590722396 block_5832002498000415319 block_6889488315428893905 block_9120115089645350905  Its all still there still.  I can subsequently do the likes of the following:  stack@bregeon:~/checkouts/hadoop$  ./bin/hadoop fs -fs s3://ID:SECRET@BUCKET -rmr /fromfile/diff.txt  ... and the delete will succeed and looking at the bucket with alternate tools shows that it has actually been remove, and so on up the hierarchy.",fs
S3FileSystem should retry if there is a communication problem with S3,File system operations currently fail if there is a communication problem (IOException) with S3. All operations that communicate with S3 should retry a fixed number of times before failing.,fs
Create fsck tool for S3 file system,A fsck tool for S3 would help diagnose data problems and also collect statistics on the use of a S3 volume.  The existing 'bin/hadoop fsck' invocation should be extended to support S3 (currently it supports only HDFS) rather than adding another command. It should be possible to do this by extracting the filesystem from the path (required first argument) and delegating to the relevant fsck tool.  ,fs
Create scripts to run Hadoop on Amazon EC2,"It is already possible to run Hadoop on Amazon EC2 (http://wiki.apache.org/lucene-hadoop/AmazonEC2), however it is a rather involved, largely manual process. By writing scripts to automate (as far as is possible) image creation and cluster launch it will make it much easier to use Hadoop on EC2.",scripts
thousands of TimerThreads created by metrics API,"When running the smallJobsBenchmark with 180 maps and hadoop metrics logging to a file  (ie hadoop-metrics.properties file contains       dfs.class=org.apache.hadoop.metrics.file.FileContext      mapred.class=org.apache.hadoop.metrics.file.FileContext) then I get this error:  org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread   java.lang.Thread.start0(Native Method)   java.lang.Thread.start(Thread.java:574)   org.apache.hadoop.ipc.Client.getConnection(Client.java:517)   org.apache.hadoop.ipc.Client.call(Client.java:452)   org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)   org.apache.hadoop.dfs.$Proxy0.isDir(Unknown Source)   org.apache.hadoop.dfs.DFSClient.isDirectory(DFSClient.java:325)   org.apache.hadoop.dfs.DistributedFileSystem.isDirectory(DistributedFileSystem.java:167)   org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:82)   org.apache.hadoop.dfs.DistributedFileSystem.copyToLocalFile(DistributedFileSystem.java:222)   org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:842)   org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:86)   org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1338)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   org.apache.hadoop.ipc.RPC$Server.call(RPC.java:337)   org.apache.hadoop.ipc.Server$Handler.run(Server.java:538)   org.apache.hadoop.ipc.Client$Connection.run(Client.java:258)  Using jconsole, I see that 2000+ of these threads were created:  Name: Timer-101 State: TIMED_WAITING on java.util.TaskQueue@1501026 Total blocked: 0  Total waited: 5 Stack trace:  java.lang.Object.wait(Native Method) java.util.TimerThread.mainLoop(Timer.java:509) java.util.TimerThread.run(Timer.java:462)  The only use of the java.util.Timer API is in org.apache.hadoop.metrics.spi.AbstractMetricsContext. ",metrics
metrics API should enforce some restrictions on tag and metric names,"The metrics API currently allows any string to be a metric name or tag name (see org.apache.hadoop.metrics.Metrics.createRecord(...) and org.apache.hadoop.metrics .Metrics.report(...)).  Such unrestricted names makes it difficult to implement metrics providers that generate source code to manipulate these metrics, since many characters are invalid as method or variable names.  I'd like to propose that metric names be restricted to letters, digits, and underscore (A-Za-z0-9_) and this restriction be documented and enforced by the API.",metrics
SequenceFile constructors should not accept a FileSystem parameter,"The SequenceFile constructors accept a FileSystem, Path and Configuration.  The FileSystem is redundant, as the combination of the Configuration and the Path determine the FileSystem.  These methods should thus be deprecated and replaced by FileSystem-less versions.",io
DFS unit tests have duplicate code,"A number of the DFS-related unit tests have a bunch of copied code.  These include TestRestartDFS, TestFileCorruption, TestFsck and TestCopyFiles.  Even within each test there is a lot of duplicated code.  Maintaining these as APIs evolve is arduous.  They should instead use a common base class.",test
Update tag and metric names to conform to HADOOP-887,"In preparation for HADOOP-887, update the current tag and metric names that use currently use '-'.  Convert all '-' to '_'.",metrics
Need a simpler way to specify arbitrary options to java compiler while building Hadoop,"Currently, if one has to specify arbitrary command-line options, such as ""-Xlint:unchecked"", to javac, one has to edit build.xml. There should be a property, javac.args, that can be specified on the ant command-line, as follows:  ant compile -Djavac.args=""-Xlint:unchecked""  Patch forthcoming.",build
Make S3FileSystem do recursive renames,"From Mike Smith:  I went through the S3FileSystem.java codes and fixed the renameRaw() method. Now, it iterates through the folders recursively and rename those. Also, in the case of existing destination folder, it moves the src folder under the dst folder.  Here is the piece code that should be replaced in S3FileSystem.java. renameRaw() method should be replaced by the following methods:   @Override  public boolean renameRaw(Path src, Path dst) throws IOException {    Path absoluteDst = makeAbsolute(dst);   Path absoluteSrc = makeAbsolute(src);    INode inode = store.getINode(absoluteDst);   // checking to see of dst folder exist. In this case moves the   // src folder under the existing path.   if (inode != null && inode.isDirectory()) {    Path newDst = new Path(absoluteDst.toString ()+""/""+absoluteSrc.getName());    return renameRaw(src,newDst,src);   } else {   // if the dst folder does not exist, then the dst folder will be created.    return renameRaw(src,dst,src);   }  }   // recursively goes through all the subfolders and rename those.  public boolean renameRaw(Path src, Path dst,Path orgSrc) throws IOException {     Path absoluteSrc = makeAbsolute(src);     Path newDst = new Path(src.toString().replaceFirst(orgSrc.toString(), dst.toString()));     Path absoluteDst = makeAbsolute(newDst);     LOG.info(absoluteSrc.toString());     INode inode = store.getINode (absoluteSrc);     if (inode == null) {       return false;     }     if (inode.isFile()) {      store.storeINode(makeAbsolute(absoluteDst), inode);     } else {       store.storeINode (makeAbsolute(absoluteDst), inode);       Path[] contents = listPathsRaw(absoluteSrc);       if (contents == null) {         return false;       }       for (Path p : contents) {         if (! renameRaw(p,dst,orgSrc)) {           return false;         }        }     }     store.deleteINode(absoluteSrc);     return true; }",fs
Code to qualify inputDirs doesn't affect path validation,"This code, at JobClient:306, doesn't seem to validate the fully qualified inputDirs, since inputDirs is a newly created arrray:          Path[] inputDirs = job.getInputPaths();           // make sure directories are fully qualified before checking them         for(int i=0; i < inputDirs.length; ++i) {           if (inputDirs[i].toUri().getScheme() == null) {             inputDirs[i] = userFileSys.makeQualified(inputDirs[i]);           }         }          // input paths should exist.          job.getInputFormat().validateInput(job);  ",fs
Unable to set Replication factor on SequenceFile,There does not seem to be a way to set the replication factor for a SequenceFile when creating one.  The API doesn't provide a way to pass it in and a call to FileSystem.setReplication() does not have any effect if called immediately after a call to SequenceFile.createWriter() ,io
NPE in org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue,After nutch started using hadoop 0.10.1 the following Exception started to appear:  java.lang.NullPointerException   org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:2158)   org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:1892)   org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:498)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:191)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1367)  Anyone know the cure?,io
tail of file not checked for checksum errors,"The checksum is only verified every bytesPerSum (512) bytes.  For the last file size % bytesPerSum bytes in a file, the checksum is not checked. ",fs
make checksums optional per FileSystem,"Checksumming is currently built into the base FileSystem class.  It should instead be optional, with each FileSystem implementation electing whether to use the Hadoop-provided checksum system, or to disable it, or to implement its own custom checksum system.  To implement this, a ChecksumFileSystem implementation can be provided that wraps another FileSystem implementation, implementing checksums as in Hadoop's current mandatory implementation (i.e., as a separate crc file per file that's elided from directory listings).  The 'raw' FileSystem methods would be removed.  FSDataInputStream and FSDataOutputStream would be made interfaces. ",fs
Add support for reading regular (non-block-based) files from S3 in S3FileSystem,"People often have input data on S3 that they want to use for a Map Reduce job and the current S3FileSystem implementation cannot read it since it assumes a block-based format.  We would add the following metadata to files written by S3FileSystem: an indication that it is block oriented (""S3FileSystem.type=block"") and a filesystem version number (""S3FileSystem.version=1.0""). Regular S3 files would not have the type metadata so S3FileSystem would not try to interpret them as inodes.  An extension to write regular files to S3 would not be covered by this change - we could do this as a separate piece of work (we still need to decide whether to introduce another scheme - e.g. rename block-based S3 to ""s3fs"" and call regular S3 ""s3"" - or whether to just use a configuration property to control block-based vs. regular writes). ",fs
Make writes to S3FileSystem world visible only on completion,Currently files written to S3 are visible to other processes as soon as the first block has been written. This is different to DFS which only makes files world visible after the stream writing to the file has closed (see FSNamesystem.completeFile).  We could implement this by having a piece of inode metadata that indicates the visibility of the file.,fs
File locking interface and implementation should be remvoed.,HADOOP-726 was filed on the same issue. HADOOP-726 only deprecates file locking interface. We should remove all the code related these locks in hadoop. ,fs
too many SequenceFile.createWriter() methods,"There are too many SequenceFile.createWriter() method signatures.  This method has two required paramters: a Configuration and a Path.  It has one obsolete parameter: a FileSystem.  And it has five optional parameters: CompressionType, CompressionCodec, Progress, replication, and metadata.  We should remove the obsolete parameter and make all optional parameters into setters.",io
Enhancements to Hadoop record I/O - Part 1,"Hadoop record I/O can be used effectively outside of Hadoop. It would increase its utility if developers can use it without having to import hadoop classes, or having to depend on Hadoop jars. Following changes to the current translator and runtime are proposed.  Proposed Changes:  1. Use java.lang.String as a native type for ustring (instead of Text.) 2. Provide a Buffer class as a native Java type for buffer (instead of BytesWritable), so that later BytesWritable could be implemented as following DDL: module org.apache.hadoop.io {   record BytesWritable {     buffer value;   } } 3. Member names in generated classes should not have prefixes 'm' before their names. In the above example, the private member name would be 'value' not 'mvalue' as it is done now. 4. Convert getters and setters to have CamelCase. e.g. in the above example the getter will be:   public Buffer getValue(); 5. Generate clone() methods for records in Java i.e. the generated classes should implement Cloneable. 6. Make generated Java codes for maps and vectors use Java generics.  These are the proposed user-visible changes. Internally, the translator will be restructured so that it is easier to plug-in translators for different targets. ",record
Coding style issues ,"I would like to recommend some mainly stylistic changes in the recent fix of http://issues.apache.org/jira/browse/HADOOP-886.  The file in question is CodeFactory.java, and the reasons for the changes are:     * It is generally preferable to avoid multiple return statements.    * It is nearly always preferable to use curly braces and a newline after an if (condition).    * There's no benefit to doing the hash lookup twice in the common case (by calling contains and then get).  (1) and (2) are commonly found in Java coding style guidelines as they make the code more readable.   I'll attach the fix shortly. ",metrics
Create a public (shared) Hadoop EC2 AMI,"HADOOP-884 makes it easy to run Hadoop on an EC2 cluster, but building an AMI (Abstract Machine Image) can take a little while. Amazon EC2 supports shared AMIs (http://developer.amazonwebservices.com/connect/entry.jspa?entryID=530&ref=featured), so we could provide publically available AMIs for each Hadoop release.",scripts
Metrics should offer complete set of static report methods or none at all,org.apache.hadoop.metrics.Metrics currently has one report method.  I should either have report methods for all underlying MetricsRecord or no report methods at all.,metrics
"Metrics.report() metricValue parameter type should be float, not long","org.apache.hadoop.metrics.Metrics.report() takes a long metricValue parameter that gets mapped by Java in the method implementation to MetricsRecord.setMetric(String,float) method.  This should be made explicit in the Metrics.report() method parameters by changing the type of metricValue to float.",metrics
DFSShell/Wen UI should return the metadata of a file,"There should be a utility to check whether a file is a sequence file or not. If so, return the metadata associated with the file. This feature should be added to  DFSShell and Web UI.  ",fs
Incorrect number of map tasks when there are multiple input files,"This problem happens with hadoop-streaming and possibly elsewhere.  If there are 5 input files, it will create 130 map tasks, even if mapred.map.tasks=128.  The number of map tasks is incorrectly set to a multiple of the number of files.  (I wrote a much more complete bug report, but Jira lost it when it had an error, so I'm not in the mood to write it all again)",documentation
Hadoop EC2 scripts are not executable,The build script needs to make the scripts under src/contrib/bin executable when packaging them up in the tar file.  Also worth making the small changes mentioned by Doug in HADOOP-884 (adding a README and a template config file) at the same time.,scripts
improve the stack trace returned by RPC client,"Currently, the RemoteException thrown from calls to RPCs include the stack trace from the RPC thread rather than the user's thread. ",ipc
flip boolean to have rpc clients send a header,This is the continuation of HADOOP-667. I forgot to flip it for 0.10. *smile*,ipc
Reduce CPU usage of hadoop ipc package,There are a couple of optimizations that could be done to reduce CPU consumption.  1. The method Server.cleanupConnections() could be invoked less often. 2. The method Server.cleanupConnections() uses a List to manage all active connections and uses  connectionList.get(i) to iterate. Locating the ith element essentially translates to traversing the list from the beginning to the ith position. 3. The current DFS heartbeattime is 3 seconds whereas ipc.client.connection.maxidletime is set to 1 second. The proposal is to change the default value of ipc.client.connection.maxidletime to something larger than the heartbeat interval. This also has to suit the heartbeat periodicity of map-reduce software. 4. Evaluate epoll() added in JDK 1.5.10 (this is a java cmd line option)      http://java.sun.com/j2se/1.5.0/ReleaseNotes.html#150_10 ,ipc
A couple setter functions and toString method for BytesWritable.,I wrote a couple of setters and a workable toString for BytesWritable.,io
The mini/mr cluster for testing always uses the local file system rather than the namenode that was passed in,"The mini map/reduce cluster that is used for testing was incorrectly setting the attribute for the filesystem, causing all of the tests to run against the local file system instead of the mini dfs cluster. This patch fixes that and also adds a new method to the mini mr cluster to generate an appropriate JobConf to submit against that cluster. That makes sure that all of the appropriate fields are set.",test
Implement S3 retry mechanism for failed block transfers,"HADOOP-882 improves S3FileSystem so that when certain communications problems with S3 occur the operation is retried. However, the retry mechanism cannot handle a block transfer failure, since blocks may be very large and we don't want to buffer them in memory. This improvement is to write a wrapper (using java.lang.reflect.Proxy if possible - see discussion in HADOOP-882) that can retry block transfers.",fs
"The ""-local"" option does work properly with test programs","The test programs take a param ""-local"" to force the local file system. This does not work in cases where the file(of type Path) is asked for it's FS. This affects teh testsequencefile program and cud affect other test programs as well. ",test
"Names used for map, reduce, and shuffle metrics should be unique","The names used for map, reduce, and shuffle metrics currently overlap:  Map metrics:   input_bytes    input_records    output_bytes    output_records   Recduce metrics:    input_records     output_records   Shuffle metrics:    input_bytes   I propose that the metric names be unique:   map_input_bytes    map_input_records    map_output_bytes    map_output_records    shuffle_input_bytes    reduce_input_records    reduce_output_records ",metrics
navigation on wiki front page should be above the fold,"The Hadoop wiki's front page currently has a long textual description of the project above the navigational links.  The front page is a starting point not just for project newbies, but also for experienced developers.  Thus one should be able to navigate the site quickly from it.  I propose that we have a short introductory paragraph at the top of the page, the move the longer introduction either: to a separate ""about"" page; or underneath the navigational links.  I have a slight preference for a separate ""about"" page.  Thoughts?",documentation
Optimization: Reduce Overhead from ReflectionUtils.newInstance,"I found that a significant amount of time on my project was being spent in creating constructors for each row of data. I dramatically optimized this performance by creating a simple WeakHashMap to cache constructors by class. For example, in a sample job I find that ReflectionUtils.newInstance takes 200 ms (2% of total) with the cache enabled, but it uses 900 ms (6% of total) without the cache.  ",util
record io should have a build target and a test target,The C++ record IO (jute) library should have a build target and a test target in build.xml,record
TestMiniMRLocalFS and TestMiniMRCaching broken on Windows,"After fixing HADOOP-761, TestMiniMRLocalFS and TestMiniMRCaching and now broken on Windows.  I belive that fixing them is dependent on fixing HADOOP-1020.  There are multiple problems in a utility class used by these 2 tests, MRCaching.  1) the ""dfs"" uri's should be ""hdfs"" for TestMiniMRCaching 2) the ""dfs"" uri's should be ""file"" for TestMiniMRLocalFS 3) the cache directory should be different for each test",test
better links to mailing list archives,"The archive links on http://lucene.apache.org/hadoop/mailing_lists.html only point to files, not to websites where the archives can be browsed and searched. I suggest to use these links (additionally or instead of the existing ones):  users: http://mail-archives.apache.org/mod_mbox/lucene-hadoop-user/ http://www.mail-archive.com/hadoop-user%40lucene.apache.org/  dev: http://mail-archives.apache.org/mod_mbox/lucene-hadoop-dev/ http://www.mail-archive.com/hadoop-dev%40lucene.apache.org/  commits: http://mail-archives.apache.org/mod_mbox/lucene-hadoop-commits/ http://www.mail-archive.com/hadoop-commits%40lucene.apache.org/  mail-archives.apache.org seems to be the official archive, mail-archive.com has a search feature. ",documentation
remove dead code in Server.java,"There's some dead code in Server.java.  The callDequeued field is no longer used, yet it is populated, synchronized on and notified.",ipc
"in unit tests, set ipc timeout in one place","The unit test code currently decreases ipc.client.timeout in three different places in order to speed the execution of unit tests (since daemon startup and shutdown time is related to this value).  The value for unit tests should be set in only a single place.  Also, the value used when testing should perhaps be increased from one second to two seconds, to make tests more reliable.",test
"Provide ""swiggable"" C binding for Hadoop Record I/O"," Provide a 'swiggable' C binding, so that processing the generated C code with swig allows it to be used in scripting languages such as Python and Perl.  ",record
Rewrite AmazonEC2 wiki page,"The wiki page that describes running Hadoop on Amazon EC2 (http://wiki.apache.org/lucene-hadoop/AmazonEC2) includes a large number of instructions for manually building images and launching instances using EC2 tools. We now have a set of automated scripts for doing these tasks (see HADOOP-884, HADOOP-952), so I propose removing the manual instructions (also, they are likely to become inaccurate over time, and may make the idea of using Hadoop on EC2 daunting to new users).  As a part of this I will keep the parts of the documentation that explain the concepts, and document what the scripts do in more detail.",scripts
TestDecommission fails because it attempts to transfer block to a dead datanode,"There are two iterations in TestDecommission.  After the first iteration, one datanode will be shut down because it was decommissioned.  In the second iteration, while decommissioning the node, if it attempts to transfer blocks to the shut down node, the test will fail.  http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/29/console   ",test
race condition in setting up ipc connections,"While running svn head, I get:  [junit] 2007-02-27 19:11:17,707 INFO  ipc.Client (Client.java:run(281)) - java.lang.NullPointerException     [junit]   org.apache.hadoop.ipc.Client$Connection.run(Client.java:251)  There is a race condition between when the threads are created above and when the IO streams are set up below.",ipc
Add checkstyle target to ant build file,"As discussed in HADOOP-948, add a target to allow people to run style checks on the codebase.","build,test"
Make Record I/O functionally modular from the rest of Hadoop,"This issue has been created to separate one proposal originally included in HADOOP-941, for which no consensus could be reached. For earlier discussion about the issue, please see HADOOP-941.  I will summarize the proposal here.  We need to provide a way for some users who want to use record I/O framework outside of Hadoop.",record
S3 listSubPaths bug,"I had problem with the -ls command in s3 file system. It was returning inconsistence number of ""Found Items"" if you rerun it different times and more importantly it returns recursive results (depth 1) for some folders.   I looked into the code, the problem is caused by jets3t library. The inconsistency problem will be solved if we use : S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER);  instead of   S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER , 0);  in listSubPaths of Jets3tFileSystemStore class (line 227)! This change will let GET REST request to have a ""max-key"" paramter with default value of 1000! It seems s3 GET request is sensetive to this paramater!   But, the recursive problem is because the GET  request doesn't execute the delimiter constraint correctly. The response contains all the keys with the given prefix but they don't stop at the path_delimiter. You can simply test this by making couple folder on hadoop s3 filesystem and run -ls. I followed the generated GET request and it looks all fine but it is not executed correctly at the s3 server side.I still don't know why the response doesn't stop at the path_delimiter.   Possible casue: Jets3t library does URL encoding, why do we need to do URL encoding in Jets3tFileSystemStore class!?  example:  Original path is   /user/root/folder  and it will be encoded to %2Fuser%2Froot%2Ffolder is Jets3tFileSystemStore class. Then, Jets3t will reencode this to make the REST request. And it will be rewritten as %252Fuser%252Froot%252Ffolder, so the the generated folder on the S3 will be %2Fuser%2Froot%2Ffolder after decoding at the amazon side. Wouldn't be better to skip the encoding part on Hadoop. This strange structure might be the reason that the s3 doesn't stop at the path_delimiter.   ",fs
Checksum error in InMemoryFileSystem,"Getting the following error in the tasktracker log on 2 attempts: 2007-03-05 14:59:50,320 WARN  mapred.TaskRunner - task_0001_r_000005_0 Intermediate Merge of the inmemory files threw an exception: org.apache.hadoop.fs.ChecksumException: Checksum error: /trank/n utch-0.9-dev/filesystem/mapred/local/task_0001_r_000005_0/map_2.out at 16776192         at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.verifySum(ChecksumFileSystem.java:250)         at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:207)         at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:163)         at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)         at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)         at java.io.BufferedInputStream.read(BufferedInputStream.java:317)         at java.io.DataInputStream.readFully(DataInputStream.java:178)         at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:57)         at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:91)         at org.apache.hadoop.io.SequenceFile$Reader.readBuffer(SequenceFile.java:1300)         at org.apache.hadoop.io.SequenceFile$Reader.seekToCurrentValue(SequenceFile.java:1363)         at org.apache.hadoop.io.SequenceFile$Reader.nextRawValue(SequenceFile.java:1656)         at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawValue(SequenceFile.java:2579)         at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2351)         at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2226)         at org.apache.hadoop.mapred.ReduceTaskRunner$InMemFSMergeThread.run(ReduceTaskRunner.java:820)  When I changed fs.inmemory.size.mb to 0 (was 75 - default) the reduce completes successfully. Could it be related to HADOOP-1027 or HADOOP-1014?  - Espen",fs
MiniDFSCluster exists a race condition that lead to data node resources are not properly released,"In MiniDFSCluster, there is a possibility that a data node gets shutted down before it is constructed. This leads to the situation that the data node's resources are not properly released. In Cygwin I observe that the data node directory is still kept locked after a miniDFSCluster is shut down.",test
http://lucene.apache.org/hadoop/ front page is not user-friendly,"One of our tech writers has been looking at the state of Hadoop documentation, and suggests reworking the Hadoop front page to be more than just a release page (as it seems to be now).  Especially in comparison with other other apache project pages, ours is uninviting to users unfamiliar with the premise of the project. She's suggesting updating the page to include more meta-information about the site, in addition to most of the links and news items currently on the page.  ",documentation
Compile fails if Checkstyle jar is present in lib directory,"HADOOP-1051 added a checkstyle target. However, the compile target fails if the checkstyle jar is present in the lib directory since it includes an earlier version of Commons CLI which Hadoop doesn't compile against.  The simplest solution is to exclude the checkstyle jar from the classpath.",build
Rename Hadoop record I/O to Jute,"jute was the original name of the hadoop record i/o component. IMHO, it is easier to pronounce, easier to remember and has already stuck among its users. This renaming should be done while there isn't a large codebase using jute, otherwise it will be very difficult later. rcc will be renamed jrc (jute record compiler).",record
VersionMismatch should be VersionMismatchException,"org.apache.hadoop.ipc.RPC$VersionMismatch extends IOException.  It's name should follow the Java naming convention for Exceptions, and thus be VersionMismatchException.",ipc
Ensure that all test-cases using MiniDFSCluster & MiniMRCluster do not start the datanodes/tasktrackers before namenode/jobtracker due to port-rolling problems.,The MiniDFSCluster & MiniMRCluster seem to fail with problems arising from port-rolling in our patch-process; stopping the test-cases from starting the NN/JT before the DNs/TTs should help as a short-term fix.,test
Cygwin path translation should occur earlier in bin/hadoop,"When native Linux libraries are present, and bin/hadoop is running under Cygwin, the part of the script that sets up java.library.path uses un-translated CLASSPATH. This leads to the (in)famous message:  Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName  The fix is to perform the Cygwin translation of the CLASSPATH earlier, before checking for the native libs.",scripts
JAVA_PLATFORM with spaces (i.e. Mac OS X-ppc-32) breaks bin/hadoop script,"Thus says Brian Whitman in NUTCH-432:  ""In some later nightly in the past few weeks (not sure when) the bin/nutch script stopped working on my Macs with Exception in thread ""main"" java.lang.NoClassDefFoundError: OS On any command. I tracked it down to the JAVA_PLATFORM env variable that is used to try to find a native hadoop library. The line JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName` in bin/nutch returns ""Mac OS X-ppc-32"", which then appears as -Djava.library.path=/Users/bwhitman/Desktop/nn/lib/native/Mac OS X-ppc-32 in the java command line to start a nutch tool. Not sure the best way to fix this, but I manually put JAVA_PLATFORM='MacOSX/PPC' and the error went away. ""  The same problem occurs in bin/hadoop.  I propose the following fix:    JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName | sed -e 's/ /_/g'`  The alternative would be to fix this in PlatformName, but then we may want to get the real platform name in some other places. We could also add a cmd-line switch to PlatformName.",scripts
Remove 'port rolling' from Mini{DFS|MR}Cluster,The rolling of ports in these 2 clusters lead to a lot of timing issues and failed test cases; as witnessed in our patch process.  The way around is to let the OS pick the port for the NameNode/JobTracker and let the the DataNode/TaskTracker query them for the port to connect to and then use that port.,test
Csv and Xml serialization for buffers do not work for byte value of -1,"If the buffer contains a byte with value -1, the CSV and XML formats do not serialize correctly. Patch follows.",record
The c++ version of write and read v-int don't agree with the java versions,"The serialization of vints is inconsistent between C++ and Java. Since the Java has been fixed recently, I'll move the C++ implementation to match the current Java implementation.",record
Optimize readFields and write methods in record I/O,"Optimize generated write() and readFields() methods, so that they do not have to create BinaryOutputArchive or BinaryInputArchive every time these methods are called on a record.  These will be done via static ThreadLocal variable in Binary Archives.",record
Provide ByteStreams in C++ version of record I/O," Implement ByteInStream and ByteOutStream for C++ runtime, as they will be needed for using Hadoop Record I/O with forthcoming C++ MapReduce framework (currently, only FileStreams are provided.) ",record
Rename InputArchive and OutputArchive and make them public,"Currently hadoop.record.RecordReader and RecordWriter act as factories for various InputArchive and OutputArchive recently. In the original design, this was done in order to have tight control over various serialization formats. This has proven to be counterproductive. For wider usage of record I/O one should be able to use their own serialization formats. The proposed changes make it possible. They are as follows:   1. Eliminate current record.RecordReader and record.RecordWriter.   2. rename InputArchive as RecordInput, and OutputArchive as RecordOutput.   3. rename various archives accordingly. e..g. BinaryInputArchive -> BinaryRecordInput etc.",record
Bug in XML serialization of strings in record I/O,XML serialization of strings in record I/O has a bug whicha makes the XmlInputArchive crash with SAXParseExcpetion (generated XML contains invalid Unicode characters http://www.w3.org/TR/REC-xml/#charsets).,record
bin/hadoop script clobbers CLASSPATH,The bin/hadoop script distributed with hadoop clobbers the user's CLASSPATH.  This prevents ad-hoc appending to the CLASSPATH. ,scripts
"Add maxmemory=""256m"" in the junit call of build-contrib.xml","When testing some very custom code, TestSymlink failed due to an OutOfMemoryError.",build
LocalFileSystem gets a NullPointerException when tries to recover from ChecksumError,NullPointerException occurs when run a large sort java.lang.NullPointerException   org.apache.hadoop.fs.FSDataInputStream$Buffer.seek(FSDataInputStream.java:74)   org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:121)   org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:221)   org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:167)   org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:41)   java.io.BufferedInputStream.fill(BufferedInputStream.java:218)   java.io.BufferedInputStream.read(BufferedInputStream.java:237)   org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:93)   java.io.DataInputStream.readInt(DataInputStream.java:370)   org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1616)   org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:2567)   org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2353)   org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:180)   org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:149)   org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:41)   org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:313)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)  ,fs
ChecksumFileSystem does not handle ChecksumError correctly,"When handle ChecksumError, the checksumed file system tries to recover by rereading from a different replica.  I have three comments: 1. One bug in the code is that when retrying, the object that computes checksum does not get restored to the old state. 2. The code also assumes that the first byte read and the byte being read when ChecksumError occurs are in the same block.  3. It would be more efficient if we roll back to the first byte in the chunk that's being checksumed instead of rolling back to the first byte that was read.",fs
IPC client fails to override the close method for FilterOutputStream,"Classes that extend FilterOutputStream and BufferedOutputStream must override the close method because the default implementation ignores IOExceptions that occur during the flush method. This has caused us to lose critical information about the sources of errors. *sigh* While looking over the code base for such problems, I found the code in IPC Client.java with this problem.  More generally, we should probably have util classes that replace the Java ones that have a better default for close. (and a better default for FilterOutputStream.write(byte[], int, int) too...)",ipc
Deadlock bug involving the o.a.h.metrics package,"Hi David,  Our nightly benchmarks are occasionally failing (2 to 4 of them per night) due to this deadlock in the JT that looks to be caused by Simon.  Do you have time to fix this in the morning?  Thanks, Nige    Found one Java-level deadlock: ============================= ""expireLaunchingTasks"":   waiting to lock monitor 0x08141b44 (object 0x57eafdd0, a org.apache.hadoop.mapred.JobTracker),   which is held by ""IPC Server handler 8 on 50020"" ""IPC Server handler 8 on 50020"":   waiting to lock monitor 0x08141630 (object 0x57de46b8, a com.yahoo.simon.hadoop.metrics.SimonContext),   which is held by ""Timer-0"" ""Timer-0"":   waiting to lock monitor 0x08141b44 (object 0x57eafdd0, a org.apache.hadoop.mapred.JobTracker),   which is held by ""IPC Server handler 8 on 50020""  Java stack information for the threads listed above: =================================================== ""expireLaunchingTasks"":         at org.apache.hadoop.mapred.JobTracker$ExpireLaunchingTasks.run(JobTracker.java:152)         - waiting to lock <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)         at java.lang.Thread.run(Thread.java:619) ""IPC Server handler 8 on 50020"":         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.createRecord(AbstractMetricsContext.java:192)         - waiting to lock <0x57de46b8> (a com.yahoo.simon.hadoop.metrics.SimonContext)         at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:130)         at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1383)         - locked <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559) ""Timer-0"":         at org.apache.hadoop.mapred.JobTracker.getRunningJobs(JobTracker.java:943)         - waiting to lock <0x57eafdd0> (a org.apache.hadoop.mapred.JobTracker)         at org.apache.hadoop.mapred.JobTracker$JobTrackerMetrics.doUpdates(JobTracker.java:429)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:275)         - locked <0x57de46b8> (a com.yahoo.simon.hadoop.metrics.SimonContext)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:48)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:242)         at java.util.TimerThread.mainLoop(Timer.java:512)         at java.util.TimerThread.run(Timer.java:462)  Found 1 deadlock. ",metrics
XmlRecordInput class should be public,None,record
need improved release process,"Hadoop's release process needs improvement.  We should better ensure that releases are stable, not releasing versions that have not been proven stable on large clusters, and we should better observe Apache's release procedures.  Once agreed on, this process should be documented in http://wiki.apache.org/lucene-hadoop/HowToRelease.  Here's a proposal:  . candidate release builds should be placed in lucene.apache.org/hadoop/dev/dist . candidate artifacts should be accompanied by a md5 and pgp signatures . a 72-hour vote for the release artifact should be called on hadoop-dev. . 3 binding +1 votes and a majority are required . if the vote passes, the release can then posted to www.apache.org/dist/lucene/hadoop for mirroring  This would bring us into accord with Apache's requirements, and better permit large-cluster validation.  We should also build consensus for a release before we commence this process.  Perhaps we should aim for releases every two months instead of every month.  We should perhaps develop more elaborate branching and merging conventions around releases.  Currently we mostly lock-out changes intended for release X+1 from trunk until release X is complete, which can be awkward.  How can we better manage that?   ",build
Ganglia metrics reporting is misconfigured,"In hadoop-metrics.properties, I set mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext.  If I then get the gmond xml feed from the gmond server, I get this:  <METRIC NAME=""load_one"" VAL=""1.04"" TYPE=""float"" UNITS="""" TN=""28"" TMAX=""70"" DMAX=""0"" SLOPE=""both"" SOURCE=""gmond""/> ... <METRIC NAME=""datanode.myhostname.bytes_read"" VAL=""657927"" TYPE=""int32"" UNITS="""" TN=""5696"" TMAX=""60"" DMAX=""0"" SLOPE=""both"" SOURCE=""gmetric""/>  Because the bytes_read metric has the datanode.hostname prefix, it will not aggregate with metrics from other hosts properly. ",metrics
TestReplicationPolicy doesn't use port 0 for the NameNode,"The TestReplicationPolicy test doesn't use mini-dfs and specifies a fixed port, so it can crash with address in use:  java.net.BindException: Address already in use         at sun.nio.ch.Net.bind(Native Method)         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)         at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:184)         at org.apache.hadoop.ipc.Server.<init>(Server.java:622)         at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:323)         at org.apache.hadoop.ipc.RPC.getServer(RPC.java:293)         at org.apache.hadoop.dfs.NameNode.init(NameNode.java:181)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:207)         at org.apache.hadoop.dfs.TestReplicationPolicy.<clinit>(TestReplicationPolicy.java:36) ",test
Code for toString in code generated by Record I/O Compiler can be generic,The generated toString method for every record is identical (it calls csv serialization on the record.) It can be moved to the Record class from which every generated record inherits.,record
InMemoryFileSystem uses synchronizedtMaps with maps that are locked anyways,The InMemoryFileSystem uses synchronizedMaps and then guards each access to them by locking the FileSystem. There is also insufficient synchronization in create.,fs
CopyFiles skips src files of s3 urls,"When given a source file of items to copy, the CopyFiles utility tries each of its supported schemes in turn looking for matching entries in the passed file.  Of the 4 schemes -- file, hdfs, http, and s3 -- we never make it to the s3 test.  We skip out early.  Also, even if we did make it to the search-for-s3-URLs code, the list of s3 srcPaths would always be empty because of a copy/paste error.",util
dynamically change log levels,I would like to switch on the debug log level of the namenode (or other components) without restarting it. This is needed to analyze a production system.  Can somebody please advice on how to set the loglevel dyncamically on a running namenode? I was thinking of enhancing dfsadmin to make an RPC to the namenode to set a specified logging level. But the apache common logging APi does not export an API to change logging levels.,util
deadlock in Abstract Metrics Context,"There appears to be a lock-inversion deadlock in AbstractMetricsContext.  When using ganglia metrics, sometimes the jobtracker will start timing out requests.  The logs then reveal:  2007-03-30 13:59:50,942 WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call heartbeat(org.apache.hadoop.mapred.Task TrackerStatus@1c19919, false, true, 407) from 10.255.62.129:50215  A kill -QUIT dump shows:  ""IPC Server handler 6 on 10001"" daemon prio=1 tid=0x08515c08 nid=0x526a waiting for monitor entry [0x4e6f4000..0x4e6f4f40]         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.createRecord(AbstractMetricsContext.java:192)         - waiting to lock <0x5a562c98> (a org.apache.hadoop.metrics.ganglia.GangliaContext)         at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:130)         at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1384)         - locked <0x5a446330> (a org.apache.hadoop.mapred.JobTracker)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)         at java.lang.reflect.Method.invoke(Unknown Source)         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:336)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:559) ... ""Timer-0"" prio=1 tid=0x08664040 nid=0x5274 waiting for monitor entry [0x4e36d000..0x4e36df40]         at org.apache.hadoop.mapred.JobTracker.getRunningJobs(JobTracker.java:944)         - waiting to lock <0x5a446330> (a org.apache.hadoop.mapred.JobTracker)         at org.apache.hadoop.mapred.JobTracker$JobTrackerMetrics.doUpdates(JobTracker.java:429)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:275)         - locked <0x5a562c98> (a org.apache.hadoop.metrics.ganglia.GangliaContext)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:48)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:242)         at java.util.TimerThread.mainLoop(Unknown Source)         at java.util.TimerThread.run(Unknown Source) ",metrics
"The misleading Configuration.set(String, Object) should be removed","I think that the confusing methods in Configuration:   set(String, Object)   getObject(String)   get(String, Object) should be deprecated. Users expect them to work and in a distributed environment, there is almost no way to make them work correctly. If some user really needs the functionality, we should implement it right and require that the objects be Writable and serialize them into the Configuration.  Thoughts?",conf
ipc.client.timeout of 2000ms for test cases seems too small; causes too many timeouts and leads to hung test cases,We should increase the timeout slightly... what do other think? 5000ms? 10000ms?,test
UpgradeUtilities should use MiniDFSCluster to start and stop NameNode/DataNodes,UpgradeUtilities used by DFS tests should use MiniDFSCluster to start and stop NameNode/DataNodes.,test
UpgradeUtilities doesn't roll ports properly,UpgradeUtilities doesn't properly set NameNode and DataNode ports to 0,test
Record I/O should support comments in the DDL,The Record I/O record description language should support comments.,record
When RPC call fails then log call message detail,"When an RPC call fails, it would be helpful to have more information about the call.  Currently, this is all I get.      [junit] 2007-04-05 06:19:19,518 WARN  ipc.Server (Server.java:run(594)) - handler output error     [junit] java.nio.channels.ClosedChannelException     [junit]   sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:125)     [junit]   sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:294)     [junit]   org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)     [junit]   org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)     [junit]   java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)     [junit]   java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)     [junit]   java.io.DataOutputStream.flush(DataOutputStream.java:106)     [junit]   org.apache.hadoop.ipc.Server$Handler.run(Server.java:592)",ipc
Specify a junit test timeout in build.xml files,"To enable a more stable patch process, I'd like to be able to timeout a junit test when it takes too long to complete.",build
Record IO C++ binding: buffer type not handled correctly,"  I added this code to the test, which currently only tests serialization/deserialization of an empty buffer.     std::string& b = r1.getBufferVal();     static char buffer[] = {0, 1, 2, 3, 4, 5};     for (int i = 0; i < 6; i++) {       b.push_back(buffer[i]);     }  The csv test fails.  The generated file looks like this. T,102,4567,99344109427290,3.145000,1.523400,',# 0 1 2 3 4 5 0 1 2 3 4 5,v{},m{}  The xml test passes, but the data in the xml file is wrong:  <value><string>000102030405000102030405000102030405</string></value>  ",record
Record IO C++ binding: non-empty vector of strings does not work,"It works in the binary case, but not in CSV or XML.  Here is the code to put some strings in the vector.      std::vector<std::string>& v = r1.getVectorVal();     v.push_back(""hello"");     v.push_back(""world"");  In the CSV file, the strings appear twice, for some reason.  In the XML file they appear three times.  ",record
Record IO class should provide a toString(String charset) method," Currently, the toString() function returns the csv format serialized form of the record object. Unfortunately, all the fields of Buffer type are serialized into hex string. Although this is a loss less conversion, it is not  the most convenient form, when perople use Buffer to store international texts. With a new function toString(String charset) , the user can pass a charset to indicate the desired way to convert a Buffer to a String. ",record
Record IO C++ binding: cannot write more than one record to an XML stream and read them back,"I tried just writing the same record twice and then reading it back twice, and got a segmentation fault.  This works fine in the binary and csv cases.  ",record
Eclipse project files,"I've created Eclipse project files for Hadoop (to be attached). I've found them very useful for exploring Hadoop and running the unit tests.  The project files can be included in the source repository to make it easy to import Hadoop into Eclipse.  A few features:  - Eclipse automatically calls the Ant build to generate some of the necessary source files - Single unit tests can be run from inside Eclipse - Basic Java code style formatter settings for the Hadoop conventions (still needs some work)  The following VM arguments must be specified in the run configuration to get unit tests to run:  -Xms256m -Xmx256m -Dtest.build.data=${project_loc}\build\test\data  Some of the unit tests don't run yet, possibly due to some missing VM flags, the fact that I'm running Windows, or some other reason(s).  TODO:  - Specify native library location(s) once I investigate building of Hadoop's native library - Get all the unit tests to run",build
maps_running metric is only updated at the end of the task,"The maps_running and reduces_running metrics aren't getting into ganglia correctly - they always appear to be underestimated.  A quick look at the TaskTracker code shows that these metrics are only updated when a task is finished.  Of course, once a task is finished, the reporting will not note the completed task (since it's not running anymore).",metrics
Classes in src/test/testjar need package name,The three classes in src/test/testjar should have a package name. Eclipse complains because they don't.,test
a distributed junit test runner,"It would be nice to have a distributed junit runner that would run 100 instances of each test a separate map, and use counters to count how many times each test passed/failed/hung.",test
TestCheckpoint test case doesn't wait for MiniDFSCluster to be active,TestCheckpoint is missing a waitForCluster call in one place.,test
need code review guidelines,"Following the improvements to the Hadoop release process (HADOOP-1161), it would be helpful to better document what a code review should entail.  I've posted a proposal here (not yet linked to from anywhere):  http://wiki.apache.org/lucene-hadoop/CodeReviewChecklist  Thoughts?",build
TaskTracker won't bind to localhost,"Connecting to the TaskTracker with the default configuration fails with the firewall settings as mentioned in the Environment.  This means the job seems to start, but then will hang with all tasks at 0%  Also, setting mapred.tasktracker.dns.interface to 'lo' has no effect.  I would expect this bound the TaskTracker in such way that local connections would be made.  To make it work, I have to explicitly poke a hole in the firewall: # hadoop iptables -A INPUT --protocol tcp --destination-port 50050 -j ACCEPT iptables -A OUTPUT --protocol tcp --destination-port 50050 -j ACCEPT  While in practise a Hadoop will often run on a cluster (so the firewall has to be opened anyway), I don't think this should be the default behaviour, because it is highly confusing.",ipc
"change default config to be single node rather than ""local"" for both map/reduce and hdfs","I propose that we change the default config to be set up for a single node rather than the current ""local"", which uses direct file access and the local job runner.",conf
Upgrade Jetty to 6.x,We should try again to upgrade to Jetty 6 (previous attempt was HADOOP-565 and HADOOP-736) because it should substantially help the cpu usage on the TaskTracker. See http://www.infoq.com/news/jetty-6-release for a discussion of the improvements to Jetty 6.,util
The class generated by Hadoop Record rcc should provide a static method to return the DDL string,"The method will look like:  public static string getDDL();  With this class, when a map/reduce job write out sequence file swith such a generated class as its value class, the job can also save the DDL of the class into a file.  With such a file around, we can implement a record reader that can generate the required class on demand, thus, can read  a sequence file of Hadoop Records without having the class a priori. ",record
ChecksumFileSystem : Can't read when io.file.buffer.size < bytePerChecksum," Looks like ChecksumFileSystem fails to read a file when bytesPerChecksum is larger than io.file.buffer.size. Default for bytesPerChecksum  and buffer size are 512 and 4096, so default config might not see the problem.  I noticed this problem when I was testing block level CRCs with different configs.  How to reproduce with latest trunk: Copy a text  file larger than 512 bytes to dfs : bin/hadoop fs -copyFromLocal ~/tmp/x.txt x.txt then set io.file.buffer.size to something smaller than 512 (say 53). Now try to read the file :   bin/hadoop dfs -cat x.txt  This will print only the first 53 characters.  The following code or comment at  ChecksumFileSystem.java:163 seems suspect. But not sure if more changes are required: {code}     public int read(byte b[], int off, int len) throws IOException {       // make sure that it ends at a checksum boundary       long curPos = getPos();       long endPos = len+curPos/bytesPerSum*bytesPerSum;       return readBuffer(b, off, (int)(endPos-curPos));     } {code} ",fs
Fix unchecked warnings in main Hadoop code under Java 6.,Reported by Tahir Hashmi:  I get the following warning while building:      [javac] /home/tahir/Desktop/Trunk/trunk/src/test/org/apache/hadoop/dfs/ClusterTestDFS.java:439: warning: [unchecked] unchecked call to getConstructor(java.lang.Class<?>...) as a member of the raw type java.lang.Class     [javac]         randomDataGeneratorCtor = clazz.getConstructor(new Class[]{Long.TYPE});     [javac]                                                       ^     [javac] Note: Some input files use or override a deprecated API.,test
hadoop-config.sh resolving symlinks leads to errors ,"My company uses a versioned deployment system where the final results are symlinked.   For example:The hadoop package would be located at this location on all boxes /apollo/env/Hadoop/  This is a symlink generated by the system.  The hardlink can look like this: box1: /apollo/env/Hadoop -> /apollo/version/Hadoop-11114456 box2: /apollo/env/Hadoop -> /apollo/version/Hadoop-10039445  This piece of script from hadoop-config.sh resolves symlinks into hard links:  while [ -h ""$this"" ]; do   ls=`ls -ld ""$this""`   link=`expr ""$ls"" : '.*-> \(.*\)$'`   if expr ""$link"" : '.*/.*' > /dev/null; then     this=""$link""   else     this=`dirname ""$this""`/""$link""   fi done  I am not sure why this is done.  Commenting out the code makes things work for our system.    I assume that was put in for a reason.  Is there a solution for the original need for this code to that can work with our use case?",scripts
Improve interface to FileSystem.getFileCacheHints,"The FileSystem interface provides a very limited interface for finding the location of the data. The current method looks like:  String[][] getFileCacheHints(Path file, long start, long len) throws IOException  which returns a list of ""block info"" where the block info consists of a list host names. Because the hints don't include the information about where the block boundaries are, map/reduce is required to call the name node for each split. I'd propose that we fix the naming a bit and make it:  public class BlockInfo extends Writable {   public long getStart();   public String[] getHosts(); }  BlockInfo[] getFileHints(Path file, long start, long len) throws IOException;  So that map/reduce can query about the entire file and get the locations in a single call.",fs
adding user info to file,"I'm working on adding a permissions model to hadoop's DFS. The first step is this change, which associates user info with files. Following this I'll assoicate permissions info, then block methods based on that user info, then authorization of the user info.   So, right now i've implemented adding user info to files. I'm looking for feedback before I clean this up and make it offical.   I wasn't sure what release, i'm working off trunk. ",fs
"Once RPC.stopClient has been called, RPC can not be used again","Calls to RPC.stopClient render RPC subsequently unusable -- at least until a reload of RPC class so static initializer has a chance to run again.  I am trying to write unit tests for a little cluster built atop hadoop RPC class.  Intent is to spin up the little cluster before each test is run.  Post unit test, the cluster is torn down.  Part of the takedown includes invocation of RPC.stopClient to clean up any outstanding connections, etc.  I've found that when the second unit test runs, RPC Client is horked.  The 'running' flag is false so Connections can't work and 'Connection Culler' is not running.",ipc
copyToLocal cast exception for S3 filesystem,"The copyToLocal method in FsShell.java class does the following casting:  ((DistributedFileSystem)fs).copyToLocalFile(srcs[i], dst, copyCrc);  which in the case of S3 filesystem, this returns ClassCastException. This can be fixed by replacing the above line by:   fs.copyToLocalFile(srcs[i], dst);  Which of course does not check for the CRC.  ",fs
Classification of various configuration knobs,"Given the ever-increasing number of configuration knobs for both dfs and mapred, it would be very useful to classify each as EXPERT or not so as to clearly demarcate them.   Thoughts?",documentation
"Bug in BytesWritable.set(byte[] newData, int offset, int length) ","Current implementation:    public void set(byte[] newData, int offset, int length) {     setSize(0);     setSize(length);     System.arraycopy(newData, 0, bytes, 0, size);   }   Correct implementation:   public void set(byte[] newData, int offset, int length) {     setSize(0);     setSize(length);     System.arraycopy(newData, offset, bytes, 0, size);   }  please fix. ",io
Doc on Streaming,None,documentation
UndeclaredThrowableException when TaskTracker shutting down,"We've been seeing this exception in unit tests on Windows since HADOOP-1191 was committed.  For instance in TestMiniMRClasspath.      [junit] 2007-05-03 21:19:21,447 ERROR mapred.TaskTracker (TaskTracker.java:offerService(783)) - Caught exception: java.lang.reflect.UndeclaredThrowableException     [junit]   org.apache.hadoop.mapred.$Proxy6.heartbeat(Unknown Source)     [junit]   org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:834)     [junit]   org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:736)     [junit]   org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1121)     [junit]   org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:140)     [junit]   java.lang.Thread.run(Thread.java:595)     [junit] Caused by: java.lang.InterruptedException     [junit]   java.lang.Object.wait(Native Method)     [junit]   org.apache.hadoop.ipc.Client.call(Client.java:466)     [junit]   org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:165)     [junit]  ... 6 more  It doesn't seem to cause the tests to fail, but it is logged as an error when a TaskTracker is being shutdown.",ipc
"Deprecate the Configuration.set(String,Object) method and make Configuration Iterable","When I was removing the Configuration methods for putting objects into Configurations, I missed one of them and the associated callers. I also extend Configuration to implement Iterable<Map.Entry<String,String>> so that you can iterate through the configuration.",conf
Configuration XML bug: empty values,"The configuration parser doesn't handle empty values well:  if (""value"".equals(field.getTagName()) && field.hasChildNodes())  This logic makes it impossible to 'unset' a field when loading multiple configurations.",conf
Configuration XML bug: comments inside values,The logic of reading the data of the value causes (very) unexpected behaviour:  value = ((Text)field.getFirstChild()).getData();  When commenting out a part of the value only the part till the comment will be processed.  ,conf
Null pointer dereference of paths in FsShell.dus(String),FsShell.java line 372 has a guaranteed NPE.  Perhaps && was supposed to be ||,fs
"Call to equals() comparing different types in CopyFiles.cleanup(Configuration, JobConf, String, String)","CopyFiles.java line 369  The call to equals is comparing a Path and a String, which will never be equal.",util
Possible null pointer dereference of thisAuthority in FileSystem.checkPath(Path),FileSystem.java line 227      if (!(this.getUri().getScheme().equals(uri.getScheme()) &&           (thisAuthority == null && thatAuthority == null)           || thisAuthority.equals(thatAuthority)))  I'm not convinced that this couldn't produce a NPE on thisAuthority.  This logic should be simplified to ensure it's correctness,fs
seek calls in 3 io classes ignore result of skipBytes(int),UTF8.java line 113 Text.java line 233 WritableUtils.java line 51  These method calls ignore the return values of java.io.InputStream.skip() which may skip fewer bytes than requested. ,io
Inconsistent synchronization of SequenceFile$Reader.noBufferedValues; locked 66% of time,SequenceFile.java line 1600  The noBufferedValues field appears to be accessed inconsistently with respect to synchronization.,io
Inconsistent synchronization of Client$Connection.shouldCloseConnection; locked 66% of time,Client.java line 244  The shouldCloseConnection field of this class appears to be accessed inconsistently with respect to synchronization.,ipc
Inconsistent synchronization of Server$Listener.acceptChannel; locked 50% of time,Server.java line 290  The acceptChannel field of this class appears to be accessed inconsistently with respect to synchronization.,ipc
Inconsistent synchronization of NetworkTopology.distFrom; locked 50% of time,org.apache.hadoop.net.NetworkTopology.java line 556  The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,io
Remove phased file system,It was deprecated in version 13 and should be removed in 14. There is a little bit of cleanup in streaming.,fs
checkPath() throws IllegalArgumentException,"This was introduced recently in one of the patches committed around 05/15 or 05/14. I am running TestDFSIO on a two node cluster. Here is the exception I get  07/05/15 19:14:53 INFO mapred.TaskInProgress: Error from task_0001_m_000007_0: java.lang.IllegalArgumentException: Wrong FS: hdfs://MY-HOST:7017/benchmarks/TestDFSIO/io_control/in_file_test_io_7, expected: hdfs://my-host:7017     at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:230)     at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.getPath(DistributedFileSystem.java:110)     at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.exists(DistributedFileSystem.java:170)     at org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:168)     at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:335)     at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1162)     at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1156)     at org.apache.hadoop.mapred.SequenceFileRecordReader.<init>(SequenceFileRecordReader.java:40)     at org.apache.hadoop.mapred.SequenceFileInputFormat.getRecordReader(SequenceFileInputFormat.java:54)     at org.apache.hadoop.mapred.MapTask.run(MapTask.java:149)     at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1709)  I confess, my config on one of the machines specifies name-node ""MY-HOST:7017"" and on the other one ""my-host:7017"". But that was acceptable before and should stay that way in the future afaiu.",fs
Integrate Findbugs into nightly build process,"I think we should integrate Findbugs (http://findbugs.sourceforge.net) into our nightly test runs at http://lucene.zones.apache.org:8080/hudson/ and/or our patch process builds.  Findbugs uses static analysis to look for bugs in Java code.  It is licensed under Lesser GNU Public License so the build target will need to be optional, similar to the checkstyle target. ",test
We should have a util.Subprocess class with utilities for starting subprocesses,"I'd like a utility class for launching subprocesses. In particular, I want there to be a list of launched subprocesses and when the jvm is shutdown, a hook will destroy all of the subprocesses. Otherwise, when a task is killed, the subprocesses continue on.",util
The distance between sync blocks in SequenceFiles should be configurable rather than hard coded to 2000 bytes,Currently SequenceFiles put in sync blocks every 2000 bytes. It would be much better if it was configurable with a much higher default (1mb or so?).,io
MD5Hash has a bad hash function,"The MD5Hash class has a really bad hash function, that will cause most most md5s to hash to 0xFFFFFFxx leaving only the low order byte as meaningful. The problem comes from the automatic sign extension when promoting from byte to int.",io
"Inconsistent Synchronization cleanup for {Configuration, TaskLog, MapTask, Server}.java","findbugs reported inconsistent synchronization for the files {Configuration, TaskLog, MapTask, Server}.java. ","conf,ipc"
Exclude some Findbugs detectors,Exclude these detectors from Findbugs:  - May expose internal representation by returning reference to mutable object  - May expose internal representation by incorporating reference to mutable object  - Comparator doesn't implement Serializable,build
Typo in GzipCodec.createInputStream - bufferSize,"Theres a typo (I think) in GzipCodec:  {code}        Decompressor decompressor =          new ZlibDecompressor(ZlibDecompressor.CompressionHeader.AUTODETECT_GZIP_ZLIB,             64*1-24); {code} gives a buffersize of 40, should be: {code}        Decompressor decompressor =          new ZlibDecompressor(ZlibDecompressor.CompressionHeader.AUTODETECT_GZIP_ZLIB,             64*1024); {code} ",io
ChecksumFileSystem : some operations implicitly not supported.," It looks like {{skip()}} would skip bytes on data stream. That assures next read is going to be a checksum error. If skip() is not supported, ideally it should throw an exception with clear message.  Also positional reads ({{read(long position, buf)}}) don't seem to checksum the data. It might be ok, but it is not obvious to users. ",fs
RPC Server won't go quietly,"Trying to do a controlled shutdown of hbase, the RPC Server spews the following ugly output:  unknown-208-76-47-46:~/Documents/checkouts/hadoop-trunk stack$ ./src/contrib/hbase/bin/hbase master stop 07/05/24 12:53:47 INFO ipc.Server: Stopping server on 60000 07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 0 on 60000 caught: java.lang.InterruptedException java.lang.InterruptedException         at java.lang.Object.wait(Native Method)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541) 07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 0 on 60000: exiting unknown-208-76-47-46:~/Documents/checkouts/hadoop-trunk stack$ 07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 1 on 600 00 caught: java.lang.InterruptedException java.lang.InterruptedException         at java.lang.Object.wait(Native Method)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541) 07/05/24 12:53:47 INFO ipc.Server: IPC Server handler 2 on 60000 caught: java.lang.InterruptedException java.lang.InterruptedException         at java.lang.Object.wait(Native Method)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:541) ...  You get the same noise when if run the TestIPC unit test.",ipc
FileSystem.globPaths should not create a Path from an empty string,"Got the following error when tried to list a directory in a home directory using a relative path name  hadoop dfs -ls in_jar  Exception in thread ""main"" java.lang.IllegalArgumentException: Can not create a Path from an empty string         at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)         at org.apache.hadoop.fs.Path.<init>(Path.java:90)         at org.apache.hadoop.fs.FileSystem.globPaths(FileSystem.java:561)         at org.apache.hadoop.fs.FileSystem.globPaths(FileSystem.java:540)         at org.apache.hadoop.fs.FsShell.ls(FsShell.java:314)         at org.apache.hadoop.fs.FsShell.doall(FsShell.java:842)         at org.apache.hadoop.fs.FsShell.run(FsShell.java:1016)         at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)         at org.apache.hadoop.fs.FsShell.main(FsShell.java:1092) ",fs
Redesign Tool and ToolBase API and releted functionality,"With the discussion from HADOOP-1425, we need better abstraction and better tool runner utilities.   1. Classes do not need to extend ToolBase  2. functions for parsing general HadoopCommands (-fs, -conf, -jt) should be public 3. We need a ToolRunner, or similar functionality  4. Also we need each class (implementing Tool) to be runnable (main method) 5. CLI objects can be passed to run method of the Tool class (arguable)",util
Grammatical / wording / copy edits for Hadoop Distributed File System: Architecture and Design white paper,"Overall, the white paper is quite good, but I think it could use some revisions to enhance its clarity and readability.  I will post a patch of the Forrest file in src/docs/src/documentation/content/xdocs/hdfs_design.xml",documentation
checksums should be closer to data generation and consumption,"ChecksumFileSystem checksums data by inserting a filter between two buffers.  The outermost buffer should be as small as possible, so that, when writing, checksums are computed before the data has spent much time in memory, and, when reading, checksums are validated as close to their time of use as possible.  Currently the outer buffer is the larger, using the bufferSize specified by the user, and the inner is small, so that most reads and writes will bypass it, as an optimization.  Instead, the outer buffer should be made to be bytesPerChecksum, and the inner buffer should be the user-specified buffer size.",fs
IPC server should not log thread stacks at the info level,"Currently when IPC server get a call which becomes too old, i.e. the call has not been served for too long time, it dumps all thread stacks to logs at the info level. Because the size of all thread stacks size might be very big, it would be better to log them at the debug level.",ipc
Rework FSInputChecker and FSOutputSummer to support checksum code sharing between ChecksumFileSystem and block level crc dfs,"Comment from Doug in HADOOP-1134: I'd prefer it if the CRC code could be shared with CheckSumFileSystem. In particular, it seems to me that FSInputChecker and FSOutputSummer could be extended to support pluggable sources and sinks for checksums, respectively, and DFSDataInputStream and DFSDataOutputStream could use these. Advantages of this are: (a) single implementation of checksum logic to debug and maintain; (b) keeps checksumming as close to possible to data generation and use. This patch computes checksums after data has been buffered, and validates them before it is buffered. We sometimes use large buffers and would like to guard against in-memory errors. The current checksum code catches a lot of such errors. So we should compute checksums after minimal buffering (just bytesPerChecksum, ideally) and validate them at the last possible moment (e.g., through the use of a small final buffer with a larger buffer behind it). I do not think this will significantly affect performance, and data integrity is a high priority.  ",fs
distcp skips empty directory when copying,None,util
Input file get truncated for text files with \r\n,"When input file has \r\n, LineRecordReader uses mark()/reset() to read one byte ahead to check if \r is followed by \n.   This probably caused the BufferedInputStream to issue a small read request (e.g., 127 bytes).   The  ChecksumFileSystem.FSInputChecker.read() code  {code}    public int read(byte b[], int off, int len) throws IOException {      // make sure that it ends at a checksum boundary      long curPos = getPos();      long endPos = len+curPos/bytesPerSum*bytesPerSum;      return readBuffer(b, off, (int)(endPos-curPos));    } {code} tries to truncate ""len"" to checksum boundary.  For DFS, bytesPerSum is 512.  So for small reads, the truncated length become negative (i.e., endPos - curPos is < 0).   The underlying DFS read returns 0 when length is negative.  However, readBuffer changes it to -1 assuming end-of-file has been reached.   This means effectively, the rest of the input file did not get read.  In my case, only 8MB of a 52MB file is actually read.   Two sample stacks are appended.  One related issue, if there are assumptions (such as len >= bytesPerSum) in FSInputChecker's read(), would it be ok to add a check that throws an exception when the assumption is violated?   This assumption is a bit unusal and as code changes (both Hadoop and Java's implementation of BufferedInputStream), the assumption may get violated.  This silently dropping large part of input seems really difficult for people to notice (and debug) when people start to deal with terabytes of data.   Also, I suspect the performance impact for such a check would not be noticed.  bwolen  Here are two sample stacks.  (i have readbuffer throw when it gets 0 bytes, and have inputchecker catches the exception and rethrow both.  This way, I catch the values from both caller and callee (see the callee one starts with ""Caused by"")  ------------------------------------- {code} java.lang.RuntimeException: end of read() in=org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker len=127 pos=45223932 res=-999999        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:50)        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)        at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:116)        at java.io.FilterInputStream.read(FilterInputStream.java:66)        at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:132)        at org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:124)        at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:108)        at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:168)        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:44)        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1720)   Caused by: java.lang.RuntimeException: end of read() datas=org.apache.hadoop.dfs.DFSClient$DFSDataInputStream pos=45223932 len=-381 bytesPerSum=512 eof=false read=0        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:200)        at org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:175)        at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:47)        ... 11 more ---------------  java.lang.RuntimeException: end of read()  in=org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker len=400 pos=4503 res=-999999   org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:50)   java.io.BufferedInputStream.fill(BufferedInputStream.java:218)   java.io.BufferedInputStream.read(BufferedInputStream.java:237)   org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:116)   java.io.FilterInputStream.read(FilterInputStream.java:66)   org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:132)   org.apache.hadoop.mapred.LineRecordReader.readLine(LineRecordReader.java:124)   org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:108)   org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:168)   org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:44)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1720)  Caused by: java.lang.RuntimeException: end of read()  datas=org.apache.hadoop.dfs.DFSClient$DFSDataInputStream pos=4503 len=-7 bytesPerSum=512 eof=false read=0   org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.readBuffer(ChecksumFileSystem.java:200)   org.apache.hadoop.fs.ChecksumFileSystem$FSInputChecker.read(ChecksumFileSystem.java:175)   org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:47)  ... 11 more  {code}",io
"After successful distcp, couple of checksum error files","Tried copying 700,000 files  with distcp. 8 mappers per node.  Single dfs.client.buffer.dir. Distcp ran on 25 nodes mapreduce.  Couple of tasks failed, but job was successful.   When checked, 12  files were corrupted. (Checksum error)  This is repeatable.  I'll add more information as we find.    ",util
Test coverage target in build files using emma,Test coverage targets for Hadoop using emma.  Test coverage will help in identifying the components which are not poperly covered in tests and write test cases for it.  Emma (http://emma.sourceforge.net/) is a good tool for coverage. If you have something else in mind u can suggest.  I have a patch ready with emma. ,build
distcp creating extra directory when copying one file.,"If I try to copy one file by distcp       hadoop distcp   hdfs://aaaa:9999/abc/efg hdfs://bbbb:9999/abc/efg   on the target cluster, the file is  copied to   /abc/efg/efg  creating one extra depth.  I wasn't sure if this was an expected behavior. ",util
terminate-hadoop-cluster may be overzealous,"If folks are using EC2 for things besides Hadoop, then the terminate-hadoop-cluster script (in src/contrib/s3/bin) will kill all instances, not just Hadoop instances.  This could cause loss of work.  http://developer.amazonwebservices.com/connect/thread.jspa?threadID=15699",fs/s3
distcp not preserving the replication factor and block size of source files,Myabe not a bug but a feature request. It would be nice if the source file and the target file have the same replication factor and block size.   ,util
"distcp skipping healthy files with  ""-i"" option","Copied about million files.  8 files were corrupted. (checksum errors)  When I used distcp  ""-i"", (skip read error files), it skipped about 300 files.  I need to check if it was really read errors or if distcp is skipping files more than necessary.  ",util
ant Task for FsShell operations,"This issue will document the requirements, design and implementation of an ant Task providing FsShell functionality within that framework.","build,fs"
A likely race condition between the creation of a directory and checking for its existence in the DiskChecker class,"Got this exception in a job run. It looks like the problem is a race condition between the creation of a directory and checking for its existence. Specifically, the line: if (!dir.exists() && !dir.mkdirs()), doesn't seem safe when invoked by multiple processes at the same time.   2007-06-21 07:55:33,583 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 1 2007-06-21 07:55:33,818 WARN org.apache.hadoop.fs.AllocatorPerContext: org.apache.hadoop.util.DiskChecker$DiskErrorException: can not create directory: /export/crawlspace/kryptonite/ddas/dfs/data/tmp   org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:26)   org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createPath(LocalDirAllocator.java:211)   org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:248)   org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:276)   org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:155)   org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:1171)   org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:1136)   org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:342)   org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.create(DistributedFileSystem.java:145)   org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.(ChecksumFileSystem.java:368)   org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)   org.apache.hadoop.fs.FileSystem.create(FileSystem.java:254)   org.apache.hadoop.io.SequenceFile$Writer.(SequenceFile.java:675)   org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:165)   org.apache.hadoop.examples.RandomWriter$Map.map(RandomWriter.java:137)   org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:189)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1740)  2007-06-21 07:55:33,821 WARN org.apache.hadoop.mapred.TaskTracker: Error running child",fs
Java Service Wrapper for Hadoop,"I'm attaching Java Service Wrapper configuration files for namenode, datanode and secondarynamenode to this issue. These should probably go in the conf/ directory.  I'm also attaching batch files to run these services in the console that can be used to check that the configuration works. These should probably go in the bin/ directory.  By setting wrapper.app.parameter.2=-format the first time namenode.bat is run, the user do the necessary format.  Apache ActiveMQ includes the Java Service Wrapper binaries in the tarball they ship, so doing this for Hadoop seems feasible.  More about Java Service Wrapper:  http://wrapper.tanukisoftware.org/doc/english/introduction.html  P. S.  It seems the ordering of the classpath is very important.",conf
Distcp should support verification modes,distcp doesnot currently support any verification after copying files. It should support  1. verify quick (vq) mode - which compares the source and destination CRCs 2. verify long (vl) mode - which in addition to verify quick should read the entire destination file to catch DFS block level errors,util
Distcp should log to specified location,Distcp errors are now not logged anywhere (other that reporter.setStatus which gets overwritten). Add support to mention a log URI where the log files should go.,util
distcp overwrites destination files by default,"This is risky. distcp should instead accept a flag that indicates if destination files can be overwritten (overwrite all files, overwrite only if dest timestamp is older than source etc). The default behavior should be to not overwrite any files.",util
Support file exclusion list in distcp,There should be a way to ignore specific paths (eg: those that have already been copied over under the current srcPath). ,util
Fix a few more FindBugs issues,"I'm attaching a patch to fix a few more FindBugs issues. Most of these fixes are relatively minor. I also went ahead and fixed some issues in the tests.  This patch includes some fixes for ""inconsistent synchronization"" reported by FindBugs. However, FindBugs still reports some cases of inconsistent synchronization that I'm not quite sure how to fix.  Also, in ChecksumFileSystem.copyToLocalFile, there is an instanceof ChecksumFileSystem check that FindBugs says will always return true. I'm not quite sure what was intended here.      ",fs
RPC server should log exceptions that are not sent to client.,"RPC server logs and sends the exceptions that occur inside the call. But if there is an exception during construction of objects, the exception is not logged. Clients just see EOF because server closes the connection.  I got bitten by this since couple of my objects did not have default constructors (and same case if readFields() throws an NPE). This is a silly mistake on my part but I think it is easy to make the mistake but hard to see where the problem is.  Server can still close the connection but should log it at the server.   ",ipc
Report Java VM metrics," It would be useful to have each Java process in Hadoop (JobTracker, TaskTracker, NameNode and DataNode) report some Java VM metrics.  E.g. heap/non-heap memory used/committed, number of garbage collections and percentage of time spent in GC, number of threads that are runnable/blocked/waiting/etc.  ",metrics
Create FileSystem implementation to read HDFS data via http,There should be a FileSystem implementation that can read from a Namenode's http interface. This would have a couple of useful abilities:   1. Copy using distcp between different versions of HDFS.   2. Use map/reduce inputs from a different version of HDFS. ,fs
NameNode Schema for HttpFileSystem,"This issue will track the design and implementation of (the first pass of) a servlet on the namenode for querying its filesystem via HTTP. The proposed syntax for queries and responses is as follows.  *Query* {noformat}GET http://<nn>:<port>/ls.jsp[<?option>[&option]*] HTTP/1.1{noformat}  Where _option_ may be any of the following query parameters: _path_ : String (default: '/') _recursive_ : boolean (default: false) _filter_ : String (default: none)  *Response* The response will be returned as an XML document in the following format: {noformat} <listing path=""..."" recursive=""(yes|no)"" filter=""...""          time=""yyyy-MM-dd hh:mm:ss UTC"" version=""..."">   <directory path=""...""/>   <file path=""..."" modified=""yyyy-MM-dd hh:mm:ss"" blocksize=""...""         replication=""..."" size=""...""         dnurl=""http://dn:port/streamFile?...""/> </listing> {noformat}",fs
distcp should use the Path -> FileSystem interface like the rest of Hadoop,"DistCp should use the standard Path to FileSystem API that the rest of Hadoop uses. By explicitly testing the protocol string, it is much more brittle.",util
Add a per-job configuration knob to control loading of native hadoop libraries ,"As it exists today, native-hadoop libraries are loaded automatically if libhadoop.so is present; however we have sporadically seen issues (HADOOP-1545) since native direct-buffers aren't very well understood. The only way to switch off usage of these is to remove the native libraries which is a maintenence issue for large clusters...  Hence I propose we add a per-job config knob: {{hadoop.native.lib}} (set to {{true}} by default) which can be used to control usage of native libraries even when the libraries are present e.g. we can have hadoop installed with native libraries present and then use this knob to switch off their usage in rare cases we see issues with them; thus aiding maintenence.",io
Bug in readFields of GenericWritable,"When getTypes() returns more than 127 entries, read of classes with index > 127 will fail.  {code}   public void readFields(DataInput in) throws IOException {     type = in.readByte();     Class clazz = getTypes()[type];     ...   } {code}  {code}   public void readFields(DataInput in) throws IOException {     type = in.readByte();     Class clazz = getTypes()[type & 0xff];     ...   } {code} ",io
GenericWritable should use generics,GenericWritable should use generics to clarify the requirement that getTypes() must return classes that extend Writable.,io
Tasks run by MiniMRCluster don't get sysprops from TestCases,While it seems a general problem it is surfacing in streaming TestSymLink testcase with patch for HADOOP-1558  The contrib testcases use src/contrib/test/hadoop-site.xml.  The property 'mapred.system.dir' is this file is defined as with a variable '${contrib.name}'. The src/build/build-contrib.xml ant file sets the sysproperty 'contrib.name' to the name of the contrib component for the JVM running the testcase.  The problem is that when a testcase uses MiniMRCluster the TaskRunner forks a JVM for the task and in this JVM (which uses the above hadoop-site.xml) the variable 'contrib.name' is undefined.  If I hardcode 'streaming' in the hadoop-site.xml for the TestSymLink the testcase works fine.  ,test
FsShell should work with paths in non-default FileSystem,"If the default filesystem is, e.g., hdfs://foo:8888/, one should still be able to do 'bin/hadoop fs -ls hdfs://bar:9999/' or 'bin/hadoop fs -ls s3://cutting/foo'.  Currently these generate a filesystem mismatch exception.  This is because FsShell assumes that all paths are in the default FileSystem.  Rather, the default filesystem should only be used for paths that do not specify a FileSystem.  This would easily be accomplished by using Path#getFileSystem().",fs
TestCopyFiles with IllegalArgumentException on Windows,"3 of the TestCopyFiles test cases fail on Windows since  http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/152/ Looks like they all failed due to:  Copy failed: java.lang.IllegalArgumentException: Wrong FS: file://C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/srcdat, expected: file:///   org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:204)   org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:50)   org.apache.hadoop.fs.RawLocalFileSystem.exists(RawLocalFileSystem.java:217)   org.apache.hadoop.fs.FilterFileSystem.exists(FilterFileSystem.java:156)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:832)   org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:166)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)",fs
GenericWritable should use ReflectionUtils.newInstance to avoid problems with classloaders,"GenericWritable currently uses Class.newInstance and it should use hadoop.utils.ReflectionUtils.newInstance. Furthermore, GenericWritable should be Configurable and should configure the nested objects. This will prevent a lot of classloader issues and allow the objects to get a configuration.",io
FSInputChecker attempts to seek past EOF,"I'm not sure which class in the stack trace below is responsible for attempting to seek past the end of file.   2007-07-16 20:31:40,598 INFO org.apache.hadoop.mapred.TaskInProgress: Error from task_200707162028_0014_m_000000_0: java.io.IOException: Cannot seek after EOF   org.apache.hadoop.dfs.DFSClient$DFSInputStream.seek(DFSClient.java:1040)   org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:37)   org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:188)   org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)   org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)   org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)   org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)   org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:353)   org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:331)   org.apache.hadoop.fs.FSInputChecker.skip(FSInputChecker.java:306)   java.io.FilterInputStream.skip(FilterInputStream.java:125)   java.io.FilterInputStream.skip(FilterInputStream.java:125)   com.yahoo.pig.impl.io.InputStreamPosition.skip(InputStreamPosition.java:55)   java.io.BufferedInputStream.skip(BufferedInputStream.java:349)   java.io.FilterInputStream.skip(FilterInputStream.java:125)   com.yahoo.pig.impl.builtin.RandomSampleLoader.getNext(RandomSampleLoader.java:34)   com.yahoo.pig.impl.mapreduceExec.PigInputFormat$PigRecordReader.next(PigInputFormat.java:169)   org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:171)   com.yahoo.pig.impl.mapreduceExec.PigMapReduce.run(PigMapReduce.java:98)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:189)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1771)",fs
Make FileStatus a concrete class,The existing implementaitons of FileStatus may be easily abstracted into a single base class.,fs
EC2 launch-hadoop-cluster awk Problem,"With Amazon 'ec2-api-tools-1.2-11797' and Hadoop 0.13.0 the output from ec2-describe-instances is being incorrectly AWK'd on the last few lines of the launch-hadoop-cluster script.  Specifically, on the """""" MASTER_EC2_HOST=`ec2-describe-instances | grep INSTANCE | grep running | awk '{if ($7 == 0) print $4}'` """""" line, it should be comparing column $8 rather than column $7.  When this command fails, the hostname of the master doesn't get set correctly and the IP lookup finds incorrect results. The last lines of output from the script look something like this: """""" Appointing master Master is . Please set up DNS so blah.gotdns.org points to D.ROOT-SERVERS.NET. I.ROOT-SERVERS.NET. F.ROOT-SERVERS.NET. M.ROOT-SERVERS.NET. J.ROOT-SERVERS.NET. C.ROOT-SERVERS.NET. H.ROOT-SERVERS.NET. E.ROOT-SERVERS.NET. A.ROOT-SERVERS.NET. K.ROOT-SERVERS.NET. G.ROOT-SERVERS.NET. B.ROOT-SERVERS.NET. L.ROOT-SERVERS.NET.. Press return to continue. """"""",contrib/cloud
Keypair Name Hardcoded,"The keypair name is hardcoded as 'gsg-keypair'  on the """""" OUTPUT=`ec2-run-instances $AMI_IMAGE -k gsg-keypair` """""" line in 'create-hadoop-image'. And again on the  """""" RUN_INSTANCES_OUTPUT=`ec2-run-instances $AMI_IMAGE -n $NO_INSTANCES -g $GROUP -k $KEY_NAME -d ""$NO_INSTANCES,$MASTER_HOST"" | grep INSTANCE | awk '{print $2}'` """""" line in 'launch-hadoop-cluster'.   The lines should read """""" OUTPUT=`ec2-run-instances $AMI_IMAGE -k $KEY_NAME` """""" and """""" RUN_INSTANCES_OUTPUT=`ec2-run-instances $AMI_IMAGE -n $NO_INSTANCES -g $GROUP -k $KEY_NAME -d ""$NO_INSTANCES,$MASTER_HOST"" | grep INSTANCE | awk '{print $2}'` """""" respectively. ",contrib/cloud
Master node unable to bind to DNS hostname,"With a release package of Hadoop 0.13.0 or with latest SVN, the Hadoop contrib/ec2 scripts fail to start Hadoop correctly. After working around issues HADOOP-1634 and HADOOP-1635, and setting up a DynDNS address pointing to the master's IP, the ec2/bin/start-hadoop script completes.  But the cluster is unusable because the namenode and tasktracker have not started successfully. Looking at the namenode log on the master reveals the following error: {quote} 2007-07-19 16:54:53,156 ERROR org.apache.hadoop.dfs.NameNode: java.net.BindException: Cannot assign requested address         at sun.nio.ch.Net.bind(Native Method)         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)         at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:186)         at org.apache.hadoop.ipc.Server.<init>(Server.java:631)         at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:325)         at org.apache.hadoop.ipc.RPC.getServer(RPC.java:295)         at org.apache.hadoop.dfs.NameNode.init(NameNode.java:164)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:211)         at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:803)         at org.apache.hadoop.dfs.NameNode.main(NameNode.java:811) {quote}  The master node refuses to bind to the DynDNS hostname in the generated hadoop-site.xml. Here is the relevant part of the generated file: {quote} <property>   <name>fs.default.name</name>   <value>blah-ec2.gotdns.org:50001</value> </property>  <property>   <name>mapred.job.tracker</name>   <value>blah-ec2.gotdns.org:50002</value> </property> {quote}  I'll attach a patch against hadoop-trunk that fixes the issue for me, but I'm not sure if this issue is something that someone can fix more thoroughly.",contrib/cloud
Add contrib jars to general hadoop CLASSPATH,A mapreduce job that depends on any of the hadoop contrib jars  must bundle the contrib jar into its job jar or copy the contrib jar to the lib dir across the cluster because hadoop contribs are not on the general hadoop CLASSPATH.  It would be an improvement if such as the included hbase mapreduce tasks did not require the running of this extra step.,scripts
IOUtils class,"In the current situation, {{FileUtil}} class includes both file related and io related functionality. This issue intends to separate the two. ",io
The FsShell Object cannot be used for multiple fs commands.,"The FsShell object is used to execute an fs command. In its present incantation, this object can be used to invoke only one fs command. Programs that wants to invoke FsShell has to create a FsShell object for each command that it wants to execute.",fs
organize CHANGES.txt messages into sections for future releases,The entries in CHANGES.txt should have the following sections per release:  INCOMPATIBLE CHANGES NEW FEATURES OPTIMIZATIONS BUG FIXES  This should make it easier for folks to read this file.,documentation
add INCOMPATIBLE CHANGES section to CHANGES.txt for Hadoop 0.14,"HADOOP-1134 and some other Jira's have introduced incompatible changes into Hadoop 0.14.  As per HADOOP-1667 the CHANGES.txt file will include sections for these kind of changes for Hadoop 0.15 and beyond.  As a stop gap for 0.14, the CHANGES.txt file needs an INCOMPATIBLE CHANGES section.",documentation
Hadoop records can't serialize zero length string,java.lang.NullPointerException   org.apache.hadoop.record.Utils.toBinaryString(Utils.java:307)   org.apache.hadoop.record.BinaryRecordOutput.writeString(BinaryRecordOutput.java:95)   com.facebook.infrastructure.profiles.users_extended.serialize(users_extended.java:153)   org.apache.hadoop.record.Record.serialize(Record.java:58)   org.apache.hadoop.record.Record.write(Record.java:72)   org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:365)   com.facebook.infrastructure.profiles.conv_users_extended$MapClass.map(conv_users_extended.java:133)   org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:186)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1707)      final int strlen = str.length();     byte[] bytes = new byte[strlen*4]; // Codepoints expand to 4 bytes max          barfs on trying to allocate zero length array? ,record
Release notes point to browsable JIRA,"Currently, the web pages on lucene.apache.org/hadoop link ""release notes"" as a pointer to JIRA. http://issues.apache.org/jira/browse/HADOOP?report=com.atlassian.jira.plugin.system.project:changelog-panel  The problem with this is that JIRA's really not very user-friendly. Most of the Hadoop JIRA tickets describe the underlying cause, not the user exposed symptoms, so users who are nsidering a new release (say, looking for API changes) can't make head or tails of half of the tickets without being familiar with Hadoop internals. ",documentation
TestCrcCorruption hangs on windows,"TestCrcCorruption times out on windows saying just that it timed out. No other useful information in the log. Some kind of timing issue, because if I run it with output=yes then it succeeds. ",test
.sh scripts do not work on Solaris,"the EXPORT commands in the scripts didn't play nicely w/the default shell & Hadoop wouldn't start (""... is not an identifier"").  I changed the first #! line of bin/hadoop-daemon.sh and bin/hadoop to specify bash, then everything worked:   #!/bin/bash  ksh is probably another option that will work, but we did not try that",scripts
DfsTask cache interferes with operation,"Some users have experienced problems until they disable the Configuration cache. Since there was no demonstrable need to add it, it should be removed.",util
Create a utility to convert binary (sequence and compressed) files to strings.,"It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings. It could then be hooked up to ""bin/hadoop fs cat"" and the web/ui to textify sequence and compressed files.",io
TestDFSUpgradeFromImage fails on Windows,"2007-08-13 18:48:42,036 INFO  dfs.TestDFSUpgradeFromImage (TestDFSUpgradeFromImage.java:setUp(72)) - Unpacking the tar file C:\hudson\workspace\Hadoop-WindowsTest-0.14\branch-0.14/build/test/cache/hadoop-12-dfs-dir.tgz  java.io.IOException: tar (child): Cannot execute remote shell: No such file or directory   org.apache.hadoop.fs.Command.run(Command.java:33)   org.apache.hadoop.fs.Command.execCommand(Command.java:89)   org.apache.hadoop.dfs.TestDFSUpgradeFromImage.setUp(TestDFSUpgradeFromImage.java:74)",test
TestCopyFiles.testCopyFromLocalToLocal fails on Windows,"3 different exceptions.  Not sure which is relevant:   java.io.FileNotFoundException: C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/destdat/five/eight/8850688331351221910   org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:126)   org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:109)   org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:266)   org.apache.hadoop.fs.FileSystem.open(FileSystem.java:244)   org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:135)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:169)  Standard Output: 2007-08-13 19:31:41,191 INFO  ipc.Server (Server.java:run(568)) - IPC Server handler 9 on 1420, call open(/destdat/four/zero/2221298292449454108, 0, 671088640) from 127.0.0.1:1424: error: java.io.IOException: Cannot open filename /destdat/four/zero/2221298292449454108 java.io.IOException: Cannot open filename /destdat/four/zero/2221298292449454108   org.apache.hadoop.dfs.NameNode.open(NameNode.java:269)   sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   org.apache.hadoop.ipc.RPC$Server.call(RPC.java:340)   org.apache.hadoop.ipc.Server$Handler.run(Server.java:566)  Standard Error: Copy failed: java.lang.NullPointerException   org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.setup(CopyFiles.java:321)   org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:773)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:854)   org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:166)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567) Waiting for the Mini HDFS Cluster to start...",test
TestDFSUpgradeFromImage fails on Solaris ,"TestDFSUpgradeFromImage is broken on Solaris so all patch builds will fail until it is fixed.  I believe Raghu is working on a patch which will remove the non-standard tar -z dependency.  From Enis Soztutar: TestDFSUpgradeFromImage fails for hadoop-patch and hudson-nightly builds on hudson.  The error thrown is : {noformat} java.io.IOException: tar: z: unknown function modifier   org.apache.hadoop.fs.Command.run(Command.java:33)   org.apache.hadoop.fs.Command.execCommand(Command.java:89)   org.apache.hadoop.dfs.TestDFSUpgradeFromImage.setUp(TestDFSUpgradeFromImage.java:75)  Standard Output  2007-08-15 13:22:38,601 INFO  dfs.TestDFSUpgradeFromImage (TestDFSUpgradeFromImage.java:setUp(72)) - Unpacking the tar file /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/test/cache/hadoop-12-dfs-dir.tgz {noformat} ",test
Test coverage target in build files using clover,"Moving Simon Willnauer clover patch from HADOOP-1496 to a new Jira.  From 1496:  Simon Willnauer - 27/Jun/07 12:18 PM We have a donated clover license at the ""private"" repository. It is located right here. https://svn.apache.org/repos/private/committers/donated-licenses/clover/ You could have a look at the lucene ant files to include clover into the hadoop build managment. The license is a ""ant - only"" license and can only be used on ""org.apache.*"" packages.  Simon Willnauer - 27/Jun/07 01:56 PM Clover integration into Hadoop. I added the clover report task to the build.xml. We did that in the Lucene project a while ago and I had to do it for work anyway so I added the tasks to the hadoop project as well. To generate the reports the clover.jar an clover.license from the ""commiter"" repository must be available on the ANT Path. I had problems with the jar file located in the apache repository so I use the current version from the cenqua website (http://www.cenqua.com/download.jspa - clover for ant-1.3.13) I created the reports running: ant -Drun.clover=true clean test generate-clover-reports ",build
Make ...hbase.io.MapWritable more generic so that it can be included in ...hadoop.io,"The class org.apache.hadoop.hbase.io.MapWritable could be made more generic through the use of ReflectionUtils so that it could support more Map key and value classes. Currently it supports Map<WritableComparable, Writable> only.  When more generalized, submit for consideration to be included in org.apache.hadoop.io ",io
javadoc warnings in trunk: Complaints about missing ant dependency,Trunk is throwing javadoc warnings complaining about ant dependency. Its causing patch failures. {code} [javadoc] Constructing Javadoc information...   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:23: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.BuildException;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:24: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.DirectoryScanner;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:25: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.Project;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:26: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.Task;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:27: package org.apache.tools.ant.types does not exist   [javadoc] import org.apache.tools.ant.types.FileSet;   [javadoc]                                   ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:50: cannot find symbol   [javadoc] symbol: class Task   [javadoc] public class RccTask extends Task {   [javadoc]                              ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:55: cannot find symbol   [javadoc] symbol  : class FileSet   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   private final ArrayList<FileSet> filesets = new ArrayList<FileSet>();   [javadoc]                           ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:98: cannot find symbol   [javadoc] symbol  : class FileSet   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   public void addFileset(FileSet set) {   [javadoc]                          ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:105: cannot find symbol   [javadoc] symbol  : class BuildException   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   public void execute() throws BuildException {   [javadoc]                                ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:124: cannot find symbol   [javadoc] symbol  : class BuildException   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   private void doCompile(File file) throws BuildException {   [javadoc]                                            ^   [javadoc] Standard Doclet version 1.5.0_11   [javadoc] Building tree for all the packages and classes...   [javadoc] Building index for all the packages and classes...   [javadoc] Building index for all classes...   [javadoc] Generating /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/api/stylesheet.css...   [javadoc] 10 warnings {code},documentation
Removing ant jar causes javadoc errors so builds fail,"When the ant jar was removed from lib, builds now experience javadoc errors. For example from Hadoop-Patch #567 console log: http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/567/console    [javadoc] Constructing Javadoc information...   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:23: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.BuildException;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:24: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.DirectoryScanner;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:25: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.Project;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:26: package org.apache.tools.ant does not exist   [javadoc] import org.apache.tools.ant.Task;   [javadoc]                             ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:27: package org.apache.tools.ant.types does not exist   [javadoc] import org.apache.tools.ant.types.FileSet;   [javadoc]                                   ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:50: cannot find symbol   [javadoc] symbol: class Task   [javadoc] public class RccTask extends Task {   [javadoc]                              ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:55: cannot find symbol   [javadoc] symbol  : class FileSet   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   private final ArrayList<FileSet> filesets = new ArrayList<FileSet>();   [javadoc]                           ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:98: cannot find symbol   [javadoc] symbol  : class FileSet   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   public void addFileset(FileSet set) {   [javadoc]                          ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:105: cannot find symbol   [javadoc] symbol  : class BuildException   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   public void execute() throws BuildException {   [javadoc]                                ^   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/record/compiler/ant/RccTask.java:124: cannot find symbol   [javadoc] symbol  : class BuildException   [javadoc] location: class org.apache.hadoop.record.compiler.ant.RccTask   [javadoc]   private void doCompile(File file) throws BuildException {   [javadoc]                                            ^   [javadoc] Standard Doclet version 1.5.0_11   [javadoc] Building tree for all the packages and classes...   [javadoc] Building index for all the packages and classes...   [javadoc] Building index for all classes...   [javadoc] Generating /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/api/stylesheet.css...   [javadoc] 10 warnings",build
Hadoop-Patch build should also compute the number of existing javadoc warnings in the pre-build stage,"The Hadoop-Patch build does a ""pre-build"" phase to determine the number of javac and findbugs warnings so it can tell if the patch generates new warnings:  ######################################################## Pre-building trunk to determine current number of javac and Findbugs warnings ########################################################  It would be nice if it also computed the number of javadoc warnings so that a new test would not get a -1 because javadoc warnings already exist.",build
A testimonial page for hadoop?,Should we create a testimonial page on hadoop wiki with a link from Hadoop home page so that people  could share their experience of using Hadoop? I see some satisfied users out there. :),documentation
Add toString() methods to some Writable types,"Add missing toString() methods to Writable types that wrap primitive values. Also, add Counters.toString(), which is useful when retrieving counters at the end of the job and displaying them in a UI.",io
processing escapes in a jute record is quadratic,"The following code appears in hadoop/src/c++/librecordio/csvarchive.cc :   static void replaceAll(std::string s, const char *src, char c) {   std::string::size_type pos = 0;   while (pos != std::string::npos) {     pos = s.find(src);     if (pos != std::string::npos) {       s.replace(pos, strlen(src), 1, c);     }   } }  This is used in the context of replacing jute escapes in the code:   void hadoop::ICsvArchive::deserialize(std::string& t, const char* tag) {   t = readUptoTerminator(stream);   if (t[0] != '\'') {     throw new IOException(""Errror deserializing string."");   }   t.erase(0, 1); /// erase first character   replaceAll(t, ""%0D"", 0x0D);   replaceAll(t, ""%0A"", 0x0A);   replaceAll(t, ""%7D"", 0x7D);   replaceAll(t, ""%00"", 0x00);   replaceAll(t, ""%2C"", 0x2C);   replaceAll(t, ""%25"", 0x25);  }  Since this replaces the entire string for each instance of the escape sequence, practically anything would be better.  I would propose that within deserialize we allocate a char * [since each replacement is smaller than the original], scan for each %, and either do a general hex conversion in place or look for one of the six patterns, and after each replacement move down the unmodified text and scan for the % fom that starting point.  -dk ",record
"Possible StackOverflowError in FileSystem.get(Uri uri, Configuration conf) method","When calling the method Filesytem.get(Uri uri, Configuration conf) with an URI without scheme -> StackOverflowError  {noformat} Exception in thread ""Main Thread"" java.lang.StackOverflowError:         at java.util.regex.Matcher.<init>(Matcher.java:201)         at java.util.regex.Pattern.matcher(Pattern.java:879)         at org.apache.hadoop.conf.Configuration.substituteVars(Configuration.java:182)         at org.apache.hadoop.conf.Configuration.get(Configuration.java:247)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143)         at org.apache.hadoop.fs.FileSystem.getNamed(FileSystem.java:118)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:90)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:143) {noformat}",fs
Hadoop does not run in Cygwin in Windows ,the hostname commands are slightly different in linux and cygwin.  Will work if use $HOSTNAME,scripts
"Hadoop build (ant) hangs while setting up init target, build process hangs","Ant hangs during a build process, eventually (in hours) dies of heap exhaustion.  The problematic lines in build.xml seem to be in the init target (fileset operations):  -    <touch datetime=""01/25/1971 2:00 pm"">  -      <fileset dir=""${conf.dir}"" includes=""**/*.template""/>  -      <fileset dir=""${contrib.dir}"" includes=""**/*.template""/>  -    </touch>  -    <copy todir=""${conf.dir}"" verbose=""true"">  -      <fileset dir=""${conf.dir}"" includes=""**/*.template""/>  -      <mapper type=""glob"" from=""*.template"" to=""*""/>  -    </copy>  -    <copy todir=""${contrib.dir}"" verbose=""true"">  -      <fileset dir=""${contrib.dir}"" includes=""**/*.template""/>  -      <mapper type=""glob"" from=""*.template"" to=""*""/>  -    </copy>    Commenting them out or deleting allows build to proceed to sucessful completion.  Not being an expert,  in either xml or ant, I'm not sure what exactly I'm missing with those lines gone, but at least I'm able to compile.  ",build
MapWritable and SortedMapWritable - Writable problems,"When using the Writable interface for MapWritable and SortedMapWritable there are two errors: - in readFields, if there are a number of entries of a class that is not one of the ""predefined"" classes, the following exception is thrown: java.lang.IllegalArgumentException: Class <not predefined class name> already registered - readFields did not set the number of non-predefined classes. Consequently, making a copy of a copy that had entries of non-predefined  classes would fail because the second copy would not receive the mapping from id to class and a NullPointerException would be thrown ",io
keyToPath in Jets3tFileSystemStore needs to return absolute path,"The keyToPath method probably needs to:  1. take the bucket identifier as a parameter. 2. set the returned Path object's protocol plus authority (bucket). Currently, APIs such as <i>listSubPaths</i> return relative paths (for a directory listing). This in turn breaks map reduce operations if the default file system is set to be something other than S3 (via fs.default.name, for example).      ",fs/s3
mapred.system.dir parameter needs documentation,"Following a user@ discussion :  {noformat} Thomas Friol wrote: > Other question, : Why the 'hadoop.tmp.dir' is user.name dependant ?  We need a directory that a user can write and also not to interfere with other users.   If we didn't include the username, then different users would share the same tmp directory.   This can cause authorization problems, if folks' default umask doesn't permit write by others.   It can also result in folks stomping on each other, when they're, e.g., playing with HDFS and re-format their filesystem.  We should better document configuring things for fully distributed operation.  In particular,  we should probably recommend setting mapred.system.dir in the ""Fully Distributed Operation"" section of:  http://lucene.apache.org/hadoop/api/overview-summary.html#overview_description  The wiki should also probably describe this better.  {noformat}","conf,documentation"
Cleanup local files command(s),"It would be good if we had some clean up command to cleanup all the local directories that any component of hadoop uses. That way before the cluster is restarted again, or when a machine is decided to be pulled out of the cluster, we can cleanup all the local files.   i propose we add  {noformat} bin/hadoop datanode -format bin/hadoop tasktracker -format bin/hadoop jobtracker -format {noformat} ",util
df command doesn't exist under windows,"My code use to work with previous version of hadoop, I upgraded to 0.14 and now: java.io.IOException: CreateProcess: df -k ""C:\Documents and Settings\Benjamin\Local Settings\Temp\test14906test\mapredLocal"" error=2   java.lang.ProcessImpl.create(Native Method)   java.lang.ProcessImpl.<init>(Unknown Source)   java.lang.ProcessImpl.start(Unknown Source)   java.lang.ProcessBuilder.start(Unknown Source)   java.lang.Runtime.exec(Unknown Source)   java.lang.Runtime.exec(Unknown Source)   org.apache.hadoop.fs.DF.doDF(DF.java:60)   org.apache.hadoop.fs.DF.<init>(DF.java:53)   org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:198)   org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:235)   org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:124)   org.apache.hadoop.mapred.MapOutputFile.getSpillFileForWrite(MapOutputFile.java:88)   org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpillToDisk(MapTask.java:373)   org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:593)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:190)   org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:137)   org.apache.hadoop.mapred.LocalJobRunner.submitJob(LocalJobRunner.java:283)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:397) ...",fs
Generalize making contrib bin content executable in ant package target,"In binary distributions of hadoop, hbase bin content are not executable.",build
DfsTask no longer compiles,"HADOOP-1436 changed the Tool and Toolbase APIs, but the HDFS antlib was not updated.",build
DfsTask lacks unit tests,"The HDFS antlib neither has unit tests written for it, nor is it part of the nightly build.","build,test"
Hudson should run test-contrib even if test-core fails,"Currently, if test-core fails, Hudson does not run test-contrib.  It is entirely possible that a contrib patch does not depend on the failing test-core test. If test-contrib passes even when test-core fails, a patch that only effects contrib files could be committed. At the very least, the test-results and console output would be useful.",build
TestIPC and TestRPC should use dynamically allocated ports,TestIPC and TestRPC currently use fixed port numbers for tests.  This can fail if that port is for some reason unavailable.  They should instead let the OS allocate a free port number dynamically.,ipc
Separate client and server jars,"For the ease of deployment, one should not have to change the server jars, and restart clusters, when minor features on the client side are changed. This requireds separating client and server jars for hadoop. Version numbers appended to hadoop jars can reflect the compatibility. e.g. the server jar could be at 0.13.1, and the client jar could be at 0.13.2. In short, we can treat the part following 0. as the ""major"" version number for now.  This allows major client frameworks such as streaming and Pig happy. To my knowledge, Pig uses hadoop's default jobclient. Whereas streaming uses its own jobclient. I would love to change streaming to use the default hadoop jobclient, if I can make modifications to it (e.g. to print more stats that are available from TaskReport, for example), if I do not have to deploy the new version of the whole jar to the backend and restart the mapreduce cluster.  (I thought there was already a bug filed for separating the client and server jar, but I could not find it. Hence the new Jira. Sorry about duplication, if any.)",build
Allow SOCKS proxy configuration to remotely access the DFS and submit Jobs,The purpose of this issue is to introduce a new configuration entry to setup SOCKS proxy for DFS and JobTracker clients. This enable users to remotely access the DFS and submit Jobs as if they were directly connected to the cluster Hadoop runs on. ,ipc
hadoop-daemon.sh script fails if HADOOP_PID_DIR doesn't exist,"If I try to bring up a datanode on a fresh machine, it will fail with this error message:  starting datanode, logging to /b/hadoop/logs/hadoop-me-datanode-example.com.out /p/share/hadoop/bin/hadoop-daemon.sh: line 99: /b/hadoop/pid/hadoop-me-datanode.pid: No such file or directory ",scripts
Dynamic Number of IPC/RPC server handler,"Curently, the IPC server runs a listener and fixed number of handlers for processing calls. As part of the discussions in HADOOP-1763, we have raised the issue of whether the number of handlers could be made dynamic.   The proposed way to do this is to configure minimum and maximum number of threads, and find an optimum strategy to allocate/deallocate handler threads. Any comments on analysis/design/implementation/benchmark are welcome. ",ipc
Hudson should kill long running tests,"Hudson should kill long running tests. (I believe it is supposed to but doesn't quite seem to do the job if the test is really hung up).  It would be nice if, when the timer goes off, Hudson did a {code}kill -QUIT{code} (to try to get a thread dump) and then followed that with a {code}kill -9{code}  (See the section ""Killing a hung test"" at http://wiki.apache.org/lucene-hadoop/HudsonBuildServer )",build
IPC server should write repsonses asynchronously,Hadoop's IPC Server currently writes responses from request handler threads using blocking writes.  Performance and scalability might be improved if responses were written asynchronously.,ipc
c++ libs need test targets that are called by ant,"There should be a 'test-c\+\+' target in ant that runs unit tests for each of the C\+\+ libraries (libhdfs, librecordio, libpipes, utils, etc.).  This target should be run by the ""test"" target when ""compile.c\+\+"" is specified.","build,record"
Remove deprecated code in Configuration/JobConf,Remove Configuration/JobConf apis deprecated by HADOOP-785 in in the 0.15.0 release - in particular: Configuration.add{Default|Final}Resource.,conf
IPC server max queue size should be configurable,"Currently max queue size for IPC server is set to (100 * handlers). Usually when RPC failures are observed (e.g. HADOOP-1763), we increase number of handlers and the problem goes away. I think a big part of such a fix is increase in max queue size. I think we should make maxQsize per handler configurable (with a bigger default than 100). There are other improvements also (HADOOP-1841).    Server keeps reading RPC requests from clients. When the number in-flight RPCs is larger than maxQsize, the earliest RPCs are deleted. This is the main feedback Server has for the client. I have often heard from users that Hadoop doesn't handle bursty traffic.    Say handler count is 10 (default) and Server can handle 1000 RPCs a sec (quite conservative/low for a typical server), it implies that an RPC can wait for only for 1 sec before it is dropped. If there 3000 clients and all of them send RPCs around the same time (not very rare, with heartbeats etc), 2000 will be dropped. In stead of dropping the earliest RPCs, if the server delays reading new RPCs, the feedback to clients would be much smoother, I will file another jira regd queue management.    For this jira I propose to make queue size per handler configurable, with a larger default (may be 500).  ",ipc
FSInputDataStream.getPos throws null pointer exception when file has been closed,"If an FSInputDataStream object has been closed, invoking getPos() will cause a NullPointerException. This is because BufferedInputStream.close() sets in to null, and Buffer.getPos() does not check for in being null.",fs
I'd like a log4j appender that can write to a Hadoop FileSystem,"It would be convenient to be able to write log files to HDFS and other file systems directly. For large clusters, it will produce too many files, but for small clusters it should be usable.",fs
distcp requires large heapsize when copying many files,"Trying to distcp 1.5 million files with 1G client heapsize,  failed with outofmemory.   Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded        at java.util.regex.Pattern.compile(Pattern.java:1438)        at java.util.regex.Pattern.<init>(Pattern.java:1130)        at java.util.regex.Pattern.compile(Pattern.java:846)        at java.lang.String.replace(String.java:2208)        at org.apache.hadoop.fs.Path.normalizePath(Path.java:147)        at org.apache.hadoop.fs.Path.initialize(Path.java:137)        at org.apache.hadoop.fs.Path.<init>(Path.java:126)        at org.apache.hadoop.dfs.DfsPath.<init>(DfsPath.java:32)        at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.listPaths(DistributedFileSystem.java:214)        at org.apache.hadoop.fs.FileSystem.listPaths(FileSystem.java:483)        at org.apache.hadoop.fs.FileSystem.listPaths(FileSystem.java:496)        at org.apache.hadoop.fs.ChecksumFileSystem.listPaths(ChecksumFileSystem.java:539)        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.setup(CopyFiles.java:327)        at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:762)        at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:808)        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)        at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:818)  It would be nice if distcp doesn't require gigs of heapsize when copying large amount of files.   ",util
multiple dfs.client.buffer.dir directories are not treated as alternatives,"When specifying multiple directories in the value for dfs.client.buffer.dir, jobs fail when the selected directory does not exist or is not writable. Correct behaviour should be to create the directory when it does not exist and fail over to an alternative directory when it is not writable.",fs
Update documentation for hadoop's configuration post HADOOP-785,"With significant changes to hadoop's configuration since HADOOP-785 the documentation for it needs to be completely overhauled: a) Exhaustive and accurate javadocs, including some specific examples. b) Update the wiki: http://wiki.apache.org/lucene-hadoop/HowToConfigure to  c) Importantly: Put up a page describing hadoop's configuration on the hadoop website (via forrest).  Any thing else folks can think of?",documentation
Race condition in MiniDFSCluster shutdown,"Hudson has been sporadically failing tests that start- or follow tests that start- multiple datanodes in MiniDFSCluster, particularly on Solaris and Windows. The following appears to be at least partially responsible (much credit to Nigel for helping to discern this).  A common error: {noformat} java.io.IOException: Cannot remove data directory: /export/home/hudson/hudson/jobs/Hadoop-Nightly/workspace/trunk/build/test/data/dfs/data   org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:126)   org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:80)   org.apache.hadoop.dfs.TestFsck.testFsckNonExistent(TestFsck.java:96) {noformat}  MiniDFSCluster starts multiple DataNodes by calling DataNode::createDataNode, which creates and starts a DataNode thread, assigns the instance to a static member, and returns the Runnable. Of course, each call from MiniDFSCluster overwrites this instance. Since DataNode::shutdown() calls join() on the same Thread, each subsequent join is essentially a noop after the first DataNode finishes. When MiniDFSCluster::shutdown() returns, it may not have released its resources, so the next MiniDFSCluster may fail to start.",test
Undocumented parameters in FilesSystem,"Multiple create methods in public FileSystem class lack documentation for the following 2 parameters.  - long blockSize,  - Progressable progress  ",fs
Fix path in EC2 scripts for building your own AMI,None,contrib/cloud
locking for the ReflectionUtils.logThreadInfo is too conservative,"When the RPC servers get into trouble with their call queues backing up, they occasionally dump the call stacks. These are very useful for identifying hot spots, but the locking is too conservative and so all of the handlers are blocked while the thread call stacks are dumped.",util
the metrics system in the job tracker is running too often,The metrics system in the JobTracker is defaulting to every 5 seconds computing all of the counters for all of the jobs. This work is a substantial amount of work showing up as running in 20% of the snapshots that I've seen. I'd like to lower the default interval to once every 60 seconds and make it a low priority thread.,metrics
JobConf should warn about the existance of obsolete mapred-default.xml.,"Since the mapred-default.xml is ignored after HADOOP-785, we should generate a warning when it is present. Otherwise users forget to move the values to hadoop-site.xml and get  confused when things don't work right.",conf
"FSShell put or CopyFromLocal incorrectly treats "".""",The following dfs shell command {code} bin/hadoop dfs -put README.txt . {code} results in creating a file /user/<user name> with the contents of README.txt. A correct behavior would be creating a directory and a file in it: /user/<user name>/README.txt The put command works correctly if /user/<user name> already exists. So the following sequence of command leads to the desired result: {code} bin/hadoop dfs -mkdir . bin/hadoop dfs -put README.txt . {code} ,fs
Need configuration guides for Hadoop,"We've recently had a spate of questions on the users list regarding features such as rack-awareness, the trash can etc. which are not clearly documented from a user/admins perspective. There is some Javadoc present but most of the ""documentation"" exists either in JIRA or in the default config files themselves.  We should generate top down configuration and use guides for map/reduce and HDFS. These should probably be in forest and accessible from the project website (Javadoc isn't always approachable to our non-programmer audience). Committers should look for user documentation before accepting patches.",documentation
Test dfs.TestFileCreation.testFileCreation failed on Windows,Fails with this assert error:  junit.framework.AssertionFailedError: filestatus.dat should be of size 16384   org.apache.hadoop.dfs.TestFileCreation.testFileCreation(TestFileCreation.java:137),test
Need extensive shell interface,"My project, Pig, provides an interactive shell for users where, among other things, they can do some basic DFS operations such as changing firectories, copying files, etc. It would be great if Hadoop could provide a consistent interface to support basic shell operations.",fs
TestDFSUpgradeFromImage doesn't shut down its MiniDFSCluster,"TestDFSUpgradeFromImage doesn't call shutdown() when it's finished with its MiniDFSCluster, so its resources aren't reclaimed before it exits causing subsequent tests to fail.",test
NPE in IPC handler.,"Noticed a few of the following traces during upgrade of a large cluster to 0.14.1 :  {noformat} 2007-09-26 17:21:40,134 WARN org.apache.hadoop.ipc.Server: IPC Server handler 20 on 8020, call  processUpgradeCommand(org.apache.hadoop.dfs.BlockCrcUpgradeUtils$CrcInfoCommand@2b9c5c9d)  from 192.0.0.100:40500: output error java.lang.NullPointerException         at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)         at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)         at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)         at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)         at java.io.DataOutputStream.flush(DataOutputStream.java:106)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:585) {noformat}   ",ipc
start task tracker and stop task tracker scripts,we should have scripts for starting and stopping just the task tracker,scripts
Code contribution of Kosmos Filesystem implementation of Hadoop Filesystem interface,"Kosmos Filesystem (KFS) is an open source implementation targeted towards applications that are required to process large amounts of data.  KFS has been integrated with Hadoop using Hadoop's filesystem interfaces.  This issue is filed with the intent of getting our code, namely, fs/kfs classes, to be included in the next Hadoop release.",fs
Wildcard input syntax (glob) should support {},"We have users who have organized data by day and would like to select several days in a single input specification.  For example they would like to be able to say:  '/data/2007{0830,0831,0901}/typeX/'  To input 3 days data into map-reduce (or Pig in this case).   (Also the use of regexp to resolve glob paterns looks like it might introduce some other bugs.  I'd appreciate it if someone took another look at the code to see if there are any file characters that could be interpreted as regexp ""instructions"").",fs
"org.apache.hadoop.io.Text uses static ChasetDecoders, but they aren't thread-safe","org.apache.hadoop.io.Text uses static instances Text.DECODER and Text.ENCODER for all encoding and decoding, but these classes are not thread-safe.  Multiple threads calling Text.toString() at the same time can cause the decoders to output jumbled and garbage data.  ",io
Path can not handle a file name that contains a back slash,"When normalizing a path name, Path incorrectly converts a back slash to a path separator even if  the path name is of the unix style. This prohibs a glob from using a back slash to escape a special character. A fix is to make path normalization file system dependent.",fs
Re-write NNBench to use MapReduce,The proposal is to re-write the NNBench benchmark/test to measure Namenode operations using MapReduce. Two buckets of measurements will be done:  1. Transactions per second  2. Average latency  for these operations - Create and Close file - Open file - Rename file - Delete file  ,test
configure script for compiling hadoop native doesn't set lzo lib name correctly,"Looks like this was already reported (but not resolved on the the list): http://tinyurl.com/2rwu6x  I would like to compile libhadoop on amd64/Fedora and everything seems kosher until I hit this compile error:       [exec] /home/jssarma/fbprojects/hadoop-0.13.1/src/native/src/org/apache/hadoop/io/compress/lzo/LzoCompressor.c:116: error: syntax error before ',' token  the line in question is:      // Load liblzo2.so                                                                                         liblzo2 = dlopen(HADOOP_LZO_LIBRARY, RTLD_LAZY | RTLD_GLOBAL);  seems like this is being set by:  configure:#define HADOOP_LZO_LIBRARY ${ac_cv_libname_lzo2}  I tried executing the relevant part of configure by hand:    if test -z ""`${CC} -o conftest conftest.c -llzo2 2>&1`""; then         if test ! -z ""`which objdump`""; then       ac_cv_libname_lzo2=""`objdump -p conftest | grep NEEDED | grep lzo2 | sed 's/\W*NEEDED\W*\(.*\)\W*$/\ \""\1\""/'`""  This is not working on my system, since: > objdump -p conftest | grep NEEDED                            NEEDED      libc.so.6  So that would explain the compile error. Editing the configure script manually for now works. ",build
TestLocalDirAllocator fails on Windows,The following tests failed on Windows. It passed on Linux  org.apache.hadoop.fs.TestLocalDirAllocator.test0 org.apache.hadoop.fs.TestLocalDirAllocator.test1 org.apache.hadoop.fs.TestLocalDirAllocator.test2 org.apache.hadoop.fs.TestLocalDirAllocator.test3  Exception thrown (org.apache.hadoop.fs.TestLocalDirAllocator.test0):  junit.framework.AssertionFailedError   org.apache.hadoop.fs.TestLocalDirAllocator.validateTempDirCreation(TestLocalDirAllocator.java:67)   org.apache.hadoop.fs.TestLocalDirAllocator.test0(TestLocalDirAllocator.java:84),fs
Instantiating a FileSystem object should guarantee the existence of the working directory,"Issues like HADOOP-1891 and HADOOP-1916 illustrate the need for this behavior.  In HADOOP-1916 the problem is that the default working directory for a user on HDFS '/user/<username>' does not exist. This results in the command 'hadoop dfs -copyFromLocal foo ."" creating a *file* called /user/<username> and copying the contents of the file 'foo' into this file.  HADOOP-1891 is basically the same problem. The problem that Olga observed was that copying a file to '.' on HDFS when her 'home directory' did not exist resulted in the creation of a file with the path as her home directory. The problem is incorrectly filed as a bug in the Path class. The behavior of Path is correct, as Doug points out, it is perfectly reasonable for Path(""."") to convert to an empty path. When this empty path is resolved in HDFS or any other filesystem the resolution to '/user/<username>' is also correct (at least for HDFS). The problem IMO is that the existence of the working directory is not guaranteed.  When I log in to a machine my default working directory is '/home/sameerp' and filesystem operations that I execute with relative paths all work correctly because this directory exists. My home directory lives on a filer, in the event of it being unmountable the default working directory I get is '/' which also is guaranteed to exist.  In the context of Hadoop, instantiating a FileSystem object is the analogue of logging in and should result in a working directory whose existence has been validated. In the case of HDFS this should be '/user/<username>' or '/' if the directory does not exist. ",fs
FileSystem should provide byte ranges for file locations,"FileSystem's getFileCacheHints should be replaced with something more useful. I'd suggest replacing getFileCacheHints with a new method:  {code} BlockLocation[] getFileLocations(Path file, long offset, long range) throws IOException; {code}  and adding  {code} class BlockLocation implements Writable {   String[] getHosts();   long getOffset();   long getLength(); } {code} ",fs
distcp fails if log dir not specified and destination not present,"The default location for distcp logs is in the destination directory; when that doesn't exist, distcp exits with an error.",util
distcp split generation does not work correctly," With the current implementation, distcp will always assign multiple files to one mapper to copy, no matter how large  are the files. This is because the CopyFiles class uses a sequencefile to store the list of files to be copied,  one record per file. CopyFile class correctly generates one split per record in the sequence file. However,  due to  the way the sequence file record reader works, the minimum unit for splits is the segments between the  ""syncmarks"" in the sequence file.  This results in the strange behavior that some mappers get zero records (zero files to copy) even though their  split lengths are non-zero, while other mappers get multiple records (multiple filesto copy) from their split (and beyond to the next sync mark).   When CopyFile class creates the sequencefile, it does try to place a sync mark between splitable segments in the sequence file by calling sync() function of the sequence file record writer.  Unfortunately, the sync() function is a no-op for files that are not block compressed.  Naturally, after I changed the compression type for the sequence file to block compression, mappers got the correct records from their splits. So a simple fix is to change the compression tye to CompressionType.BLOCK:  {code} // create src list     SequenceFile.Writer writer = SequenceFile.createWriter(         jobDirectory.getFileSystem(jobConf), jobConf, srcfilelist,         LongWritable.class, FilePair.class,         SequenceFile.CompressionType.BLOCK);. {code}  ",util
NPE in JvmMetrics.doThreadUpdates,It showed on task's stderr.  Didn't fail the task directly.  java.lang.NullPointerException at org.apache.hadoop.metrics.jvm.JvmMetrics.doThreadUpdates(JvmMetrics.java:129) at org.apache.hadoop.metrics.jvm.JvmMetrics.doUpdates(JvmMetrics.java:79) at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:284) at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50) at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249) at java.util.TimerThread.mainLoop(Timer.java:512) at java.util.TimerThread.run(Timer.java:462),metrics
0.14.2 release compiled with Java 1.6 instead of Java 1.5,"Hadoop is compatible with Java 1.5, but the 0.14.2 release was compiled with Java 1.6, so the distributed binaries do not work with Java 1.5.  Folks can recompile to get things to work under Java 1.5, but it really should work out-of-the-box.",build
credits page should have more information,"The hadoop credits page should permit folks to list their organization, timezone, role, etc, as is done for many other projects, e.g.:  http://harmony.apache.org/contributors.html http://jackrabbit.apache.org/team-list.html http://db.apache.org/whoweare.html  Thus I propose we add a table to this page, with columns for username, name, organization, timezone and roles.  I don't think we need explicit ""website"" or ""email"" columns.  Folks can make their name a link if they have a website or blog, and email addresses are derivable from Apache username (by adding @apache.org).",documentation
Documentation: improve mapred javadocs,"I'd like to put forward some thoughts on how to structure reasonably detailed documentation for hadoop.  Essentially I think of atleast 3 different profiles to target: * hadoop-dev, folks who are actively involved improving/fixing hadoop. * hadoop-user ** mapred application writers and/or folks who directly use hdfs ** hadoop cluster administrators  For this issue, I'd like to first target the latter category (admin and hdfs/mapred user) - where, arguably, is the biggest bang for the buck, right now.  There is a crying need to get user-level stuff documented, judging by the sheer no. of emails we get on the hadoop lists...  ----  *1. Installing/Configuration Guides*  This set of documents caters to folks ranging from someone just playing with hadoop on a single-node to operations teams who administer hadoop on several nodes (thousands). To ensure we cover all bases I'm thinking along the lines of:  * _Download, install and configure hadoop_ on a single-node cluster: including a few comments on how to run examples (word-count) etc. * *Admin Guide*: Install and configure a real, distributed cluster.  * *Tune Hadoop*: Separate sections on how to tune hdfs and map-reduce, targeting power admins/users.  I reckon most of this would be done via forrest, with appropriate links to javadoc.  ---  *2. User Manual*  This set is geared for people who use hdfs and/or map-reduce per-se. Stuff to document:  * Write a really simple mapred application, just fitting the blocks together i.e. maybe a walk-through of a couple of examples like word-count, sort etc. * Detailed information on important map-reduce user-interfaces: *- JobConf *- JobClient *- Tool & ToolRunner *- InputFormat  *-- InputSplit *-- RecordReader *- Mapper *- Reducer *- Reporter *- OutputCollector *- Writable *- WritableComparable *- OutputFormat *- DistributedCache * SequenceFile *- Compression types: NONE, RECORD, BLOCK * Hadoop Streaming * Hadoop Pipes  I reckon most of this would land up in the javadocs, specifically package.html and some via forrest.  ----  Also, as discussed in HADOOP-1881, it would be quite useful to maintain documentation per-release, even on the hadoop website i.e. we could have a main documentation page link to documentation per-release and to the trunk.  ----  Thoughts?",documentation
distcp does not fail if source directory has files with missing blocks,"I copied a directory using distcp (to another directory on the same file system).  There were 9 data blocks missing in the files in the source directory, which caused distcp to print messages like the following:  ... 07/10/13 00:09:16 INFO mapred.JobClient:  map 1% reduce 0% 07/10/13 00:09:16 INFO mapred.JobClient: Task Id : task_200710120717_0081_m_000020_0, Status : FAILED java.io.IOException: Could not obtain block: blk_6787282547149034655 file=/srcdir/file1         at org.apache.hadoop.dfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1136)         at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:988)         at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1094)         at java.io.DataInputStream.read(DataInputStream.java:83)         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:289)         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:348)         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:216)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1753) ...  The corresponding tasks failed, but the retries were successful (all files with missing blocks in the source directory were copied as empty files in the target directory).  I think that distcp should fail if it cannot successfully copy all the files (at least when no command-line options are given).  This is critical for us as we intend to use distcp to copy databases from one dfs to another, and if silent failures can happen then we would have to monitor each distcp manually to ensure that it succeeded.",util
Allow adding additional datanodes to MiniDFSCluster,Currently MiniDFSCluster allows first starting namenode and then datanodes. It would be nice that it also allows additional datanodes to a running MiniDFSCluster.,test
Command to pull corrupted files,"Before 0.14, dfs -get didn't perform checksum checking.    Users were able to download the corrupted files to see if they want to delete them.  After 0.14, dfs -get also does the checksumming.   Requesting a command for no-checksum-get command.",fs
DFS Summary Page,"DFS Summary page should consists of the DFS summary data and *links* to Live Datanodes and Dead datanodes instead of loading ""all nodes"" information in one page. It would be great if you can give the summary in XML format also.  ",util
RawLocalFileStatus is causing Path problems ,"In RawLocalFileStatus of the RawLocalFileSystem class, files were getting changed to URIs then to string which would cause some files to appear as C:/somethingfile://anotherpath.  This is a simple change and it just needs to be converted as toString and opposed to toUri().toString().  The problem area is line 324 of the org.apache.hadoop.fs.RawLocalFileSystem.java file.  I am testing a patch currently and will submit as soon as it passes all current unit tests.",fs
ChecksumFileSystem checksum file size incorrect.,"Periodically, reduce tasks hang. When the log for the task is consulted, you see a stacktrace that looks like this:  2007-10-18 17:02:04,227 WARN org.apache.hadoop.mapred.ReduceTask: java.io.IOException: Insufficient space   org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryOutputStream.write(InMemoryFileSystem.java:174)   org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:39)   java.io.DataOutputStream.write(DataOutputStream.java:90)   java.io.FilterOutputStream.write(FilterOutputStream.java:80)   org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:326)   org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:140)   org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:122)   org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:310)   org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:49)   org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:64)   org.apache.hadoop.mapred.MapOutputLocation.getFile(MapOutputLocation.java:253)   org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:685)   org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:637)  The problem stems from a miscalculation of the checksum file created in the InMemoryFileSystem associated with the data being copied from a completed mapper task to the reducer task.  The method used for calculating checksum file size is the following (ChecksumFileSystem:318):  ((long)(Math.ceil((float)size/bytesPerSum)) + 1) * 4 + CHECKSUM_VERSION.length;  The issue here is the cast to float.  Floating point numbers have only 24 bits of precision, thus will return short values on any size over 0x1000000.  The fix is to replace this calculation with something that doesn't cast to float.  (((size+1)/bytesPerSum) + 2) * 4 + CHECKSUM_VERSION.length  ",fs
"Configuration getInt, getLong, and getFloat replace invalid numbers with the default value",The current behavior of silently replace invalid numbers in the configuration with the default value leads to hard to diagnose problems. The methods should throw an exception to signal that the input was invalid.,conf
hadoop-daemon.sh script fails if HADOOP_PID_DIR doesn't exist,"HADOOP-1825 didn't fix this right...   However it isn't critical (i.e. not a regression on previous releases) and 0.15.0 feature freeze is in place (it's nearly out of the door), I'm moving it to a separate jira and marking it for 0.16.0.",scripts
ToolBase doesn't keep configuration,ToolBase which has been superceded by ToolRunner doesn't pass in an existing configuration object and therefore won't pick up initial configuration resources.  One consequence of this is the nutch default and site.xml files are ignored.,util
HADOOP-2046 caused some javadoc anomalies,"Configuration.java, Mapper.java, and WritableComparable.java have either misformatted or missing fragments of javadoc.",documentation
clover description attribute suppresses all other targets in -projecthelp,"Running ""ant -projecthelp"" should print a list of available targets; instead it outputs:  {noformat} Buildfile: build.xml  Main targets:   clover  Instrument the Unit tests using Clover.  Requires a Clover license and clover.jar in the ANT classpath.  To use, specify -Drun.clover=true on the command line. Default target: compile {noformat}  When ant has ""main"" targets- i.e. targets with descriptions- by default it only outputs those targets when run with -projecthelp. Since clover is the only target in build.xml with a description, it's the only target reported to the user. The description should either be removed or descriptions should be added to some subset of targets.",build
NullPointerException in JVMMetrics for OOM killed task,"I had a reduce task run out of memory and die in such a way that JVMMetrics.doThreadUpdates() throws a NullPointerException.  The aparent cause seems to be that the call to threadMXBean.getThreadInfo() on JVMMetrics:119 returns an array of ThreadInfo whose elements may be null.  Here's a relevant quote from the javadoc: This method returns an array of the ThreadInfo objects,      * each is the thread information about the thread with the same index      * as in the ids array.      * If a thread of the given ID is not alive or does not exist,      * null will be set in the corresponding element       * in the returned array.  A thread is alive if       * it has been started and has not yet died.  My stacktrace looks like this: java.lang.NullPointerException   org.apache.hadoop.metrics.jvm.JvmMetrics.doThreadUpdates(JvmMetrics.java:129)   org.apache.hadoop.metrics.jvm.JvmMetrics.doUpdates(JvmMetrics.java:79)   org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:284)   org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50)   org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249)   java.util.TimerThread.mainLoop(Timer.java:512)   java.util.TimerThread.run(Timer.java:462)  On line 129,  there's an attempt to dereference the potientially null threadInfo value to get its current state.  The naive solution here is to check for null and count null values as ""terminated""... but it seems clear that a thread state of TERMINATED and a null ThreadInfo value are distinct cases and may need special treatment.  Guessing that this is a ""minor"" issue because it seems more cosmetic than mission critical.  I'm not sure what the upstream effects are of this method throwing the NPE, so i didn't set it to ""trivial"".",metrics
"Add ""-text"" command to FsShell to decode SequenceFile to stdout",FsShell should provide a command to examine SequenceFiles.,fs
dfs -getMerge does not do what it says it does,"dfs -getMerge, which calls FileUtil.CopyMerge, contains this javadoc:    {code}  Get all the files in the directories that match the source file pattern     * and merge and sort them to only one file on local fs      * srcf is kept.  {code}    However, it only concatenates the set of input files, rather than merging them in sorted order.    Ideally, the copyMerge should be equivalent to a map-reduce job with IdentityMapper and IdentityReducer with numReducers = 1. However, not having to run this as a map-reduce job has some advantages, since it increases cluster utilization during reduce phase.    ","documentation,fs"
distcp between two clusters does not work if it is run on the target cluster,"I am trying to copy a directory (~100k files, ~500GB) between two clusters A and B (~70 nodes), using a command like:  hadoop distcp -log /logdir hdfs://namenode-of-A:8600/srcdir hdfs://namenode-of-B:8600/targetdir   I tried 4 ways of doing it:  1) Copy from A to B, by running distcp on A 2) Copy from A to B, by running distcp on B 3) Copy from B to A, by running distcp on B 4) Copy from B to A, by running distcp on A  Invocations 1 and 3 succeeded, but 2 and 4 failed.  I got a lot of errors of the type below:  07/10/30 20:52:11 INFO mapred.JobClient: Running job: job_200710180049_0115 07/10/30 20:52:12 INFO mapred.JobClient:  map 0% reduce 0% 07/10/30 20:54:41 INFO mapred.JobClient:  map 1% reduce 0% 07/10/30 20:56:52 INFO mapred.JobClient:  map 2% reduce 0% 07/10/30 20:57:41 INFO mapred.JobClient: Task Id : task_200710180049_0115_m_000184_0, Status : FAILED java.io.IOException: Some copies could not complete. See log for details.         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:407)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)  followed by the job failing:  07/10/30 22:07:41 INFO mapred.JobClient:  map 99% reduce 100% Copy failed: java.io.IOException: Job failed!         at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:688)         at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:481)         at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:555)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:54)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:67)         at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:566)",util
Remove developer-centric requirements from overview.html,"Doug's comments on HADOOP-2105:  {blockquote} There are still some asymmetries in overviewlhtml, e.g., subversion is only mentioned for Windows, and ant is mentioned in a separate section. I suggest that we drop mention of subversion and ant here altogether, as this is documentation for Hadoop users, not for Hadoop's developers. We might separately add mention of subversion and ant to Hadoop's contributor documentation, if we feel that's needed. {blockquote}  I'm moving Jim's patch here since the old patch has already been committed to subversion.",documentation
need 'doc' target that runs forrest,"We should have an 'ant doc' target that runs forrest.  Ideally we'd check the forrest jars into subversion, so that this task works out of the box, w/o having to separately install forrest.  The ant task could both build the docs and copy them into the docs/ directory, rather than requiring this to be done manually.",build
Benchmark directory and build target.,"Currently hadoop benchmarks are located in the test directory. Since the number of benchmarks is growing we should probably consider a separate directory for benchmarks.  The benchmark directory should be structured in the  same way the test or example directories are that is be respective packages. The core-benchmark target(s) should be also created in build.xml which should include compile-core, and should be included in core-test. This is because benchmarks also require unit tests.",build
separate website from user documentation,"Currently the website only contains the documentation for a single release, the current release.  It would be better if the website also contained documentation for past releases, since not everyone is using the current release.  To implement this we should move the top-level of the website, including project and developer information, from the subversion trunk into a separate tree, so that only the user documentation is branched per release.",documentation
"PositionCache was removed from FSDataInputStream, causes extremely bad MapFile performance",The PositionCache in FSDataInputStream seems to have been removed in HADOOP-1470. This causes for example MapFile.get usage to be  extremely slow as the file position isn't cached in memory.,fs
distcp throws a NullPointerException in the close() method of mapper class due to the Reporter becoming invalid,"distcp occasionally throws a NullPointerException in the close() method of the mapper class, when the copy of the Reporter handle becomes invalid:  java.lang.NullPointerException        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.updateStatus(CopyFiles.java:242)        at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:402)        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1760)  This can easily be fixed by checking whether the Reporter is null before updating the status. Also, the status could be updated once the invocation of copy(srcstat, dstpath, out, reporter); returns on line 373 of CopyFiles.java. Marking this as critical for a 0.15.1 release as Chris requested.",util
RPC Support for user permissions and authentication.," Update 11/13/2007: What is proposed for 0.16.0 :  The client can set a user ticket (as defined in HADOOP-1701) for each connection and that ticket is made available to RPC calls at the server. The client can replace the ticket at any time. The main advantage is that rest of the the client RPCs don't need to be aware of the user tickets.  What RPC would ideally support in future :  In the current version of RPC, there is no authentication or data protection.  We propose to change the RPC framework, so that secure communication is possible.  The new RPC should: - Compatible with current RPC - Allow a pluggable security implementations (see HADOOP-1701) - Support both secure and non-secure modes.  Here is a rough idea: - Store security information (e.g. username, keys) in a ticket - Use the ticket to establish a RPC connection - Create secure sockets by the (subclass of) SocketFactory corresponding to the selected security implementations - Send the data and RPC parameters with the secure sockets  When authentication is supported, the RPC callee should also initialize caller information during RPC setup and execute the RPC on the caller's behalf.",ipc
Server ports: to roll or not to roll.,"Looked at the issues related to port rolling. My impression is that port rolling is required only for the unit tests to run. Even the name-node port should roll there, which we don't have now, in order to be able to start 2 cluster for testing say dist cp.  For real clusters on the contrary port rolling is not desired and some times even prohibited. So we should have a way of to ban port rolling. My proposition is to # use ephemeral port 0 if port rolling is desired # if a specific port is specified then port rolling should not happen at all, meaning that a  server is either able or not able to start on that particular port.  The desired port is specified via configuration parameters. - Name-node: fs.default.name = host:port - Data-node: dfs.datanode.port - Job-tracker: mapred.job.tracker = host:port - Task-tracker: mapred.task.tracker.report.bindAddress = host   Task-tracker currently does not have an option to specify port, it always uses the ephemeral port 0,    and therefore I propose to add one. - Secondary node does not need a port to listen on.  For info servers we have two sets of config variables *.info.bindAddress and *.info.port except for the task tracker, which calls them *.http.bindAddress and *.http.port instead of ""info"". With respect to the info servers I propose to completely eliminate the port parameters, and form  *.info.bindAddress = host:port Info servers should do the same thing, namely start or fail on the specified port if it is not 0, and start on any free port if it is ephemeral.  For the task-tracker I would rename tasktracker.http.bindAddress to mapred.task.tracker.info.bindAddress For the data-node the info dfs.datanode.info.bindAddress should be included into the default config. Is there a reason why it is not there?  This is the summary of proposed changes: || Server || current name = value || proposed name = value || | NameNode | fs.default.name = host:port | same | | | dfs.info.bindAddress = host | dfs.http.bindAddress = host:port | | DataNode | dfs.datanode.bindAddress = host | dfs.datanode.bindAddress = host:port | | | dfs.datanode.port = port | eliminate | | | dfs.datanode.info.bindAddress = host | dfs.datanode.http.bindAddress = host:port | | | dfs.datanode.info.port = port | eliminate | | JobTracker | mapred.job.tracker = host:port | same | | | mapred.job.tracker.info.bindAddress = host | mapred.job.tracker.http.bindAddress = host:port | | | mapred.job.tracker.info.port = port | eliminate | | TaskTracker | mapred.task.tracker.report.bindAddress = host | mapred.task.tracker.report.bindAddress = host:port | | | tasktracker.http.bindAddress = host | mapred.task.tracker.http.bindAddress = host:port | | | tasktracker.http.port = port | eliminate | | SecondaryNameNode | dfs.secondary.info.bindAddress = host | dfs.secondary.http.bindAddress = host:port | | | dfs.secondary.info.port = port | eliminate |  Do we also want to set some uniform naming convention for the configuration variables? Like having hdfs instead of dfs, or info instead of http, or systematically using either datanode or data.node would make that look better in my opinion.  So these are all +*api*+ changes. I would +*really*+ like some feedback on this, especially from  people who deal with configuration issues on practice.",conf
FileSystem should return location information with byte ranges,"The FileSystem interface should provide location information with byte ranges rather than a String[][] of locations. I suggest that we deprecate FileSystem.getFileCacheHints and replace it with: {code} abstract public class FileSystem {    ...    public static class BlockInformation implements Writable {       public BlockInformation(long start, String[] locations) {...}       public String[] getHosts() {...}       public long getStartingOffset() {...}    }    BlockInformation[] getFileLocations(Path f, long start, long length) { ... } } {code} This will allow us to fix the FileInputFormat in map/reduce to make just one call per a file to the name node instead of one per a block.",fs
RPC should send a ping rather than use client timeouts,"Current RPC (really IPC) relies on client side timeouts to find ""dead"" sockets. I propose that we have a thread that once a minute (if the connection has been idle) writes a ""ping"" message to the socket. The client can detect a dead socket by the resulting error on the write, so no client side timeout is required. Also note that the ipc server does not need to respond to the ping, just discard it.",ipc
Regenerate entire hadoop website since site.xml was changed by HADOOP-1917,"HADOOP-1917 changed src/docs/src/documentation/content/xdocs/site.xml, but did not regenerate docs/hdfs_design.html and docs/mailing_lists.html to reflect those changes.",documentation
java.lang.ArithmeticException: / by zero in ChecksumFileSystem.open,"The ChecksumFileSystem uses a default bytesPerChecksum value of zero.  This number appears as a divisor in ChecksumFileSystem.getSumBufferSize, if it is not overriden in config.",fs
Change documentation in cluster_setup.html and mapred_tutorial.html post HADOOP-1274,Documentation has to be changed in cluster_setup.html and mapred_tutorial.html to reflect usage of mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum instead of mapred.tasktracker.tasks.maximum.,documentation
Generalize StatusHttpServer so webdav server can use it,"I'd like to make HADOOP-496 stand alone, so that I can make a hadoop-webdav jar that works against stock hadoop.  The latest HADOOP-496 patch has only a small patch against StatusHttpServer, which generalizes it a little bit to make some private methods protected and changes HttpServlet to Servlet -- the rest is new files.  I'd like to get the part against StatusHttpServer committed.",fs
Configuration.toString is broken,{{Configuration.toString}} doesn't string-ify the {{Configuration.resources}} field which was added in HADOOP-785.,conf
Provide a simple login implementation,Give a simple implementation of HADOOP-1701.    Hadoop clients are assumed to be started within a Unix-like network which provides user and group management.  This implementation read user information from the OS and send them to the NameNode in plaintexts through RPC (see also HADOOP-2184).  NameNode trusts all information given and uses them for permission checking.,fs
"ShellCommand, in particular 'df -k', sometimes hang","We noticed that some pipes applications writing to dfs using libhdfs have about 6% chance of hanging when executing 'df -k' to find out whether there is enough space available on the local filesystem before opening a file for write.  Why not using File.getFreeSpace() or File.GetUsableSpace()?  The call stack is: Exception in thread ""main"" java.io.IOException          at org.apache.hadoop.fs.ShellCommand.runCommand (ShellCommand.java:52)          at org.apache.hadoop.fs.ShellCommand.run(ShellCommand.java:42)          at org.apache.hadoop.fs.DF.getAvailable(DF.java:72)          at org.apache.hadoop.fs.LocalDirAllocator $AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:264)          at org.apache.hadoop.fs.LocalDirAllocator $AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:294)          at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite (LocalDirAllocator.java:155)          at org.apache.hadoop.dfs.DFSClient $DFSOutputStream.newBackupFile(DFSClient.java:1470)          at org.apache.hadoop.dfs.DFSClient $DFSOutputStream.openBackupStream(DFSClient.java:1437)          at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.writeChunk (DFSClient.java:1579)          at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk (FSOutputSummer.java:140)          at org.apache.hadoop.fs.FSOutputSummer.write1 (FSOutputSummer.java:100)          at org.apache.hadoop.fs.FSOutputSummer.write (FSOutputSummer.java:86)          at org.apache.hadoop.fs.FSDataOutputStream $PositionCache.write(FSDataOutputStream.java:39)          at java.io.DataOutputStream.write(DataOutputStream.java:90)          at java.io.FilterOutputStream.write(FilterOutputStream.java:80)  ",fs
Add option to disable nagles algorithm in the IPC Server,"While investigating hbase performance, I found a bottleneck caused by Nagles algorithm. For some reads I would get a bi-modal distribution of read times, with about half the times being around 20ms, and half around 200ms. I tracked this down to the well-known interaction between Nagle's algorithm and TCP delayed acknowledgments.   I found that calling setTcpNoDelay(true) on the server's socket connection dropped all of my read times back to a constant 20 ms.  I propose a patch to have this TCP_NODELAY option be configurable. The attacked patch allows one to set the TCP_NODELAY option on both the client and the server side. Currently this is defaulted to false (i.e., with Nagle's enabled).  To see the effect, I have included a Test which provokes the issue by sending a MapWriteable over an IPC call. On my machine this test shows a speedup of 117 times when using TCP_NODELAY.  These tests were done on OSX 10.4. Your milage may very with other TCP/IP implementation stacks.",ipc
General example for modeling m/r load in Java,"This matches the hadoop.sort.(map|reduce).keep.percent interface in HADOOP-2127 and includes an ""indirect"" sort matching some user apps (when locality information is unavailable). It mostly, merely merges parts of the RandomWriter and Sort examples to effect sample loads in map/reduce.",test
MapWritable.readFields needs to clear internal hash else instance accumulates entries forever,"A common framework pattern is to get an instance of a Writable, usually by reflection, and then just keep calling readFields to make new 'instances' of the particular Writable.  For example, the spill-to-disk that is run at the end of a map task gets instances of map output keys and values and then loops over the (sorted) map output calling readFields to make instances to write out to the filesystem (See around line #470 in the spill method).  If the particular Writable is an instance of MapWritable, currently we get funny results.  It has an internal hash map that is created on instantiation.  Each time the readFields method is called, the newly deserialized entries are added to the internal map.  The map needs to be reset when readFields is called so it doesn't just keep growing ad infinitum.",io
"In CHANGES.txt, move HADOOP-1851 & HADOOP-1231 to INCOMPATIBLE CHANGES section","HADOOP-1851 redefines the way one would control compression for the intermediate and the final outputs of a job. HADOOP-1231 adds Generics to the framework and user code should be aware of that (at least for code that requires compilation with 0.15). In CHANGES.txt these two appear in NEW FEATURES and IMPROVEMENTS sections respectively. Ideally, they should be part of the INCOMPATIBLE CHANGES section.",documentation
Title: DFS submit client params overrides final params on cluster ,"hdfs client params over-rides the params set as final on hdfs cluster nodes.   default valuesv of cleint side hadoop-site.xml values override the final prameters of hdfs hadoop-site.xml . oberved the following cases -: 1. dfs.trash.root=/recycle, dfs.trash.interval=10 and dfs.replication=2 marked final under hadoop-site.xml on hdfs cluster.    When fsShel command ""hadoop dfs -put local_dir dest"" fired from submission host    Files will still get replicated 3 times (default) instead of final dfs.replication=2.    Similarly when ""hadoop dfs -rmr dfs_dir OR hadoop dfs -rm file_path "" fired from submit client the file/driectory diectly got deleted without being moved to /recycle.    Here hadoop-site.xml on submit client does not specify dfs.trash.root, dfs.trash.interval and dfs.replication.       Same is the case when we submit mapred JOB from client and job.xml dispalys default values which overrides the lsuter values.  2. dfs.trash.root=/recycle, dfs.trash.interval=10 and dfs.replication=2 marked final under hadoop-site.xml on hdfs cluster.    And     dfs.trash.root=/rubbish, dfs.trash.interval=2 and dfs.replication=5 under hadoop-site.xml on submit client.     When fsShel command ""hadoop dfs -put local_dir dest"" fired from submit client    Files will  get replicated 5 times instead of final dfs.replication=2.    Similarly when ""hadoop dfs -rmr dfs_dir OR hadoop dfs -rm file_path "" fired from submit client the file/driectory diectly will be moved to /rubbish instead of /recycle.        Same is the case when we submit mapred job from client, job.xml displays following values -:    dfs.trash.root=/rubbish, dfs.trash.interval=2 and dfs.replication=5   ",conf
chmod in ant package target fails,"If you checkout the trunk to a folder long than /tmp (in my case it is ""/home/adrian/workspace/hadoop-trunk"") and you execute the command:  ant clean package  You get the following error:  BUILD FAILED /home/adrian/workspace/hadoop-trunk/build.xml:730: Execute failed: java.io.IOException: Cannot run program ""chmod"": java.io.IOException: error=7, Argument list too long  I also tried this from the folder ""/tmp/trunk"" and the package target worked fine so I imagine the argument list is becoming too long due to the folder being longer. A simple fix for this should be to set the ""parallel"" attribute on chmod to false.",build
findbugs currently fails due to hadoop-streaming having moved,"If you do a fresh checkout of the trunk and try run findbugs you get the following error:  findbugs:  [findbugs] Running FindBugs...  [findbugs] Exception in thread ""main"" java.util.zip.ZipException: Error opening /tmp/x/trunk/build/hadoop-0.16.0-dev-streaming.jar  [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ZipFileCodeBase.<init>(ZipFileCodeBase.java:61)  [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassFactory.createFilesystemCodeBase(ClassFactory.java:96)  [findbugs]     at edu.umd.cs.findbugs.classfile.impl.FilesystemCodeBaseLocator.openCodeBase(FilesystemCodeBaseLocator.java:63)  [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassPathBuilder.processWorkList(ClassPathBuilder.java:381)  [findbugs]     at edu.umd.cs.findbugs.classfile.impl.ClassPathBuilder.build(ClassPathBuilder.java:192)  [findbugs]     at edu.umd.cs.findbugs.FindBugs2.buildClassPath(FindBugs2.java:432)  [findbugs]     at edu.umd.cs.findbugs.FindBugs2.execute(FindBugs2.java:160)  [findbugs]     at edu.umd.cs.findbugs.FindBugs.runMain(FindBugs.java:1521)  [findbugs]     at edu.umd.cs.findbugs.FindBugs2.main(FindBugs2.java:731)  [findbugs] Output saved to /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml      [xslt] Processing /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml to /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.html      [xslt] Loading stylesheet /opt/java/findbugs/src/xsl/default.xsl      [xslt] : Error! Premature end of file.      [xslt] : Error! com.sun.org.apache.xml.internal.utils.WrappedRuntimeException: Premature end of file.      [xslt] Failed to process /tmp/x/trunk/build/test/findbugs/hadoop-findbugs-report.xml  BUILD FAILED /tmp/x/trunk/build.xml:599: javax.xml.transform.TransformerException: javax.xml.transform.TransformerException: com.sun.org.apache.xml.internal.utils.WrappedRuntimeException: Premature end of file.  ----  This is because   build/hadoop-0.16.0-dev-streaming.jar   is the wrong location, it should be   build/contrib/streaming/hadoop-0.16.0-dev-streaming.jar   I think this also explains why Hudson is currently giving a -1 to all new patches as findbugs is failing.",build
website should link to ASF sponsor page,"The Hadoop website should link to the following pages:  http://www.apache.org/foundation/thanks.html http://www.apache.org/foundation/sponsorship.html  This should be done in the boilerplate of the site, so that every page links to these pages.  This is a requirement of all ASF sites. ",documentation
Change FileSystem API to support access control.,#NAME?,fs
ant target without source and docs ,Can we have an ant target or a -D option to build the hadoop tar without the source and documentation? This brings down the tar size from 11.5 MB to 5.6 MB. This would speed up distribution. ,build
"SocketFactory support for flexible RPC connections (authentication, encryption ...)","Currently Hadoop uses SocketFactories to support proxies for RPC connections.  HADOOP-2184 introduces a _ticket_ associated with each RPC that is transfered to Server with each connection. The ticket is transfered transparently to the user RPCs. Hadoop is expected to support other user authentication and encryption like Kerberos in near future. It would be nice to have Hadoop SocketFactory intefaces and implementations that implement different types of connections and data encryptions, that happens transparently to RPC layer itself.   There was some discussion about this in HADOOP-2184 and will include the excepts in the next comment. ",ipc
All C++ builds should use the autoconf tools,"Currently we have -Dcompile.native and -Dcompile.c++ build flags.  In addition, builds for pipes and libhadoop use autoconf tools, but libhdfs does not, nor does 64bit libhdfs compile work.  All these builds should use autoconf tools, support 64bit compilation, and should occur when a single flag is present (-Dcompile.c++ seems like the better choice).  ",build
Build both 32 and 64 bit native libraries when compiling with a 64 bit JVM,"When a 32 bit JVM is used to build Hadoop, the 32 bit  native libraries are build (providing -Dcompile.native=true is present).  Likewise, a 64 bit JVM automatically builds a 64 bit native library.  It would be helpful if a 64 bit JVM built both 32 and 64 bit native libraries.",build
Require Java 6,"We should require Java 6 for release 0.17.  Java 6 is now available for OS/X.  Hadoop performs much better on Java 6.  And, finally, there are features of Java 6 (like 'df') that would be nice to use.",build
Shell commands to access and modify file permissions, Hadoop 0.16 includes file permissions in DFS and we need FsShell to support common file permissions related commands : - chown - chgrp - chmod  Also output from some of the commands like {{ls -l}} will change to reflect new file properties. Aim is to make the above commands look like its Unix/Linux couterparts. They will of course support only the subset of the options.,fs
Remove AC_LIB_CHECK from src/native/configure.ac to ensure libhadoop.so doesn't have a dependency on libz.so/liblzo.so,"We should AC_LIB_CHECK from src/native/configure.ac to ensure libhadoop.so doesn't have a dependency on libz.so/liblzo.so, the check for libz/liblzo headers via AC_HEADERS_CHECK is a stronger guarantee anyway.",native
hadoop version wrong in 0.15.1,"I downloaded 0.15.1 release, recompiled and executed ./bin/hadoop version. It says 0.15.2-dev picking it from build.xml",build
Unit tests fail if there is another instance of Hadoop,"If you are running another Hadoop cluster or DFS, many unit tests fail because Namenode in MiniDFSCluster fails to bind to the right port. Most likely HADOOP-2185 forgot to set right defaults for MiniDFSCluster.",test
Space in the value for dfs.data.dir can cause great problems,"The following configuration causes problems:  <property>   <name>dfs.data.dir</name>   <value>/mnt/hstore2/hdfs, /home/foo/dfs</value>     <description>   Determines where on the local filesystem an DFS data node  should store its bl ocks.  If this is a comma-delimited  list of directories, then data will be stor ed in all named  directories, typically on different devices.  Directories that  do not exist are ignored.     </description> </property>  The problem is that the space after the comma causes the second directory for storage to be "" /home/foo/dfs"" which is in a directory named <SPACE> which contains a sub-dir named ""home"" in the hadoop datanodes default directory.  This will typically cause the user's home partition to fill, but will be very hard for the user to understand since a directory with a whitespace name is hard to understand.  My proposed solution would be to trimLeft all path names from this and similar property after splitting on comma.  This still allows spaces in file and directory names but avoids this problem. ",conf
Representative mix of jobs for large cluster throughput benchmarking,The benchmarking load will consist of a set of map/reduce jobs of varying types and sizes. The mix of jobs will emulate observed user loads on large Hadoop clusters.,test
Support permission information in FileStatus,"In HADOOP-2288,  FileSystem API is changed to support access control.  FileStatus should also be changed to support permission information.",fs
include hadoop-default.html in subversion,"The hadoop-default.html file is included in releases, but is not included with other documents in subversion.  This makes publishing released documentation more difficult, since this file must be manually copied into place.  Thus I propose that this file be created by the ""docs"" target and stored in subversion with the other documentation.",documentation
Document the user-controls for intermediate/output compression via forrest,"We should document the user-controls for compressing the intermediate and job outputs, including the types (record/block) and the various codecs in the hadoop website via forrest (mapred_tutorial.html).",documentation
Lzo compression compresses each write from TextOutputFormat,"Outputting with TextOutputFormat and Lzo compression generates a file such that each key, tab delimiter, and value are compressed separately.","io,native"
HADOOP-2185 breaks compatibility with hadoop-0.15.0,"HADOOP-2185 removed the following configuration parameters:  {noformat} dfs.secondary.info.port dfs.datanode.port dfs.info.port mapred.job.tracker.info.port tasktracker.http.port {noformat}  and changed the following configuration parameters: {noformat} dfs.secondary.info.bindAddress dfs.datanode.bindAddress dfs.info.bindAddress mapred.job.tracker.info.bindAddress mapred.task.tracker.report.bindAddress tasktracker.http.bindAddress {noformat}  without a backward-compatibility story.  Lots are applications/cluster-configurations are prone to fail hence, we need a way to keep things working as-is for 0.16.0 and remove them for 0.17.0.",conf
Micro-benchmark to measure read/write times through InputFormats,"The attached test writes/reads XGB to/from the default filesystem through SequenceFileInputFormat and TextInputFormat, using LzoCodec, GzipCodec, and without compression, using both block and record compression for SequenceFiles.  The following results using 10GB of data through RawLocalFileSystem with 5 word keys, 20 word values (as generated by RandomTextWriter with the same seed for each file) are pretty stable:  Writes: || Format || Compression || Type || Time (sec) || Filesize (bytes) || | SEQ | LZO | BLOCK | 318 | 8 604 288 397 | | SEQ | LZO | RECORD | 367 | 11 689 969 413 | | SEQ | ZIP | BLOCK | 929 | 2 827 697 769 | | SEQ | ZIP | RECORD | 1737 | 9 324 730 365 | | SEQ |  |  | 201 | 11 282 745 683 | | TXT | LZO |  | 742 | 12 671 065 769 | | TXT | ZIP |  | 1320 | 2 597 397 680 | | TXT |  |  | 392 | 10 818 058 643 |  Reads: || Format || Compression || Type || Time (sec) || | SEQ | LZO | BLOCK | 150 | | SEQ | LZO | RECORD | 281 | | SEQ | ZIP | BLOCK | 155 | | SEQ | ZIP | RECORD | 548 | | SEQ |  |  | 209 | | TXT | LZO |  | 620 | | TXT | ZIP |  | 355 | | TXT |  |  | 284 |   Of note: - Lzo compressed TextOutput is larger than the uncompressed output (HADOOP-2402); lzop cannot read it. - Zip compression is expensive. Short values are responsible for the unimpressive compression for record-compressed SequenceFiles. - TextInputFormat is slow (HADOOP-2285). TextOutputFormat also looks suspect.","fs,test"
Make EC2 image independent of Hadoop version,"Instead of building a new image for each released version of Hadoop, install Hadoop on instance start up. Since it is a small download this would not add significantly to startup time. Hadoop releases would be mirrored on S3 for scalability (and to avoid bandwidth costs). The version to install would be found from the instance metadata - this would be a download URL.   More generally, the instance could retrieve a script to run on start up from a URL specified in the metadata. The script would install and configure Hadoop, but it could be extended to do cluster-specific set up.",contrib/cloud
Make EC2 cluster nodes more independent of each other,"The cluster start up scripts currently wait for each node to start up before appointing a master (to run the namenode and jobtracker on), and copying private keys to all the nodes, and writing the private IP address of the master to the hadoop-site.xml file (which is then copied to the slaves via rsync). Only once this is all done is hadoop started on the cluster (from the master). This can fail if any of the nodes fails to come up, which can happen as EC2 doesn't guarantee that you get a cluster of the size you ask for (I've seen this happen).  The process would be more robust if each node was told the address of the master as user metadata and then started its own daemons. This is complicated by the fact that the public DNS alias of the master resolves to a public IP address so cannot be used by EC2 nodes (see http://docs.amazonwebservices.com/AWSEC2/2007-08-29/DeveloperGuide/instance-addressing.html). Instead we need to use a trick (http://developer.amazonwebservices.com/connect/message.jspa?messageID=71126#71126) to find the private IP, and what's more we need to attempt to resolve the private IP in a loop until it is available since the DNS will only be set up after the master has started.  This change will also mean the private key doesn't need to be copied to each node, which can be slow and has dubious security. Configuration can be handled using the mechanism described in HADOOP-2409.",contrib/cloud
Add support for larger EC2 instance types,"Need to configure Hadoop to exploit the resources available on larger instance types: 64bit, extra CPUs, larger memory. See http://docs.amazonwebservices.com/AWSEC2/2007-08-29/DeveloperGuide/instance-types.html",contrib/cloud
Use exit code to detect normal errors while excuting 'ls' in Local FS,"Local FileSystem runs {{ls -ld}} find file permissions, owner and group for a file. Currently it parses message in the exception to check if the command returned an error due to expected conditions like missing file.   HADOOP-2344 add interface to get error code returned by the external process. Local FS should use that. ",fs
Release JDiff report of changes between different versions of Hadoop,"Similar to LUCENE-1083, it would be useful to report javadoc differences (ala [JDiff|http://www.jdiff.org/]) between Hadoop releases.",documentation
lzop compatible CompressionCodec,"LzoCodec currently outputs at most {{io.compression.codec.lzo.buffersize}} (default 64k)- less the compression overhead- bytes per write (HADOOP-2402) in the following format:  {noformat} [uncompressed block length(32)] [compressed block length(32)] [compressed block] {noformat}  lzop (lzo-backed command-line utility) writes blocks in the following format:  {noformat} [uncompressed block length(32)] [compressed block length (32)] [Adler-32|CRC-32 checksum of uncompressed block (32)] [Adler-32|CRC-32 checksum of compressed block (32)] [compressed block] {noformat}  There's an additional ~32 byte header to the file. I don't know of a standard, but the lzop source should suffice.  Since we're using "".lzo"" as the default extension, it's worth considering being compatible with lzop, but not necessarily for all lzo-compressed blocks. For example, SequenceFiles should use the existing LzoCodec format.","io,native"
TestLocalFileSystemPermission fails in mac,"In Mac, group name can be more than 8 characters, e.g. ""appserveradm"".  However, ""ls -ld"" truncates it to 8 characters, i.e. it shows ""appserve"" for the previous group name.  Therefore, the test fails since ""appserveradm"".equals(""appserve"") returns false.  Not sure whether similar problem exists in other OS.",fs
Test HDFS File Permissions,This jira is intended to provide junit tests to HADOOP-1298.,test
MapFile.get on HDFS in TRUNK is WAY!!! slower than 0.15.x,"Stall happens down in SequenceFile in the first call to getPos inside readRecordLength.  I tried the johano patch from HADOOP-2172 that restores the positional cache but that didn't seem to be the issue here.  Here is data to support my assertion.  I wrote a little program to make a MapFile of 1M records.  I then did 1M random reads from same file.  Below are timings from a 0.15.0 and TRUNK as of this afternoon run.  0.15.x branch:  {code} [stack@aa0-000-12 branch-0.15]$ ./bin/hadoop org.apache.hadoop.io.TestMapFile .07/12/15 01:29:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 07/12/15 01:29:02 INFO io.TestMapFile: Writing 1000000 rows to testMapFileRandomAccess 07/12/15 01:32:04 INFO io.TestMapFile: Writing 1000000 records took 182009ms 07/12/15 01:32:04 INFO io.TestMapFile: Reading 1000000 random rows 07/12/15 01:48:02 INFO io.TestMapFile: Reading 1000000 random records took 958243ms Time: 1,140.652 OK (1 test) {code}  For the below test using TRUNK r604352, I amended the test so it output a log message every 100k reads:  {code} [stack@aa0-000-12 hadoop-trunk]$ ./bin/hadoop org.apache.hadoop.io.TestMapFile .07/12/15 01:56:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 07/12/15 01:56:34 INFO io.TestMapFile: Writing 1000000 rows to testMapFileRandomAccess 07/12/15 01:59:38 INFO io.TestMapFile: Writing 1000000 records took 183986ms 07/12/15 01:59:38 INFO io.TestMapFile: Reading 1000000 random rows ......... {code}  After 20 minutes it still hadn't printed out the 'read 100k messages' (I had to leave -- will fill in final figures later)",io
"HQL tutorial for SELECT, CREATE, INSERT doesnt work on the HQL test db",http://shell.hadoop.co.kr/PHPClient.php  only thing that works is the HELP command  http://wiki.apache.org/lucene-hadoop/Hbase/HbaseShell,documentation
Unit test failed: org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner,Unit test  failed on Linux:  Test: org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner  junit.framework.ComparisonFailure: expected:<users> but was:<>   org.apache.hadoop.fs.TestLocalFileSystemPermission.testLocalFSsetOwner(TestLocalFileSystemPermission.java:130)  Standard Output  foo: rw-r--r-- bar: rw-r--r-- ,fs
TestHDFSServerPorts fails.,This might be because I already have Namenode running on my machine. Its better if the unit tests could tolerate another DFS instance running on the same machine.  Otherwise we might get used to seeing unit test failures and miss the new failures.  I will attach the test output. ,test
Eclipse plug-in build.xml issue,The build.xml for the Eclipse plug-in has a bad reference to the generated hadoop-core-*.jar and prevents the build from succeeding. ,build
Add a 'forrest.home' property for the 'docs' target in build.xml,"I propose we add a *forrest.home* property which is used as the base for the Apache Forrest installation for the _docs_ target, currently it relies on forrest being on the PATH.  $ ant -Dforrest.home=<forrest installation> docs",documentation
Running 'ant docs tar' includes src/docs/build in the resulting tar file,src/docs/build (which is where forrest puts the built site) should not be included in the tar in this location.  It's already included under the top level docs directory.,build
"Configuration should trim property names and accept decimal, hexadecimal, and octal numbers","I suggest two improvements in reading configuration: - Suppose we have the following property in a conf file. {code} <property> <name> testing.property</name> <value>something</value> </property> {code} Try to get it by {code} Configuration conf = new Configuration(); String value = conf.get(""testing.property""); //value == null here {code} We will get null since there is an eol in  {code} <name> testing.property</name> {code} I suggest to trim all property names.  - I also suggest configuration to accept decimal, hexadecimal, and octal numbers (e.g. 011 is 9, 0xA is 10) It can be easily done by replacing Integer.parseInt(...) with Integer.decode(...) in Configuration.getInt(...), similarly, in Configuration.getLong(...).",conf
"MiniMRCluster does not utilize multiple local directories in ""mapred.local.dir""","My hadoop-site.xml specifies 4 local directories {code} <property>   <name>mapred.local.dir</name>   <value>${hadoop.tmp.dir}/mapred/local1, ${hadoop.tmp.dir}/mapred/local2,           ${hadoop.tmp.dir}/mapred/local3, ${hadoop.tmp.dir}/mapred/local4</value> </property> {code} and I am looking at MiniMRCluster.TaskTrackerRunner  There are several things here: # localDirBase value is set to {code} ""/tmp/h/mapred/local1, /tmp/h/mapred/local2, /tmp/h/mapred/local3, /tmp/h/mapred/local4"" {code} and I get a hierarchy of directories with commas and spaces in the names.  I think this was not designed to work with multiple dirs. # Further down, all new directories are generated with the same name {code}         File ttDir = new File(localDirBase,                                Integer.toString(trackerId) + ""_"" + 0); {code} So in fact only one directory is created. I think the intension was to have i instead of 0 {code}         File ttDir = new File(localDirBase,                                Integer.toString(trackerId) + ""_"" + i); {code} # On windows MiniMRCluster.TaskTrackerRunner in this case throws an IOException,  which is silently ignored by all but the TestMiniMRMapRedDebugScript   MiniMR tests. {code} java.io.IOException: Mkdirs failed to create  /tmp/h/mapred/local1, /tmp/h/mapred/local2, /tmp/h/mapred/local3, /tmp/h/mapred/local4/0_0   org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.<init>(MiniMRCluster.java:124)   org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:293)   org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:244)   org.apache.hadoop.mapred.TestMiniMRClasspath.testClassPath(TestMiniMRClasspath.java:163)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:478)   org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:344)   org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196) {code}  I am marking it as ""Major"" because we actually do not test multiple local directories. Looks like it was introduced rather recently by HADOOP-1819.",test
WritableUtils.clone should take Configuration rather than JobConf,"I'd like to use WritableUtils.clone but I'm not in a mapred context so don't have a JobConf to hand.  I do have a Configuration.   Looking inside the clone implementation, it doesn't need JobConf; a Configuration will do.",io
EC2 termination script should support termination by group,"The termination script currently terminates all hadoop instances (after user confirmation). It is therefore not suitable when running multiple, independent clusters on EC2. This change would make it possible to run independent clusters in separate groups and terminate them independently.",contrib/cloud
Automate EC2 DynDNS setup,Use the DynDNS webservice (https://www.dyndns.com/developers/specs/syntax.html) to automatically set up DNS for the EC2 cluster master node. If no DynDNS credentials are set (in hadoop-ec2-env.sh) then prompt the user to set up DNS manually (the existing behaviour).,contrib/cloud
Unit test fails on Windows: TestCopyFiles.testCopyFromLocalToLocal,"Unit test fails on Windows: org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal  Exception:  java.io.FileNotFoundException: C:/hudson/workspace/Hadoop-WindowsTest/trunk/build/test/data/destdat/three/three/7695082211392925393   org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:144)   org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:117)   org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:274)   org.apache.hadoop.fs.FileSystem.open(FileSystem.java:262)   org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:144)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:238)  Standard Error: With failures, global counters are inaccurate; consider running with -i Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)   org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)   org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)   org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)   org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)   org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:235)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)  Waiting for the Mini HDFS Cluster to start... With failures, global counters are inaccurate; consider running with -i Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)   org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)   org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)   org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)   org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)   org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:283)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)  Waiting for the Mini HDFS Cluster to start... ",fs
Unit test fails on Windows: TestCopyFiles.testCopyFromLocalToDfs,"Unit test failed on Windows: org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs  Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /destdat/one/eight/3479184736143758567   org.apache.hadoop.dfs.NameNode.open(NameNode.java:234)   org.apache.hadoop.ipc.RPC$Server.call(RPC.java:401)   org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)   org.apache.hadoop.ipc.Client.call(Client.java:509)   org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:198)   org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)   org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)   org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)   org.apache.hadoop.dfs.$Proxy0.open(Unknown Source)   org.apache.hadoop.dfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:839)   org.apache.hadoop.dfs.DFSClient$DFSInputStream.<init>(DFSClient.java:831)   org.apache.hadoop.dfs.DFSClient.open(DFSClient.java:263)   org.apache.hadoop.dfs.DistributedFileSystem.open(DistributedFileSystem.java:114)   org.apache.hadoop.fs.FileSystem.open(FileSystem.java:262)   org.apache.hadoop.fs.TestCopyFiles.checkFiles(TestCopyFiles.java:144)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:288)  Standard Error: With failures, global counters are inaccurate; consider running with -i Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)   org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)   org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)   org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)   org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)   org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToLocal(TestCopyFiles.java:235)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)  Waiting for the Mini HDFS Cluster to start... With failures, global counters are inaccurate; consider running with -i Copy failed: java.lang.IllegalArgumentException: length != 10(unixSymbolicPermission=drwxrwxrwx+)   org.apache.hadoop.fs.permission.FsPermission.valueOf(FsPermission.java:160)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:405)   org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.write(RawLocalFileSystem.java:429)   org.apache.hadoop.util.CopyFiles$FilePair.write(CopyFiles.java:133)   org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:956)   org.apache.hadoop.util.CopyFiles.setup(CopyFiles.java:727)   org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:475)   org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:550)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.fs.TestCopyFiles.testCopyFromLocalToDfs(TestCopyFiles.java:283)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:585)   junit.framework.TestCase.runTest(TestCase.java:154)   junit.framework.TestCase.runBare(TestCase.java:127)   junit.framework.TestResult$1.protect(TestResult.java:106)   junit.framework.TestResult.runProtected(TestResult.java:124)   junit.framework.TestResult.run(TestResult.java:109)   junit.framework.TestCase.run(TestCase.java:118)   junit.framework.TestSuite.runTest(TestSuite.java:208)   junit.framework.TestSuite.run(TestSuite.java:203)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)   org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)  Waiting for the Mini HDFS Cluster to start...",fs
NNBench should periodically report its progress,"When I run NNBench on a 100-node cluster, some map tasks fail with the error  message ""Task xx failed to report status for yy seconds. Killing!"". Map tasks should periodically reports its progress to prevent itself being killed.",test
ConcurrentModificationException in org.apache.hadoop.ipc.Server.Responder,"I was running hadoop on 800 machines and after running a couple of jobs, and running 100% of the maps of the current job, the JobTracker stopped responding - *all* tasktrackers were lost ... When I looked at the JT logs, these seemed alarming: 2007-12-26 19:18:30,185 WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException Following the above exception, I saw a whole lot of exceptions like: 2007-12-26 19:23:10,926 WARN org.apache.hadoop.ipc.Server: Call queue overflow discarding oldest call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@5a05f9, false, true, 1758) from 1.2.3.4:1234  From the number of exceptions to do with call queue overflow, it seemed like the jobtracker was not processing RPCs after it got the ConcurrentModificationException, and around that time the tasktrackers started getting timeouts on RPCs...  There were two occurrences of the ConcurrentModificationException but the first instance seemed to not have any effect on the call queue...  ",ipc
Set +x on contrib/*/bin/* in packaged tar bundle,"If you download a nightly, the hbase scripts are not executable.",scripts
sequence file does not detect corruption in key/value lengths,"From:   jason@attributor.com Subject: Re: question on Hadoop configuration for non cpu intensive jobs - 0.15.1 Date: December 28, 2007 7:20:11 AM PST To:   hadoop-user@lucene.apache.org  Our OOM was being caused by a damaged sequence data file. We had assumed that the sequence files had checksums, which appears to be in correct. The deserializer was reading a bad length out of the file and trying to allocate 4gig of ram.  ",io
Implement utility-tools for working with SequenceFiles,"It would be nice to implement a bunch of utilities to work with SequenceFiles:   * info (print-out header information such as key/value types, compression type/codec etc.)  * cat  * head/tail  * merge multiple seq-files into one  * ...  I'd imagine this would look like: {noformat} $ bin/hadoop seq -info /user/joe/blah.seq $ bin/hadoop seq -head -n 10 /user/joe/blah.seq {noformat} ",io
Add rat target to build,The ARAT tool should be run before each release: java -cp rat-0.5.1.jar rat.Report hadoop-x.y.z  It's available from: http://code.google.com/p/arat/  The output for 0.15.2 shows some missing license headers which must be fixed for 0.16.0: http://people.apache.org/~siren/hadoop-0.15.2-rc0/rat-hadoop-0.15.2.txt ,build
HADOOP-2344 introduced a javadoc warning,"{noformat}   [javadoc] /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/src/java/org/apache/hadoop/util/Shell.java:70: warning - @param argument ""Interval"" is not a parameter name. {noformat} ",documentation
error stream handling in Shell executor ,"Fix a couple of issues while handling error stream in Shell (added in HADOOP-2344) :  # fix typo in {{System.getProperty(""line.seperator"")}}, currently it adds ""null"" instead of ""\n"". # completed is not set to {{true}} when a process exits with an error. # In normal error case, it reads errMsg (to create IOException) before waiting for errThread to complete, which results in in consistent error message. I will attach a patch.",util
DFS User Guide,"We need a user guide for DFS on the lines of http://lucene.apache.org/hadoop/docs/r0.15.1/mapred_tutorial.html . This could be the starting point for new users.  Though this is marked for 0.16, I think it could go in even after feature freeze date.",documentation
File manager frontend for Hadoop DFS (with proof of concept).,"I had problems classifying this, but since it's not an improvement and neither a task, I thought I'd put it under ""wishes"". I like command line, but using hadoop fs -X ... leaves my fingers hurt after some time. I though it would be great to have a file manager-like front end to DFS. So I modified muCommander (Java-based) a little bit and voila -- it works _great_, especially for browsing/ uploading and deleting stuff.  I uploaded the binary and WebStart-launchable version here:  http://project.carrot2.org/varia/mucommander-hdfs  Look at screenshots, they will give you a clue about how it works. I had some thoughts about publishing the source code -- muCommander is GPLed... so I guess it can't reside in Hadoop's repository anyway, no matter what we do. If you need sources, let me know.  Finally, a few thoughts stemming from the coding session:      *  DF utility does not work under Windows. This has been addressed recently on the mailing list (HADOOP-33), so it's not a big issue I guess.      * I support the claim that it would be sensible to introduce a client interface to DFS and provide two implementations -- one with intelligent spooling on local disk (using DF) and one with some simpler form of spooling (in /tmp for example). Note the funky shape of the upload chart above resulting from delay between spooling and chunk upload. I don't know if this can be worked around in any way.      * Incompatible protocol version causes exceptions. Since the protocol changes quite frequently (isn't it version 20 at the moment?), some way of choosing the connection protocol to Hadoop and keeping the most recent versions around would be very useful for external clients.",io
make build process compatible with Ant 1.7.0,"This section of build.xml behaves differently between Ant 1.6.5 and Ant 1.7.0:  {code:xml}     <touch datetime=""01/25/1971 2:00 pm"">       <fileset dir=""${conf.dir}"" includes=""**/*.template""/>       <fileset dir=""${contrib.dir}"" includes=""**/*.template""/>     </touch> {code}  In Ant 1.6.5, if the fileset is empty (which it is if you're building from a released tar.gz distribution) then this section silently passes.  In Ant 1.7.0, if the fileset is empty, the following error is raised and the build stops: {quote} BUILD FAILED /home/ndaley/Downloads/hadoop-0.15.1/build.xml:151: Specify at least one source--a file or resource collection. {quote}  ",build
remove use of 'magic number' in build.xml,"build.xml has a call to touch task as   <touch datetime=""01/25/1971 2:00 pm""> it should be changed to  <touch millis=""0"">   It will avoid using magic values like ""01/25/1971 2:00 pm"" and make the script easier to understand ",build
hadoop-env.sh needs finer granularity,"We often configure our HADOOP_OPTS on the name node to have JMX running so that we can do JVM monitoring.  But doing so means that we need to edit this file if we want to run other hadoop commands, such as fsck.  It would be useful if hadoop-env.sh was refactored a bit so that there were different and/or cascading HADOOP_OPTS dependent upon which process/task was being performed.  ",scripts
"globPaths does not support {ab,cd} as it claims to","Olga reports:   According to 0.15 documentation, FileSystem::globPaths supports {ab,cd} matching. However, when I tried to use it with pattern /data/mydata/{data1,data2} I got no results even though I could find the individual files. ",fs
Remove deprecated FileSystem#listPaths(),"FileSystem#listPaths() has been deprecated for a few releases, and we should now remove it, upgrading everything to use FileSystem#listStatus(). ",fs
need FileSystem#globStatus method,"To remove the cache of FileStatus in DFSPath (HADOOP-2565) without hurting performance, we must use file enumeration APIs that return FileStatus[] rather than Path[].  Currently we have FileSystem#globPaths(), but that method should be deprecated and replaced with a FileSystem#globStatus().  We need to deprecate FileSystem#globPaths() in 0.16 in order to remove the cache in 0.17. ",fs
add FileSystem#getHomeDirectory() method,"The FileSystem API would benefit from a getHomeDirectory() method.  The default implementation would return ""/user/$USER/"".  RawLocalFileSystem would return System.getProperty(""user.home"").  HADOOP-2514 can use this to implement per-user trash. ",fs
javac generates a warning in test/o.a.h.io.FileBench,FileBench generates the following warning in trunk:  {quote}     [javac] {...}/src/test/org/apache/hadoop/io/FileBench.java:341: warning: [unchecked] unchecked cast     [javac] found   : java.lang.Enum     [javac] required: T     [javac]       set.add((T)fullmap.get(c).get(s));     [javac]                                    ^ {quote}  This should be suppressed.,test
bugs in mapred tutorial,"Sam Pullara sends me: {noformat} Phu was going through the WordCount example... lines 52 and 53 should have args[0] and args[1]:  http://lucene.apache.org/hadoop/docs/current/mapred_tutorial.html  The javac and jar command are also wrong, they don't include the directories for the packages, should be:  $ javac -classpath ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-core.jar -d classes WordCount.java  $ jar -cvf /usr/joe/wordcount.jar WordCount.class -C classes .  {noformat}",documentation
Shell command ls should support option -d,"Shell command ""ls"" currently does not support option -d. Meanwhile the shell supports command ""stat"", but needs to supply the print format specifying which attributes to print on the screen. It's hard to use and in addition it does not support printing the file permissions. So I would like to propose that  the shell supports ""ls -d"" and deprecates the command ""stat"".",fs
Potential Eclipse plug-in UI loop when editing location parameters,"The UI might enter an infinite loop, when propagating parameters asynchronously. Some functions are not yet implemented",contrib/eclipse-plugin
add SequenceFile.createWriter() method that takes block size as parameter,"Currently it is not possible to create a SequenceFile.Writer using a block size other than the default.  The createWriter() method should be overloaded with a signature receiving block size as parameter should be added to the the SequenceFile class.  With all the current signatures for this method there is a significant code duplication, if possible the createWriter() methods  should be refactored to avoid such duplication.",io
Patch process should determine issue number differently,"When an email comes in, the processHadoopPatchEmail.sh script (http://svn.apache.org/viewvc/lucene/hadoop/nightly/processHadoopPatchEmail.sh?view=markup) determines the issue number based on the first occurance of ""HADOOP-#*"".  This picks the wrong issue number when there is a different issue number in the synopsis of the issue (such as HADOOP-2570).  Instead, the script should look for ""Key: HADOOP-#*"" ",build
leading slash in mapred.task.tracker.report.bindAddress,TaskTracker incorrectly sets mapred.task.tracker.report.bindAddress with a slash in front of the host:port pair. This described in more details here: [Deveraj|http://issues.apache.org/jira/browse/HADOOP-2404#action_12554551] and [Konstantin|http://issues.apache.org/jira/browse/HADOOP-2404#action_12554859],conf
Reading sequence file consumes 100% cpu with maximum throughput being about 5MB/sec per process," I did some tests on the throughput of scanning block-compressed sequence files. The sustained throughput was bounded at 5MB/sec per process, with the cpu of each process maxed at 100%.  It seems to me that the cpu consumption is too high and the throughput is too low for just scanning files.  ",io
'bin/hadoop fs -help' does not list file permissions commands.,"'{{bin/hadoop fs -help}}' does not list chmod, chown, and chgrp. But '{{bin/hadoop fs -help chmod}}' etc  work as expected.",fs
RawLocalFileStatus is badly handling URIs,"as a result, files with special characters (that get encoded when translated to URIs) are badly handled using a local filesystem.  {{new Path(f.toURI().toString()))}} should be replaced by {{new Path(f.toURI().getPath()))}}  IMHO, each call to {{toURI().toString()}} should be considered suspicious. There's another one in the class CopyFiles at line 641.",fs
"If local file included a '%' character in a file name, we can't copy to dfs becuase RawLocalFileSystem.getPath() returns urlencoded '%25'","{code} FileStatus fileStatus = new RawLocalFileStatus(new File(""udanax/Ageha100%.html""), getDefaultBlockSize()); LOG.info(fileStatus.getPath()); {code}  Log : file:/root/workspace/hadoop/udanax/Ageha100%25.html",fs
Add rat report to the Hudson patch process,"Sami Siren suggests: ""How about adding [the rat header license] check to the patch queue? That way users submitting patches would become responsible for adding proper headers""  Good suggestion.  I think the Rat tool needs some enhancements for this to work reliably, for instance: * an exit code that indicates failure/success * greater customization of which files need and don't need headers  Rat is entering incubation.  Once there, these improvements can be filed.  ",build
SortValidator broken with fully-qualified working directories,"The sort validator is broken by HADOOP-2567.  In particular, it no longer works when DistributedFileSystem#getWorkingDirectory() returns a fully-qualified path. ",test
Design and Implement a Test Plan to support appends to HDFS files,"HADOOP-1700 describes the design of supporting appends to HDFS files. This feature needs extensive testing, especially because the design explicitly analyzes many failure scenarios. A detailed test plan and test cases are needed to make this feature a reality.",test
"Buffer class' toString method should accept a code name for ""true"" utf-8 codeName "," Currently, if one call toString(""UTF-8""), a String object is created using Java's converion code. That does not work properly for some rare but still true utf-8 bytes. Hadoop has its own utf-8 conversion  code for string serialization/deserialization. The same code should be used here.",record
lzop-compatible CompresionCodec,The current lzo codec is not compatible with the standard .lzo file format used by lzop.,io
Hudson is not picking up newly submitted patches and is not running patch builds,Hudson is not picking up newly submitted patches and putting them in to the Hadoop-Patch build queue.  Consequently no patches are getting built.,build
Hadoop-Nightly does not run contrib tests if core tests fail,"Unlike Hadoop-Patch, Hadoop-Nightly does not run the contrib tests if the core tests fail. Even if the core tests fail, it is useful to know if there has been a regression in contrib.",build
"NullPointerException in TaskRunner.java when system property ""hadoop.log.dir"" is not set","Currently, NullPointerException exception is thrown on line 321 in TaskRunner.java when system property ""hadoop.log.dir"" is not set.  Instead of a NullPointerException exception, I expected a default value for ""hadoop.log.dir"" to be used, or to see a more meaningful error message that could have helped me figure out what was wrong (like, telling me that I needed to set ""hadoop.log.dir"" and how to do so).  Here is one instance of such exceptions:  WARN mapred.TaskRunner: task_200801181719_0001_m_000000_0 Child Error java.lang.NullPointerException   java.io.File.<init>(File.java:222)   org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:321)",conf
RegEx support for expressing datanodes in the slaves conf files,"It will be very handy if datanodes and task trackers can be expressed in the slave conf file as regular expressions.  For example,   machine[1-200].corp machine[400-679].corp  ","conf,scripts"
Adding support into build.xml to build a special hadoop jar file that has the MiniDFSCluster and MiniMRCluster classes among others necessary for building and running the unit tests of Pig on the local mini cluster,"In order to build Pig and run its unit tests with ant, Pig needs to be built and run against a hadoop jar file that not only has the MiniDFSCluster and MiniMRCluster classes but also has classes from several 3rd party libraries.  I added a target to the build.xml file of Hadoop for that purpose (see the ""Compile code for Pig"" section in the attached build.xml).   I also attached the resulted hadoop jar with this report. ",build
Problems with Hudson, Build 1657 failed after 9 minutes and failed to run the tests Build 1656 completed normally (build failed due to findbugs errors)  When I try to look at the test results for builds 1655 and 1658 I get:: {code} HTTP Status 500 -  type Exception report  message  description The server encountered an internal error () that prevented it from fulfilling this request.  exception  javax.servlet.ServletException: Servlet execution threw an exception  hudson.security.ChainedServletFilter$1.doFilter(ChainedServletFilter.java:52)  hudson.security.UnwrapSecurityExceptionFilter.doFilter(UnwrapSecurityExceptionFilter.java:28)  hudson.security.ChainedServletFilter$1.doFilter(ChainedServletFilter.java:55)  org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:166)  hudson.security.ChainedServletFilter$1.doFilter(ChainedServletFilter.java:55)  org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)  hudson.security.ChainedServletFilter$1.doFilter(ChainedServletFilter.java:55)  hudson.security.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:89)  hudson.security.ChainedServletFilter$1.doFilter(ChainedServletFilter.java:55)  hudson.security.ChainedServletFilter.doFilter(ChainedServletFilter.java:44)  hudson.security.HudsonFilter.doFilter(HudsonFilter.java:69)  root cause  java.lang.OutOfMemoryError: Java heap space  note The full stack trace of the root cause is available in the Apache Tomcat/5.5.25 logs. Apache Tomcat/5.5.25 {code},build
io.file.buffer.size should default to a value larger than 4k,"Tests using HADOOP-2406 suggest that increasing this to 32k from 4k improves read times for block, lzo compressed SequenceFiles by over 40%; 32k is a relatively conservative bump.",conf
Doap still refers to lucene and nagoya,The DOAP file at  http://hadoop.apache.org/core/doap.rdf  still mentions lucene.apache.org and nagoya.apache.org,documentation
Fix trivial typeos in EC2 scripts,"The ec2-run-instances needs a capital K not a lower case k flag.  Index: src/contrib/ec2/bin/create-hadoop-image =================================================================== --- src/contrib/ec2/bin/create-hadoop-image     (revision 608611) +++ src/contrib/ec2/bin/create-hadoop-image     (working copy) @@ -11,7 +11,7 @@  AMI_IMAGE=`ec2-describe-images -a | grep fedora-core4-base | awk '{print $2}'`   echo ""Starting a fedora core base AMI with ID $AMI_IMAGE."" -OUTPUT=`ec2-run-instances $AMI_IMAGE -k $KEY_NAME` +OUTPUT=`ec2-run-instances $AMI_IMAGE -K $KEY_NAME`  BOOTING_INSTANCE=`echo $OUTPUT | awk '{print $6}'` ",contrib/cloud
Review and document '_' prefix convention in input directories,"We use files and directories prefixed with '_' to store logs, metadata and other info that might be useful to the owner of a job within the output directory.  The standard input methods then ignore such files by default.  HADOOP-2391 lead to some discussion of the '_' convention in output directories.  No all developers input formats are supporting this.  We should review the convention and document it well so that future input methods support it.  Or we should come up with an alternate approach.    My hope is that after some discuss we will close this bug by creating a documentation patch explaining the convention.  It sounds like the convention is implemented via some input filter classes.  We should discuss if this generic solution is helping or obscuring the intent of the convention.  Perhaps we should just have a non-configurable filter, so '_' prefixed files are treated like '.' prefixed files by most unix tools.  ",documentation
Update HOD in Hadoop 0.16,"Between the time of submission of the HOD patch to Hadoop SVN and now, there have been a bunch of changes made to local repositories of HOD. This patch is to synchronize the source changes. Post this patch, HOD will only be developed out of Hadoop SVN.",contrib/hod
Distcp truncates some files when copying,"We used distcp to copy ~100 TB of data across two clusters ~1400 nodes each.  Command used (it was run on the src cluster): hadoop distcp -log /logdir/logfile hdfs://src-namenode:8600//src-dir-1 hdfs://src-namenode:8600//src-dir-2 ... hdfs://src-namenode:8600//src-dir-n hdfs://tgt-namenode:8600//dst-dir  Distcp completed without errors, but when we checked the file sizes on the src and tgt clusters, we noticed differences in file sizes for 9 files (~6 GB).  src-file-1 666762714 bytes -> tgt-file-1 134217728 bytes src-file-2 673791814 bytes -> tgt-file-2 536870912 bytes src-file-3 692172075 bytes -> tgt-file-3 0 bytes  All target files are truncated at block boundaries (some have 0 size).   I looked at the log files, and noticed a few things:  1. There are 31059 log files (same as the number of Maps the job had).  2. 246 of the log files are non-empty.  3. All non-empty log files are of the form:  SKIP: hdfs://src-namenode/src-dir-a/src-file-x SKIP: hdfs://src-namenode/src-dir-b/src-file-y SKIP: hdfs://src-namenode/src-dir-c/src-file-z  4. All 9 files which were truncated were included in the log files as skipped files.  5. All 9 files were the last entry in their respective log files.  e.g. Non-empty logfile 1:  SKIP: hdfs://src-namenode/src-dir-a/src-file-x SKIP: hdfs://src-namenode/src-dir-b/src-file-y SKIP: hdfs://src-namenode/src-dir-c/src-file-z  <-- Truncated file  Non_empty logfile 2: SKIP: hdfs://src-namenode/src-dir-p/src-file-m SKIP: hdfs://src-namenode/src-dir-q/src-file-n  <-- Truncated file",util
Update HOD documentation,"Update the HOD documentation to provide some more details on usage, setup and configuration.",contrib/hod
"ab{5[6-9],[6-9][6-9]}.gz should not be treated as an illegal glob","Running  hadoop dfs -ls ""/xx/ab{5[6-9],[6-9][6-9]}.gz""  returns the following error message: ls: Illegal file pattern: Unexpected end of a group for glob ab{5[6-9],[6-9][6-9]}.gz at 20",fs
Compiler warnings in TestClusterMapReduceTestCase and TestJobStatusPersistency,"The following four warnings were introduced by HADOOP-1876. {code}     [javac] hadoop/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:26: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]       collector.collect(key, value);     [javac]                        ^     [javac] hadoop/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:42: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]         collector.collect(key, value);     [javac]                          ^     [javac] hadoop/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:26: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]       collector.collect(key, value);     [javac]                        ^     [javac] hadoop/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:42: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]         collector.collect(key, value);     [javac]                          ^     [javac] Note: Some input files use or override a deprecated API.     [javac] Note: Recompile with -Xlint:deprecation for details.     [javac] 4 warnings {code} ",test
docs link to lucene.apache.org,The forrest documentation still links to lucene.apache.org.  This should be updated to hadoop.apache.org.,documentation
HOD fails to allocate nodes if the hadoop version is a string ( e.g Hadoop full ),"HOD is not able to allocate nodes if the hadoop version  is a string ( like full ) . The ring master logs are  given below. Probably we can improve the error logging for this  error too.  RING MASTER LOG:- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ [2008-01-29 05:44:55,420] CRITICAL/50 ringMaster:571 - Exception in  creating Hdfs and Map/Reduce descriptor objects:        <type 'exceptions.TypeError'> int() argument must be a string or a number, not 'NoneType'. [2008-01-29 05:44:55,431] CRITICAL/50 ringMaster:956 - Traceback (most  recent call last):    File  ""###Path to Ringmaster directory###/ringMaster.py"",  line 947, in main      rm = RingMaster(cfg, log)    File  ""###Path to Ringmaster directory###/ringMaster.py"",  line 556, in __init__      hdfs = Hdfs(hdfsDesc, workDirs, 0, version=int(hadoopVers['minor'])) TypeError: int() argument must be a string or a number, not 'NoneType'  [2008-01-29 05:44:55,437] ERROR/40 ringmaster:332 - bin/ringmaster  failed to start.<type 'exceptions.Exception'> int() argument must be a  string or a number, not 'NoneType'. Stack trace follows: Traceback (most recent call last):    File ""ringmaster"", line 328, in <module>      ret = main(ringMasterOptions,log)    File  ""###Path to Ringmaster directory###/ringMaster.py"",  line 957, in main      raise Exception(e) Exception: int() argument must be a string or a number, not 'NoneType' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",contrib/hod
Text is not subclassable because set(Text) and compareTo(Object) access the other instance's private members directly,"Text objects should not access other Text objects private members directly. Both set(Text) and compareTo(Object) do. Because these two methods access private members of the other object, Text is not subclassable. Either these two methods should be modified to use the accessors that are already available, or Text should be declared as a final class, because as it exists today it is not subclassable.",io
SequenceFile and MapFile should either be subclassable or they should be declared final,Neither SequenceFile nor MapFile are currently subclassable as there are no accessor methods to their private members or member functions. Either protected accessor methods should be added or the member variables should be declared as protected instead of private.  OR  both SequenceFile and MapFile should be declared as final.,io
Modify HOD to work with changes mentioned in HADOOP-2404,HADOOP-2404 proposes a change to configuration variable names introduced in HADOOP 0.16. These are used by HOD. This bug is to modify HOD to use the new names. Both these bugs must be committed together for things to work.,contrib/hod
Better documentation of controls for memory limits on hadoop daemons and Map-Reduce tasks,We have had a spate of questions about memory usage of hadoop daemons and Map-Reduce jobs and how to configure them.   We should better document *mapred.child.java.opts* in the Map-Reduce tutorial and cluser_setup.html and link to http://hadoop.apache.org/core/docs/r0.15.3/cluster_setup.html#Configuring+the+Environment+of+the+Hadoop+Daemons. ,"documentation,scripts"
[HOD] No way to set HADOOP_OPTS environment variable to the Hadoop daemons through HOD,"For purposes of performance tuning it should be possible to set some environment variables that Hadoop honors before launching the Hadoop daemons. For e.g. to set the heap size of the JVM, we should set the HADOOP_HEAPSIZE variable. These can be configured through hodrc using the gridservice-mapred.envs environment variable. This works for everything *except* HADOOP_OPTS, which is also useful, for e.g. to pass in Garbage collection parameters to the JVM, like -XX:+UseParallelGC. This is because HOD tries to set HADOOP_OPTS from the gridservice-mapred.java-opts variable, but it does not read the java-opts variable anywhere properly.  ",contrib/hod
TestNNThroughputBenchmark should not used a fixed http port,HADOOP-2601 only fixed the main namenode port. It also needed to fix the namenode http port. I get failures like:  {quote} Testcase: testNNThroughput took 1.048 sec         Caused an ERROR Address already in use java.net.BindException: Address already in use         at java.net.PlainSocketImpl.socketBind(Native Method)         at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:359)         at java.net.ServerSocket.bind(ServerSocket.java:319)         at java.net.ServerSocket.<init>(ServerSocket.java:185)         at org.mortbay.util.ThreadedServer.newServerSocket(ThreadedServer.java:391)         at org.mortbay.util.ThreadedServer.open(ThreadedServer.java:477)         at org.mortbay.util.ThreadedServer.start(ThreadedServer.java:503)         at org.mortbay.http.SocketListener.start(SocketListener.java:203)         at org.mortbay.http.HttpServer.doStart(HttpServer.java:761)         at org.mortbay.util.Container.start(Container.java:72)         at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:182)         at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:294)         at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:235)         at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:130)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:175)         at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:161)         at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:843)         at org.apache.hadoop.dfs.NNThroughputBenchmark.<init>(NNThroughputBenchmark.java:74)         at org.apache.hadoop.dfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:769)         at org.apache.hadoop.dfs.TestNNThroughputBenchmark.testNNThroughput(TestNNThroughputBenchmark.java:32) {quote},test
[HOD] Put in place unit test framework for HOD,"HOD does not have any unit tests in place currently. This issue is to decide on a framework that would be effective for our python code base. Something on the lines of pyUnit would be good. The fix should put in place any dependencies needed for running the tests, and should also define some tests that demonstrate how to write further tests. We would not be defining a complete unit test suite for the entire code base right now, but would incrementatlly add tests as changes are made.",contrib/hod
Web interface uses internal hostnames on EC2,"The web interface, for example http://$MASTER_HOST:50030/machines.jsp, uses internal hostnames when running on EC2.  This makes it impossible to access from outside EC2.  The slaves file has the public names listed.  Resolving a public name inside EC2 returns the private IP (which would reverse to the internal DNS name).",contrib/cloud
build scripts broken by moving hbase to subproject,The build scripts fail with hbase moving out.,build
hod/hodlib/Common/xmlrpc.py uses HodInterruptException without importing it,"And because of this no-import of HodInterruptException in hod/hodlib/Common/xmlrpc.py, HOD fails with an exception, when it gets interrupted by user(e.g ^C) while it is performing any xmlrpc request. One line fix.",contrib/hod
Race condition in ipc.Server prevents responce being written back to client.,"I encountered a race condition in ipc.Server when writing the response back to the socket. Sometimes the write SelectKey is being canceled when it should not be, and thus the full response never gets written. This results in clients timing out on the socket while waiting for the response.  I am attaching a unit test that demonstrates the problem. It follows closely the TestIPC test, however the socket output buffer is set smaller than the result being sent back, so that partial writes occur. I also put random sleep in the client to help provoke the race condition.  On my machine this fails over half of the time.  Looking at the code in ipc.Server.java. The problem is manifested in Responder.doAsyncWrite(). If I comment out the key.cancel() line, then everything works fine.   So we need to identify when to safely cancel the key.  I tried the following:  {noformat}     private void doAsyncWrite(SelectionKey key) throws IOException {       Call call = (Call)key.attachment();       if (call == null) {         return;       }       if (key.channel() != call.connection.channel) {         throw new IOException(""doAsyncWrite: bad channel"");       }       if (processResponse(call.connection.responseQueue)) {           synchronized(call.connection.responseQueue) {               if (call.connection.responseQueue.size() == 0) {                   LOG.info(""Cancelling key for call ""+call.toString()+ "" key: ""+ key.toString());                   key.cancel();          // remove item from selector.               } else {                   LOG.warn(""NOT REALLY DONE: ""+call.toString()+ "" key: ""+ key.toString());               }           }       }     } {noformat}  And this does catch some of the cases (EG, the LOG.warn message gets hit), but i still hit the race condition. ",ipc
"hod list command throws python exception, when clusters.state file contains some directory path which actually does not exist ","hod list command throws following python exception when clusters.state file contains some directory path which actually does not exits -: CRITICAL/50 hod:340 - op: list failed: <type 'exceptions.OSError'> [Errno 2] No such file or directory: '[Path to non-exstent directory'  Following are the steps to repro -: 1. Create directory tree cdir/clusterDir say at $HOME dir. 2. Run hod allocate as -: hod -c <confPath> -b 4 -o ""allocate ~/cdir/clusterDir 5""  3. rename cdir to cdir1. 4. Run hod list as -: hod -o list     hod will list  the allocated directories up to the the point it encounters non-existant path entry in ~/.hod/clusters.state file afer that hod will throw python exception and will stop Note -: There is more issue. if cdir exists but not clusrterDir. Then hod will create clusterDir (a new empty directory)     ",contrib/hod
hod alloation command sometimes fails with wrong exit code,"Hod client exit code sometimes exits with wrong exit code - 5 when ringmaster fails to start it should be exting with exiting 6. Run hod as -: hod -c <confpath> -b4 -o"" allocate ~/clusterDir 5"" -t ~/tars/tarfile. where tarfile is invalid file. Sometimes hod client exits with exit code 5 saying ""No job found, Ringmaster failed to run"" and when checked about are torque job the exit status it shows the correct value of 6. It seems that it is the timing issue b/w when job went into complete state and hod client checks for its status.   Note -: It is not always reproducible ",contrib/hod
For script option hod should exit with distinguishable exit codes for script code and hod exit code.,"For hod script option, the exit code should distinguishable between hod exit code and script exit code. e.g. If script command contains the streaming command at end and that fails due to input path not found, its value exit cod will 5 which overlaps with hod exit code 5 which means ""job execution failure"" It would hod throws some distinguishable exit codes  e.g For above examples 64 +5 =69 and we should this to get exact exit code of hod script command user should subtract 64 from exit code  ",contrib/hod
Replace org.apache.hadoop.io.Closeable with java.io.Closeable,"Since java.io.Closeable is provided in Java 1.5,  we should replace org.apache.hadoop.io.Closeable.",io
SetFile.Writer deprecated by mistake?,"The SetFile.Writer class is deprecated with the description ""pass a Configuration too"". I don't see any reason why that class should be deprecated, I assume the first constructor were supposed to be deprecated and the tag was misplaced.",io
"Fix javac warnings shown in ""ant test-core""","When running ""ant test-core"", javac shows the following warnings: {code}   [javac] Compiling 170 source files to /trunk/build/test/classes     [javac] /trunk/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:43: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]       collector.collect(key, value);     [javac]                        ^     [javac] /trunk/src/test/org/apache/hadoop/mapred/TestClusterMapReduceTestCase.java:59: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]         collector.collect(key, value);     [javac]                          ^     [javac] /trunk/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:43: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]       collector.collect(key, value);     [javac]                        ^     [javac] /trunk/src/test/org/apache/hadoop/mapred/TestJobStatusPersistency.java:59: warning: [unchecked] unchecked call to collect(K,V) as a member of the raw type org.apache.hadoop.mapred.OutputCollector     [javac]         collector.collect(key, value);     [javac]                          ^     [javac] Note: Some input files use or override a deprecated API.     [javac] Note: Recompile with -Xlint:deprecation for details.     [javac] 4 warnings {code}",test
Formatable changes log as html,"We could do a better job of generating useful release notes.  As a first step, I suggest we follow Lucene's lead and add html formatting to our CHANGES.txt.  An example output is here:  http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/trunk/build/docs/changes/Changes.html  This should then be included with our release documentation that is published on the website for each release.  After this is committed, the next step is for committers to agree on a CHANGES.txt convention that will flag major features/improvements (such as a well placed *).  Then the flagged Jira's can be further highlighted when generating the html.  Thoughts?",documentation
distcp creating a file instead of a target directory (with single file source dir),"Source file.   [knoguchi@~]$ hadoop dfs -ls a/b Found 1 items /user/knoguchi/a/b/c    <r 3>   8       2008-02-10 20:58  Ran distcp   1) run distcp hdfs://nn1:____/user/knoguchi/a/b   hdfs://nn2:____/user/knoguchi/newdir  2) run distcp hdfs://nn1:____/user/knoguchi/a/b/c hdfs://nn2:____/user/knoguchi/newfile  Result % run dfs -ls new\* /user/knoguchi/newdir   <r 3>   8       2008-02-11 04:59 /user/knoguchi/newfile  <r 3>   8       2008-02-11 05:06    For (1), I expected /user/knoguchi/newdir/c  (which is the case in version 0.14) Related? HADOOP-1499  ",util
"[HOD] Syslog configuration, syslog-address, does not work in HOD 0.4",Specify the parameter syslog-address as host:port in the ringmaster or hodring sections in a hod configuration file. Run a hod allocation. It fails with error code 5/6. The ringmaster fails to start. No logs are created.  The problem seems to be that we are passing in the syslog-address in a format different from what is expected to the hodLogger object.,contrib/hod
Need new Hadoop Core logo,We need a new Hadoop core logo.,documentation
Unit test fails on Linux: org.apache.hadoop.fs.TestDU.testDU,Unit test fails on Linux: org.apache.hadoop.fs.TestDU.testDU. This is a regression  Changes that went in yesterday were: HADOOP-2725. Modify distcp to avoid leaving partially copied files at the destination after encountering an error. HADOOP-2193. 'fs -rm' and 'fs -rmr' show error message when the target file does not exist.  Here is the error from the test run: org.apache.hadoop.util.Shell$ExitCodeException: /usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_i0ebq9/.nfs00000000004aa47f00001ba0': No such file or directory /usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_i0ebq9/.nfs00000000004aa48000001b9f': No such file or directory /usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_r3r1fo/.nfs00000000004aa48200001ba2': No such file or directory /usr/bin/du: cannot access `/home/xxxxxx/workspace/Hadoop-LinuxTest/trunk/build/test/mapred/system/distcp_r3r1fo/.nfs00000000004aa48300001ba1': No such file or directory    org.apache.hadoop.util.Shell.runCommand(Shell.java:161)   org.apache.hadoop.util.Shell.run(Shell.java:100)   org.apache.hadoop.fs.DU.getUsed(DU.java:53)   org.apache.hadoop.fs.TestDU.testDU(TestDU.java:62)   org.apache.hadoop.fs.TestDU.testDU(TestDU.java:71),fs
Remove deprecated classes in util,"The classes ShellUtil and ToolBase in 'util' need to removed, as they are deprecated.",util
"SimpleCharStream.getColumn(),  getLine() methods to be removed.","SimpleCharStream.getColumn(),  getLine() methods need to be removed, as they are deprecated",record
Remove deprecated NetUtils.getServerAddress,This deprecated method was introduced in release 0.16 via HADOOP-2404. It should be removed in 0.17,"conf,util"
Remove deprecated methods in Configuration.java,"The following methods in Configuration.java needs to be removed as they are deprecated. getObject(String name) setObject(String name, Object value) get(String name, Object defaultValue) set(String name, Object value) Iterator entries()",conf
"""hadoop fs -help ..."" should not require a NameNode to show help messages","For example, if we do ""hadoop fs -help get"" before started a NameNode, we will get  {code} bash-3.2$ ./bin/hadoop fs -help get 08/02/14 15:59:52 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 1 time(s). 08/02/14 15:59:54 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 2 time(s). 08/02/14 15:59:56 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 3 time(s). 08/02/14 15:59:58 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 4 time(s). 08/02/14 16:00:00 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 5 time(s). 08/02/14 16:00:02 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 6 time(s). 08/02/14 16:00:04 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 7 time(s). 08/02/14 16:00:06 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 8 time(s). 08/02/14 16:00:08 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 9 time(s). 08/02/14 16:00:10 INFO ipc.Client: Retrying connect to server: some-host:some-port. Already tried 10 time(s). Bad connection to FS. command aborted. {code}",fs
Remove deprecated methods in FileSystem,Remove deprecated methods like listPath and globPath and fix all the use of these deprecated methods in FsShell.,fs
Gridmix test script fails to run java sort tests,The gridmix test script fails to run the java sort tests  The script makes use of APP_JAR instead of EXAMPLE_JAR to run the sort job,test
A SequenceFile.Reader object is not closed properly in CopyFiles,"In CopyFiles line 186, the SequenceFile.Reader sl is never closed.",util
dfsadmin disk utilization report on Solaris is wrong,"dfsadmin reports 2x disk utilization on some platforms (Solaris, MacOS). The reason for this is that org.apache.hadoop.fs.DU is relying on du's default block size when reporting sizes and assuming they are 1024 byte blocks. This works fine on Linux, but du Solaris and MacOS uses 512-byte blocks to report disk usage.  DU should use ""du -sk"" instead of ""du -s"" to force the command to report sizes based on 1024 byte blocks. ",fs
[HOD] Idle cluster cleanup does not work if the JobTracker becomes unresponsive to RPC calls,"In some erroneous conditions, the Hadoop JobTracker becomes unresponsive to RPC calls (for e.g. if a misconfiguration causes the JobTracker to run out of memory). In such cases, a cluster allocated by HOD no longer runs any jobs and is wastefully holding up nodes. The usual idle cluster cleaner should deallocate the cluster ideally, but it does not.  ",contrib/hod
"[HOD] If a cluster directory is deleted, hod -o list must show it, and deallocate should work.","Currently if the cluster directory is deleted, all state about the cluster is lost. While this in itself is not a problem, at least recovery in the sense of being able to list the torque job id and deallocation to clear up the nodes should happen correctly.",contrib/hod
[HOD] Support for multiple levels of configuration,"Currently HOD provides a 2 level of configuration hierarchy - the hod configuration file, and command line options, with command line options overriding. At a minimum, there must be atleast an option to have an admin configured, non-overridable hod configuration file which can be used in addition to the 2 levels above. We could also generalize this to multiple levels in the same manner.",contrib/hod
[HOD] Split the operations of allocation from provisioning in HOD.,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently.  This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",contrib/hod
Bogus logging messages in Configration object constructors,"The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement:  if (LOG.isDebugEnabled()) {       LOG.debug(StringUtils.stringifyException(new IOException(""config()"")));     }  I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.",conf
Update gridmix to avoid artificially long tail,"The MaxEntropy test in the gridmix benchmark is submitted late into the queue, iterating past the point where the cluster is saturated. This dilutes the throughput measurement- the purpose of this benchmark- by making the tail overly dependent on the performance of a single job.",test
Add Writable for very large lists of key / value pairs,"Some map-reduce jobs need to aggregate and process very long lists as a single value. This usually happens when keys from a large domain are mapped into a small domain, and their associated values cannot be aggregated into few values but need to be preserved as members of a large list. Currently this can be implemented as a MapWritable or ArrayWritable - however, Hadoop needs to deserialize the current key and value completely into memory, which for extremely large values causes frequent OOM exceptions. This also works only with lists of relatively small size (e.g. 1000 records).  This patch is an implementation of a Writable that can handle arbitrarily long lists. Initially it keeps an internal buffer (which can be (de)-serialized in the ordinary way), and if the list size exceeds certain threshold it is spilled to an external SequenceFile (hence the name) on a configured FileSystem. The content of this Writable can be iterated, and the data is pulled either from the internal buffer or from the external file in a transparent way.",io
[HOD] HOD fails to allocate a cluster if the tarball specified is a relative path,"Run hod -t my-tar.tar.gz -o ""allocate hod-cluster 3"". Ringmaster fails to come up. The log shows the exception as an invalid URL for the tarball file. Basically HOD should translate the relative path to an absolute path and send it to the ringmaster.  ",contrib/hod
[HOD] The Idle cluster clean-up code should check for idleness based on last job's end time,"Currently, HOD polls the jobTracker to get list of jobs and checks how many jobs are running. This has the flip side that very short running jobs are missed by HOD. In practise, this has not been a problem. However, it can definitely be made more foolproof and we can check based on the completion time of the last job run.",contrib/hod
"""ant tar"" should not copy the modified configs into the tarball","When generating releases, it is counter-intuitive that the tarball contains the configuration files from the developer's test environment.",build
[HOD] Improve the user interface for the HOD commands,"The current command line for HOD is not user-friendly or conventional. Following improvements are required:  - We shouldn't require the quotes around the operation. - Can we define a HOD_CLUSTER_DIR environment variable which would be used by default ?  With these two, one can imagine a very simple command line for HOD. Something like:  hod-allocate 3 hod-deallocate  hod-allocate 3 ~/hod-clusters/test hod-deallocate ~/hod-clusters/test  This would also be backward compatible for people who're used to the current model.",contrib/hod
[HOD] Support PBS env vars in hod configuration,"In some batch environments, eg using Torque PBS, scratch spaces are provided on cluster nodes for where jobs should put their temporary files. These are automatically cleaned up when the job exists by an epilogue script.  For instance, in our local Torque cluster, all nodes have a /scratch partition. For each job, the prologue script creates a scratch folder owned by the user at /scratch/pbstmp.$PBS_JOBID - $PBS_JOBID is then the env var containing the job id, as set by pbs_mom.  Would it be possible to use these env vars in the configuration of hod. For instance, say I want to create an hdfs on demand using hod, but that the hdfs space should be in /scratch/pbstmp.$PBS_JOBID, not in /tmp/hod say. This would involve HOD supporting env vars in configuration, but knowing when to substitute the env var with it's current value (ie not until running on the correct node where the operation should take place).",contrib/hod
FSDataOutputStream should not flush() inside close().,"Why does FSDataOutputStream.close() call flush()? This stream itself does not store any data that it needs to flush. It is a wrapper and it should just invoke its outputstream's close().  For. e.g one bad side effect is that, in the case of DFSOutputStream which extends FSOutputSummer, flush() inside close sends the current data even though FSOutputSummer might have some data.. this left over data will be sent in side close() (so it sends data in two different packets instead of one). Other filesystems might have similar side effects.  I will submit a patch.  ",fs
Improve the Scalability and Robustness of IPC,"This jira is intended to enhance IPC's scalability and robustness.   Currently an IPC server can easily hung due to a disk failure or garbage collection, during which it cannot respond to the clients promptly. This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout. On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously, the server may be swarmed by requests and cannot work responsively.   The proposed changes aim to  # provide a better client/server coordination #* Server should be able to throttle client during burst of requests. #* A slow client should not affect server from serving other clients. #* A temporary hanging server should not cause catastrophic failures to clients. # Client/server should detect remote side failures. Examples of failures include: (1) the remote host is crashed; (2) the remote host is crashed and then rebooted; (3) the remote process is crashed or shut down by an operator; # Fairness. Each client should be able to make progress. ",ipc
Datanode.shutdown() and Namenode.stop() should close all rpc connections,"Currently this two cleanup methods do not close all existing rpc connections. If a mini dfs cluster gets shutdown and then restarted as we do in TestFileCreation, RPCs in second mini cluster reuse the unclosed connections opened in the first run but there is no server running to serve the request. So the client get stuck waiting for the response forever if client side timeout gets removed as suggested by hadoop-2811.",ipc
HOD should print the default file system to the user,"Currently, while running HOD, I have to look in the hadoop-site.xml to find the default file system. It should be printed for the user.",contrib/hod
Track individual RPC metrics.,"There is currently no mechanism to track performance metrics at the granularity of a specific RPC. So For e.g. if we wanted to capture average latency for the openFile RPC or for the createFile RPC the current infrastructure does not support that.  The implementation involves having a simple HashMap where every new Rpc metric being added would be inserted into the HashMap. Since there is a mechanism to obtain RPC latencies already (without the name of the specific RPC), the identification of what RPC is involved would be done by doing a lookup on the HashMap. ",metrics
Enhancements to gridmix scripts,I would like to propose enhancements to the gridmix scripts to make it: 1. easier to setup parameters for the test run and data generation (makes it easier to automate the runs using something like hudson) 2. ensure the benchmarks wait until they are completed (makes it easier to automate the runs using something like hudson)  Here are the details: Ability to override these parameters in gridmix-env * HADOOP_HOME * GRID_MIX_HOME * EXAMPLE_JAR * APP_JAR * STREAM_JAR * GRID_MIX_DATA * GRID_MIX_PROG  Ability to override these parameters in generateData.sh * COMPRESSED_DATA_BYTES * UNCOMPRESSED_DATA_BYTES * INDIRECT_DATA_BYTES  Ability for the tests submitted to the same cluster to wait until they are done. Changes will be in: * submissionScripts/monsterQueriesToSameCluster * submissionScripts/maxentToSameCluster * submissionScripts/textSortToSameCluster * submissionScripts/webdataScanToSameCluster * submissionScripts/webdataSortToSameCluster,test
checksum exceptions on trunk,"While running jobs like Sort/WordCount on trunk I see few task failures with ChecksumException Re-running the tasks on different nodes succeeds.   Here is the stack {noformat} Map output lost, rescheduling: getMapOutput(task_200802251721_0004_m_000237_0,29) failed : org.apache.hadoop.fs.ChecksumException: Checksum error: /tmps/4/mapred-tt/mapred-local/task_200802251721_0004_m_000237_0/file.out at 2085376   at org.apache.hadoop.fs.FSInputChecker.verifySum(FSInputChecker.java:276)   at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)   at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189)   at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)   at java.io.DataInputStream.read(DataInputStream.java:132)   at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2299)   at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)   at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)   at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)   at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)   at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)   at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)   at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)   at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)   at org.mortbay.http.HttpServer.service(HttpServer.java:954)   at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)   at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)   at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)   at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)   at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)   at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)  {noformat}   another stack {noformat} Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: /tmps/4/mapred-tt/mapred-local/task_200802251721_0004_r_000110_0/map_367.out at 21884416   at org.apache.hadoop.fs.FSInputChecker.verifySum(FSInputChecker.java:276)   at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)   at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)   at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)   at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)   at java.io.DataInputStream.readFully(DataInputStream.java:178)   at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:56)   at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)   at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1930)   at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:2958)   at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2716)   at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.getNext(ReduceTask.java:209)   at org.apache.hadoop.mapred.ReduceTask$ValuesIterator.next(ReduceTask.java:177)   ... 5 more {noformat}  both with local files",fs
[HOD] Remove script option from the core hod framework,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g.  hod -m 3 -z ~/hadoop.script  allocates 3 nodes, and runs ~/hadoop.script, then deallocates  This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because:  - hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. - even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. - users can free up clusters as soon as they are done.  The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean.  One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",contrib/hod
HOD should allow setting MapReduce UI ports within a port range,"HOD currently does now allow to explicitly specify ports or a port range in the MapReduce and HDFS sections, but this could be useful. A typical example would involve firewall settings that allow only a certain range of ports. ",contrib/hod
[HOD] hdfs:///mapredsystem directory not cleaned up after deallocation ,"Each submitted job creates a hdfs:///mapredsystem directory, created by (I guess) the hodring process. Problem is that it's not cleaned up at the end of the process; a use case would be:  - user A allocates a cluster, the hodring is svrX, so a /mapredsystem/srvX directory is created  - user A deallocates the cluster, but that directory is not cleaned up  - user B allocates a cluster, and the first node chosen as hodring is svrX, so hodring tries to write hdfs:///mapredsystem but it fails  - allocation succeeds, but there's no hodring running; looking at 0-jobtracker/logdir/hadoop.log under the temporary directory I can read:  2008-02-26 17:28:42,567 WARN org.apache.hadoop.mapred.JobTracker: Error starting tracker: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=B, access=WRITE, inode=""mapredsystem"":hadoop:supergroup:rwxr-xr-x  I guess a possible solution would be to clean up those directories during the deallocation process.  ",contrib/hod
distributedcache:  Can we create symlink without mapred.create.symlink = yes?,"I always thought   <property>   <name>mapred.cache.archives</name>   <value>/archives/arc1.zip#symlink</value> </property>  would create a symlink on the current working directory, but it also requires   <property>   <name>mapred.create.symlink</name>   <value>yes</value> </property>  to be set.  Can we simplify it so that we always create a symlink whenever there's '#wwww' entry? Or is 'mapred.create.symlink' necessary since '#' is a valid dfs filename? ",util
"replace accesss of ""fs.default.name"" with FileSystem accessor methods","HADOOP-1967 added accessor methods to set the default filesystem.  We should start using them.  While doing this, we should also replace uses of ""local"" and ""host:port"" with proper URIs, e.g., ""file:///"" and ""hdfs://host:port/"".  This will silence warnings about the use of old-format names. ",fs
Data type mismatch exception raised from pushMetric,incrMetric takes a int argument. However in pushMetric since getPreviousIntervalAverageTime() returns a long an exception was raised.  Fix is to cast getPreviousIntervalAverageTime() to an int,metrics
forrest docs for dfs shell commands and semantics.,add forrest documents for dfs shell command behaviours and semantics.,documentation
Improve IPC idle connection management,"IPC server determines if a connection is idle or not by checking if the connection does not have any IO activity for a predefined max idle time. An idle connection will be closed even if the connection still has outstanding requests or replies. This causes RPC failures when a server becomes slow or if a request takes a long time to be served. In jira, I'd like to propose the following changes to IPC idle management: 1. Add data structures to the IPC server that keep track of outstanding requests. 2. IPC server does not close a connection that has outstanding requests/replies even when it has no IO activities for a while. 3. The default client-side max idle time should be in several minutes not 1 second.  4. The server-side max idle time should be greater than the client-side max idle time, for example, twice of the client-side max idle time. So server mainly deals with clients that are crashed without closing  its connections.  ",ipc
Throttle IPC Client/Server during bursts of requests or server slowdown,"I propose the following to avoid an IPC server being swarmed by too many requests and connections 1. Limit call queue length or limit the amount of memory used in the call queue. This can be done by including the size of a request in the header and storing unmarshaled requests in the call queue.  2. If the call queue is full or queue buffer is full, stop reading requests from sockets. So requests stay at the server's system buffer or at the client side and thus eventually throttle the client.  3. Limit the total number of connections. Do not accept new connections if the connection limit is exceeded. (Note: this solution is unfair to new connections.)  4. If receive out of memory exception, close the current connection.   ",ipc
[HOD] Make the information printed by allocate and info commands less verbose and clearer,"Currently hod prints the following information as part of the allocate command at an 'INFO' verbose level:  Service Registry started Torque job id Ringmaster .. HDFS UI at .. Mapred UI at ..  The ""service registry started"" line is misleading and isn't very useful. This should be removed. Torque job id is misleading in terminology because job id also clashes with Mapred ""job id"". The proposal is to make it something like 'cluster id' or something similar. Ringmaster information is useful for debugging, but not for regular users.  Similar changes should be made to the hod info command as well. The naming should be clear and consistent. There, we print all of the above information and in addition, also print the number of nodes as min,max. Because we don't really support the min,max number of nodes, we can remove that and probably print only the number of nodes allocated.  All information must continue to be printed at a higher verbosity level.  ",contrib/hod
HOD is trying to bring up task tracker on  port which is already in close_wait state,"While bringing up task tracker using random ports, HOD is not checking whether the port is in CLOSE_WAIT state. So when it starts task tracker, we will be getting an address bind error on that port. We can avoid this error if we check for CLOSE_WAIT state on that port before starting the tasktracker.  ",contrib/hod
[HOD] Create mapred system directory using a naming convention that will avoid clashes in multi-user shared cluster scenario.,"Currently, HOD generates the name of the mapredsystem directory using the name /mapredsystem/hostname-of-jobtracker.  In HADOOP-2899, we ran into a scenario where this naming convention could lead to problems in case dfs permissions are enabled. While the bug should ideally be addressed in Hadoop M/R, it will be better that HOD does not generate names that can potentially clash across runs. One way to solve the problem is to do what HOD already does for local log and temp directories - name it using username.torque-job-id, which is going to be pretty unique mostly.",contrib/hod
Unit test fails on Windows: org.apache.hadoop.fs.TestDU.testDU,Unit test fails on Windows: org.apache.hadoop.fs.TestDU.testDU  Here is the output from the test: org.apache.hadoop.fs.TestDU.testDU: junit.framework.AssertionFailedError: expected:<2048> but was:<4096>   org.apache.hadoop.fs.TestDU.testDU(TestDU.java:82)  The failure points to the new test code in TestDU.java that just went in as part of HADOOP-2845,fs
need regression tests for setting up cluster and running jobs with different users,"Currently, there is no regression tests for setting up cluster and running jobs with different users.  So, some bugs like HADOOP-2915 cannot be discovered in the regression tests.    HADOOP-2915 is due to some changes after the permission patches were committed.  This bug did not exist in earlier builds.  For example, build #378 (Jan 24, 2008), which is a build after the permission patches were committed, works fine.  The problem of HADOOP-2915 cannot be reproduced in build #378.  http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/378/ ",test
"make {start,stop}-balancer.sh work even if hadoop-daemon.sh isn't in the PATH","The {start,stop}-balancer.sh scripts assume that hadoop-daemon.sh is in the PATH.  Added same boilerplate as other start/stop scripts to remove this assumption (attaching patch).",scripts
"Trash initialization generates ""deprecated filesystem name"" warning even if the name is correct.","HADOOP-1967 made it mandatory to prefix the value of ""fs.default.name"" with ""hdfs://"". # During name-node initialization the value of the ""fs.default.name"" is set to <host>:<port> without the prefix even if the original name was prefixed with ""hdfs://"". This makes the Trash constructor, which is called with the modified configuration print the following warning: {code} 08/03/03 17:29:36 WARN fs.FileSystem: ""<host>:<port>"" is a deprecated filesystem name. Use ""hdfs://<host>:<port>/"" instead. 08/03/03 17:29:36 WARN fs.FileSystem: ""<host>:<port>"" is a deprecated filesystem name. Use ""hdfs://<host>:<port>/"" instead. {code} The warning is printed twice because FileSystem.getDefaultUri() is called twice during new Trash() and then inside Trash.getEmptier(). # Other than that the name-node never checks the correctness of the ""fs.default.name"", which it should.  As a side note the Trash class should be rearranged IMO.  - The Trash.getEmtier() should be replaced by  {code}   static public Runnable getEmptier(Configuration conf) throws IOException {code} - Trash should have only one private constructor, the one that is called in Emtier.run(). - Then we can replace  {code} new Trash(conf).getEmptier() {code} with {code} Trash.getEmptier(conf) {code} in order to avoid unnecessary creation of the Trash object on the stack.","conf,fs"
HOD should generate hdfs://host:port on the client side configs.,"with Hadoop-1967,  if the fs.default.name is just host:port it prints out warnings when hadoop shell commands are run. Hod should change this to hdfs://host:port so that users do not see these warnings...",contrib/hod
some of the fs commands don't globPaths.,Some of the 'hadoop fs' commands don't globPaths. e.g:  {noformat} $ bin/hadoop fs -ls '/user/rangadi/2Gb-*' /user/rangadi/2Gb-1     <r 3>   808587264       2008-03-05 00:36        rw-r--r--       rangadi supergroup /user/rangadi/2Gb-2     <r 3>   812191744       2008-03-05 00:36        rw-r--r--       rangadi supergroup $ bin/hadoop fs -rm '/user/rangadi/2Gb-*' rm: cannot remove /user/rangadi/2Gb-*: No such file or directory. {noformat}  Mostly related to HADOOP-2063. I think all the commands that use {{DelayedExceptionThrowing}} are affected.,fs
Make the Hudson patch process an executable ant target,The automatic Hadoop patch testing does some fairly intricate processing.  It would be useful if this was exposed to the developers as an ant target.,test
redesigned plugin has missing functionality,"It is not possible to add a new Hadoop server to the list of servers in the Hadoop eclipse plugin.  In org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java, the private class variable ""createNewPage"" is never initialized (although it should be at line 97), and therefore clicking on ""Finish"" in the wizard results in a NullPointerException when this variable is accessed.  After fixing this in a local client, I had further problems connecting to a server using the SOCKS protocol.",contrib/eclipse-plugin
[HOD] Hod sometimes leaves behind temp directories related to ringmaster work-dirs.,None,contrib/hod
"[HOD] Hod should not check for the pkgs directory in gridservice-hdfs, if the external option is specified","If the gridservice-hdfs option is specified, the pkgs option is not used. So, it need not be validated.",contrib/hod
[HOD] Hod should redirect stderr and stdout of Hadoop daemons to assist debugging,"Copied from internal bug details from Koji:  ========================== Sometimes JobTracker/TaskTracker starts consuming 99% cpu and stops responding to 'jstack' call.  In those cases, usually it still responds to kill -QUIT signal which forces the jvm to dump the stack to stdout.    Please have the stdout of JT/TT redirected to a file.   Adding stderr.  If thread has an uncaught exception, it prints out to stderr and dies. ==========================",contrib/hod
"[HOD] HOD can print information in the info command in some logical order, rather than in a random order","From internal bug filed by Karam:  Current when we file hod info as -: hod -c ~/conf/hodrc -o ""info /a/b/c/clusterDir1""  Output is displayed as -: [         INFO - /a/b/c/clusterDir1         INFO - hdfs     http://foo.bar.com:55030         INFO - jobid    3553.foo1.bar.com         INFO - mapred   http://foo2.bar.com:52413         INFO - max      100         INFO - min      100         INFO - ring     http://foo3:54461/ ]   It would better if hod while displaying output preforms some logical grouping  e.g.  [         INFO - jobid    33553.foo1.bar.com         INFO - hdfs     http://foo2.bar.com:52413         INFO - mapred   http://foo.bar.com:55030 ]  ",contrib/hod
"[HOD] Hod should not check for the pkgs directory in gridservice-hdfs or mapred sections, if tarball is specified.","Ringmaster validates the pkgs path for gridservice-mapred and gridservice-hdfs section when tarball option is used and hodrc also contians pkgs path. This is not necessary, as it does not use the pkgs path.",contrib/hod
[HOD] Identifiers for map/reduce in the logs uploaded to the DFS,"Filed from an internal bug filed by Amar.  ======================================  On DFS we get the task logs which contain logs per node. If some one is interested in just MAP or REDUCE task logs, it becomes difficult to get it. If there are some identifiers for the task-log-types in the name or in the UI it will be a lot easier.  ",contrib/hod
Test utility no longer works in trunk,"Filebench no longer works in trunk, due to HADOOP-2391 performing a check for the existence of the output directory (it improperly sets it to the file location, which works due to URI.resolve semantics)",test
[HOD] Hod expects port info though external host is not mentioned.,"When external host is not specified in gridservice-mapred or hdfs sections, there is no point in HOD validating the port numbers or hostnames.",contrib/hod
[HOD] HOD fails to start if hodrc contains commented lines not beginning with a # or a ;,"for e.g. lines like ""  # testing"" or ""       ; testing"" in the hod configuration fail make hod fail to start.",contrib/hod
[HOD] hod temp-dir problems,"Filed from couple of related internal bugs from Karam: ====================================== - hod should validate directory provided for hod.temp-dir if exists then it must be a directory - If a tempdir path is specified with '~', it is not expanded as usual. - If there's a problem in creating a tempdir, hod exits with an error but does not deallocate the cluster.  ==================================================================================  ",contrib/hod
[HOD] Have a hod available command to replace checknodes,Easier for users to remember just one command.,contrib/hod
[HOD] Job logs which are fetched when a cluster is deallocated should have names corresponding to Hadoop jobs.,From internal bug from Arkady.. ============================= Map reduce jobs have names. The log files use mysterious names like jobfailures.jsp_jobid_job_200712150418_0001_kind_map_cause_failed.html They should use real job names so that users can find the logs they  need.,contrib/hod
Relative path for script option is not handled properly.,Ran hod as -: hod allocate -d cdir -n 5 -s scripts/hodscript.sh  hod throws following error -: [ CRITICAL - Invalid script file (--hod.script or -s) specified : scripts/hdp ]  It works for absolute path. hod should also handle relative path for script file ,contrib/hod
hod gives wrong exit code if provided invalid directory and invalid number of nodes.,Run hod as -: hod allocate -d invalid_cdir -n 3 (where invalid_cdir does not exist) or  hod allocate -d cdir -n -2 (or ab)  hod exit with exit code 1 (Configuration error) where it should exit with exit code 3 (Invalid operation arguments) ,contrib/hod
"When hodrc contains ""/"" as values for mapred-system-dir-root, hod behaves unexpectedly ","Ran hod with hodrc containing mapred-system-dir-root = /.  Allocation was successful but ringmaster command contains --hodring.mapred-system-dir-root with no value and hadoop-site.xml  on submssion node contains value for mapred.system.dir as ""user_name/mapredsystem/clusrer_id"" While hadoop-site.xml on Jobtracker contains value for mapred.system.dir like ""--hodring.pkgs/user_name/mapredsystem/cluser_id""  Here static dfs was used ",contrib/hod
Hod admin and user guide should elaborate more about setup when used with static dfs and dfs premissions on.,"Hod admin guide should also what all directory setups should done by admin before using hod with static dfs and dfs permissions on.  Hod user guide should talk about directories that could be setup inside home directories in DFS, so users are aware how they get created.",contrib/hod
Wrong class definition for hodlib/Hod/hod.py for Python < 2.5.1,"Running HOD with python 2.5 (<2.5.1) leads to the following error:   Traceback (most recent call last):   File ""hod"", line 47, in ?     from hodlib.Hod.hod import hodRunner   File ""/mnt/scratch/grid/hod/hod-trunk-421/hodlib/Hod/hod.py"", line 488     class hodHelp():                   ^ SyntaxError: invalid syntax  ",contrib/hod
SocketTimeoutException in unit tests," TestJobStatusPersistency failed and contained DataNode stacktraces similar to the following :  {noformat} 2008-03-07 21:27:00,410 ERROR dfs.DataNode (DataNode.java:run(976)) - 127.0.0.1:57790:DataXceiver: java.net.SocketTimeoutException: 0 millis  timeout while waiting for Unknown Addr (local: /127.0.0.1:57790) to be ready for read         at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:188)         at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:135)         at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:121)         at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)         at java.io.BufferedInputStream.read(BufferedInputStream.java:237)         at java.io.DataInputStream.readInt(DataInputStream.java:370)         at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2434)         at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1170)         at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:953)         at java.lang.Thread.run(Thread.java:619) {noformat}  This is mostly related to HADOOP-2346. The error is strange. socket.getRemoteSocketAddress() returned null implying this socket is not connected yet. But we have already read a few bytes from it!.  ",io
ipc unit tests fail due to connection errors,"ipc unit tests fail due to connection errors:  Failing tests: org.apache.hadoop.ipc.TestIPC.unknown org.apache.hadoop.ipc.TestIPCServerResponder.unknown org.apache.hadoop.ipc.TestRPC.testSlowRpc org.apache.hadoop.ipc.TestRPC.testCalls  Changes: # HADOOP-2346. Utilities to support timeout while writing to sockets. DFSClient and DataNode sockets have 10min write timeout. # HADOOP-2906. Add an OutputFormat capable of using keys, values, and config params to map records to different output files. # HADOOP-2756. NPE in DFSClient while closing DFSOutputStreams under load. # HADOOP-2934. The namenode was encountreing a NPE while loading leases from the fsimage. Fixed. # HADOOP-2925. Fix HOD to create mapred system directory using a naming convention that will avoid clashes in multi-user shared cluster scenario. # HADOOP-2911. Make the information printed by the HOD allocate and info commands less verbose and clearer. # HADOOP-2883. Write failures and data corruptions on HDFS files. The write timeout is back to what it was on 0.15 release. Also, the datnodes flushes the block file buffered output stream before sending a positive ack for the packet back to the client. # HADOOP-2861. INCOMPATIBLE CHANGE. Improve the user interface for the HOD commands. Command line structure has changed.  Error logs:  [junit] Running org.apache.hadoop.ipc.TestIPC     [junit] 2008-03-07 10:50:04,291 INFO  metrics.RpcMetrics (RpcMetrics.java:<init>(53)) - Initializing RPC Metrics with hostName=0, port=4785     [junit] 2008-03-07 10:50:04,354 INFO  ipc.Server (Server.java:run(443)) - IPC Server Responder: starting     [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(303)) - IPC Server listener on 4785: starting     [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 0 on 4785: starting     [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 1 on 4785: starting     [junit] 2008-03-07 10:50:04,369 INFO  ipc.Server (Server.java:run(861)) - IPC Server handler 2 on 4785: starting     [junit] 2008-03-07 10:50:04,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 1 time(s).     [junit] 2008-03-07 10:50:04,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 1 time(s).     [junit] 2008-03-07 10:50:05,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 2 time(s).     [junit] 2008-03-07 10:50:05,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 2 time(s).     [junit] 2008-03-07 10:50:06,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 3 time(s).     [junit] 2008-03-07 10:50:06,432 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 3 time(s).     [junit] 2008-03-07 10:50:07,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 4 time(s).     [junit] 2008-03-07 10:50:07,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 4 time(s).     [junit] 2008-03-07 10:50:08,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 5 time(s).     [junit] 2008-03-07 10:50:08,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 5 time(s).     [junit] 2008-03-07 10:50:09,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 6 time(s).     [junit] 2008-03-07 10:50:09,433 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 6 time(s).     [junit] 2008-03-07 10:50:10,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 7 time(s).     [junit] 2008-03-07 10:50:10,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 7 time(s).     [junit] 2008-03-07 10:50:11,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 8 time(s).     [junit] 2008-03-07 10:50:11,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 8 time(s).     [junit] 2008-03-07 10:50:12,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 9 time(s).     [junit] 2008-03-07 10:50:12,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 9 time(s).     [junit] 2008-03-07 10:50:13,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 10 time(s).     [junit] 2008-03-07 10:50:13,434 INFO  ipc.Client (Client.java:setupIOstreams(177)) - Retrying connect to server: /0.0.0.0:4785. Already tried 10 time(s).     [junit] 2008-03-07 10:50:14,435 FATAL ipc.TestIPC (TestIPC.java:run(92)) - Caught: java.net.BindException: Cannot assign requested address: no further information     [junit] 2008-03-07 10:50:14,435 FATAL ipc.TestIPC (TestIPC.java:run(92)) - Caught: java.net.BindException: Cannot assign requested address: no further information  ",ipc
IPC server should not allocate a buffer for each request,Currently the IPC server allocates a buffer for each incoming request. The buffer is thrown away after the request is serialized. This leads to very inefficient heap utilization. It would be nicer if all requests from one connection could share a same common buffer since the ipc server has only one request is being read from a socket at a time.,ipc
[HOD] checknodes should look for free nodes without the jobs attribute,Modify the checknodes logic to compute number of available nodes by looking for the number of free nodes without a jobs attribute. This is more accurate than current computation.,contrib/hod
[HOD] local_fqdn() returns None when gethostbyname_ex doesnt return any FQDNs.,"For some reason (probably in our local DNS setup) gethostbyname_ex() does not return any fully qualified hostnames. This has never been an issue, everything has worked fully with the hostnames.  However, this causes HOD to fail, as local_fqn() in util.py returns None.   {noformat} Python 2.5.1 (r251:54863, Sep 21 2007, 16:05:06) [GCC 3.4.6 20060404 (Red Hat 3.4.6-3)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import socket, os >>> socket.gethostbyname_ex(os.uname()[1]) ('bohol', [], ['130.209.252.70']) >>> {noformat}  The solution is to fix local_fqdn in until.py, such that it falls back to the contents of the variable me if fqdn is still None.  Ta muchly.  Craig",contrib/hod
Distcp should have forrest documentation,We really should have a page on how to use distcp.,util
Trash not being deleted on time,"On one of our cluster, we set the Trash interval to 6 hrs.   However, sometimes the namenode doesn't delete the Trash dir on time.  -bash-3.00$ hadoop dfs -ls /Trash Found 3 items /Trash/0711201201       <dir>           2007-11-20 06:00 /Trash/0711201800       <dir>           2007-11-20 12:15 /Trash/Current  <dir>           2007-11-20 18:01  In our current setting, we're supposed to have only one 'current' and one previous snapshot.    Grepping shows that /Trash/0711201201 was not even touched. My guess is that 1800 - 1201 = 5 hrs 59min  < 6hrs.   ",fs
Sequential distributed upgrades.,The distributed upgrades were tested in the case of one distributed upgrade only. It turned out to be that current implementation of the framework does not support multiple upgrades. This feature is needed for the appends. See HADOOP-2656.,test
Specify which JAVA_HOME should be set,"Quickstart page (http://hadoop.apache.org/core/docs/current/quickstart.html) specifies that user should set JAVA_HOME under ""1."" of ""Required Software"".  But, it does not specify where this should be set.  My instinct (as would be the instinct of any un*x user) was to set this in his/her shell/profie/rcfile.  Please specify that this should be specified in conf/hadoop-env.sh.  Yes, this is clarified later under ""Download"", but is easy to miss, especially since the user will think that he/she had already set JAVA_HOME. ",documentation
add new logo to project site,HADOOP-2810 added a new logo for Core to the released documenation.  The same logo should also be used on the Core project web site.,documentation
FileSystems should track how many bytes are read and written,It would be nice if the file systems could track the number of bytes read and written for each class of FileSystem. Map/Reduce could then report these numbers back as counters to provide information about how much data is read and written.,fs
FileSystem cache key should be updated after a FileSystem object is created,"In FileSystem.get(uri, conf), it first creates a cache key from the uri and the conf and then lookups the corresponding FileSystem object in the cache.  If the object is not found, it initializes a FileSystem object and put it to the cache with the key.  However, during FileSystem creation, the conf might be modified.  In such case, the key should be updated before putting it to the cache.",fs
HOD allocate command does not accept '~',"hod allocate seems to throw error when I specify directory relative to my home [lohit@hod]$ hod allocate --hod.clusterdir=""~/clusterdir"" --hod.nodecount=5 error: bin/hod failed to start. error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): ~/clusterdir error: 1 problem found. Check your command line options and/or your configuration file /hod/conf/hodrc",contrib/hod
HOD script command should take arguments,"hod script command take script as argument, the script itself does not take arguments. It would be good to have the ability to pass arguments to the script as well.",contrib/hod
ConcurrentModificationException from org.apache.hadoop.ipc.Server$Responder in JobTracker,I see lot of these in my JobTracker log  {noformat} WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException         at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)         at java.util.HashMap$KeyIterator.next(HashMap.java:827)         at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)         at org.apache.hadoop.ipc.Server$Responder.run(Server.java:475) {noformat},ipc
Distcp deleting target directory,I believe this happens when   1) src is a file and dst is a directory.  2) -update is set  % hadoop  distcp -p -update  hdfs://srcnn:9999/user/knoguchi/fileA    hdfs://dstnn:9999/user/knoguchi/dir    This will delete the hdfs://dstnn:9999/user/knoguchi/dir .  ,util
dfs -mv file to user home directory fails silently if the user home directory does not exist,"dfs -mv file to user home directory fails silently if the user home directory does not exist. This was working before. It will move the file if the user home directory exists.  Here is the sequence: 1. bin/hadoop dfs -lsr / /file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup  2. bin/hadoop dfs -mv /file1 file2  3. bin/hadoop dfs -lsr / /file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup  4. hadoop dfs -lsr / /file1  <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup /user   <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup /user/hadoopqa  <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup /user/hadoopqa/file0    <r 3>   0       2008-03-13 20:07        rw-r--r--       hadoopqa        supergroup  5. bin/hadoop dfs -mv /file1 file2  6. bin/hadoop dfs -lsr / /user   <dir>           2008-03-13 20:07        rwxr-xr-x       hadoopqa        supergroup /user/hadoopqa  <dir>           2008-03-13 20:08        rwxr-xr-x       hadoopqa        supergroup /user/hadoopqa/file0    <r 3>   0       2008-03-13 20:07        rw-r--r--       hadoopqa        supergroup /user/hadoopqa/file2    <r 3>   0       2008-03-13 19:54        rw-r--r--       hadoopqa        supergroup  In step #2, it fails to move the file. In step #5, it moves the file as /user/hadoopqa directory exists.",fs
HOD Error message points to --hod.clusterdir when -d is used,"HOD Error message points to --hod.clusterdir when -d is used  When the clusterdir directory is not correctly specified using the -d option, HOD throws an error about the --hod.clusterdir  Here is the command: {quote}hod -c /perf info -d /doesnotexist{quote}  And the output: {quote}error: bin/hod failed to start. error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): /doesnotexist error: 1 problem found. Check your command line options and/or your configuration file /perf{quote}",contrib/hod
Eclipse plugin fails to compile due to missing RPC.stopClient() method,HADOOP-2870 removed the RPC.stopClient() method which is called by org.apache.hadoop.eclipse.dfs.DFSLocationsRoot.,contrib/eclipse-plugin
"Nightly and Patch builds should include native, c++, and eclipse-contrib","The nightly build should build the native, c++ and eclipse plugin support:  ant -Dcompile.native=yes -Dcompile.c++=yes -Declipse.home=/path/to/eclipse clean package-libhdfs tar  The build machine will need to be setup with some libraries, etc. to make this possible.",build
[HOD] build file requires exact python version,"To run the HOD pyunit tests, the build file checks that an exact python version is present:  +      <condition property=""python.versionmatched""> +        <!--- Currently check for only 2.5.1 --> +        <equals arg1=""${hodtest.pythonVersion}"" arg2=""Python 2.5.1"" /> +      </condition>  This is too restrictive (we have 2.5.2 on our Solaris build machine).  I suggest that you don't check the python version and instead issue a warning message if the pyunit test fail stating that python 2.5.1 or great is required (and optionally showing them what version they tried to use).",build
ChecksumFileSystem needs to support the new delete method,"The method FileSystem.delete(path) has been deprecated in favor of the new method delete(path, recursive). Temporary files gets created in the MapReduce framework and when the time for deletion comes, they are deleted via delete(path, recursive). This doesn't delete the associated checksum files. This has a big impact when the FileSystem is the InMemoryFileSystem, where space is at a premium and wasting space here might hurt the performance of MapReduce jobs overall. One solution to this problem is to implement the method delete(path, recursive) in the ChecksumFileSystem but is there is a reason why it was left out as part of HADOOP-771?",fs
Quickstart link in homepage returns URL not found - http://hadoop.apache.org/core/docs/current/quickstart.html,"Clicking on the Quickstart link on the Core home page - http://hadoop.apache.org/core/docs/current/quickstart.html returns ""Page Not Displayed"".  The same page can be reached thru the ver 0.16 folder. http://hadoop.apache.org/core/docs/r0.16.1/quickstart.html  This seems to be a missing link  The directory current at - http://hadoop.apache.org/core/docs/current  has the text - ""link r0.16.1"" looks like the \current folder wasn't properly linked to \r0.16.1",documentation
InMemoryFileSystem.reserveSpaceWithChecksum does not look at failures while reserving space for the file in question,"The return statement code in InMemoryFileSystem.reserveSpaceWithCheckSum looks like {noformat}   return (mfs.reserveSpace(f, size) && mfs.reserveSpace(getChecksumFile(f), checksumSize)); {noformat}  This should be broken up to check for successful reserveSpace for each of the components. In some cases, we might reserve space for the first component and fail while doing the same for the second (checksum file). This will lead to wastage of space since we don't un-reserve the space we got for the first component. This usually won't happen due to the policy associated with creating a file in the InMemoryFileSystem (since the checksum component is usually very small) but still it should be fixed.",fs
Fix findBugs warnings in UpgradeUtilities.,There are 2 findBugs warning in UpgradeUtilities. - toString() applied to a String - not closing InputStream. ,test
Hudson needs to add src/test for checking javac warnings,I think src/test is not added in javac warnings checker.  HADOOP-3031 looks at warnings introduced. Hudson needs to add src/test for checking javac warnings,build
[HOD] Ringmaster does not automatically create log directory if it does not exist.,"In the hod configuration, if the directory specified in ringmaster.log-dir does not exist, verification fails, and ringmaster exits. In the code, it tries to create the log dir after the verification is through. The order is misplaced.  This problem can also occur if any parameter in the ringmaster configuration is invalid.",contrib/hod
Update the Javadoc in JobConf.getOutputPath to reflect the actual temporary path,The javadoc needs to be fixed to include the _temporary in the job's temp output path.,documentation
NNBench does not use the right configuration for the mapper,NNbench does not use the job configuration for getting property values in the Mapper. Therefore all the pre-configured property values do not get passed to the mapper.,test
hod -t option fails silently if the tarball specified does not exist.,None,contrib/hod
Stringifier,"Storing arbitrary objects in the configuration has been discussed before in HADOOP-449 and HADOOP-1873. Although enabling such functionality has the risk of encouraging people to put big binary objects in the configuration, for some use cases passing objects to tasks is absolutely necessary.   This issue will track the implementation of a Stringifier interface which stringifies and destringifies objects. Using this implementation, developers can store objects in the configuration and restore them later.   Any thoughts ?",io
New Server Framework for Hadoop RPC,This is a new Server framework for Hadoop RPC which replaces the current Server class.  This new framework uses a reactor model to allow better throughput and better handling of clients.  ,ipc
distcp seems to be broken in 0.16.1,"Starting from 0.16.1 distcp no longer works when running on source between two 0.16.1 installations. It seems to copy okay, but then at the end all copied files are deleted. Log files are empty. The job ends successfully.  BTW. so far, even with 0.16.0, we were unsuccessful to run distcp on the target successfully, except for small amounts of data (< 2 TB).",util
Hadoop DFS to report more replication metrics,"Currently, the namenode and each datanode reports 'blocksreplicatedpersec.'  We'd like to be able to graph pending replications, vs number of under replicated blocks, vs. replications per second, so that we can get a better idea of the replication activity within the DFS. ",metrics
Writable for single byte and double,Implementations of Writable for a single byte and a double.,io
Need to capture the metrics for the network ios generate by dfs reads/writes and map/reduce shuffling  and break them down by racks ,"In order to better understand the relationship between hadoop performance and the network bandwidth, we need to know  what the aggregated traffic data in a cluster and its breakdown by racks. With these data, we can determine whether the network  bandwidth is the bottleneck when certain jobs are running on a cluster. ",metrics
BloomMapFile - fail-fast version of MapFile for sparsely populated key space,"The need for this improvement arose when working with large ancillary MapFile-s (essentially used as external dictionaries). For each invokation of map() / reduce() it was necessary to perform several look-ups in these MapFile-s, and in case of sparsely populated key-space the cost of finding that a key is absent was too high.  This patch implements a subclass of MapFile that creates a Bloom filter from all keys, so that accurate tests for absence of keys can be performed quickly and with 100% accuracy.  Writer.append() operations update a DynamicBloomFilter, which is then serialized when the Writer is closed. This filter is loaded in memory when a Reader is created. Reader.get() operation first checks the filter for the key membership, and if the key is absent it immediately returns null without doing any further IO.",io
"Trash not being expunged, Trash Emptier thread gone by NPE","We noticed that the users' trash were not being expunged by the namenode.   jstack didn't show the Trash.Emptier thread and .out file showed   Exception in thread ""Trash Emptier"" java.lang.NullPointerException   at org.apache.hadoop.fs.Trash.expunge(Trash.java:146)   at org.apache.hadoop.fs.Trash$Emptier.run(Trash.java:233)   at java.lang.Thread.run(Thread.java:619)   It seems like this happens when it hits the user's directory which doesn't contain the .Trash.",fs
SocketOutputStream.close() should close the channel.,None,ipc
URLStreamHandler for the DFS,"This issue aims at providing a handler to resolve DFS URLs (""hdfs://host:port/file/to/path""), so that such URLs can be read using the URL API (mainly {{InputStream url.openStream()}}).  This allows the use of a {{URLClassLoader}} which would serve classes directly from the DFS. ",util
"[HOD] If a cluster directory is specified as a relative path, an existing script.exitcode file will not be deleted.","Create a cluster directory, and use it in a script which fails so that the script.exitcode file is created in this directory Now using the same directory, try to use the script option again, specifying the cluster directory as a relative path. This will fail to remove the script.exitcode file.",contrib/hod
[HOD] Minor changes to unit tests,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed.  As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",contrib/hod
distcp fails for files with zero length,"distcp fails for files with zero length. This is a regression from 0.15.3  distcp hftp://<namenode:port>/dir1/file1 file2          08/03/24 23:09:45 INFO util.CopyFiles: srcPaths=[hftp://<namenode:port>/dir1/file1] 08/03/24 23:09:45 INFO util.CopyFiles: destPath=file2 08/03/24 23:09:45 INFO util.CopyFiles: srcCount=1 08/03/24 23:09:46 INFO mapred.JobClient: Running job: job_200803242306_0001 08/03/24 23:09:47 INFO mapred.JobClient:  map 0% reduce 0% 08/03/24 23:10:01 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_0, Status : FAILED java.io.IOException: Copied: 0 Skipped: 0 Failed: 1         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)  08/03/24 23:10:18 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_1, Status : FAILED java.io.IOException: Copied: 0 Skipped: 0 Failed: 1         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)  08/03/24 23:10:33 INFO mapred.JobClient: Task Id : task_200803242306_0001_m_000000_2, Status : FAILED java.io.IOException: Copied: 0 Skipped: 0 Failed: 1         at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.close(CopyFiles.java:448)         at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:53)         at org.apache.hadoop.mapred.MapTask.run(MapTask.java:208)         at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2084)  ^@08/03/24 23:10:48 INFO mapred.JobClient:  map 100% reduce 100% With failures, global counters are inaccurate; consider running with -i Copy failed: java.io.IOException: Job failed!         at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:894)         at org.apache.hadoop.util.CopyFiles.copy(CopyFiles.java:526)         at org.apache.hadoop.util.CopyFiles.run(CopyFiles.java:596)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)         at org.apache.hadoop.util.CopyFiles.main(CopyFiles.java:612)",util
pushMetric() method of various metric util classes should catch exceptions," pushMetric() method of various metric util classes should catch exception. Otherwise, any misconfigured metric will cause the entire metric data not sent out. ",metrics
"Show counter values from ""job -status"" command","It would be useful if the ""job -status"" command showed counter values, like the JobClient does (when the job has completed).",scripts
BytesWritable.toString prints bytes above 0x80 as FFFFFF80,The printing of BytesWritable as a series of bytes does the wrong thing with negative bytes and expands them out to 32 bits before printing them.,io
Validating input paths and creating splits is slow on S3,"A call to listPaths on S3FileSystem results in an S3 access for each file in the directory being queried. If the input contains hundreds or thousands of files this is prohibitively slow. This method is called in FileInputFormat.validateInput and FileInputFormat.getSplits. This would be easy to fix by overriding listPaths (all four variants) in S3FileSystem to not use listStatus which creates a FileStatus object for each subpath. However, listPaths is deprecated in favour of listStatus so this might be OK as a short term measure, but not longer term.  But it gets worse: FileInputFormat.getSplits goes on to access S3 a further six times for each input file via these calls:  1. fs.isDirectory 2. fs.exists 3. fs.getLength 4. fs.getLength 5. fs.exists (from fs.getFileBlockLocations) 6. fs.getBlockSize  So it would be best to change getSplits to use listStatus, and only access S3 once for each file. (This would help HDFS too.) This change would require some care since FileInputFormat has a protected method listPaths which subclasses can override (although, in passing I notice validateInput doesn't use listPaths - is this a bug?).  For input validation, one approach would be to disable it for S3 by creating a custom FileInputFormat. In this case, missing files would be detected during split generation. Alternatively, it may be possible to cache the input paths between validateInput and getSplits.","fs,fs/s3"
Improve documentation about the Task Execution Environment in the Map-Reduce tutorial,We should improve the 'Task Execution & Environment' section in the Map-Reduce tutorial with information missing from http://wiki.apache.org/hadoop/TaskExecutionEnvironment.,documentation
"dfs -chown does not like ""_"" underscore in user name",":~$ hadoop dfs -chown aaa_bbb  /user/knoguchi/test.txt chown: 'aaa_bbb' does not match expected pattern for [owner][:group].  in 0.16.1, only alphabets and numbers are allowed.  Shouldn't '_' be allowed?  I couldn't find any standard, but in Solaris10, it's defined as   http://docs.sun.com/app/docs/doc/816-5174/6mbb98uhg?a=view  bq. The login (login) and role (role) fields accept a string of no more than eight bytes consisting of characters from the set of alphabetic characters, numeric characters, period (.), underscore (_), and hyphen (-). The first character should be alphabetic and the field should contain at least one lower case alphabetic character. A warning message is displayed if these restrictions are not met.  ",fs
"Need new options in distcp for preserving ower, group and permission","Currently, distcp -p preserves replication# and block size.  Since permissions are introduced, distcp should provide options for preserving ower, group and permission.",util
Develop tests to test the DFS command line interface,Develop tests to test the DFS command line interface. The DFS commands to test are:             [-ls <path>]            [-lsr <path>]            [-du <path>]            [-dus <path>]            [-mv <src> <dst>]            [-cp <src> <dst>]            [-rm <path>]            [-rmr <path>]            [-expunge]            [-put <localsrc> <dst>]            [-copyFromLocal <localsrc> <dst>]            [-moveFromLocal <localsrc> <dst>]            [-get [-crc] <src> <localdst>]            [-getmerge <src> <localdst> [addnl]]            [-cat <src>]            [-copyToLocal [-crc] <src> <localdst>]            [-moveToLocal [-crc] <src> <localdst>]            [-mkdir <path>]            [-setrep [-R] [-w] <rep> <path/file>]            [-touchz <path>]            [-test -[ezd] <path>]            [-stat [format] <path>]            [-tail [-f] <file>],test
[HOD] Hadoop.tmp.dir should not be set to cluster directory,"Currently HOD generates  hadoop.tmp.dir pointing to the cluster directory(hod.clusterdir), which users typically create in their home directories. This hadoop.tmp.dir is used on the client side when a jar is run(org/apache/hadoop/util/RunJar.java). The user home directories might be NFS mounted in some environments, which might result in a hit, for example when running a jar.  So, HOD *should not* generate hadoop.tmp.dir pointing to cluster directory. Instead, it should use hod.temp-dir, which *is* the location that dfs.client.buffer.dir uses to alleviate any such NFS hit problems. Further, though currently hadoop.tmp.dir is not used on hadoop daemons side, it would be good if HOD generates it on daemons side too, and thus preclude any related problems in future.",contrib/hod
Update documentation in mapred_tutorial to add Debugging,Update mapred_tutorial to add How to debug map reduce programs with http://wiki.apache.org/hadoop/HowToDebugMapReducePrograms,documentation
TestDFSShell fails on Windows.,"TestDFSShell.testPut() fails with the following exception: {code} Pathname /C:/Hadoop/build/test/data/f1 from C:/Hadoop/build/test/data/f1 is not a valid DFS filename. java.lang.IllegalArgumentException: Pathname /C:/Hadoop/build/test/data/f1 from C:/Hadoop/build/test/data/f1 is not a valid DFS filename.   org.apache.hadoop.dfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:112)   org.apache.hadoop.dfs.DistributedFileSystem.exists(DistributedFileSystem.java:184)   org.apache.hadoop.dfs.TestDFSShell.testPut(TestDFSShell.java:214) {code} Looks like that Path treats windows drive letter incorrectly. Not sure which path introduced it, but needs to be fixed for 0.17 imo.",fs
Speculative execution configuration is confusing,"In Hadoop 0.16 the single switch for speculative execution was replaced by 2 switches for maps and reduces. The old switch was retained as well and overrides the new switches if it is turned on. This is confusing. We should go back to a single config variable that has one value from the set  -- 'true, false, map, reduce'.  ",conf
"dfs -lsr fail with ""Could not get listing """,(This happened a lot when namenode was extremely slow due to some other reasons.)  % hadoop dfs -lsr /     failed with   > Could not get listing for /aaa/bbb/randomfile  It's probably because  file  was deleted between  items = fs.listStatus( and ls(items[i]   I think lsr should ignore this case and continue. ,fs
test-patch target should check @SuppressWarnings(...),"The Java annotation @SuppressWarnings(...) tag can be used to get rid of compiler warnings. In our patch process, QA should check @SuppressWarnings(...) tag to prevent abusing this tag.",build
Build native libraries on Solaris,The current native build scripts need to be changed to run on Solaris.,build
rm /user/<username>/.Trash/____ only moves it back to .Trash ,"In 0.15 or before, any dfs -rm call for  files under Trash were deleted.  From 0.16, it just renames it back to Trash.  In Trash.java:moveToTrash()  {noformat} if (path.toString().startsWith(trash.toString())) {noformat}  seems like trash.toString() is fully qualified, and path.toString() is not.",fs
[hod] hod leaves mapredsystem directories,"In hod, mapredsystem dir is created for each torque job using the torque jobid.  I believe we now create it under /user/<username>/mapredsystem/<torque jobid>   Files are deleted after job is done, but the directory remain untouched. Hod should delete these directories when the cluster is deallocated.",contrib/hod
[HOD] hodring.log-destination-uri should be set to a default value,"In the current setup, we do not insist a value for hodring.log-destination-uri which is used to store tarred hadoop logs. Now, if there is a hadoop start up problem, the cluster could get automatically deallocated. However, the hadoop logs would be lost too, as the above configuration parameter does not have a default value.  Possibly this should be set to atleast file:///<local file system> path so that the logs will be saved.",contrib/hod
[HOD] Update hod version number,At a minimum update the version of hod in the VERSION file to indicate new version. The best solution is if we can pick it automated from Hadoop build.,contrib/hod
periodic ConcurrentModificationException in IPC Server,"Seeing this exception in the Namenode log (429 of them yesterday alone)  2008-03-30 00:00:32,332 WARN org.apache.hadoop.ipc.Server: Exception in Responder java.util.ConcurrentModificationException         at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)         at java.util.HashMap$KeyIterator.next(HashMap.java:827)         at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)         at org.apache.hadoop.ipc.Server$Responder.run(Server.java:475)",ipc
Increase the number of open files for unit tests under Hudson,"Lucene.zones.apache.org and I assume hudson.zones limit the number of file handles to 256. TestMiniMRDFSSort runs better with more file handles. (I set it to 10000 in my testing, but a smaller value is probably ok.) I suggest the QA scripts increase the ulimit before running the unit tests with ""ulimit -n 10000"".",scripts
Decrease the number of slaves in TestMiniMRDFSSort to 3.,Hudson.zones gets pretty overloaded making TestMiniMRDFSSort flaky. I think we should drop the number of slaves (ie. DataNodes and TaskTrackers) from 4 to 3.,test
HOD command should allow to allow to specify a hadoop command in the command line,"The current version allows only to specify a ""script"". So if one needs to generate a single command, one has to create a file for this, which very inconvenient -- generating temp file name, printing, deleting the file. It also incompatible with hod 0.3  ",contrib/hod
build-contrib.xml should inherit hadoop version parameter from root build.xml,"This is needed in HOD (and may be useful in other contrib projects), which, in some cases, may be compiled and built separately. After HADOOP-3137, HOD will obtain its version from build parameter ${version}, and this will fail to give proper version when built independently(at src/contrib/hod level).",build
Hod should have better error messages.,"(1) gsgw1037:/homes/arkady/CDSR> hod allocate -n 4 -d xx error: bin/hod failed to start. error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): xx error: 1 problem found. Check your command line options and/or your configuration file /grid/0/kryptonite/hod/conf/hodrc     * Directory 'xx' does not exist - the message shuold say ""Non-existing directory"" (or ""directory not found""), not ""invalid""    * checking the configuration file will not help, the advice is confusing    * checking the command line options is the obvious thing to do -- the advise is unnecessary.  (2) when hod does not understand the command, it prints the whole help options message -- which clobbers the screen and is confusing. It should give a meaningful message -- and may say ""type hod help for more info""   ",contrib/hod
Make index interval configuable when using MapFileOutputFormat for map-reduce job,"Per discussion with Doug Cutting on hadoop user mailing around Mar 21, thread title ""MapFile and MapFileOutputFormat"". Currently, there is no way  to change the index interval for the output MapFile in a map-reduce job. As suggested, adding a static method MapFile(Configuration, int) to set the index interval and stores in Configuration, then MapFile.Writer constructor reads the setting from configuration may be a good idea.   I also noticed that Hbase did similar things in HBASE-364.",io
[HOD] Hod should deallocate cluster if there's a problem in writing information to the state file,"Consider a scenario where hod runs allocate successfully, but isn't able to save teh allocated information to the clusters.state file. In such a case, it gets an error and exits. But the cluster remains allocated, and unfortunately the user cannot deallocate the cluster now unless he knows the cluster directory.  It is better if HOD can deallocate the cluster in such an error condition.",contrib/hod
TestMiniMRLocalFS fails in trunk on Windows,TestMiniMRLocalFS fails with this stacktrace on Windows  Testcase: testWithLocal took 299.637 sec   Caused an ERROR No FileSystem for scheme: C java.io.IOException: No FileSystem for scheme: C   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1278)   at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:53)   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1292)   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:191)   at org.apache.hadoop.filecache.DistributedCache.getTimestamp(DistributedCache.java:414)   at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:605)   at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:700)   at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:973)   at org.apache.hadoop.mapred.MRCaching.launchMRCache(MRCaching.java:196)   at org.apache.hadoop.mapred.TestMiniMRLocalFS.testWithLocal(TestMiniMRLocalFS.java:56)  ,test
TestFileAppend fails on Mac since HADOOP-2655 was committed,org.apache.hadoop.dfs.TestFileAppend.testCopyOnWrite fails with this exception:  java.io.IOException: stat: illegal option -- c   org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:557)   org.apache.hadoop.dfs.DatanodeBlockInfo.detachBlock(DatanodeBlockInfo.java:117)   org.apache.hadoop.dfs.FSDataset.detachBlock(FSDataset.java:652)   org.apache.hadoop.dfs.TestFileAppend.testCopyOnWrite(TestFileAppend.java:188)  Mac command should be stat -f$l,test
FS.mkdirs()'s contract is unclear.,{{FileSystem.mkdirs()}} returns a boolean but does not specify what a return value of true or false means. For e.g. HDFS returns never returns false (until HADOOP-3138).  Should it be consistent with java.io.File.mkdirs()?,fs
speculative execution should not have default value on hadoop-default.xml bundled in the Hadoop JAR,"Having a default value for speculative execution in the hadoop-default.xml bundled in the Hadoop JAR file does not allow a cluster to control the default behavior.   *ON in hadoop-default.xml (current behavior)*  * ON in JT hadoop-site.xml  * present in job.xml, job's value is used  * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf (*) * ON FINAL in the JT hadoop-site.xml  * present or not present in the job.xml, ON is used * OFF in JT hadoop-site.xml  * present in job.xml, job's value is used  * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf * OF FINAL in the JT hadoop-site.xml  * present or not present in the job.xml, OFF is used  *OFF in hadoop-default.xml (not current behavior)*  * ON in JT hadoop-site.xml  * present in job.xml, job's value is used  * not-present in job.xml, OFF is taken as default from the hadoop-default.xml present in the client's JAR/conf (*) * ON FINAL in the JT hadoop-site.xml  * present or not present in the job.xml, ON is used * OFF in JT hadoop-site.xml  * present in job.xml, job's value is used  * not-present in job.xml, ON is taken as default from the hadoop-default.xml present in the client's JAR/conf * OF FINAL in the JT hadoop-site.xml  * present or not present in the job.xml, OFF is used  ---  Still is desirable for the JT to have a default value. To avoid having to support 2 hadoop-default.xml files, one for the JT and other for the clients, the easiest why is to remove it from the hadoop-default.xml and have the default value in the code when getting the config property (thing that may be already happening). ",conf
Improve documentation and supply an example for MultiFileInputFormat,"MultiFileInputFormat has been discussed several times in mailing lists, and it is clear that documentation could be improved and an example showing the usage of it could be helpful. ",documentation
"""-get file -"" does not work","{noformat} $ bin/hadoop fs -get file - > /dev/null Usage: java FsShell -get [-ignoreCrc] [-crc] <src> <localdst> get: Illegal option - {noformat} In 0.16, it used to write 'file' to stdout. ",fs
gridmix scripts for small and medium jobs need to be changed to handle input paths differently,"The gridmix scripts failed to run small and medium jobs as tracked in  HADOOP-3162. Based on that fix, the gridmix scripts need to be changed to handle input paths differently.  The scripts that submit the small and medium jobs need to be changed. Example of a change: ${VARINFLTEXT}/part-00000,${VARINFLTEXT}/part-00001,${VARINFLTEXT}/part-00002 nees to be changed to ${VARINFLTEXT}/{part-00000,part-00001,part-00002}",test
add name of faulty class to WritableName.getClass IOException upon ClassNotFoundException ,"WritableName.getClass raises an IOException when it cannot load the class. The message is ""WritableName can't load class"" without the name of the class.  Although one can get this from getCause, easier to see it just from the message.  propose change it to:        IOException newE = new IOException(""WritableName can't load class: "" + name);  I will attach a patch. ",io
"HOD gracefully exclude ""bad"" nodes during ring formation","HOD clusters sometimes fail to allocate due to a single ""bad"" node. During ring formation, the entire ring should not be dependent upon every single node being good. Instead, it should either exclude any ring member that does not adequately join the ring in a specified amount of time.  This is a frequent HOD user issue (although not directly caused by HOD).  Examples of bad nodes: Missing java, incorrect version of HOD or Hadoop, local name-cache corrupt, slow network links, drives just beginning to fail, etc.  Many of these conditions are known, and we can monitor for those separately, but this enhancement would shield users from unknown failure conditions that we haven't yet anticipated. This way, a user will get a cluster, instead of hanging indefinitely. ",contrib/hod
Performance benchmark to compare HDFS with local utilities,Test to compare performance of HDFS with local utilities like {{cat}} and {{dd}}. ,test
TestFileSystem fails randomly,"TestFileSystem sometimes fails.  For example, https://issues.apache.org/jira/browse/HADOOP-1911?focusedCommentId=12585857#action_12585857 Add a sleep() to make it more robust.  See also https://issues.apache.org/jira/browse/HADOOP-3139?focusedCommentId=12585821#action_12585821",test
"Deprecate org.apache.hadoop.fs.FileUtil.fullyDelete(FileSystem fs, Path dir)","We should use FileSystem.delete(path, recursive=true) instead. See also https://issues.apache.org/jira/browse/HADOOP-771?focusedCommentId=12586566#action_12586566",fs
Read multiple chunks directly from FSInputChecker subclass into user buffers,Implementations of FSInputChecker and FSOutputSummer like DFS do not have access to full user buffer. At any time DFS can access only up to 512 bytes even though user usually reads with a much larger buffer (often controlled by io.file.buffer.size). This requires implementations to double buffer data if an implementation wants to read or write larger chunks of data from underlying storage.    We could separate changes for FSInputChecker and FSOutputSummer into two separate jiras.    ,fs
Remove deprecated class UTF8,Deprecated class UTF8  needs to be removed. And references to it can be replaced by Text.,io
Provide Hadoop Pipes tutorial,"Hadoop pipes is a neat (and more efficient than streaming) way of writing C++ Hadoop map-reduce applications. It would be great to have a tutorial for beginners and advanced users, similar to the Java MapRed tutorial.",documentation
[HOD] Handle Torque error codes related to security / credential errors,"There a bunch of credential / security related errors that come from Torque server, possibly under high load. HOD already handles one of this code specially, by retrying a bunch of times and giving up. We should probably do the same for other such errors. One of the frequently occuring one is error code 159. Other ones which Rajiv identified are:  PBSE_IVALREQ  PBSE_TOOMANY PBSE_UNKREQ PBSE_PERM PBSE_SYSTEM PBSE_INTERNAL PBSE_BADSTATE PBSE_BADCRED PBSE_EXPIRED PBSE_BADUSER PBSE_QUEBUSY PBSE_NOCONNECTS PBSE_ROUTEREJ PBSE_RESCUNAV PBSE_BADGRP PBSE_BADACLHOST  ",contrib/hod
[HOD] Be less agressive when querying job status from resource manager.,"After a job is submitted, HOD queries torque periodically until it finds the job to be running / completed (due to error). The initial rate of query is once every 0.5 seconds for 20 times, and then once every 10 seconds. This is probably a tad too aggressive as we find that Torque sometimes returns some odd errors under heavy load in the cluster (HADOOP-3216). It may be better to query at a more relaxed rate. ",contrib/hod
Add command line access to named counters,It would be convenient to be able to access counters by name from the command line. Example usage:  bin/hadoop job -counter <job-id> <group-name> <counter-name>,scripts
move Stringifier to org.apache.hadoop.io.serialization package and rename DefaultStringifier to Stringifier,"The Stringinfier already uses the Serializer under the hood, and thus doesn't really need to be parameterized further. I propose that we move the DefaultStringifier to org.apache.hadoop.io.serializer.Stringifier and remove the Stringifier interface.",io
TestJobShell should not create files in the current directory,"After TestJobShell is done, a file ""files_tmp"" will be created in the current directory.  Testing files should be created under the test directory and be cleaned up once the test is done.",test
DoubleWritable,None,io
FTP client over HDFS,An FTP client that stores content directly into HDFS allows data from FTP serves to be stored directly into HDFS instead of first copying the data locally and then uploading it into HDFS. The benefits are apparent from an administrative perspective as large datasets can be pulled from FTP servers with minimal human intervention. ,util
gridmix scripts have a few bugs, maxentToSameCluster should use NUM_OF_LARGE_JOBS_FOR_ENTROPY_CLASS  webdata_sort.large's input dir spec need to use glob syntax. ,test
Extend FileSystem API to allow appending to files,Provide an API to allow applications to append data to pre-existing files.,fs
Reduce the number of public FileSystem.create() methods,Currently we have around 10 different public FileSystem.create() methods. It would be good to see if we can reduce this. ,fs
Path should handle all characters,"Currently Path is limited by URI semantics in the sense that one cannot create files whose names include characters such as "":"" etc.  HADOOP-2066 & HADOOP-3256 are manifestations of this problem. It would be nice if Path handled all characters correctly...",fs
Configuration.substituteVars() needs to handle security exceptions,"Inside Configuration.substituteVars(), there is a call to System.getProperty(var); this contains the implicit assumption that the JVM will never block access to a system property, because if that is the case -such as when the Configuration is running under a restrictive security manager, a SecurityException gets thrown. This will get thrown all the way up the tree.   Better to have some plan to handle it in situ, such as a log@warn level then leave the property unexpanded. ",conf
make Hadoop compile under Apache Harmony,Some small changes are required to get Hadoop Core to compile with Apache Harmony. ,build
Remove deprecated API getFileCacheHints,"Following HADOOP-2027, please remove getFileCacheHints deprecated API",fs
"Remove HOD changes from CHANGES.txt, as they are now inside src/contrib/hod","HOD changes were originally documented in the CHANGES.txt file.  However, to assist ease of commiting patches - as hod committers (me) don't have access to the main CHANGES.txt file, and also because HOD is typically deployed and installed to separate locations, it made sense to have a separate CHANGES.txt for HOD.  This is already present under src/contrib/hod. This issue is to remove the older entries in the main CHANGES.txt, so there is no duplication.",contrib/hod
TestUrlStreamHandler.testFileUrls fails on Windows,org.apache.hadoop.fs.TestUrlStreamHandler.testFileUrls fails on Windows with this exception:  java.net.URISyntaxException: Illegal character in authority at index 7: file://C:\hudson\workspace\Hadoop-WindowsTest\trunk\build\test/thefile   java.net.URI$Parser.fail(URI.java:2816)   java.net.URI$Parser.parseAuthority(URI.java:3154)   java.net.URI$Parser.parseHierarchical(URI.java:3065)   java.net.URI$Parser.parse(URI.java:3021)   java.net.URI.<init>(URI.java:578)   org.apache.hadoop.fs.TestUrlStreamHandler.testFileUrls(TestUrlStreamHandler.java:119)  ,test
The default constructor of BytesWritable should not create a 100-byte array.,"The default byte array should be an empty array and the empty array should be stored in a static final variable.  Then, there is no unnecessary memory allocation.",io
Got org.apache.hadoop.mapred.JobTracker$IllegalStateException while running hadoop job on cluster allocated by hod.,"Allocated cluster using hod of 5 nodes . It returned the both Mapred and Hdfs UI addresses. Also checked out of nodes on 3 taskterakers nodes were using getTaskTrackers() api.  Then tried to run wordcount example and got following -: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.mapred.JobTracker$IllegalStateException: Job tracker still initializing         at org.apache.hadoop.mapred.JobTracker.ensureRunning(JobTracker.java:1722)         at org.apache.hadoop.mapred.JobTracker.getNewJobId(JobTracker.java:1730)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)          at org.apache.hadoop.ipc.Client.call(Client.java:557)         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)         at $Proxy1.getNewJobId(Unknown Source)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)         at $Proxy1.getNewJobId(Unknown Source)         at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:696)   (Here dfs was static with permissions on.) On further checking there was problem in static dfs setting, but Web UI was getting started resulting successful allocation, but was not able create mparedsystem directory causing JobTracker$IllegalStateException. So, It seems if Web UI up then hod assumes that Mapred JT is running (This is rare condition) ",contrib/hod
hod should better errors message when deallocate is fired on non allocated directory.,Current when deallocate on a directory which not allocated hod shows following error -: [ CRITICAL - '<cluster_dir_path>' is not a valid cluster directory. ]  It should show some better error message e.g. CRITICAL - '<cluster_dir_path>' directory is not allocated. ,contrib/hod
hod should check python version before starting any operation.,Hod requires python version should be greater then 2.5.1 (pre-requisite). hod should check python version before doing any operation. ,contrib/hod
bin/hadoop script should check class name before running java,"When the first parameter ($1) cannot be matched with one of existing hadoop commnads, the parameter will be considered as a class name and the script will pass it to java.  For examples, {noformat} bash-3.2$ ./bin/hadoop -version java version ""1.5.0_14"" Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_14-b03) Java HotSpot(TM) Client VM (build 1.5.0_14-b03, mixed mode) bash-3.2$ ./bin/hadoop -help    Usage: java [-options] class [args...]            (to execute a class)    or  java [-options] -jar jarfile [args...]            (to execute a jar file) ... {noformat} The behavior above is confusing.  We should check whether the parameter is a valid class name before passing it to java.",scripts
TestCheckpoint occasionally fails because of the port issues.,"This is related to timing issues. If MiniDFSCluster does not stop fast enough the name-node won't start because the port is busy. So the tests should reset the configuration port properties before each server startup, because servers change those values to the actual port numbers they run. ",test
[HOD] hod does not follow symlinks correctly,"On the local file system, we have a mount point /opt, inside /opt, we have symlink called ""0"" -> . HOD is installed in /opt/0/hod. All the directory values inside the hodrc file utilize the /opt/0/hod values.   When trying to allocate the cluster, it gave the following result:  $ hod allocate -d mycluster -n 5 CRITICAL - No job found, ringmaster failed to run. CRITICAL - Cannot allocate cluster /homes/jim/mycluster  No other additional error messages was produced. To fix this issue, the symlink 0 -> . was removed.   An actual directory called 0 was created in its place and hod was put inside of it.   After the cluster was able to be allocate.    ",contrib/hod
Gridmix jobs'  output dir names may collide,"  Gridmix jobs use time suffix to differentiate output dir names.  The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.  ",test
Being able to set default job configuration values on the jobtracker,"The jobtracker hadoop-site.xml carries custom configuration for the cluster and the 'final' flag allows to fix a value ignoring any override by a client when submitting a job.  There are several properties for which a cluster may want to set some default values (different from the ones in the hadoop-default.xml), for example:   * enabling/disabling compression  * type of compression, record/block  * number of task retries  * block replication factor  * job priority  * tasks JVM options  The cluster default values should apply to submitted jobs when the job submitter does not care about those values. When the job submitter cares, it should include its preferred values. Using the final flag on the jobtracker hadoop-site.xml will lock the value ignoring the value set in the client jobconf.  Currently the only way of doing this is to distribute the jobtracker hadoop-site.xml to all clients and make sure they use it when creating the job configuration.  There are situations where this is not practical:   * In a shared cluster with several clients submitting jobs. It requires redistributing the hadoop-site.xml to all clients.  * In a cluster where the jobs are dispatched by a webapp application. It requires rebundling and redeploying the webapp.  The current behavior happens because the jobconf when serialized, to be sent to the jobtracker, sends all the values found in the hadoop-default.xml bundled with the hadoop JAR file. On the jobtracker side, all those values override all but the 'final' properties of the jobtracker hadoop-site.xml.  According to the javadocs of the Configuration.write(OutpuStream) this should not happen ' Writes non-default properties in this configuration.'  If taken the javadocs as the proper behavior this is a bug in the current implementation and it could be easily fixed by avoiding writing default values on write.  This is a generalization of the problem mentioned in Hadoop-3171. ",conf
Invalid log-destination-uri throws an error messages saying local:// instead of file://,"Hod internally uses the file:// scheme for log-destination-uri (and any URI in general). But when we specify an invalid log-destination-uri, it misguides, asking to use local:// scheme.  {quote} error: invalid 'log-destination-uri' specified in section hodring (--hodring.log-destination-uri): local:// error: additional info: The log destiniation uri must be of type local:// or hdfs://. {quote}  Also, there is a typo - _destiniation_ - in the second message.  ",contrib/hod
Add StackWritable and QueueWritable classes,Adds Writable classes for FIFO Queu and LIFO Stack data structures.,io
distcp leaves empty blocks afte successful execution,"I copied around 40 TB between two hadoop clusters, with distcp running on source.  Job was *successful*, but one destination file was empty because of its only block being empty. None of the distcp log files have any mentioning of this file.  There were a couple of messages in the namenode server log of the destination cluster referencing the file:  hadoop-xxxnamenode-yyy.log.2008-04-19:2008-04-19 02:19:15,666 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: destinationDir/_distcp_tmp_z0g93p/fileName. blk_-9209890281741927376 hadoop-xxx-namenode-yyy.log.2008-04-19:2008-04-19 02:54:45,820 WARN org.apache.hadoop.dfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on destinationDir/_distcp_tmp_z0g93p/fileName file does not exist.  distcp should not rely on the user to double-check. Would it make sense to add a reducer  to compare destination file sizes with source files sizes and do some appropriate action?",util
Allow TextOutputFormat to use configurable separators,"TextOutputFormat use hardcoded tab as key-value separator. We should allow configurable separators like ^A, etc. ",io
org.apache.hadoop.mapred.join.CompositeInputFormat does not initialize  TextInput format files with the configuration resulting in an NullPointerException,"The input formats are not initialized with the Configuration object before the isSplitable method is called.  bin/hadoop jar hadoop-0.16.3-examples.jar  join -r 1 -inFormat org.apache.hadoop.mapred.KeyValueTextInputFormat -outFormat org.apache.hadoop.mapred.TextOutputFormat  -joinOp outer datajoin/input datajoin/output -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.mapred.join.TupleWritable 08/04/22 15:05:33 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId= Job started: Tue Apr 22 15:05:33 GMT-08:00 2008 08/04/22 15:05:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized 08/04/22 15:05:33 INFO mapred.FileInputFormat: Total input paths to process : 2 java.lang.NullPointerException   org.apache.hadoop.mapred.KeyValueTextInputFormat.isSplitable(KeyValueTextInputFormat.java:44)   org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:185)   org.apache.hadoop.mapred.join.Parser$WNode.getSplits(Parser.java:304)   org.apache.hadoop.mapred.join.Parser$CNode.getSplits(Parser.java:374)   org.apache.hadoop.mapred.join.CompositeInputFormat.getSplits(CompositeInputFormat.java:129)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:542)   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:803)   org.apache.hadoop.examples.Join.run(Join.java:149)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.examples.Join.main(Join.java:158)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)   org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)   org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:52)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.util.RunJar.main(RunJar.java:155) ",io
Misleading error message when S3 URI contains hostname containing an underscore,"As reported in http://www.nabble.com/Not-able-to-back-up-to-S3-td16737029.html#a16737029, when the hostname in an S3 URI contains an underscore the exception reports a problem with S3 credentials. In the absence of URI complaining (see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6587184) we can check that the host is non-null before checking user info (since URI.getHost returns null if the host contains an underscore). ",fs/s3
"[HOD] logcondense fails if DFS has files that are not log files, but match a certain pattern","logcondense works by listing files in dfs and match them against a certain pattern. This pattern is incorrect in the sense that it can potentially match files that are not log files. And this can cause it to fail and not delete files correctly as it should.  It should use a correct regular expression that will only list log files. Also, possibly it should log a stack trace if it happens, but continue to delete files and not stop.",contrib/hod
Publish hadoop-core to the apache repository with an appropriate POM file,"To let people downstream build/test with hadoop, using Apache Ivy or Apache Maven2 to pull it down, hadoop-core needs to be published to the apache repository with a .pom file that lists its mandatory dependencies.  In an automated build process, this means -having a template XML pom defining all included dependencies (and excluded transient dependency artifacts) -having a property file driving version numbering of all artifacts -copying this template with property expansion to create the release POM file -public releases only: sticking this POM file up on people.apache.org in the right place, along with the JAR and some .md5 checksums  There's a risk that if the hadoop team dont do this, someone else will (as mahout are doing under http://people.apache.org/~kalle/mahout/maven2/org/apache/hadoop/ ) This is bad as hadoop end up fielding the support calls from someone elses files.   Before automating the process, existing hadoop-core JARs can be pushed out with hand-encoded POM files. The repository police dont allow pom files ever to be changed, so supporting existing releases (.16.2, 0.16.3 ... ) is a way of beta testing the POMs. ",build
Archives in Hadoop.,This is a new feature for archiving and unarchiving files in HDFS. ,fs
[HOD] Miscellaneous logcondense bugs,"Some miscellaneous logcondense related bugs:    - If we don't specify hadoop configuration(via -c), logcondense doesn't behave sanely, and throws an exception. It should instead print an error message and exit, similar to how it behaves with a missing hadoop package(-p).    - version(logcondense.py --version) isn't updated as expected; should sync with HoD version(?).    - help messages for (-d, --days) and (-l, --logs) should be better, they aren't that clear/obvious now.    - We currently don't check the return status of ""hadoop -lsr <path>"" command now. When -lsr command fails with an error (for e.g when JAVA_HOME isn't set), logcondense doesn't report anything and exits silently; this behavior should change .",contrib/hod
[Hod] logcondense improvements/features,"Some improvements/good_to_have features in logcondense    - logcondense currently doesn't give us a way to purge jobtracker/namenode logs, admins might want to have something like this, thought it might be used only rarely.    - Currently when purging logs, it does dfs deletes per cluster specific directly, these deletes could be batched to reduce load on and round trips to namenode.    - logcondense could take hadoop package (-p, --package) from HADOOP_HOME env var, and hadoop config (-c, --config) from HADOOP_CONF_DIR.",contrib/hod
New binary file format,SequenceFile's block compression format is too complex and requires 4 codecs to compress or decompress. It would be good to have a file format that only needs ,io
[HOD]checknodes prints errors messages on stdout,"When pbsnodes doesn't exist in the path or when a queue name is missing, checknodes prints errors messages on stdout instead of stderr. This should change.",contrib/hod
Hadoop rpc metrics do not get pushed to the MetricsRecord,The MetricsRecord for rpc metrics do not get pushed to the client library. Fix is simple and involves invoking the update() method of the MetricsRecord class.,metrics
Hudson should also count javac warnings generated by test code,"Hudson should \-1 patches that add javac warnings in test code, as it does with code in core and contrib.","build,test"
"Return code for ""hadoop dfs -test"" does not match standard return codes from System.exit","The output of ""hadoop dfs -test"" does the opposite of what normal Unix commands to (which is also the opposite of the recommended return codes in the javadocs for System.exit). Normal commands return zero for success and non-zero for error, but ""hadoop dfs -test"" does this opposite. This makes writing shell scripts that use ""hadoop dfs -test"" clunky and unnatural since you can't do standard stuff like this:  {noformat} if hadoop dfs -test -e /missing/file/name; then     # Do something only if the file exists else     # Do something else if the file is missing fi {noformat}  Creating a patch for this would introduce a breaking change and would require changing the Ant DFS task as well.",fs
'make clean' in src/c++/libhdfs does 'rm -rf /*'," If 'make clean' is invoked from the command line in src/c++/libhdfs, rather than by the ant build, it tries to remove all files on your system.  I just discovered this the hard way. ",build
hadoop dfs metrics shows 0,Hadoop dfs Mbean has the following metrics showing as 0.  BlocksReplicated (unimplemented)  ReadMetadataOpNum HeartbeatsNum BlockReportsAverageTime,metrics
"libhdfs: always builds 32bit, even when x86_64 Java used","The makefile for libhdfs is hard-coded to compile 32bit libraries. It should perhaps compile dependent on which Java is set.  The relevant lines are:  LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m32 -Wl,-x CPPFLAGS = -m32 -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)  $OS_ARCH can be (e.g.) amd64 if you're using a 64bit java on the x86_64 platform. So while gcc will try to link against the correct libjvm.so, it will fail because libhdfs is to be built 32bit (because of -m32)  {noformat}      [exec] /usr/bin/ld: skipping incompatible /usr/java64/latest/jre/lib/amd64/server/libjvm.so when searching for -ljvm      [exec] /usr/bin/ld: cannot find -ljvm      [exec] collect2: ld returned 1 exit status      [exec] make: *** [/root/def/hadoop-0.16.3/build/libhdfs/libhdfs.so.1] Error 1 {noformat}  The solution should be to specify -m32 or -m64 depending on the os.arch detected.  There are 3 cases to check:  * 32bit OS, 32bit java => libhdfs should be built 32bit, specify -m32  * 64bit OS, 32bit java => libhdfs should be built 32bit, specify -m32  * 64bit OS, 64bit java => libhdfs should be built 64bit, specify -m64",build
Enhance the hudson-test-patch target,Add eclipse and python testing to patch builds. Clean up Jira message to put +1/-1 at the beginning of each line. Fix some defects in the hudson-test-patch target.,build
TestUrlStreamHandler hangs on LINUX,"TestUrlStreamHandler sets setURLStreamHandlerFactory as {noformat} FsUrlStreamHandlerFactory factory =         new org.apache.hadoop.fs.FsUrlStreamHandlerFactory();     java.net.URL.setURLStreamHandlerFactory(factory); {noformat}  After this, MiniDFSCluster seems to hang while Datanodes tries to register in setNewStorageID, specifically at {noformat} rand = SecureRandom.getInstance(""SHA1PRNG"").nextInt(Integer.MAX_VALUE); {noformat}  jstack output shows that the main thread is stuck in RawLocalFileSystem$LocalFSFileInputStream.read  (Attaching the jstack)",fs
Test Failure on trunk with timeout error,"TestEditLog failed on trunk. Only suspecting exception from log is  {noformat}     [junit] 2008-05-06 11:34:31,441 INFO  ipc.Server (Server.java:run(899)) - IPC Server handler 1 on 36391: exiting     [junit] Exception in thread ""IPC Client (47) connection to localhost/127.0.0.1:36385 from an unknown user"" java.lang.NoClassDefFoundError: org/apache/hadoop/io/IOUtils     [junit]   org.apache.hadoop.ipc.Client$Connection.close(Client.java:527)     [junit]   org.apache.hadoop.ipc.Client$Connection.run(Client.java:430)     [junit] 2008-05-06 11:34:31,444 INFO  ipc.Server (Server.java:run(499)) - Stopping IPC Server Responder     [junit] 2008-05-06 11:34:31,550 INFO  dfs.DataBlockScanner (DataBlockScanner.java:run(567)) - Exiting DataBlockScanner thread.     [junit] Running org.apache.hadoop.dfs.TestEditLog     [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec     [junit] Test org.apache.hadoop.dfs.TestEditLog FAILED (timeout) {noformat}  Here is the console output (http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/482/console)  There are also these NoClassDefFoundError in many of the timeout test failures. ",test
Configuration should accept decimal and hexadecimal values,"This issue is created after the discussions in HADOOP-2461. To summarise, Configuration should also accept hexadecimal values such as 0x1000, in addition to decimal values. Octal representation is not supported, since it may confuse some users.",conf
Implement renames for NativeS3FileSystem,Amazon S3 now supports a copy object operation (http://docs.amazonwebservices.com/AmazonS3/2006-03-01/UsingCopyingObjects.html). We can use this to properly support renames in NativeS3FileSystem.,fs/s3
SequenceFile.Sorter.MergeQueue.next does an unnecessary copy of the key,"SequenceFile.Sorter.MergeQueue.next does an unnecessary copy of the *rawKey*. We should remove that copy, and move code which adjusts the _heap_ from {{next()}} to {{getKey()}}, which should then just return {{ms.getKey()}}, thereby eliminating the extra copy.",io
"hod dealliocate does not xecute properly and throws Operation not permitted error, when hod allcoated cluster is shared with multiple users.","1. Allocate cluster using hadoop with dfs permissions on and this cluster is used by two users.  2. Ran randomtextwriter and distcp jobs. 3. When tried to deallocate hod deallocate threw ""Operation not permitted"" but exitted with exit code 0.    Following the output of deallocate operation -:    [  [2008-05-07 15:01:47,503] DEBUG/10 hadoop:595 - hadoop-ui-log-dir not specified. Skipping Hadoop UI log collection.  [2008-05-07 15:01:47,512] DEBUG/10 hadoop:616 - calling rm.stop  [2008-05-07 15:01:47,559] DEBUG/10 hadoop:618 - completed rm.stop  [2008-05-07 15:01:47,564] CRITICAL/50 hod:517 - op: deallocate cluster_dir failed: <type 'exceptions.OSError'> [Errno 1] Operation not permitted: '<path of hod.temp-dir>/<userid>.<cluster_id>'  [2008-05-07 15:01:47,569] DEBUG/10 hod:518 - Traceback (most recent call last):     File ""/grid/0/hodqa/hod/hod-dev-20080414/hodlib/Hod/hod.py"", line 510, in operation        getattr(self, ""_op_%s"" % opList[0])(opList)     File ""/grid/0/hodqa/hod/hod-dev-20080414/hodlib/Hod/hod.py"", line 365, in _op_deallocate        self.__cluster.deallocate(clusterDir, clusterInfo)     File ""/grid/0/hodqa/hod/hod-dev-20080414/hodlib/Hod/hadoop.py"", line 624, in deallocate        shutil.rmtree(tempDir)     File ""/export/crawlspace/kryptonite/comps//python-2.5.1/lib/python2.5/shutil.py"", line 178, in rmtree        onerror(os.rmdir, path, sys.exc_info())     File ""/export/crawlspace/kryptonite/comps//python-2.5.1/lib/python2.5/shutil.py"", line 176, in rmtree        os.rmdir(path)   OSError: [Errno 1] Operation not permitted: '<path of hod.temp-dir>/<userid>.<clusrter_id>'  [2008-05-07 15:01:47,511] DEBUG/10 hod:522 - return code: 0    ]     Torque got comleted, hod list shows clsuter as dead cluster.    It seems when mapred job is run by other user then the user who allocated the cluster. hdo.temp-dir is getting created with ownership of mapred who ran maped jobs.     So when deallocate operation is fired, by trhe user who allcoated the cluser, hod tries to removes <hod.temp-dir>/<useruid>.<cluster_id> durectory which fails causing dellocate operation to behave oddly.  ",contrib/hod
Can commons-logging.properties be pulled from hadoop-core?,"In the root of hadoop-core.jar is a log4j.properties and a commons-logging.properties  while this provides good standalone functionality to hadoop, it complicates anyone else trying to control the logging, and use the libraries in-process.  In particular, there is a commons-logging.properties file that selects Log4J as the back end. This is not needed as  -log4j is automatically picked up if it is on the classpath   -if it is not on the classpath, asking for it is generally considered bad form If you look at the commons-logging configuration details:  http://commons.apache.org/logging/guide.html#Configuration you will see that that such a properties file takes priority over any setting through system properties, which makes it very hard to override the settings without adding multiple commons-logging.properties files and playing with their priority settings   If you pull the commons-logging.properties file from hadoop-core log4j will still be picked up by default, but it becomes easier for people to turn on different logging infrastructures if they want to. It should have no visible impact on the end user experience (unlike pulling log4j.properties)",build
MBeanUtil dumps stacktrace from registerMBean,"MBeanUtil catches dumps all exception thrown by registerMBean, which prints the stacktrace for InstanceAlreadyExistsException in most of our testcases. It should be fine to catch and ignore it and not print it.",metrics
An ipc log message is printed in FsShell,"When a fs shell command is executed, an ipc log message ""Build a connection to hostname/123.123.123.123:9000"" is printed.  For example  {noformat} bash-3.2$ ./bin/hadoop fs -ls / 08/05/12 17:31:05 INFO ipc.Client: Build a connection to hostname/123.123.123.123:9000      0           2008-05-12 17:29  drwxr-xr-x  tsz  supergroup  /user {noformat}",ipc
[HOD] HOD should have a way to detect and deal with clusters that violate/exceed resource manager limits,"Currently If we set up resource manager/scheduler limits on the jobs submitted, any HOD cluster that exceeds/violates these limits may 1) get blocked/queued indefinitely or 2) blocked till resources occupied by old clusters get freed. HOD should detect these scenarios and deal intelligently, instead of just waiting for a long time/ for ever. This means more and proper information to the submitter.  (Internal) Use Case:      If there are no resource limits, users can flood the resource manager queue preventing other users from using the queue. To avoid this, we could have various types of limits setup in either resource manager or a scheduler - max node limit in torque(per job limit), maxproc limit in maui (per user/class), maxjob limit in maui(per user/class) etc. But there is one problem with the current setup - for e.g if we set up maxproc limit in maui to limit the aggregate number of nodes by any user over all jobs, 1) jobs get queued indefinitely if jobs exceed max limit and 2) blocked if it asks for nodes < max limit, but some of the resources are already used by jobs from the same user. This issue addresses how to deal with scenarios like these.",contrib/hod
"Incorrect sort order if WritableComparable.compareTo() does not return -1, 0 or 1","I've found that incorrect sort orders are returned in some cases if the WritableComparable.compareTo() method doesn't return  -1, 0 or 1.   I believe this is a bug as the compareTo() interface states that the returned int be only a -ve or +ve number, and will potentially catch a lot of people out who decide to write a WritableComparator (well it caught me out anyway!).   I'll attach an example application to demonstrate -- I simply modified the sort example to specify a non-default comparator to sort LongWritable , i.e.:  public int compare(WritableComparable a, WritableComparable b) {   LongWritable longA =(LongWritable) a;   LongWritable longB =(LongWritable) b;    return (int) (longA.get() - longB.get());  // wrong sort order   // return (int) Math.signum(longA.get() - longB.get());  // correct sort order }   When I run the application through Hadoop on my input data it returns the incorrect sort order.",io
"Document the ""stream.non.zero.exit.status.is.failure"" knob for streaming","HADOOP-2057 added a useful feature: ""stream.non.zero.exit.status.is.failure"" to optionally treat non-zero exit code from streaming apps as fatal. We should document this on http://hadoop.apache.org/core/docs/current/streaming.html.",documentation
need comparators in serializer framework,"The new serialization framework permits Hadoop to incorporate different serialization systems, including Hadoop's Writable, Thrift, Java Serialization, etc.  It provides a generic, extensible means (SerializationFactory) to create serializers and deserializers for arbitrary Java classes.  However it does not include a generic means to create comparators for these classes.  Comparators are required for MapReduce keys and many other computations.  Thus we should enhance the serialization framwork to provide comparators too.",io
TestDatanodeBlockScanner failed while trying to corrupt replicas,Blocks now have generation stamp associated with them. This unit test does a Block.toString() to find out the name of the block. Instead it should use Block.getBlockName(). ,test
[HOD] HOD unit test RunHodCleanupTests fails on solaris,"HOD unit test RunHodCleanupTests fails on solaris and this was first observed while submitting HADOOP-3023. Hudson failed to run this test altogether, the first time. The Second time, it took 300 secs to finish, instead of returning immediately as observed on RHEL boxes. Because of this, HADOOP-3023 is blocked, thereby no hod unit test can be run at all by Hudson.",contrib/hod
Namenode.initialize() sets the default filesystem uri to an ip6 address (0:0:0:0:0:0:0:0),"When starting the namenode, I would get the following exception:  Caused by: java.io.IOException: Incomplete HDFS URI, no host/port: hdfs://0:0:0:0:0:0:0:0:50051   org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:66)   org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1275)   org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)   org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1286)   org.apache.hadoop.fs.FileSystem.get(FileSystem.java:208)   org.apache.hadoop.fs.FileSystem.get(FileSystem.java:108)   org.apache.hadoop.fs.Trash.<init>(Trash.java:62)   org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:138)   org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:180)   org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:166)   I tracked this down to NameNode.java line 127 and 128. The socetAddress returned by this.server.getListenerAddress() is an ipv6 style address with colons. Then this is set as the default filesystem which causes problems on the next call to FileSystem.get.  I replacedthe line: this.nameNodeAddress = this.server.getListenerAddress(); with this.nameNodeAddress = socAddr;  And this made it work for me. However, I gather this would break support  for ephemeral ports? Is there a better way for me to fix this, maybe disabling ipv6 elsewhere? ",fs
Debug log not removed in ipc client,I think a debug message got committed in HADOOP-2188.  {noformat} $ bin/hadoop fs -ls 08/05/15 23:50:07 INFO ipc.Client: Build a connection to localhost:127.0.0.1:8020 {noformat},ipc
"Update FileBench to use the ""work"" directory for SequenceFileOutputFormat","HADOOP-3041 changed SequenceFileOutputFormat::getRecordReader to use a different property for its output path. This is set by a package-private, static function in FileOutputFormat. Since FileBench uses this OutputFormat directly and is not in o.a.h.mapred, it either needs to specify this parameter manually or setWorkOutputPath needs to be public.",test
Document controls for profiling maps & reduces,HADOOP-2367 and further improvements added the ability to profile map and reduce tasks. We should document these in http://hadoop.apache.org/core/docs/current/mapred_tutorial.html.,documentation
Provide a query method to find all of the attributes under a given root,"It would be really nice to have a way to query all of the attributes under a given root:  {code} String[] attrs = conf.getChildAttributes(""mapred.task""); {code}  which would return all of the attributes in the configurat that start with ""mapred.task...."" ",conf
Change FSNamesytem status metrics to IntValue,"HADOOP-3058 introduced new FSNamesystem status metrics. Most of them were LongValue, but there might be cases where LongValues are not properly  handled by metrics collectors. For now, we should change it to IntValue.",metrics
SequenceFile.Reader doesn't use the Serialization framework,"Currently SequenceFile.Reader only works with Writables, since it doesn't use the new Serialization framework. This is a glaring considering that SequenceFile.Writer uses the Serializer and handles arbitrary types via the SerializationFactory.",io
Facility to query serializable types such as Writables for 'raw length',Currently we need to jump through hoops to get the 'raw length' of serializable types for e.g. SequenceFile.Writer.append needs to copy the key/value into a buffer and then check the buffer's size to figure the record/key/value lenghts. Obviously this can be improved to do away with the extra copy if we had types which could be queried for it's raw-length.  Thoughts?,io
Recover the deprecated mapred.tasktracker.tasks.maximum,"https://issues.apache.org/jira/browse/HADOOP-1274 replaced the configuration attribute mapred.tasktracker.tasks.maximum with mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum because it sometimes make sense to have more mappers than reducers assigned to each node.  But deprecating mapred.tasktracker.tasks.maximum could be an issue in some situations. For example, when more than one job is running, reduce tasks + map tasks eat too many resources. For avoid this cases an upper limit of tasks is needed. So I propose to have the configuration parameter mapred.tasktracker.tasks.maximum as a total limit of task. It is compatible with mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum.  As an example:  I have a 8 cores, 4GB, 4 nodes cluster. I want to limit the number of tasks per node to 8. 8 tasks per nodes would use almost 100% cpu and 4 GB of the memory. I have set: ?  mapred.tasktracker.map.tasks.maximum -> 8   ?mapred.tasktracker.reduce.tasks.maximum -> 8   1) When running only one Job at the same time, it works smoothly: 8 task average per node, no swapping in nodes, almost 4 GB of memory usage and 100% of CPU usage.   2) When running more than one Job at the same time, it works really bad: 16 tasks average per node, 8 GB usage of memory (4 GB swapped), and a lot of System CPU usage.  So, I think that have sense to restore the old attribute ?mapred.tasktracker.tasks.maximum making it compatible with the new ones.  Task trackers could not:  - run more than mapred.tasktracker.tasks.maximum tasks per node,  - run more than ?mapred.tasktracker.map.tasks.maximum mappers per node,   - run more than ?mapred.tasktracker.reduce.tasks.maximum reducers per node. ",conf
"Ganglia counter metrics are all reported with the metric name ""value"", so the counter values can not be seen","The JobInProgress class reports all metrics with the name ""value"". The FileMetrics class puts all of the tags into the name when reporting the individual values, but the Ganglia Context does not put the tags into the name..  This patch modifies the context to build names for the counter metrics out of the tag values. This enables the user to see the indivdual counter values with the ganglia web tool, on a per job basis",metrics
Implement 'ant test' as map/reduce task,It would be very helpful if we could make 'ant test' target to run as Map/Reduce program. It helps testing patches locally before submitting to hudson. ,build
mapred.job.tracker default value/docs appear out of sync with code,"This is the settings for mapred.job.tracker in SVN_HEAD. If you dont override these, you get a RuntimeException <property>   <name>mapred.job.tracker</name>   <value>local</value>   <description>The host and port that the MapReduce job tracker runs   at.  If ""local"", then jobs are run in-process as a single map   and reduce task.   </description> </property> ",conf
NPE if job tracker started and system property hadoop.log.dir is not set,"This is a regression. If the system property ""hadoop.log.dir"" is not set, the job tracker NPEs rather than starts up. ",metrics
Add tests to Local Directory Allocator for asserting their URI-returning capability,"Original comment:    {quote}Local directory allocator returns a bare path, without a URI specifier.  This means that calling Path.getFileSystem will do the wrong thing with the returned path.   Should really stick a ""file://"" in front.    Also it's test cases need to be improved to make sure this class works fine.  {quote}    Only the latter needed to be done (see below for discussion).",fs
test-libhdfs fails on Linux,"test-libhdfs fails on Linux  Console output:      [exec] Opened /tmp/testfile.txt for writing successfully...      [exec] Wrote 14 bytes      [exec] Current position: 14      [exec] Flushed /tmp/testfile.txt successfully!      [exec] hdfsAvailable: 14      [exec] Current position: 1      [exec] Read following 13 bytes:      [exec] ello, World!      [exec] Read following 14 bytes:      [exec] Hello, World!      [exec] hdfsCopy(remote-local): Success!      [exec] hdfsCopy(remote-remote): Success!      [exec] hdfsMove(local-local): Success!      [exec] hdfsMove(remote-local): Success!      [exec] hdfsRename: Success!      [exec] hdfsCopy(remote-remote): Success!      [exec] hdfsCreateDirectory: Success!      [exec] hdfsSetReplication: Success!      [exec] hdfsGetWorkingDirectory: hdfs://localhost:23000/user/hadoopqa      [exec] hdfsSetWorkingDirectory: Success!      [exec] hdfsGetWorkingDirectory: /tmp      [exec] hdfsGetDefaultBlockSize: 67108864      [exec] hdfsGetCapacity: 1645537427456      [exec] hdfsGetUsed: 24576      [exec] hdfsGetPathInfo - SUCCESS!      [exec] Name: /tmp, Type: D, Replication: 0, BlockSize: 0, Size: 0, LastMod: Tue May 27 03:27:35 2008      [exec] Name: hdfs://localhost:23000/tmp/newdir, Type: D, Replication: 0, BlockSize: 0, Size: 0, LastMod: Tue May 27 03:27:35 2008      [exec] Name: hdfs://localhost:23000/tmp/testfile.txt, Type: F, Replication: 2, BlockSize: 67108864, Size: 14, LastMod: Tue May 27 03:27:35 2008      [exec] Name: hdfs://localhost:23000/tmp/testfile2.txt, Type: F, Replication: 1, BlockSize: 67108864, Size: 14, LastMod: Tue May 27 03:27:35 2008      [exec] Exception in thread ""main"" java.lang.NoSuchMethodError: getFileCacheHints      [exec] stopping datanode      [exec] stopping namenode      [exec] exiting with 1      [exec] make: *** [test] Error 1",test
ipc.Client.close() throws NullPointerException,"There are two possible cases that Client.close() throws NullPointerException  - Exception in thread ""IPC Client (47) connection to localhost/127.0.0.1:3070 from tsz"" java.lang.NullPointerException     at org.apache.hadoop.ipc.Client$Connection.close(Client.java:521)     at org.apache.hadoop.ipc.Client$Connection.run(Client.java:434)  - Exception in thread ""Thread-2"" java.lang.NullPointerException     at org.apache.hadoop.ipc.Client$Connection.close(Client.java:519)     at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:317)     at org.apache.hadoop.ipc.Client$Connection.access$1700(Client.java:175)     at org.apache.hadoop.ipc.Client.getConnection(Client.java:766)     at org.apache.hadoop.ipc.Client.call(Client.java:680)     at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)     ...",ipc
IPC.Client synchronisation looks weak,"Looking at HADOOP-3453 , its clear that Client.java is inconsistently synchronized  1. the running and shouldCloseConnection flags are not always read/written in synchronized blocks, even though they are properties used to share information between threads. They should be marked as volatile for access outside synchronized blocks, and all read-check-update operations must be synchronized.  2. there are multiple calls to System.currentTimeMillis() in synchronized blocks; this is a slow native operation and should ideally be done unsynchronized.  3. Synchronizing on the (out) stream is dangerous as its value changes during the life of the class, and sometimes it is null. These blocks should all synchronize on the Client instead.  4.  There are a number of places where InterruptedExceptions are caught and ignored in a sleep-wait loop:      } catch (InterruptedException e) {       }     This isn't dangerous, but it does make the client harder to stop. These code fragments should be looked at carefully.",ipc
IPC.Client connect timeout should be configurable,"In ipc.Client.setupIOStreams, the connect timeout is hard-coded to 20 seconds              // connection time out is 20s             this.socket.connect(remoteId.getAddress(), 20000);  This could be made configurable for deployments where a longer connect time is desired, or where a shorter connect time would detect failure faster.",ipc
IPC.Client retry delay should be interruptible,"The delay of ipc.Client.handleConnectionFailure() that causes the client to block and wait is hard coded at 1 second; any interruption of the thread is treated as a shortening of the delay.          // otherwise back off and retry        try {          Thread.sleep(1000);        } catch (InterruptedException ignored) {}          1. this delay could be configurable; different clients may wish to retry more/less often. or at a slightly different rate than their peers.    2. interrupting the thread may imply the client has been requested to terminate -this could be handled by declaring failure early, rather than continuing to retry until the retry count is exceeded.",ipc
hadoop scripts don't change directory to hadoop_home,"The hadoop scripts don't change the current directory to one that should be local, which means that your home directory will likely be mounted across the cluster. It would be much better to cd in hadoop-daemon.sh to the hadoop home directory.",scripts
[HOD] HOD can improve error messages by reporting failures on compute nodes back to hod client,"This issue addresses error messages w.r.t failures on compute nodes, while HADOOP-3151 addresses error messages in hod client.",contrib/hod
The javadoc for FileSystem.deleteOnExit should have more description,#NAME?,documentation
Compile error: FTPFileSystem.java:26: cannot access org.apache.commons.net.ftp.FTP,"    [javac] Compiling 496 source files to d:\@sze\hadoop\latest\build\classes     [javac] d:\@sze\hadoop\latest\src\java\org\apache\hadoop\fs\ftp\FTPFileSystem.java:26: cannot access org.apache.commons.net.ftp.FTP     [javac] bad class file: d:\@sze\hadoop\latest\lib\commons-net-1.4.1.jar(org/apache/commons/net/ftp/FTP.class)     [javac] class file has wrong version 50.0, should be 49.0     [javac] Please remove or make sure it appears in the correct subdirectory of the classpath.     [javac] import org.apache.commons.net.ftp.FTP;     [javac]                                   ^     [javac] Note: Some input files use or override a deprecated API.     [javac] Note: Recompile with -Xlint:deprecation for details.     [javac] 1 error  ",fs
Bad coding style: The member fields in org.apache.hadoop.ipc.metrics.RpcMetrics are public,"In org.apache.hadoop.ipc.metrics.RpcMetrics, {code} //the following are member fields   public MetricsTimeVaryingRate rpcQueueTime = new MetricsTimeVaryingRate(""RpcQueueTime"");   public MetricsTimeVaryingRate rpcProcessingTime = new MetricsTimeVaryingRate(""RpcProcessingTime"");    public Map <String, MetricsTimeVaryingRate> metricsList = Collections.synchronizedMap(new HashMap<String, MetricsTimeVaryingRate>()); {code} Then, the fields are accessed directly in other classes.  For example, org.apache.hadoop.ipc.RPC.Server.call(...) {code} ...  MetricsTimeVaryingRate m = rpcMetrics.metricsList.get(call.getMethodName());   if (m != null) {   m.inc(processingTime);  }  else {   rpcMetrics.metricsList.put(call.getMethodName(), new MetricsTimeVaryingRate(call.getMethodName()));   m = rpcMetrics.metricsList.get(call.getMethodName());   m.inc(processingTime);  } {code}",metrics
TestIndexedSort sometimes fails,"After testing an array of equal values, TestIndexedSort::testAllEqual introduces a min and max value at random intervals: {noformat}     int diff = r.nextInt(SAMPLE);     values[diff] = 9;     values[(diff + r.nextInt(SAMPLE >>> 1)) % SAMPLE] = 11; {noformat}  The max value can overwrite the min value and produce a spurious failure.",test
MapFile.Reader getClosest() function returns incorrect results when before is true,"The MapFile.Reader getClosest() method returns incorrect results due to an error in seekInternal(). The test case in trunk sets the index interval of the test MapFile to 1 which obscures the issue. There are several other errors in the test case as well that assert incorrect behavior for getClosest().  I've got this fixed and tested, and will attach a patch.",io
io.sort.factor should default to 100 instead of 10,10 is *really* conservative and can make merges much much more expensive.,conf
"Need an option -rm and -rmr to *not* move to Trash, but actually delete","By default, when Trash is enabled, -rm and -rmr just move files/directories to .Trash, which are actually deleted after some time set in the config.  When DFS gets full, we need to quickly create space on it by deleting files, so it will be good to have a --force option to both -rm and -rmr, so that files are actually deleted and not moved.",fs
release tar.gz contains duplicate files,None,build
Implement configuration items useful for Hadoop resource manager (v1),"HADOOP-3421 lists requirements for a new resource manager for Hadoop. Implementation for these will require support for new configuration items in Hadoop. This JIRA is to define such configuration, and track it's implementation.",conf
Need to update Eclipse template to reflect current trunk,"Since there are new libraries (e.g. src/test/lib) and new contrib package (e.g. src/contrib/fuse-dfs) added, the current Eclipse template does not work.  Also, it would be great if there is a README for the Eclipse template.",build
[HOD] Improvements with cluster directory handling,"The following improvements are asked for from users related to cluster directory handling: - Create a new cluster directory if one does not exist. - If a cluster directory points to a dead cluster, currently allocate fails with a message asking user to deallocate it first. Instead, it should issue a warning, deallocate the cluster and automatically allocate a fresh one.",contrib/hod
"the rsync command in hadoop-daemon.sh also rsync the logs folder from the master, what deletes the datanode / tasktracker log files.",A stop and start deletes the log files on the datanodes.,scripts
Improve S3FileSystem data integrity using MD5 checksums,Make use of S3 MD5 checksums to verify writes and reads.,fs/s3
Support legacy S3 buckets containing underscores,"For bucket names containing an underscore we fail with an exception, however it should be possible to support them. See proposal in https://issues.apache.org/jira/browse/HADOOP-930?focusedCommentId=12601991#action_12601991 by Chris K Wensel.",fs/s3
TestHarFileSystem.testArchives fails,org.apache.hadoop.fs.TestHarFileSystem.testArchives fails on trunk with the following exception  java.lang.ArrayIndexOutOfBoundsException: -1   org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:255)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:684)   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:964)   org.apache.hadoop.fs.TestHarFileSystem.testArchives(TestHarFileSystem.java:184),fs
File globbing with a PathFilter is too restrictive,Consider the file hierarchy  {noformat} /a /a/b {noformat}  Calling the globStatus method on FileSystem with a path of {noformat}/*/*{noformat} and a PathFilter that only accepts {{/a/b}} returns no matches. It should return a single match: {{/a/b}}.  ,fs
File globbing alternation should be able to span path components,"For example, {/a/b,/c/d} should expand to /a/b and /c/d. This change would also permit a consistent syntax for specifying multiple input paths to MapReduce, streaming and Pig by specification of a single glob path with alternation {/a/b,/c/d}, rather than a collection of comma separated glob paths /a/b,/c/d.  This change would also make globbing more consistent with bash, which supports this feature.",fs
deprecate InMemoryFileSystem,"As of HADOOP-2095, InMemoryFileSystem is no longer used.  Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",fs
omissions in HOD documentation,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that  hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized.  I don't see any references to either in the documentation, and it'd be great to write this down.","contrib/hod,documentation"
Occasional NPE in Jets3tFileSystemStore,"In extraordinary circumstances (eg. S3 outages), calling S3FileSystem functions will throw NullPointerExceptions when trying to read from the filesystem. I've traced this down to calls to Jets3tFileSystemStore.get().  Both get() functions catch an S3ServiceException, and check its error code using a string comparison. However, the underlying libs3t library will sometimes produce an exception with a null error code string. This results in an NPE that propagates all the way to the S3FileSystem, and to the caller.  To fix this, Jets3tFileSystemStore lines 196 and 212 should be changed from: bq.         if (e.getS3ErrorCode().equals(""NoSuchKey"")) { to: bq.         if (""NoSuchKey"".equals(e.getS3ErrorCode())) {  ",fs/s3
Improve NNThroughputBenchmark log messages.,The block report benchmark should not print return values of heartbeats. It should also leave safe mode before creating files.,test
TestHarFileSystem.testArchives fails with NullPointerException,TestHarFileSystem.testArchives fails with NPE:  {code} java.lang.NullPointerException   org.apache.hadoop.mapred.JobClient.getSystemDir(JobClient.java:1478)   org.apache.hadoop.tools.HadoopArchives.archive(HadoopArchives.java:315)   org.apache.hadoop.tools.HadoopArchives.run(HadoopArchives.java:653)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.fs.TestHarFileSystem.testArchives(TestHarFileSystem.java:131) {code},test
Want NIO.2 (JSR 203) file system provider for Hadoop FileSystem,"JSR 203 (aka ""NIO.2"" or ""more NIO"") is defining a rich set of classes for interacting with files and file systems (as well as other NIO enhancements). It is scheduled to be released as a part of Java 7.  This motivation behind this issue is to see if NIO.2 can be used as an interface to Hadoop's FileSystem class before NIO.2 is finalized, thus giving Hadoop developers an opportunity to influence NIO's design (if necessary). Also, learning more about NIO.2 may inform design decisions for Hadoop filesystems.  The starting point for this work should be the java.nio.file.spi package (http://openjdk.java.net/projects/nio/javadoc/java/nio/file/spi/package-summary.html). There is an example of a filesystem provider (for ZIP files) linked from the OpenJDK page for NIO.2: http://openjdk.java.net/projects/nio/. This page also has other useful links, such as a JavaOne talk, javadoc and source code.",fs
"[HOD] If a job does not exist in Torque's list of jobs, HOD allocate on previously allocated directory fails.","HADOOP-3483 addressed the issue where a dead cluster could be reallocated without having to issue warnings to users to clean up the directory themselves, provided the job is completed. It missed one case, where the job no longer exists in the Torque queue. When tried in that case, HOD fails with a bad error message: ERROR - qstat error: exit code: 153 | signal: False | core False CRITICAL - op: allocate hod-clusters/test 3 failed: <type 'exceptions.TypeError'> 'NoneType' object is unsubscriptable  This should be addressed to avoid user concerns. ",contrib/hod
hod script command deallocate previously allocated cluster even if that cluster is alive,hod script command deallocate previously allocated cluster even if that cluster is alive Following are the steps  -: 1. Ran hod allocate -d ~/cdir -n 10 2. Again Ran hod script -d ~/cdir -n 10 -s scriptfile.sh hod allocate  fails to allocate new cluster for script and fail with error -: [ CRITICAL/50 hod:310 - Found a previously allocated cluster at cluster directory '~/cdir'. Deallocate the cluster first. CRITICAL/50 hod:647 - Error 12 in allocating the cluster. Cannot run the script. ]    but hod deallocates previous cluster and exits with exit code 0.,contrib/hod
Metrics FilesCreated and files_deleted metrics do not match. ,"This JIRA is to fix the files created vs files deleted count. Looks like files created counts number of file created while files deleted counts number of inodes deleted. For now, it would be good to fix files created to match files deleted. ",metrics
Hod does not  report job tracker failure on hod client side when job tracker fails to come up,"Hod does not  report job tracker failure on hod client side when job tracker fails to come up.  When max-master-failure > 1 hod client does not properly show why job tracker failed to come up, while in case namenode proper error message is displayed. Also in namenode failure ringmaster log contains information such as -: ""Detected errors (3) beyond allowed number of failures (2). Flagging error to client"" while no such information is there in ringmaster log for job tracker failures",contrib/hod
Create build targets to create api change reports using jdiff,"I'd like to add support for jdiff to our build scripts, to enable generating api difference reports between versions.",build
IOUtils.close needs better documentation,The JavaDoc for IOUtils.close is incorrect and misleading.,io
Cygwin: cygpath displays an error message in running bin/hadoop script,"- When running ./bin/hadoop in cygwin, we see an error message from cypath. For example, {noformat} bash-3.2$ ./bin/hadoop namenode -format cygpath: cannot create short name of :\cygdrive\d\@sze\hadoop\latest\hadoop-*-tools.jar:\cygdrive\d\@sze\hadoop\latest\build\hadoop-*-tools.jar ... {noformat}  - The output message is confusing for incorrect commands. For example, {noformat} bash-3.2$ ./bin/hadoop foo cygpath: cannot create short name of :\cygdrive\d\@sze\hadoop\latest\hadoop-*-tools.jar:\cygdrive\d\@sze\hadoop\latest\build\hadoop-*-tools.jar java.lang.NoClassDefFoundError: foo Exception in thread ""main"" bash-3.2$  {noformat} ",scripts
Namespace recovery from the secondary image should be documented.,"HADOOP-2585 introduced a new feature which lets starting name-node from an image stored on the secondary node. This need to be documented in ""HDFS Admin Guide"".",documentation
Need to increment the year field for the copyright notice,Generated documentation is still using 2007.,documentation
"The command ""archive"" is missing in the example in  docs/hadoop_archives.html (and pdf)","currently, the example reads  hadoop -archiveName foo.har /user/hadoop/dir1 /user/hadoop/dir2 /user/zoo/  it should be   hadoop archive -archiveName foo.har /user/hadoop/dir1 /user/hadoop/dir2 /user/zoo/",documentation
Improve documentation about distributing native libraries via DistributedCache,An illustrative example (with a code snippet) for distributing/loading native libraries via DistributedCache would aid a lot.,documentation
The tools.jar is not included in the distribution,The build.xml doesn't copy all of the jars from the build dir into the release.,build
forrest doc for hadoop commands,Document hadoop command line options in forrest. HADOOP-2908 covered the DFS options. This jira is for commands other than the DFS Shell.,documentation
Substitute the synchronized code in MD5Hash to avoid lock contention. Use ThreadLocal instead.,"Currently the MD5Hash class uses a singleton instance of the MessageDigester. The access to this instance is synchronized, so MD5Hash has performance problems when used from several Threads. I propose to substitute the singleton instance by a TheadLocal instances cache. I will provide a patch. ",io
Sometime after successful  hod allocation datanode fails to come up with java.net.BindException for dfs.datanode.ipc.address,"From Jira: HADOOP-3283 which introduced new conf parameter dfs.datanode.ipc.address which defaults to 50020. When static dfs of hadoop version 0.18.0 running and its conf is not having dfs.datanode.ipc.address specified, then datanode start with 50020 port for ipc, w When we  use  hod allocate without using static dfs.datanode on some machine fails to come. On further investigation  it has been found sometimes when torque provides list nodes, that list also contain some static dfs node. When hodring tries to start datanode on a machine where a static dfs datanode of hadoop 0.18.0 is running, hod's dynamic dfs datanode fails to come with exception -: java.net.BindException: Problem binding to /0.0.0.0:50020 : Address already in use beacuse hod provides ports for dfs.datanode.address and dfs.datanode.http.address.    ",contrib/hod
Don't need to use toString() on strings (code cleanup),Don't need to call toString on a String type.  This occurs in several places in the test code.  Patches below:   ,test
clover target broken after src restructuring,"After HADOOP-2916, the clover target no longer instruments the code since src/java patch no longer exists in Hadoop.",build
Support property groups in Hadoop configuration,Hadoop configuration is currently a list of key and value pairs. There are some use-cases to support configuring groups of related properties. There could also be multiple instances of such groups. The issue is for adding support for such configuration.,conf
Improve how Hadoop gets configured,"Improve how Hadoop gets configured  The current approach of a two-level XML configuration file works; it offers  * default values in (easily overridden) configuration files, rather than just Java source  * A way to override the default values  * conversion from string to float, double.  * with the ${property} evaluation, there is some ability to reference system values for some limited adaptation.  * errors show up at parse time (except for value parse problems)  * A serialization format to exchange configuration with other nodes   * the possibility of updating a local (in-memory) configuration    But it has limits [1] Requires a change to the XML files to be pushed out to every node [2] Differences between configurations can cause obscure bugs [3] No support for complex hierarchical configurations [4] No easy way to cross-reference data other than copy and paste. [5] No way for a deployed instance to update configuration data for other instances to query [6] Value type checking/dereferencing failure is not signalled by a custom error; there is no explicit exception on any of the get/set operations. [7] No consistency with names.  [8] Not easily managed by different configuration architectures/tools  This issue is to group/track the problems, then discuss solutions.    ",conf
Pull out configuration element names from inline code to interface classes for use across Hadoop and other apps,"If we had different interfaces for the different configuration names as a set of constants, instead of using inline strings  1. we could detect naming inconsistencies (and spelling errors) faster 2. it would be easier for other applications (downstream, such as HBase) or tools that set the parameters) to actually set the parameters.  3. It would be eaiser to change the names, if desired. 4. it would be easy to detect all usages within a single project.  The interface classes could be hand generated (turn existing references into constants), or they could be auto-generated from an XML template. The latter would work if the default values included a definition of every possible supported attribute, somehow grouped so that jobconf values could be kept separate from dfs constants.  There is an incomplete hand-generated example at  http://smartfrog.svn.sourceforge.net/viewvc/smartfrog/trunk/core/components/hadoop/src/org/smartfrog/services/hadoop/conf/ConfigurationAttributes.java?view=markup ",conf
Add an explicit HadoopConfigurationException that extends RuntimeException,"It is possible for a get() or set() operation to throw an exception today, especially if a security manager is blocking property access. As more complex cross-references are used, the likelihood for failure is higher.  Yet there is no way for a Configuration or subclass to throw an exception today except by throwing a general purpose RuntimeException.  I propose having a specific HadoopConfigurationException that extends RuntimeException. Classes that read in configurations can explicitly catch and handle these. The exception could * be raised on some parse error (a float attribute is not a parseable float, etc) * be raised on some error caused by an implementation of a configuration service API * wrap underlying errors from different implementations (like JNDI exceptions) * wrap security errors and other generic problems  I'm not going to propose having specific errors for parsing problems versus undefined name,value pair though that may be useful feature creep. It certainly makes bridging from different back-ends trickier.   This would not be incompatible with the existing code, at least from my current experiments. What is more likely to cause problems is having the get() operations failing, as that is not something that is broadly tested (yet). If we do want to test it, we could have a custom mock back-end that could be configured to fail on a get() of a specific option.",conf
Hardware Failure Monitoring in large clusters running Hadoop/HDFS,"At IBM we're interested in identifying hardware failures on large clusters running Hadoop/HDFS. We are working on a framework that will enable nodes to identify failures on their hardware using the Hadoop log, the system log and various OS hardware diagnosing utilities. The implementation details are not very clear, but you can see a draft of our design in the attached document. We are pretty interested in Hadoop and system logs from failed machines, so if you are in possession of such, you are very welcome to contribute them; they would be of great value for hardware failure diagnosing.    Some details about our design can be found in the attached document failmon.doc. More details will follow in a later post.",metrics
contrib/data_join needs unit tests,"To insure against future incompatibility, there should be unit tests for the contrib/data_join framework.",test
org.apache.hadoop.fs.FileUtil.checkDependencies may want to use .equals() instead of ==,"org.apache.hadoop.fs.FileUtil.checkDependencies checks for source overwriting dest by first checking for filesystem equality:  if (srcFS == dstFS) {  ..more checks here  }  So, its assuming that you can check filesystem by pointer equality, whereas if you create two filesystems and initialise them, they will still refer to the same remoteFS.  Consider assuming URI equality ==filesystem equality, and either to do a FileSystem.equals() that checks that , or do the check in FileUtil.getDependencies",fs
org.apache.hadoop.fs.FileUtil.copy() will leak input streams if the destination can't be opened,"FileUtil.copy()  relies on IOUtils.copyBytes() to close the incoming streams, which it does. Normally.  But if dstFS.create() raises any kind of IOException, then the inputstream ""in"", which was created in the line above, will never get closed, and hence be leaked.        InputStream in = srcFS.open(src);       OutputStream out = dstFS.create(dst, overwrite);       IOUtils.copyBytes(in, out, conf, true);  Some try/catch wrapper around the open operations could close the streams if any exception gets thrown at that point in the copy process.",fs
Update MapRed tutorial,This issue is about updating the mapred tutorial with the changes from 0.17 to 0.18.,documentation
SortValidator always uses the default file system irrespective of the actual input,"In SortValidator, the underlying file system should be obtained from the given inputs (sortInput and sortOutput). It is currently assumed to be the default (HDFS) always.   So, the following usage does not work  bin/hadoop jar hadoop-0.19.0-dev-test.jar testmapredsort -sortInput har:///user/jothipn/foo.har/user/jothipn/input -sortOutput  output  ",test
Added an abort on unset AWS_ACCOUNT_ID to luanch-hadoop-master,"I wanted to test out playing with the ec2 contributed shell scripts to kick off a Hadoop EC2 cluster however I found that without a set AWS_ACCOUNT_ID the script would still continue to run but not properly set the authorization groups due to the lack of an AWS Account ID parameter.  I have attached a quick patch to launch-hadoop-master to double check the existence of AWS_ACCOUNT_ID and abort the script as early as possible and present a useful error message to the user.   Below is the pach in cut and paste   --- hadoop-0.17.0/src/contrib/ec2/bin/launch-hadoop-master 2008-05-15 16:20:14.000000000 +0900 +++ hadoop-0.17.0-modified/src/contrib/ec2/bin/launch-hadoop-master 2008-06-20 11:58:59.000000000 +0900 @@ -29,6 +29,12 @@  bin=`cd ""$bin""; pwd`  . ""$bin""/hadoop-ec2-env.sh   +if [ -z $AWS_ACCOUNT_ID ]; then +  echo ""AWS_ACCOUNT_ID is not configured properly!  Please check"" +  echo ""AWS_ACCOUNT_ID: $AWS_ACCOUNT_ID"" +  exit -1 +fi +  echo ""Testing for existing master in group: $CLUSTER""  MASTER_EC2_HOST=`ec2-describe-instances | awk '""RESERVATION"" == $1 && ""'$CLUSTER_MASTER'"" == $4, ""RESERVATION"" == $1 && ""'$CLUSTER_MASTER'"" != $4'`  MASTER_EC2_HOST=`echo ""$MASTER_EC2_HOST"" | awk '""INSTANCE"" == $1 && ""running"" == $6 {print $4}'` @@ -108,4 +114,4 @@  ssh $SSH_OPTS ""root@$MASTER_EC2_HOST"" ""chmod 600 /root/.ssh/id_rsa""    MASTER_IP=`dig +short $MASTER_EC2_HOST` -echo ""Master is $MASTER_EC2_HOST, ip is $MASTER_IP, zone is $MASTER_EC2_ZONE."" \ No newline at end of file +echo ""Master is $MASTER_EC2_HOST, ip is $MASTER_IP, zone is $MASTER_EC2_ZONE."" ",contrib/cloud
Update streaming documentation,Streaming document needs to be updated with changes to 0.18,documentation
[HOD] HOD does not automatically create a cluster directory for the script option,"HADOOP-3483 fixed the allocate command to create a cluster directory if the directory does not exist. However, it missed doing this for the script command. As most of the customers use the script option this is a blocker for Hadoop 0.18.",contrib/hod
TestNetworkTopology could be cleaned up slightly,"The TestNetworkTopology test (a) has some of its private static final modifiers in the wrong order (b) in an assertEquals, has the static and dynamic values wrong, so the error message won't be correct",test
DNS.getHosts triggers an ArrayIndexOutOfBoundsException in reverseDNS if one of the interfaces is IPv6,"reverseDNS tries to split a host address string by ""."", and so fails if "":"" is the separator, as it is in IPv6. When it tries to access the parts of the address, a stack trace is seen. ",net
HoD should include timestamp in all error messages,"In order to debug and correlate errors reported by HoD with the logs of torque etc, it is essential that HoD include a timestamp in all the error messages it prints.",contrib/hod
CreateEditsLog could be improved to create tree directory structure,CreateEditsLog generates files in one directory. Processing an edits file with lot of files takes a lot of time while starting NameNode. It would be good to have the directory creation logic of NNThroughputBenchmark used in CreateEditsLog as well.,test
Create network topology plugin that uses a configured rack ip mask,"We should have a plugin that implements DNSToSwitchMapping with a configurable netmask that defines the rack. Therefore, if you use the high 26 bits to determine your switch, you could configure it with ""ff.ff.ff.c0""",util
Document the metrics produced by hadoop,"This information is needed in order to collect, monitor, and report on hadoop metrics.  Subject: Re: [Fwd: Specification of hadoop metrics?] Date: Mon, 23 Jun 2008 14:36:04 -0700 From: ""Owen O'Malley"" <oom@yahoo-inc.com> To: ""Rob Weltman"" <robw@yahoo-inc.com>  On Jun 23, 2008, at 12:40 PM, Rob Weltman wrote:  >   Is there a JIRA, forrest, or wiki document that defines all the > metrics produced by Hadoop (DFS and MR) for each release? If not, > should there be?  I don't know of any documentation of the exported metrics. There probably should be forrest documentation of the metrics, but it probably makes sense to do.  -- Owen ","documentation,metrics"
Create tests for Hadoop metrics,It would be good to have a test case for hadoop metrics. We could use FileContext or derive something out of NullContext to check the values returned via metrics are correct. ,metrics
NativeS3FsInputStream read() method for reading a single byte is incorrect,"From Albert Chern:  I think there may be a bug in the read() method of NativeS3InputStream, which looks like this:  {code} public synchronized int read() throws IOException {     int result = in.read();     if (result > 0) {         pos += result;     }     return result; } {code}  The return value of InputStream.read() should be the next byte in the range 0 to 255, or -1 if there are no more bytes.  So shouldn't this method look something like this?  {code} public synchronized int read() throws IOException {     int result = in.read();     if (result > -1) {         pos ++;     }     return result; } {code}",fs/s3
add a HadoopIOException that can be thrown in any method that has IOException on its signature,"I find myself having to throw IOExceptions a lot, and create new ones -but the classes signature varies from java1.5 to 1.6, and the base IOException is fairly meaningless. If Hadoop added a HadoopIOException, it could be thrown whenever hadoop's own code needed to create new IOExceptions, and possibly be differentiated in the catch() logic.   The biggest disadvantage of doing this is that as IOException is built into the JVM, you can be sure that the far end will be able to deserialize it under RMI, without having the rest of hadoop on the classpath. This is not a feature of hadoop, so should not be an issue. For those of us who do use RMI, well, we'd better get our classpaths right.",util
MetricsTimeVaryingRate returns wrong value for metric_avg_time,MetricsTimeVaryingRate seems to return getPreviousIntervalNumOps for metric_avg_time. It should return getPreviousIntervalAverageTime instead.,metrics
Providing bzip2 as codec,"Hadoop recognizes gzip compressed input and automatically decompresses the data before providing it to the mapper. But Hadoop can not split a gzip stream due to the very nature of the gzip compression. Consequently one gzip stream (e.g a whole file) can go to only one mapper.  On the contrary Bzip2 compressed stream can be split across its block delimiters.  We are interested in extending Hadoop to support splittable bzip2 with a codec.  (https://issues.apache.org/jira/browse/HADOOP-1823  uses input reader to split the bzip2 files, which must be provided by the user and can handle FileInputFormat.  If a user wants to use some other input format or wants to do custom record handling, he must write a new input reader!)  We have a patch now that provides a basic bzip2 codec equivalent to the current gzip codec.  We are in the process of extending that to support splitting.","conf,io"
Log4J logging of stack trace may deadlock JRockit in TestFileSystem,"This is being added as a bugrep so that other people can find it, and the workaround  1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout.  2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either  The fix for this problem was to edit conf/log4j.properties and switch to a log4J log pattern that did not print the line of the code  log4j.appender.console.layout.ConversionPattern=%-4r %-5p %c %x - %m%n  Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",conf
provide more control options for the junit run,"with a few more properties, people running the junit tests could  * choose a faster forking policy  * increase the allocated memory (useful on 64 bit java)  * ask for more console-level logging  * halt on the first failure",build
Patch to allow hadoop native to compile on Mac OS X,This patch makes the autoconf script work on Mac OS X.  LZO needs to be installed (including the optional shared libraries) for the compile to succeed.  You'll want to regenerate the configure script using autoconf after applying this patch.,native
Add replication factor for injecting blocks in the data node cluster ,Data node cluster is used for testing and benchmarking. This jira improves it as follows:  - allows the user to specify the location of the data node dirs  - allows a replication factor to be specified for injected blocks. ,benchmarks
WritableComparator newKey() fails for NullWritable,"It is not possible to use NullWritable as a key in order to suppress key value in output.  Syndrome exception: Caused by: java.lang.IllegalAccessException: Class org.apache.hadoop.io.WritableComparator can not access a member of class org.apache.hadoop.io.NullWritable with modifiers ""private""  The problem is that NullWritable is a singleton and does not provide public non-parametric constructor. The following code in WritableComparator causes the exception: return (WritableComparable)keyClass.newInstance();  Proposed simple solution is to use ReflectionUtils instead (it requires modification as well).  This issue is probably related to HADOOP-2922",io
Clean up HOD documentation,Include the changes in HOD documentation suggested by Corinne Chandel.  Terminology: -          Map/Reduce -          HDFS               (not DFS) -          NameNode -          TaskTracker -          For example      (not For e.g.)  Other changes: -          Remove most italics (except some in the User Guide) -          Various edits -          Spell-check,contrib/hod
Remove Hadoop's dependance on the cli 2 snapshot,"Currently, the Hadoop release includes a jar of the Apache commons-cli from a snapshot of 2.0. Hadoop isn't getting a major benefit from the 2.0 api, which in fact is pretty buggy, and we should just roll back to the cli 1.1 release.",util
"calls to junit Assert::assertEquals invert arguments, causing misleading error messages, other minor improvements.","JUnit Assert::assertEquals takes its expected and actual arguments in a particular order, but many unit tests invert them. The error message from a failed assertion can be misleading.",test
Hadoop dfs metric FilesListed shows number of files listed instead of operations,"Hadoop dfs metric FilesListed shows number of files listed, it should be number of fileListing operations instead. ",metrics
Fix mapred docs,Include the changes suggested by Corinne in Map/Reduce  and streaming documentation,documentation
Fix documentation for Cluster setup and Quick start guides,Include the changes suggested by Corinne in Cluster setup and quickstart guides,documentation
"Fix documentation for Archives, distcp and native libraries","Include the changes suggested by Corinne in Hadoop archives, Distcp user guide and Native libraries documentation.",documentation
"if MiniDFS startup time could be improved, testing time would be reduced","Its taking me 140 minutes to run a test build; looking into the test results its the 20s startup delay of every MiniDFS cluster that is slowing things down. If we could find out why it is taking so long and cut it down, every test case that relied on a cluster would be speeded up. ",test
[HOD] Have an ability to run multiple slaves per node,"Currently HOD launches at most one slave per node. For purposes of testing a large number of slaves on much fewer resources - for e.g. testing scalability of clusters, it will be useful if it can provision multiple slaves per node.",contrib/hod
[HOD] logcondense needs to use the new pattern of output in hadoop dfs -lsr,"In Hadoop 0.18, the format of *hadoop dfs -ls* and *hadoop dfs -lsr* commands has changed. It is a Unix like output now, with the filename being printed as the last column of the file listing as opposed to the first. logcondense.py needs to use this new format.",contrib/hod
Bash tab completion support,"The attached Bash script adds support for tab completion of most arguments to the main Hadoop script ({{bin/hadoop}}). Namely, it allows tab completion of all the command names, subcommands for the {{fs}}, {{dfsadmin}}, {{job}}, {{namenode}} and {{pipe}} commands, arguments of the {{jar}} command and most arguments to the {{fs}} subcommands (completing local and dfs paths as appropriate).  The file can be dropped into /etc/bash_completion.d/ on Debian-like distros, and it should then start working the next time Bash is started.",scripts
Provide a unified way to pass jobconf options from bin/hadoop,"Often when running a job it is useful to override some jobconf parameters from jobconf.xml for that particular job - for example, setting the job priority, setting the number of reduce tasks, setting the HDFS replication level, etc. Currently the Hadoop examples, streaming, pipes, etc take these extra jobconf parameters in different was: the examples in hadoop-examples.jar use -Dkey=value, streaming uses -jobconf key=value, and pipes uses -jobconf key1=value1,key2=value2,etc. Things would be simpler if bin/hadoop could take the jobconf parameters itself, so that you could run for example bin/hadoop -Dkey=value jar [whatever] as well as bin/hadoop -Dkey=value pipes [whatever]. This is especially useful when an organization needs to require users to use a particular property, e.g. the name of a queue to use for scheduling in HADOOP-3445. Otherwise, users may confuse one way of passing parameters with another and may not notice that they forgot to include certain properties.  I propose adding support in bin/hadoop for jobconf options to be specified with -C key=value. This would have the effect of setting hadoop.jobconf.key=value in Java's system properties. The Configuration class would then be modified to read any system properties that begin with hadoop.jobconf and override the values in hadoop-site.xml.  I can write a patch for this pretty quickly if the design is sound. If there's a better way of specifying jobconf parameters uniformly across Hadoop commands, let me know.",conf
TestMiniMRMapRedDebugScript loses exception details,"The test class org.apache.hadoop.mapred.TestMiniMRMapRedDebugScript catches and prints exception details then calls fail(), rather than relaying the exception up to the calling test framework. this make the full stack trace harder to obtain. It could be a relic of the class having its own main() entry point.  trivial to fix: add Exception to the test signature, move the catch/print into the main() method for that entry point",test
TestCLI loses exception details on setup/teardown,"The TestCLI test case catches exceptions during setup and teardown and prints them, instead of just relaying them up to the test runner, for them to be caught and logged.",test
add new JobConf constructor that disables loading default configurations,"Similar to the {{Configuration(boolean loadDefauls)}} , {{JobConf}} should have such constructor.    This would allow supporting default values on the JT side as the client would not submit its default values in the job.xml.     It would address HADOOP-3287    ",conf
"""s3:"" URLs break when Secret Key contains a slash, even if encoded","When using URLs of the form s3://ID:SECRET@BUCKET/ at the command line, distcp fails if the SECRET contains a slash, even when the slash is URL-encoded as %2F.  Say your AWS Access Key ID is RYWX12N9WCY42XVOL8WH And your AWS Secret Key is Xqj1/NMvKBhl1jqKlzbYJS66ua0e8z7Kkvptl9bv And your bucket is called ""mybucket""  You can URL-encode the Secret KKey as Xqj1%2FNMvKBhl1jqKlzbYJS66ua0e8z7Kkvptl9bv  But this doesn't work:  {noformat} $ bin/hadoop distcp file:///source  s3://RYWX12N9WCY42XVOL8WH:Xqj1%2FNMvKBhl1jqKlzbYJS66ua0e8z7Kkvptl9bv@mybucket/dest 08/07/09 15:05:22 INFO util.CopyFiles: srcPaths=[file:///source] 08/07/09 15:05:22 INFO util.CopyFiles: destPath=s3://RYWX12N9WCY42XVOL8WH:Xqj1%2FNMvKBhl1jqKlzbYJS66ua0e8z7Kkvptl9bv@mybucket/dest 08/07/09 15:05:23 WARN httpclient.RestS3Service: Unable to access bucket: mybucket org.jets3t.service.S3ServiceException: S3 HEAD request failed. ResponseCode=403, ResponseMessage=Forbidden         at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:339) ... With failures, global counters are inaccurate; consider running with -i Copy failed: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 PUT failed. XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>         at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.createBucket(Jets3tFileSystemStore.java:141) ... {noformat}",fs/s3
CompressedWritable throws OutOfMemoryError,"We were seeing OutOfMemoryErrors with stack traces like the following (Hadoop 0.17.0):  {noformat} java.lang.OutOfMemoryError         at java.util.zip.Deflater.init(Native Method)         at java.util.zip.Deflater.<init>(Deflater.java:123)         at java.util.zip.Deflater.<init>(Deflater.java:132)         at org.apache.hadoop.io.CompressedWritable.write(CompressedWritable.java:71)         at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)         at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)         at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1016)         [...] {noformat}  A Google search found the following long-standing issue in Java in which use of java.util.zip.Deflater causes an OutOfMemoryError:  [http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4797189]  CompressedWritable instantiates a Deflater, but does not call {{deflater.end()}}.  It should do that in order to release the Deflater's resources immediately, instead of waiting for the object to be finalized.  We applied this change locally and saw much improvement in the stability of memory usage of our app.  This may also affect the SequenceFile compression types, because org.apache.hadoop.io.compress.zlib.BuiltInZlib{Deflater,Inflater} extend java.util.zip.{Deflater,Inflater}.  org.apache.hadoop.io.compress.Compressor defines an end() method, but I do not see that this method is ever called.",io
metrics: FileContext support overwrite mode,"FileContext currently continually appends to the metrics log file(s), generating an ever lengthening file.  In some scenarios, it would be useful to simply write the current statistics to the file once every period, then overwrite the file for the next period.  For instance, this could be useful if an external application parsed the metrics output - e.g. Cacti to create realtime graphs.",metrics
HDFS Web UI displays comments from dfs.exclude file and counts them as dead nodes,I am putting comments in dfs.exclude file such as:    {noformat}   # 32 GB memory upgrades  {noformat}    HDFS Web UI counts each word in the commented line as a dead node.  ,util
expose static SampleMapper and SampleReducer classes of GenericMRLoadGenerator class for gridmix reuse,"Currently, the SampleMapper and SampleReducer are package static in GenericMRLoadGenerator class.  In order for gridmix to  reuse them, we need to make them public.   Also, we need a function which can returns the jobClient from the Job class. so that gridmix can get the job status information. ",test
"improve composition, submission and result collection of gridmix","Current gridmix submits jobs using a set of scripts, which is inconvenient and the results are difficult to collect.  To improve the gridmix submission and results collection, we implemented a new program  using JobControl to submit and collect the results of jobs  Also the new gridmix allows to have more different types of jobs such as, pig jobs, jobs with combiners etc. ",benchmarks
Proposed hadoop instrumentation API,"We want to evolve the Hadoop metrics subsystem into a more generic Instrumentation facility.  The ultimate goal is to add structured logging to Hadoop, with causal tags, a la X-trace.  The existing metrics framework is not quite suitable for our needs, since the implementation and interface are tightly coupled. There's no way to use the metrics instrumentation points for anything other than metricss, and there's no way for a metrics context to find out what event just happened.    We want to tease apart the generic notion of hookable instrumentation points, from the specifics of the information recording. The latter ought to  be pluggable at run-time.   ",metrics
Typos in shell output,"{noformat} [ ~/hadoop-0.18.01]$ ./bin/hadoop --config ~/1157/ jar hadoop-0.18.01-examples.jar randomtextwriter randomtextwriter [-outFormat <output format class>] <input> Generic options supported are -conf <configuration file>     specify an application configuration file -D <property=value>            use value for given property -fs <local|namenode:port>      specify a namenod -jt <local|jobtracker:port>    specify a job tracker -files <comma separated list of fiels>    specify comma separated files to be copied to the map reduce cluster -libjars <comma seperated list of jars>    specify comma separated jar files to include in the classpath. -archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.  The general command line syntax is bin/hadoop command [genericOptions] [commandOptions] {noformat} - <input> should be <output> - ""fiels"" should be ""files"" - ""namenod"" should be ""namenode"" - ""seperated "" should be ""separated """,fs
"""deprecated filesystem name"" warning on EC2","When logged in to the master node, in a Hadoop cluster on EC2, launched with src/contrib/ec2/bin/hadoop-ec2 ...  Various bin/hadoop commands (jar, distcp, fs) display this warning one or more times:  08/07/17 14:46:23 WARN fs.FileSystem: ""ip-10-251-203-207.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-203-207.ec2.internal:50001/"" instead.  ",contrib/cloud
FileSystem cache should be case-insensitive,"We cache FileSystem instances based on URI scheme & authority, plus username.  Elsewhere we compare URI schemes and authorities case-insensitively, but the cache is case-sensitive.  In particular, the Cache.Key#equals() and Cache.Key#hashCode() should be made case-insensitive.  This should not be a blocker, since the worst it causes is a few extra FileSystem instances to be cached.",fs
Changes in HOD documentation,"After reviewing bugs 3505 & 3668, following changes are required in HOD documentation.  1. HOD user guide :  =============== Term HDFS should be used instead of DFS under following sections: 3.9. Capturing HOD exit codes in Torque 4.3. hod Fails With an error code and error message  2. HOD admin guide: ================ 'Torque' should be a  hyperlink in Pre-requisites section of hod admin guide.  ",documentation
Add more unit tests to test appending to files in HDFS,"A new feature ""appends to HDFS files"" have been implemented in HADOOP-1700. There are a set of unit tests in TestFileAppend.java and TestFileAppend2.java. But we would like to have more unit tests.",test
"exit code from ""hadoop dfs -test ..."" is wrong for Unix shell","The exit code for the ""test"" command in hadoop.fs.FsShell is backwards relative to the Unix shell, which interprets an exit code of 0 as true and anything else as false.",fs
compile-core-test fails to compile,"compile-core-test fails to compile on branch-0.18  compile-core-test:    [javac] Compiling 7 source files to /branch-0.18/build/test/classes    [javac] Note: Some input files use unchecked or unsafe operations.    [javac] Note: Recompile with -Xlint:unchecked for details.    [javac] Compiling 238 source files to /branch-0.18/build/test/classes    [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:27: package org.apache.hadoop.hdfs does not exist    [javac] import org.apache.hadoop.hdfs.MiniDFSCluster;    [javac]                              ^    [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:38: cannot find symbol    [javac] symbol  : class MiniDFSCluster    [javac] location: class org.apache.hadoop.mapred.TestJobShell    [javac]     MiniDFSCluster dfs = null;    [javac]     ^    [javac] /branch-0.18/src/test/org/apache/hadoop/mapred/TestJobShell.java:45: cannot find symbol    [javac] symbol  : class MiniDFSCluster    [javac] location: class org.apache.hadoop.mapred.TestJobShell    [javac]       dfs = new MiniDFSCluster(conf, 2 , true, null);    [javac]                 ^    [javac] Note: Some input files use or override a deprecated API.    [javac] Note: Recompile with -Xlint:deprecation for details.    [javac] 3 errors",build
IPC - Heartbeat exceptions filling up log files,"We have a datanode (10.0.0.93) that is in a semi-live state.  An ssh session is able to do a connect but is unable to send or receive any data.  The connection is closed immediately after the connection is established.  Because of this, the name node's logs are full of the following message:  2008-07-21 09:36:10,800 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1on 9000, call sendHeartbeat(10.0.0.193:50010, 917638877184, 20480, 644100882925, 0, 0) from 10.0.0.193:55908: error: org.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node. Reported: -11. Expecting = -13. org.apache.hadoop.dfs.IncorrectVersionException: Unexpected version of data node. Reported: -11. Expecting = -13.         at org.apache.hadoop.dfs.NameNode.verifyVersion(NameNode.java:682)         at org.apache.hadoop.dfs.NameNode.verifyRequest(NameNode.java:669)         at org.apache.hadoop.dfs.NameNode.sendHeartbeat(NameNode.java:557)         at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:585)         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)  Approximately generating 100MB a minute.",ipc
[HOD] Include job tracker RPC in notes attribute after job submission,"Currently, after a job is submitted, HOD records the Jobtracker URL and the NameNode URL in the 'notes' attribute of the Torque resource manager. This helps in building centralized administration tools that need to get a full picture of all allocated clusters etc. Including the Jobtracker RPC port in this attribute would be useful to build such similar tools. One of them is an idle job tracker that can be centrally run to clear up any clusters that aren't automatically cleaned up due to some faulty nodes in the cluster or resource manager problems.",contrib/hod
TestCLI fails on trunk point out that moveFromLocal/put output has changed,TestCLI failed detecting a change in output of put and moveFromLocal  {noformat} Testcase: testAll took 24.112 sec   FAILED One of the tests failed. See the Detailed results to identify the command that failed junit.framework.AssertionFailedError: One of the tests failed. See the Detailed results to identify the command that failed   at org.apache.hadoop.cli.TestCLI.displayResults(TestCLI.java:272)   at org.apache.hadoop.cli.TestCLI.tearDown(TestCLI.java:141) {noformat} ,test
"[HOD] Remove dfs.client.buffer.dir generation, as this is removed in Hadoop 0.19.","In HADOOP-3756, the dfs.client.buffer.dir parameter was removed from hadoop-default.xml, as this was no longer used. HOD generates this parameter in the hadoop-site it generates. This must be removed as well.",contrib/hod
[Build] Have an ant build property to exclude sources and docs while building a hadoop tar-ball.,This helps developers.,build
KFS changes for faster directory listing,"To improve directory listing performance in KFS, the KFS client library code has been updated with new APIs.  This JIRA is for propogating those changes into the Hadoop code base.",fs
Maven build from hadoop-core,"Maven users would benefit from having Hadoop in a public maven repository, along with an archetype to quickly create the ""hello world"" maven examples. By having hadoop in Maven repositories it is simpler to handle version increments for projects that build on top of Hadoop-core. If this is of interest please vote for it and I will attempt to offer a patch into the hadoop build scripts that publish it for maven use - if no one votes, then I will not bother...",build
Not possible to access a FileSystem from within a ShutdownHook,"FileSystem uses addShutdownHook to close all FileSystems at exit.  This makes it impossible to access a FileSystem from within your own ShutdownHook threads, say for deleting incomplete output.  Using a pre-existing FileSystem object is unsafe since it may be closed by the time the thread executes.  Using FileSystem.get(...) results in an exception:  Exception in thread ""Thread-10"" java.lang.IllegalStateException: Shutdown in progress   java.lang.Shutdown.add(Shutdown.java:81)   java.lang.Runtime.addShutdownHook(Runtime.java:190)   org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1293)   org.apache.hadoop.fs.FileSystem.get(FileSystem.java:203)   org.apache.hadoop.fs.FileSystem.get(FileSystem.java:108)  ",fs
can not get svn revision # at build time if locale is not english,"My locale is zh_TW.UTF-8, so 'svn info' shows messages in chinese. But src/saveVersion.sh expects english from output.  I suggest that we add clear LANG, LC_* in saveVersion.sh before calling svn.",build
"gridmix-env has a syntax error, and wrongly defines USE_REAL_DATASET by default","Syntax error due to missing double-quotes: {noformat} export ALL_HOD_OPTIONS=""-c ${HOD_CONFIG} ${HOD_OPTIONS} {noformat}  And, the following line: {noformat} export USE_REAL_DATASET=false {noformat} is unnecessary.",benchmarks
SequenceFile's Reader.decompressorPool or Writer.decompressorPool gets into an inconsistent state when calling close() more than once,"SequenceFile.Reader uses a decompressorPool to reuse Decompressor instances. The Reader obtains such an instance from the pool on object creation and returns it back to the pool it when {{close()}} is called.  SequenceFile.Reader implements the {{java.io.Closable}} interface and it's spec on the {{close()}} method says:  {quote} Closes this stream and releases any system resources associated  with it. If the stream is already closed then invoking this  method has no effect. {quote}  This spec is violated by the Reader implementation, because calling {{close()}} multiple times has really bad implications.  When you call {{close()}} twice, one and the same Decompressor instances will be returned to the pool two times and the pool would now maintain duplicated references to the same Decompressor instances. When other Readers now request instances from the pool it might happen that two Readers get the same Decompressor instance.  The correct behavior would be to just ignore a second call to {{close()}}.  The exact same issue applies to the SequenceFile.Writer as well.  We were having big trouble with this, because we were observing sporadic exceptions from merge operations. The strange thing was that executing the same merge again usually succeeded. But sometimes it took multiple attempts to complete a merge successfully. It was very hard to debug that the root cause was some duplicated Decompressor references in the decompressorPool.  Exceptions that we observed in production looked like this (we were using hadoop 0.17.0):  {noformat} java.io.IOException: unknown compression method at org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater.decompress(BuiltInZlibInflater.java:47) at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:80) at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74) at java.io.DataInputStream.readFully(DataInputStream.java:178) at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:56) at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90) at org.apache.hadoop.io.SequenceFile$Reader.nextRawKey(SequenceFile.java:1995) at org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor.nextRawKey(SequenceFile.java:3002) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.next(SequenceFile.java:2760) at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2625) at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2644) {noformat}  or   {noformat} java.io.IOException: zero length keys not allowed at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.appendRaw(SequenceFile.java:1340) at org.apache.hadoop.io.SequenceFile$Sorter.writeFile(SequenceFile.java:2626) at org.apache.hadoop.io.SequenceFile$Sorter.merge(SequenceFile.java:2644) {noformat}  The following snippet reproduces the problem:  {code:java}     public void testCodecPool() throws IOException {         Configuration conf = new Configuration();         LocalFileSystem fs = new LocalFileSystem();         fs.setConf(conf);         fs.getRawFileSystem().setConf(conf);          // create a sequence file         Path path = new Path(""target/seqFile"");         SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, path, Text.class, NullWritable.class, CompressionType.BLOCK);         writer.append(new Text(""key1""), NullWritable.get());         writer.append(new Text(""key2""), NullWritable.get());         writer.close();          // Create a reader which uses 4 BuiltInZLibInflater instances         SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);         // Returns the 4 BuiltInZLibInflater instances to the CodecPool         reader.close();         // The second close erroneously returns the same 4 BuiltInZLibInflater instances to the CodecPool again         reader.close();          // The first reader gets 4 BuiltInZLibInflater instances from the CodecPool         SequenceFile.Reader reader1 = new SequenceFile.Reader(fs, path, conf);         // read first value from reader1         Text text = new Text();         reader1.next(text);         assertEquals(""key1"", text.toString());         // The second reader gets the same 4 BuiltInZLibInflater instances from the CodePool as reader1         SequenceFile.Reader reader2 = new SequenceFile.Reader(fs, path, conf);         // read first value from reader2         reader2.next(text);         assertEquals(""key1"", text.toString());         // read second value from reader1         reader1.next(text);         assertEquals(""key2"", text.toString());         // read second value from reader2 (this throws an exception)         reader2.next(text);         assertEquals(""key2"", text.toString());                  assertFalse(reader1.next(text));         assertFalse(reader2.next(text));     } {code}  It fails with the exception:  {noformat} java.io.EOFException   java.io.DataInputStream.readByte(DataInputStream.java:243)   org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)   org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)   org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1835)   CodecPoolTest.testCodecPool(CodecPoolTest.java:56) {noformat}  But this is just a very simple test that shows the problem. Much more weired things can happen when running in a complex production environment. Esp. heavy concurrency makes the behavior much more exciting. ;-)",io
Create more unit tests for testing HDFS appends,"Create more unit tests to test HDFS appends. One interesting unit test would be introduce ""appends"" to existing unit test TestDatanodeDeath.",test
Checkin the design document for HDFS appends into source control repository,The design document for HDFS needs to be converted into forrest and checked into repository.,documentation
Develop scripts to create rpm package to facilitate deployment of hadoop on Linux machines,"A rpm-like packing scheme to package and then install hadoop binaries is very helpful, especially when the number of machines in the cluster is huge. ",build
TestMultipleOutputs will fail if it is ran more than one times,"TestMultipleOutputs will success after ""ant clean"" but it will fail if running it more than one times. ",test
include message of local exception in Client call failures,"When Client fails with a local exception, that exception is retained, but the message is not propagated to the new exception text, which becomes simply  ""Call failed on local exception""  The forthcoming patch will change such messages to include that of the nested exception, so producing test reports containing useful data such as  java.io.IOException: Call failed on local exception: Connection refused",ipc
CreateEditsLog used for benchmark misses creating parent directories,"CreateEditsLog generates a tree of directory structure while generating random files for benchmarks. I see that when generating new directories for depth of more than 2, we get a new directory name for parent directory. This was not logged into edits makes it an invalid edits file. ",benchmarks
"If ShellCommandExecutor had a toString() operator that listed the command run, its error messages may be more meaningful","It may be easier to identify what causes error commands to be returned on an exec if the command is included in the error message. this can be done with  * a toString() operator on the class that includes the list of arguments (when non null) * a test that this works with arguments with spaces, and a null array * relevant use of the operator when an execute fails, such as in TaskTracker",util
org.apache.hadoop.http.HttpServer should support user configurable filter,"Filters provide universal functions such as authentication, logging and auditing, etc.  HttpServer should support configurable filters, so that individual site administrators could possibly configure filters for their web site.",util
Fix import of MiniDFSCluster in TestCompressedEmptyMapOutputs.java,The merge of HADOOP-3827 to branch-0.18 had the wrong import ...,test
Compare name-node performance when journaling is performed into local hard-drives or nfs.,The goal of this issue is to measure how the name-node performance depends on where the edits log is written to. Three types of the journal storage should be evaluated: # local hard drive; # remote drive mounted via nfs; # nfs filer. ,benchmarks
Make MapFile.Reader and Writer implement java.io.Closeable,Both MapFile.Reader and Writer have a close() method with the right signature. They just need to declare that they implement Closeable.,io
Create network topology plugin that uses a configured rack ip mask,"We should have a Java class that answers network topology questions by implementing DNSToSwitchMapping by using the host's IP address and a mask that defines the addresses that are on the same rack. Therefore, if your racks are defined by using the top 3 octets, you'd configure ff.ff.ff.00.",net
Improve Hadoop Jobtracker Admin,"A number of enhancements to the jobtracker jsp that allow for searching, scheduling,  and navigating a large number of jobs.  These enhancements are to be added as subtasks, to follow.",scripts
TestFileAppend2.testComplexAppend sometimes fails,This failed while testing an unrelated change:  http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2985/testReport/ ,test
IPC client doesnt time out if far end handler hangs,"This is what appears to be happening in some changes of mine that (inadventently) blocked JobTracker: if the client can connect to the far end and invoke an operation, the far end has forever to deal with the request: the client blocks too.    Clearly the far end shouldn't do this; its a serious problem to address. but should the client hang? Should it not time out after some specifiable time and signal that the far end isn't processing requests in a timely manner?     (marked as minor as this shouldn't arise in day to day operation. but it should be easy to create a mock object to simulate this, and timeouts are considered useful in an IPC)",ipc
TestFileCreation fails once in a while,TestFileCreation fails once in a while.,test
eclipse plugin build is broken with current eclipse versions,Eclipse plugin build is broken with current versions of Eclipse.  The problem seems to be that the constructor for org.eclipse.core.runtime.Status has changed. ,contrib/eclipse-plugin
"Error in javadoc of Reporter, Mapper and Progressable","The javadoc for Reporter says:    ""In scenarios where the application takes an insignificant amount of time to process individual key/value pairs""    Shouldn't this read /significant/ instead of insignificant?",documentation
Distcp help needs to be more elaborated.,"""hadoop distcp --help"" command gives help regarding the execution of the distcp command but it doesn't give the details about copying data between different versions of Hadoop.  The example displayed by the help documentation is : hadoop distcp -p -update ""hdfs://A:8020/user/foo/bar"" ""hdfs://B:8020/user/foo/baz""  while copying between different versions, distcp uses hftp protocol instead of hdfs for source path. ",documentation
distcp: PathFilter for source files,I'd like distcp to be able to skip _logs/_temporary directories and files.  ,util
Include Unix group name in JobConf,A feature that was requested for the fair scheduler was to use a user's unix group name as their pool name so their jobs automatically have the right resource allocations. This might be useful for HADOOP-3445 as well.,conf
trunk does not compile,Compilation fails saying: {code}     [javac] /home/amarsri/workspace/trunk/src/core/org/apache/hadoop/net/ScriptBasedMapping.java:34: cannot find symbol     [javac] symbol: class CachedDNSToSwitchMapping     [javac] public final class ScriptBasedMapping extends CachedDNSToSwitchMapping {code}  HADOOP-3620 missed adding new files to the trunk.,fs
TestInjectionForSimulatedStorage only works on a clean directory structure,"I've been getting intermittent failures on org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage  Timedout while waiting for all blocks to be replicated for /replication-test-file  java.io.IOException: Timedout while waiting for all blocks to be replicated for /replication-test-file at org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.waitForBlockReplication(TestInjectionForSimulatedStorage.java:100) at org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection(TestInjectionForSimulatedStorage.java:151)  Here's how to replicate it: run the test twice.   When run the second time, the trace is full of log messages like : 2008-08-05 11:12:46,108 [org.apache.hadoop.hdfs.server.datanode.DataNode$DataXceiver@3cc66e] INFO  datanode.DataNode - Receiving block blk_-5590567767202552997_1001 src: /127.0.0.1:41792 dest: /127.0.0.1:46976 2008-08-05 11:12:46,108 [org.apache.hadoop.hdfs.server.datanode.DataNode$DataXceiver@3cc66e] INFO  datanode.DataNode - writeBlock blk_-5590567767202552997_1001 received exception java.io.IOException: Block blk_-5590567767202552997_1001 is valid, and cannot be written to. 2008-08-05 11:12:46,108 [org.apache.hadoop.hdfs.server.datanode.DataNode$DataXceiver@3cc66e] ERROR datanode.DataNode - DatanodeRegistration(127.0.0.1:46976, storageID=DS-1340877566-127.0.1.1-0-1217931159994, infoPort=56552, ipcPort=60571):DataXceiver: java.io.IOException: Block blk_-5590567767202552997_1001 is valid, and cannot be written to.   org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset.writeToBlock(SimulatedFSDataset.java:370)   org.apache.hadoop.hdfs.server.datanode.DataNode$BlockReceiver.<init>(DataNode.java:2405)   org.apache.hadoop.hdfs.server.datanode.DataNode$DataXceiver.writeBlock(DataNode.java:1266)   org.apache.hadoop.hdfs.server.datanode.DataNode$DataXceiver.run(DataNode.java:1124)   java.lang.Thread.run(Thread.java:619)  Am I the only person seeing this? ",test
CLASSPATH in bin/hadoop script is set incorrectly for cygwin,"In Cygwin, paths are required to be translated by cygpath.  In the bin/hadoop script, the CLASSPATH variable may be modified after cygpath translation.  In such case, CLASSPATH will be set incorrectly.",scripts
Ordering of the output statistics in the report page (jobtracker-details for a job),"The ordering of the job statistics in the jobdetails.jsp seems very unintuitive - and not in sync with previous versions.  It seems as if the rows should be ordered by their respective function (maps, combines, reduces).  Example:  Map-Reduce Framework    Reduce input groups   0   1,936   1,936 Combine output records  0  0  0 Map input records  41,580,847  0  41,580,847 Reduce output records  0  664,803,173  664,803,173 Map output bytes  988,918,560  0  988,918,560 Map input bytes  1,100,931,203  0  1,100,931,203 Map output records  41,580,847  0  41,580,847 Combine input records  0  0  0 Reduce input records  0  41,580,847  41,580,847",util
A few tests still using old hdfs package name,"ClusterTestDFS, ClusterTestDFSNamespaceLogging, TestDistributedUpgrade are still using the old hdfs package name ""org.apache.hadoop.dfs"".",test
Are ClusterTestDFSNamespaceLogging and ClusterTestDFS still valid tests?,"ClusterTestDFSNamespaceLogging and ClusterTestDFS are not executed by ""ant test"" since their names do not begin with ""Test"". So these two tests have not be run for a long time.  I ran these two tests manually. Trunk passed ClusterTestDFSNamespaceLogging but failed on ClusterTestDFS. I am not sure whether they are still valid tests.",test
' -blocks ' option not being recognized,"Somehow depending on the order of options, GenericOptionsParser throws an error.  This fail.  {noformat} [knoguchi@gsgw2001 tmp]$ ~/branch-0.18/bin/hadoop fsck Usage: DFSck <path> [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]  [knoguchi@tmp]$ ~/branch-0.18/bin/hadoop fsck -files -blocks -locations /user/knoguchi java.io.FileNotFoundException: File -blocks does not exist.         at org.apache.hadoop.util.GenericOptionsParser.validateFiles(GenericOptionsParser.java:278)         at org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:233)         at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:315)         at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:134)         at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:119)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)         at org.apache.hadoop.dfs.DFSck.main(DFSck.java:137)  ...Status: HEALTHY  Total size:    2769 B  Total dirs:    4  Total files:   3  Total blocks (validated):      3 (avg. block size 923 B) ... {noformat}  This works. {noformat} [knoguchi@tmp]$ ~/branch-0.18/bin/hadoop fsck -blocks -locations -files /user/knoguchi /user/knoguchi/.Trash <dir> /user/knoguchi/hod-logs <dir> /user/knoguchi/hod-logs/____ <dir> /user/knoguchi/hod-logs/____/aaa.tar.gz 1024 bytes, 1 block(s):  OK 0. blk_-5724352734215884188_0 len=1024 repl=3 [11.111.11.11:11111, 22.22.22.22:11111, 33.33.33.33:11111] ... /user/knoguchi/mapredsystem <dir> Status: HEALTHY  Total size:    2769 B  Total dirs:    4  Total files:   3  Total blocks (validated):      3 (avg. block size 923 B)  {noformat}","fs,util"
Improvements for NativeS3FileSystem,"In the process of porting NativeS3FileSystem for use with Hadoop 12, I made the following changes and improvements which might be helpful (apologies if I should have opened separate issues, but I was lazy):  1. The single byte read() method of NativeS3InputStream incorrectly treats the return value of InputStream.read() as the number of bytes read, which is actually always 1.  2. It allows people to write files ending with the folder suffix.  I prevented this by doing a check in the create() method.  3. Similarly, it allows people to open directories for reading.  I prevented this by doing a check in the open() method.  4. If you write a file to a nonexistent directory tree, say /a/b/c/d/file, and then you delete/rename that file or one of its parent directories, the whole directory tree vanishes.  I fixed this by always creating the parent of a deleted/renamed file.  5. Recursive delete(), rename(), and getContentLength() can be sped up tremendously by working directly with S3 listings rather than working at the FileSystem level.  All sub-files/sub-directories should begin with the parent directory name as a prefix.  6. HADOOP-3506 is still relevant.  I don't have patches since I created a new file, but I have attached my source if Tom wants to take a look at it.",fs/s3
Create a page on the hadoop wiki for contrib/fuse-dfs,Have a link from the contrib section of http://wiki.apache.org/hadoop/   tools to mount HDFS as a standard filesystem on Linux (and some other Unix OSs)  linked to a page that describes:   1. fuse-dfs with its FAQ and README   2. fuse-j-hdfs - a link to the tar file on hadoop-4   3. hdfs-fuse - a google code project ,documentation
Clover breaks nightly builds,Since 8 July the nightly builds on Hudson have failed with the following error:     [clover] Processing files at 1.3 source level.    [clover] Updating database at '/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-trunk/workspace/trunk/build/test/clover/db/hadoop_coverage.db'    [clover] /var/tmp/clover35762.tmp/src35763.tmp/org/apache/hadoop/fs/FileUtil.java.tmp:508:10:unexpected token: OSType    [clover] line 508: unexpected token: OSType    [clover] ** Error(s) occurred and the instrumentation process can't continue.  ,build
Be able to create a Configuration out an stream (with config XML content),"While it is possible to write a {{Configuration}} as XML to a stream, it is not possible to read from a stream a {{Configuration}}.  The use case is to persist a given {{Configuration}} (more specifically a {{JobConf}}) for later retrieval an use.  Currently I can do it only by first writing the XML content to a temporary file and then give a URL to that temporary file to the {{Configuration}} as {{addResource}} or in the constructor.",conf
Extend FileSystem API to return file-checksums/file-digests,"Suppose we have two files in two locations (may be two clusters) and these two files have the same size.  How could we tell whether the content of them are the same?  Currently, the only way is to read both files and compare the content of them.  This is a very expensive operation if the files are huge.  So, we would like to extend the FileSystem API to support returning file-checksums/file-digests.",fs
Update DistCp documentation,There are a few new options added to DistCp.  We should update the documentation.,documentation
TupleWritable listed as public class but cannot be used without methods private to the package,"Reading the hadoop-core javadocs, it appears as though TupleWritable can be used outside of the org.apache.hadoop.mapred.join.* package.  A user can import TupleWritable but cannot use it correctly without the setWritten, and clearWritten methods being public.   It seems as the though the intent was to make TupleWritable hidden from the user as it is dependent on CompositeRecordReader.  As a possible solution, classes within a package can be made invisible to the user by omitting 'public' from the class definition.  In the case of TupleWritable, this removes the javadoc link from other classes in mapred.join and it's not clear if these classes should be hidden from the user. ",documentation
TestMapRed fails on trunk,"TestMapRed fails on trunk with following exception: java.io.IOException: Not a file: file:/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/mapred.loadtest/intermediateouts/part-00000/data   org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:198)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:740)   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1024)   org.apache.hadoop.mapred.TestMapRed.launch(TestMapRed.java:614)   org.apache.hadoop.mapred.TestMapRed.testMapred(TestMapRed.java:248)  The failure is introduced by HADOOP-3664. Hudson detected the failure, but with commit of 3664 failure occurs on trunk.",test
javadoc warnings: Multiple sources of package comments found for package,"Beginning from Hudson build #3046, there are javadoc warnings in trunk. See http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3046/artifact/trunk/current/patchJavadocWarnings.txt {noformat}   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.codec""   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.codec.binary""   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl"" {noformat}","build,documentation"
TestMapRed and TestMiniMRDFSSort failed on trunk,- TestMapRed failed on Windows and Linux {noformat} Testcase: testMapred took 7.36 sec  Caused an ERROR Not a file: file:/d:/@sze/hadoop/trunk/mapred.loadtest/intermediateouts/part-00000/data java.io.IOException: Not a file: file:/d:/@sze/hadoop/trunk/mapred.loadtest/intermediateouts/part-00000/data   org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:195)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:740)   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1036)   org.apache.hadoop.mapred.TestMapRed.launch(TestMapRed.java:614)   org.apache.hadoop.mapred.TestMapRed.testMapred(TestMapRed.java:248) {noformat}  - TestMiniMRDFSSort failed on Windows  {noformat} Testcase: testMapReduceSort took 21.312 sec  Caused an ERROR File does not exist: /tmp/sortvalidate/recordstatschecker/part-00000 java.io.FileNotFoundException: File does not exist: /tmp/sortvalidate/recordstatschecker/part-00000   org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:404)   org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:677)   org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1416)   org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1411)   org.apache.hadoop.mapred.SortValidator$RecordStatsChecker.checkRecords(SortValidator.java:370)   org.apache.hadoop.mapred.SortValidator.run(SortValidator.java:561)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.mapred.TestMiniMRDFSSort.runSortValidator(TestMiniMRDFSSort.java:73)   org.apache.hadoop.mapred.TestMiniMRDFSSort.testMapReduceSort(TestMiniMRDFSSort.java:99) {noformat},test
TestDataJoin references dfs.MiniDFSCluster instead of hdfs.MiniDFSCluster,The data join test doesn't compile...,test
TestMapRed ignores failures of the test case,"TestMapRed checks for correctness of the data after several map/reduce jobs, but only prints the failure to stderr rather than fail the junit test. The test has been failing for a long time and was not reported as broken.",test
[HOD] --resource_manager.options is not passed to qsub,"--resource_manager.options is ignored in HOD. It isnt passed to the qsub command. This is a problem, as it makes it impossible to add additional resource contraints for scheduling (i.e. give me 64bit nodes) ",contrib/hod
"Shell command ""fs -count"" should support paths with different file systsms","If there are two different file systems in the path list, ""fs -count"" does not work.  For example, {noformat} bash-3.2$ ./bin/hadoop fs -count / file:///               13            4              27369 hdfs://nnn:9000/ count: Wrong FS: file:/, expected: hdfs://nnn:9000 Usage: java FsShell [-count[-q] <path>] {noformat}",fs
javadoc warnings by failmon,"There are javadoc warnings caused by failmon: {noformat}   [javadoc] Loading source files for package org.apache.hadoop.contrib.failmon...   [javadoc] Constructing Javadoc information...   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl"" {noformat}",build
"Place the new findbugs warnings introduced by the patch in the /tmp directory when ""ant test-patch"" is run.","New findbugs warnings introduced by the patch should be made available in the PATCH_DIR(i.e /tmp) when ""ant test-patch"" is run. It should be available in both .html and .xml format.   ",test
test-patch can report the modifications found in the workspace along with the error message,"When test-patch is run on a developer workspace which contains modifications, it errors out by saying ""ERROR: can't run in a workspace that contains modifications"". It appears this is printed when svn stat command returns a non-empty output. It would be helpful to report this output, so that developers can be alerted to what modifications are causing this error. Sometimes these modifications may not be because of a direct code change they've done, but remnants of an incorrect cleanup of the last build, etc.",test
SequenceFile.Writer reopen (hdfs append),Allows for reopening and appending to a SequenceFile,io
linux 64-bit native libraries should always be built and included in release,"The current HowToRelease wiki document says that 64-bit linux native libs are optional for a release. However, we had them in all 0.16.x, 0.17.[0-1], and 0.18.0 releases. Provided the fact that 64-bit computers become more common nowadays. We should make this binary built and included in release.","build,native"
compile-c++ should honor the jvm size in compiling the c++ code,"The build scripts for compile-c++ and compile-c++ -examples should honor the word size of the jvm, since it is in the platform name. Currently, the platform names are ""Linux-amd64-64"" or ""Linux-i386-32"", but the C++ is always compiled in the platform default size.",build
updates to hadoop-ec2-env.sh for 0.18.0,"src/contrib/ec2/bin/hadoop-ec2-env-sh on 0.18.0 still has values for the 0.17.x branch.  Here is a diff.  I used this to create an EC2 image, which works for my hadoop streaming project.  The java version is the latest one, as instructed by the docs, but I left the java URL blank, as it looks like it isn't persistent.  ChimpBook4:bin karl$ diff hadoop-ec2-env.sh- hadoop-ec2-env.sh-foo 43c43 < HADOOP_VERSION=0.17.0 --- > HADOOP_VERSION=0.18.0 77c77 < JAVA_VERSION=1.6.0_05 --- > JAVA_VERSION=1.6.0_07",contrib/cloud
Dynamic host configuration system (via node side plugins),"The MapReduce paradigma is limited to run MapReduce jobs with the lowest common factor of all nodes in the cluster.  On the one hand this is wanted (cloud computing, throw simple jobs in, nevermind who does it) On the other hand this is limiting the possibilities quite a lot, for instance if you had data which could/needs to be fed to a 3rd party interface like Mathlab, R, BioConductor you could solve a lot more jobs via hadoop.  Furthermore it could be interesting to know about the OS, the architecture, the performance of the node in relation to the rest of the cluster. (Performance ranking) i.e. if i'd know about a sub cluster of very computing performant nodes or a sub cluster of very fast disk-io nodes, the job tracker could select these nodes regarding a so called job profile (i.e. my job is a heavy computing job / heavy disk-io job), which can usually be estimated by a developer before.  To achieve this, node capabilities could be introduced and stored in the DFS, giving you  a1.) basic information about each node (OS, ARCH) a2.) more sophisticated infos (additional software, path to software, version).  a3.) PKI collected about the node (disc-io, cpu power, memory) a4.) network throughput to neighbor hosts, which might allow generating a network performance map over the cluster  This would allow you to  b1.) generate jobs that have a profile (computing intensive, disk io intensive, net io intensive) b2.) generate jobs that have software dependencies (run on Linux only, run on nodes with MathLab only) b3.) generate a performance map of the cluster (sub clusters of fast disk nodes, sub clusters of fast CPU nodes, network-speed-relation-map between nodes)  From step b3) you could then even acquire statistical information which could again be fed into the DFS Namenode to see if we could store data on fast disk subclusters only (that might need to be a tool outside of hadoop core though)","benchmarks,conf,metrics"
Providing splitting support for bzip2 compressed files,"Hadoop assumes that if the input data is compressed, it can not be split (mainly due to the limitation of many codecs that they need the whole input stream to decompress successfully).  So in such a case, Hadoop prepares only one split per compressed file, where the lower split limit is at 0 while the upper limit is the end of the file.  The consequence of this decision is that, one compress file goes to a single mapper. Although it circumvents the limitation of codecs (as mentioned above) but reduces the parallelism substantially, as it was possible otherwise in case of splitting.    BZip2 is a compression / De-Compression algorithm which does compression on blocks of data and later these compressed blocks can be decompressed independent of each other.  This is indeed an opportunity that instead of one BZip2 compressed file going to one mapper, we can process chunks of file in parallel.  The correctness criteria of such a processing is that for a bzip2 compressed file, each compressed block should be processed by only one mapper and ultimately all the blocks of the file should be processed.  (By processing we mean the actual utilization of that un-compressed data (coming out of the codecs) in a mapper).    We are writing the code to implement this suggested functionality.  Although we have used bzip2 as an example, but we have tried to extend Hadoop's compression interfaces so that any other codecs with the same capability as that of bzip2, could easily use the splitting support.  The details of these changes will be posted when we submit the code.",io
SocketException with S3 native file system causes job to fail,"I'm running Hadoop 0.18.0 with a Amazon S3 native filesystem input (s3n URL given for input on the commandline).  I'm having mapper tasks die, which is killing the job.  The error is ""java.net.SocketException: Connection reset"".  I'm using streaming, but my code isn't using any S3 classes itself, this is being done for me by the input reader.  Traceback from the task details, and my invocation, are appended.  Several mapper tasks complete before this happens, and I've had other jobs work with input from smaller Amazon S3 buckets for the same account.  So this looks like a connectivity issue, where the input reader should realize that it's calling a web service and try again.  Traceback:  java.net.SocketException: Connection reset   java.net.SocketInputStream.read(SocketInputStream.java:168)   com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)   com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)   com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)   com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:789)   com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:746)   com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)   java.io.BufferedInputStream.read1(BufferedInputStream.java:256)   java.io.BufferedInputStream.read(BufferedInputStream.java:317)   org.apache.commons.httpclient.ContentLengthInputStream.read(ContentLengthInputStream.java:169)   java.io.FilterInputStream.read(FilterInputStream.java:116)   org.apache.commons.httpclient.AutoCloseInputStream.read(AutoCloseInputStream.java:107)   org.jets3t.service.io.InterruptableInputStream.read(InterruptableInputStream.java:72)   org.jets3t.service.impl.rest.httpclient.HttpMethodReleaseInputStream.read(HttpMethodReleaseInputStream.java:123)   org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsInputStream.read(NativeS3FileSystem.java:98)   java.io.BufferedInputStream.fill(BufferedInputStream.java:218)   java.io.BufferedInputStream.read1(BufferedInputStream.java:258)   java.io.BufferedInputStream.read(BufferedInputStream.java:317)   java.io.DataInputStream.read(DataInputStream.java:132)   java.io.BufferedInputStream.fill(BufferedInputStream.java:218)   java.io.BufferedInputStream.read(BufferedInputStream.java:237)   org.apache.hadoop.streaming.StreamXmlRecordReader.fastReadUntilMatch(StreamXmlRecordReader.java:248)   org.apache.hadoop.streaming.StreamXmlRecordReader.readUntilMatchEnd(StreamXmlRecordReader.java:123)   org.apache.hadoop.streaming.StreamXmlRecordReader.next(StreamXmlRecordReader.java:91)   org.apache.hadoop.streaming.StreamXmlRecordReader.next(StreamXmlRecordReader.java:46)   org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:165)   org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:45)   org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)   org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2209)  Part of my Hadoop invocation (connection info censored, lots of -file includes removed)  hadoop jar /usr/local/hadoop-0.18.0/contrib/streaming/hadoop-0.18.0-streaming.jar -mapper ./spinn3r_vector_mapper.py -input s3n://<key:key>@<bucket>/ -output vectors -jobconf mapred.output.compress=false -inputreader org.apache.hadoop.streaming.StreamXmlRecordReader,begin=<item>,end=</item> -jobconf mapred.map.tasks=128 -jobconf mapred.reduce.tasks=0 [...] ",fs/s3
DFS upgrade fails on Windows,"FileUtil.HardLink#createHardLink() didn't work on Windows, and DFS upgrade of Datanode fails.  The windows command 'fsutil' requires the arguments link name first as follows, > fsutil hardlink create <link name> <target> But the current FileUtil passes the target first. ",fs
failmon build seems to be recursively pulling in hadoop jar and tar.gz file into it's lib directory,"HadoopQA is taking a very long time to run patch builds and tests on hudson.zones because, I believe, failmon build is broken.  If I look at trunk/src/contrib/failmon/lib during a build I see this: -rw-r--r--   1 hudson      38015 Aug 25 16:04 commons-logging-1.0.4.jar -rw-r--r--   1 hudson      26202 Aug 25 16:04 commons-logging-api-1.0.4.jar drwxr-xr-x   2 hudson          2 Aug 25 16:04 hadoop-688101_HADOOP-3854_PATCH-12388706 -rw-r--r--   1 hudson    2222615 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-core.jar -rw-r--r--   1 hudson     123580 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-examples.jar -rw-r--r--   1 hudson    1110090 Aug 25 14:43 hadoop-688101_HADOOP-3854_PATCH-12388706-test.jar -rw-r--r--   1 hudson      50222 Aug 25 16:04 hadoop-688101_HADOOP-3854_PATCH-12388706-tools.jar -rw-r--r--   1 hudson   74019000325 Aug 25 16:03 hadoop-688101_HADOOP-3854_PATCH-12388706.tar.gz -rw-r--r--   1 hudson     391834 Aug 25 16:04 log4j-1.2.15.jar  I believe that failmon is recursively pulling these files in (note the size of the tar.gz file).   ",build
javadoc warnings: incorrect references,"- ""ant test-patch"" with an empty patch file {noformat}   [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FileSystem.java:34: package org.apache.hadoop.hdfs does not exist   [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;   [javadoc]                              ^   [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FsShell.java:46: package org.apache.hadoop.hdfs does not exist   [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;   [javadoc]                              ^   [javadoc] .../trunk/src/mapred/org/apache/hadoop/mapred/Task.java:36: package org.apache.hadoop.hdfs does not exist   [javadoc] import org.apache.hadoop.hdfs.DistributedFileSystem;   [javadoc]                              ^   [javadoc] Standard Doclet version 1.6.0   [javadoc] Building tree for all the packages and classes...   [javadoc] .../trunk/src/core/org/apache/hadoop/fs/FileSystem.java:56: warning - Tag @link: reference not found: DistributedFileSystem   [javadoc] .../trunk/src/core/org/apache/hadoop/metrics/MetricsUtil.java:34: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics   [javadoc] Building index for all the packages and classes...   [javadoc] Building index for all classes...   [javadoc] .../trunk/src/core/overview.html: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.NameNode   [javadoc] Generating /home/tsz/hadoop/trunk/build/docs/api/stylesheet.css...   [javadoc] 6 warnings {noformat}  - ""ant javadoc"" {noformat}   [javadoc] .../trunk/src/core/org/apache/hadoop/metrics/MetricsUtil.java:34: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics   [javadoc] Building index for all the packages and classes...   [javadoc] Building index for all classes...   [javadoc] .../trunk/src/core/overview.html: warning - Tag @link: reference not found: org.apache.hadoop.hdfs.server.namenode.NameNode   [javadoc] Generating .../trunk/build/docs/api/stylesheet.css...   [javadoc] 2 warnings {noformat}",documentation
"""ant test-patch"" and ""ant javadoc"" are inconsistent","The javadoc results from ""ant test-patch"" and ""ant javadoc"" are inconsistent.  See HADOOP-4023 for an example.",build
Add hama site link to hadoop related project,"The Hama (http://incubator.apache.org/hama) team which is develop a parallel matrix computational package based on Hadoop would like to add our site link to hadoop related project.   Please review this, Thanks. ",documentation
LzopCodec shouldn't be in the default list of codecs i.e. io.compression.codecs,"Since we do not ship the native lzo codecs by default, we shouldn't have it in io.compression.codecs. Users might be thrown off by the errors which come up when the framework tries to load the lzo codecs (by default).",io
Redundant deprecation warnings in hadoop logs,"Warnings in the form of  ""org.apache.hadoop.fs.FileSystem - ""localhost:57367"" is a deprecated filesystem name. Use ""hdfs://localhost:57367/"" instead."" are frequently emitted into the hadoop log. The problem is that the frequency of these warnings floods the logs and makes it difficult to discover real issues.  A short investigation reveals that while FileSystem.getFileSysName(URI) returns the file system name without the hdfs:// scheme part, the method FileSystem.fixName(String) complains about it and appends the hdfs:// back.    ",fs
IsolationRunner does not work as documented,"IsolationRunner does not work as documented in the tutorial.    The tutorial  says ""To use the IsolationRunner, first set keep.failed.tasks.files to true (also see keep.tasks.files.pattern).""    Should be:    keep.failed.task.files (not tasks)    After the above was set (quoted from my message on hadoop-core):  > After the task  > hung, I failed it via the web interface.  Then I went to the node that was  > running this task  >  >   $ cd ...local/taskTracker/jobcache/job_200808071645_0001/work  > (this path is already different from the tutorial's)  >  >   $ hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml  > Exception in thread ""main"" java.lang.NullPointerException  >         at  > org.apache.hadoop.mapred.IsolationRunner.main(IsolationRunner.java:164)  >  > Looking at IsolationRunner code, I see this:  >  >     164     File workDirName = new File(lDirAlloc.getLocalPathToRead(  >     165                                   TaskTracker.getJobCacheSubdir()  >     166                                   + Path.SEPARATOR + taskId.getJobID()   >     167                                   + Path.SEPARATOR + taskId  >     168                                   + Path.SEPARATOR + ""work"",  >     169                                   conf). toString());  >  > I.e. it assumes there is supposed to be a taskID subdirectory under the job  > dir, but:  >  $ pwd  >  ...mapred/local/taskTracker/jobcache/job_200808071645_0001  >  $ ls  >  jars  job.xml  work  >  > -- it's not there.",documentation
bin/hadoop should check `which java` to find java,"Currently, the bin/hadoop script tries to find java in JAVA_HOME/bin/java.  If JAVA_HOME is not set, it errors out.  Instead, I think it should check `which java 2>/dev/null` to see if there is a java on the user's PATH.  If a java is on the user's path, the script should just set JAVA=java. ",scripts
WritableComparator's constructor should be protected instead of private,"Currently, it isn't possible to subclass WritableComparator in a meaningful way. We should make this constructor protected:  {code} private WritableComparator(Class keyClass, boolean createInstances) {code}",io
Metrics for connection cleanup by the ipc/Server: cleanupConnections(),Request for metrics that shows the number of idle ipc connections closed from the Server side. This metrics would have helped when debugging HADOOP-4040. ,"ipc,metrics"
ipc.Client:  Log when Server side closes the socket while request is still pending,ipc/Client.java {noformat} 316       } catch (EOFException eof) { 317         // This is what happens when the remote side goes down 318       }  {noformat}  Request to log  when Server side closes the socket while some requests are still pending.  This would have helped when debugging HADOOP-4040.,ipc
clean up UGI to support savetoConf,Currently clients have to use UnixUserGroupInfo instead of UserGroupInfo because saveToConf is only defined in UnixUGI. We should add the abstract saveToConf in UGI.  {code} UserGroupInfo:   public abstract void saveToConf(Configuration conf); {code}  and the matching body in UnixUGI.,util
Add tests that try starting the various hadoop command line scripts,"Hadoop has lots of tests that start clusters, but I don't see any that test the command line scripts working. Which means that changes to the scripts and the code behind them may not get picked up rapidly, and regressions won't get picked up by Hudson and apportioned to the specific patches.  Propose: * an abstract test case that can exec scripts on startup; wait for them to finish, kill them if they take too long * test cases for every service (namenode, tasktracker, datanode, etc) * tests that try invalid commands, -help options * tests that start the services, assert that they are live, and shut them down",test
[HOD] Make HOD to roll log files on the client ,"Currently HOD writes a log file on the client in the cluster directory, named hod-<username>.log. This file is appended to for each run of hod allocation that runs on the same cluster directory. Thus, the file can become quite large - particularly if the job is queued for a long time. If there are problems with the HOD client subsequently, this could result in stale file handles pointing to this large file, and cause disk space to fill up. Another problem is that a large log file is usually unusable for a user.",contrib/hod
IPC client does not need to be synchronized on the output stream when a connection is closed,"Currently when a connection on the client side is closed, the close method is synchronized on the output stream. The synchronization is not necessary and has introduced a side effect that the socket can not be closed immediately when meanwhile applications are sending large requests.",ipc
Issue with permissions on test-libhdfs.sh while carrying out unit test on trunk /branch,"We are encountering some issue while carrying out unit test for trunk using jdk1.6.0_07-32bit (command) : ""${ANT_HOME}/bin/ant -Dlibhdfs=1 -Dtest.junit.output.format=xml -Dtest.output=yes test test-libhdfs""   -------------------------------------------------------------------------------- test-libhdfs:     [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs     [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs/logs     [mkdir] Created dir: /xx/hadoop-0.19.0-dev/build/test/libhdfs/hdfs/name      [exec] ./tests/test-libhdfs.sh      [exec] make: execvp: ./tests/test-libhdfs.sh: Permission denied      [exec] make: *** [test] Error 127  BUILD FAILED /xx/hadoop-0.19.0-dev/build.xml:1088: exec returned: 2 ",build
TestKosmosFileSystem can fail when run through ant test on systems shared by users,"TestKosmosFileSystem has some test cases that try to verify paths under /tmp/test/. If a user is running ant test on a system that is shared, this could result in test failures if these paths are created by another user who has used the system. The test cases can be modified to either use one of the standard data directories set up for tests by Hadoop, or they can atleast append the user name when referring to these directories, like /tmp/test-<username>/. ",fs
releaseaudit target must be re-written so that the rat jar file isn't in the classpath,"The nightly build and the Hudson patch testing process copies the rat-0.5.1.jar into the hadoop lib directory so that the releaseaudit target works.  This puts it in the general classpath.  Since Hive was committed, this jar file contains older versions of classes needed by Hive.  This rat jar file should be put elsewhere or configured in another way so that it doesn't need to be in the classpath. ",build
test-patch.sh should output the ant commands that it runs,Would be helpful for debugging.,build
TestKosmosFileSystem fails on trunk,org.apache.hadoop.fs.kfs.TestKosmosFileSystem.testDirs fails with error messgae: {noformat} Error Message expected:<7> but was:<1>  Stacktrace junit.framework.AssertionFailedError: expected:<7> but was:<1>   org.apache.hadoop.fs.kfs.TestKosmosFileSystem.testDirs(TestKosmosFileSystem.java:77) {noformat},fs
HFTP interface compatibility with older releases broken,"Using current trunk (Revision 692556) build and trying to distcp from a 0.18.0 cluster via HFTP, got the following NullPointerException. Seems like on line 165 in HftpFileSystem.java, a potential null return value isn't checked before using.  coursemud-lm:bin kan$ ./hadoop distcp hftp://ucdev18.inktomisearch.com:50070/output/part-00000 /copied/part-00001 08/09/05 19:00:10 INFO tools.DistCp: srcPaths=[hftp://ucdev18.inktomisearch.com:50070/output/part-00000] 08/09/05 19:00:10 INFO tools.DistCp: destPath=/copied/part-00001 With failures, global counters are inaccurate; consider running with -i Copy failed: java.lang.NullPointerException   java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1215)   java.text.DateFormat.parse(DateFormat.java:335)   org.apache.hadoop.hdfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:165)   com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:501)   com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:179)   com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:377)   com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2740)   com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:647)   com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)   com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:508)   com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)   com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)   com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)   com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)   org.apache.hadoop.hdfs.HftpFileSystem$LsParser.fetchList(HftpFileSystem.java:194)   org.apache.hadoop.hdfs.HftpFileSystem$LsParser.getFileStatus(HftpFileSystem.java:205)   org.apache.hadoop.hdfs.HftpFileSystem.getFileStatus(HftpFileSystem.java:234)   org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:649)   org.apache.hadoop.tools.DistCp.checkSrcPath(DistCp.java:614)   org.apache.hadoop.tools.DistCp.copy(DistCp.java:631)   org.apache.hadoop.tools.DistCp.run(DistCp.java:838)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.tools.DistCp.main(DistCp.java:865)  ",fs
libhdfs wiki is very out-of-date and contains mostly broken links,This thing is way out of date and seems incorrect in some places. Also note it is not linked from the wiki front page.  http://wiki.apache.org/hadoop/LibHDFS ,documentation
FileSystem support for POSIX access method,"From man access: {code}   int access(const char *pathname, int mode); {code}  DESCRIPTION        access  checks  whether  the process would be allowed to read, write or test for existence of the file (or other file system object) whose name is pathname.  If pathname is a symbolic link permissions of the file referred to by this symbolic link are tested.         mode is a mask consisting of one or more of R_OK, W_OK, X_OK and F_OK.         R_OK, W_OK and X_OK request checking whether the file exists and has read, write and execute permissions, respectively.  F_OK just requests checking for the existence of the file.         The tests depend on the permissions of the directories occurring in the path to the file, as given in pathname, and on the permissions of directories and files referred to by symbolic links encountered on the way.         The check is done with the process s real uid and gid, rather than with the effective ids as is done when actually attempting an operation.  This is to allow set-UID  programs  to        easily determine the invoking user s authority.         Only  access  bits  are checked, not the file type or contents.  Therefore, if a directory is found to be ""writable,"" it probably means that files can be created in the directory,        and not that the directory can be written as a file.  Similarly, a DOS file may be found to be ""executable,"" but the execve(2) call will still fail.         If the process has appropriate privileges, an implementation may indicate success for X_OK even if none of the execute file permission bits are set.  RETURN VALUE        On success (all requested permissions granted), zero is returned.  On error (at least one bit in mode asked for a permission that is denied, or some other error occurred),  -1  is        returned, and errno is set appropriately.  ",fs
FileSystem support for posix truncate method,"{code}      void truncate(Path path, long offset) throws IOException;  {code} from man truncate:  DESCRIPTION        The truncate and ftruncate functions cause the regular file named by path or referenced by fd to be truncated to a size of precisely length bytes.         If the file previously was larger than this size, the extra data is lost.  If the file previously was shorter, it is extended, and the extended part reads as zero bytes.         The file pointer is not changed.         If the size changed, then the ctime and mtime fields for the file are updated, and suid and sgid mode bits may be cleared.         With ftruncate, the file must be open for writing; with truncate, the file must be writable.  RETURN VALUE        On success, zero is returned.  On error, -1 is returned, and errno is set appropriately.  ",fs
Improve configurability of Hadoop EC2 instances,"Currently hadoop-site.xml for EC2 instances is stored as a part of the image and only a few properties can be controlled from the user scripts (compression, number of map/reduce tasks). Furthermore, it is not possible to rsync the configuration around the EC2 cluster with the current image, so the only way to customize the hadoop-site.xml file is to rebuild the image, which is time-consuming.  It would be much better to pass the initialization script for nodes at boot time, so that it is easy to edit the configuration before starting a cluster.",contrib/cloud
Allow access to HDFS web UI on EC2,Need to open access to ports 50070 and 50075 in the Hadoop EC2 security groups.,contrib/cloud
possible race condition in FSDataset.FSVolume,"This is only a possibility, noticed during reviewing of the possible causes of HADOOP-4128.  In the FSDataset.FSVolume constructor, attempts are made to recover from the detach and tmp directories        this.detachDir = new File(parent, ""detach"");       if (detachDir.exists()) {         recoverDetachedBlocks(currentDir, detachDir);        this.tmpDir = new File(parent, ""tmp"");       if (tmpDir.exists()) {         recoverDetachedBlocks(currentDir, tmpDir);       }  This is done before the destination directory is created       this.dataDir = new FSDir(currentDir); If either ../detach or ../tmp had valid data, but ../current didn't exist, recovery could fail without useful messages. Perhaps the existence of ../current is a prerequisite for recovery?",fs
Allow use of hadoop scripts on EC2,"Currently the Hadoop control scripts do not work on EC2 since instances do not have ssh access to one another. This is inconvenient as it makes it impossible to restart the cluster after a configuration change, for example.",contrib/cloud
NPE in GangliaContext.xdr_string (GangliaContext.java:195),"Exception in thread ""Timer thread for monitoring dfs"" java.lang.NullPointerException         at org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_string(GangliaContext.java:195)         at org.apache.hadoop.metrics.ganglia.GangliaContext.emitMetric(GangliaContext.java:138)         at org.apache.hadoop.metrics.ganglia.GangliaContext.emitRecord(GangliaContext.java:123)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecords(AbstractMetricsContext.java:304)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:290)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:50)         at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:249)         at java.util.TimerThread.mainLoop(Unknown Source)         at java.util.TimerThread.run(Unknown Source)  It looks like this caused the datanode to hang, though I accidentally killed the datanode before I could dump its stack.",metrics
[HOD] Support an accounting plugin script for HOD,Production environments have accounting packages to track usage of a cluster. HOD should provide a mechanism to plug-in an accounting related script that can be used to verify if the user is using a valid account or not.,contrib/hod
Include librecordio as part of the release,None,build
Fix javac warning in WritableUtils,There a few javac warning in WritableUtils.,io
[HOD] Uncaught exceptions can potentially hang hod-client.,"In hod-client, we have {code} sys.exit(hod.operation()) sys.exit(hod.script()) {code} sys.exit(opCode) makes sure that the client is truly cleaned up, killing unjoined threads etc. So, exceptions not caught by hodRunner.operation() or hodRunner.script(), will by-pass sys.exit method and thus can potentially hang hod-client.  For e.g., when hod allocate fails after allocation and before service-registry thread is cleaned up, hod client will hang. ",contrib/hod
Chinese translation of core docs,"There are many active Chinese users in Hadoop community, a Chinese translation of the core docs will be very helpful to Hadoop newbies and experienced Hadoop users as well. Our team(Distributed computing@Alibaba Search Center) have just worked out a translation of  r0.17.x, now we're updating it to r0.18 and r0.19.  The translation covers all the files under xdocs directory in the repos. Thus, we will have a Chinese version for each main doc, and leave the API docs, Wiki, FAQ, Mailing Lists, Release Notes, All Changes as is.  ",documentation
test-patch script is showing questionable number of Javac warnings,"test-patch is recording 881 number of javac warnings when run on trunk and 216 javac warnings when run on ""ANY"" patch. This behavior was observed even on an empty patch.",test
TestInjectionForSimulatedStorage job is failing on linux ,"TestInjectionForSimulatedStorage job is failing on linux with following errors:  [junit] 2008-09-13 00:58:09,676 INFO  dfs.DataNode (DataNode.java:run(2858)) - DatanodeRegistration(127.0.0.1:xxxxx, storageID=DS-1383424108-66.228.166.207-0-1221267471510, infoPort=xxxxx, ipcPort=xxxxxx):Transmitted block blk_3685375500187228851_1001 to /127.0.0.1:xxxxx     [junit] 2008-09-13 00:58:09,753 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:09,755 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:10,258 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:10,259 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:10,763 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:10,764 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:11,267 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:11,269 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:11,631 INFO  dfs.StateChange (FSNamesystem.java:computeReplicationWork(2362)) - BLOCK* ask 127.0.0.1:xxxxx to replicate blk_3685375500187228851_1001 to datanode(s) 127.0.0.1:xxxxx 127.0.0.1:54219     [junit] 2008-09-13 00:58:11,772 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:11,773 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:12,276 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(94)) - ugi=hadoopqa,users,search,gridlogin ip=/127.0.0.1 cmd=open src=/replication-test-file dst=null perm=null     [junit] 2008-09-13 00:58:12,278 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 2th block blk_3685375500187228851_1001 yet. Expecting 4, got 1.     [junit] 2008-09-13 00:58:12,674 INFO  dfs.DataNode (DataNode.java:transferBlocks(879)) - DatanodeRegistration(127.0.0.1:xxxxx, storageID=DS-1383424108-66.228.166.207-0-1221267471510, infoPort=54223, ipcPort=xxxxx) Starting thread to transfer block blk_3685375500187228851_1001 to 127.0.0.1:xxxxx, 127.0.0.1:54219     [junit] 2008-09-13 00:58:12,675 INFO  dfs.DataNode (DataNode.java:run(2858)) - DatanodeRegistration(127.0.0.1:xxxxx, storageID=DS-1383424108-66.228.166.207-0-1221267471510, infoPort=54223, ipcPort=54224):Transmitted block blk_3685375500187228851_1001 to /127.0.0.1:xxxxx     [junit] 2008-09-13 00:58:12,678 INFO  dfs.DataNode (DataNode.java:writeBlock(1156)) - Receiving block blk_3685375500187228851_1001 src: /127.0.0.1:54252 dest: /127.0.0.1:xxxxx     [junit] 2008-09-13 00:58:12,678 INFO  dfs.DataNode (DataNode.java:writeBlock(1302)) - writeBlock blk_3685375500187228851_1001 received exception java.io.IOException: Block blk_3685375500187228851_1001 is valid, and cannot be written to.     [junit] 2008-09-13 00:58:12,678 ERROR dfs.DataNode (DataNode.java:run(1068)) - DatanodeRegistration(127.0.0.1:xxxxx, storageID=DS-55333783-66.228.166.207-0-1221267471661, infoPort=xxxxx, ipcPort=xxxxx):DataXceiver: java.io.IOException: Block blk_3685375500187228851_1001 is valid, and cannot be written to.     [junit]   org.apache.hadoop.dfs.SimulatedFSDataset.writeToBlock(SimulatedFSDataset.java:365)     [junit]   org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:2320)     [junit]   org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1187)     [junit]   org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1045)     [junit]   java.lang.Thread.run(Thread.java:619)",test
delay in TestCLI teardown,"Before I have a go at cleaning it up, can I check to see if there is a reason for this code in TestCLI.teardown()      success = true;     Thread.sleep(2000);     assertTrue(""Error tearing down Mini DFS cluster"", success);  It sets a flag that is always true, and sleeps for 2 seconds. If shutdown is really needed, it should be checked for, and if not, the sleep and the assert culled.   ",test
Unit tests failing on Windows,The following unit tests are failing on Windows machine: TestFileCreation TestHDFSFileSystemContract TestInjectionForSimulatedStorage,test
Unit tests (mapred) failing on Windows machine,The following unit tests are failing on Windows machine: 1) TestMiniMRClasspath 2) TestMiniMRDFSSort 3) TestMiniMRLocalFS 4) TestMiniMRWithDFSWithDistinctUsers 5) TestTaskTrackerMemoryManager ,test
TestProcfsBasedProcessTree failing on Windows machine,"TestProcfsBasedProcessTree unit test is failing on Windows. Here is the output:     [junit] Running org.apache.hadoop.util.TestProcfsBasedProcessTree     [junit] 2008-09-14 21:39:24,354 INFO  util.TestProcfsBasedProcessTree (TestProcfsBasedProcessTree.java:testProcessTree(121)) - Root process pid: 5512     [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.609 sec     [junit] Test org.apache.hadoop.util.TestProcfsBasedProcessTree FAILED","test,util"
Range of ports instead of all over the map,Change the default ports so they are next to each other rather than scattered to make it easier to firewall Hadoop off.,fs
some minor things to make Hadoop friendlier to git,It would be nice to have a .gitignore file and a saveVersion that didn't fail under git.,build
Add setVerifyChecksum() method to FileSystem,None,fs
Add a testcase for jobhistory,"Changes in job history might break the history parser which in turn might break some features like jobtracker-recovery, history-viewer etc. There should be a testcase that catches these incompatible changes early and informs about the expected change.    This patch validates job history files so that issues related to jobHistory are caught when changes to JobHistory  are made.  Validates history file name, file location, parsability. Validates user log location for varoius configuration settings. Validates job status for jobs that are succeeded, failed and killed.  Validates format of history file contents and also validates contents by comparing with actual values obtained from job tracker.",test
SequenceFile.Writer close() uses compressor after returning it to CodecPool.,In function SequenceFile.Writer.close() (line 946): The first marked line returns the compressor while the second marked line will use the compressor again. This will lead to a race condition if another thread checks out the compressor between these two marked statements.  {code:title=SequenceFile.java|borderStyle=solid}     public synchronized void close() throws IOException {       CodecPool.returnCompressor(compressor); // <==== compressor returned              keySerializer.close();       uncompressedValSerializer.close();       if (compressedValSerializer != null) {         compressedValSerializer.close(); // <===== compressor used       }        if (out != null) {                  // Close the underlying stream iff we own it...         if (ownOutputStream) {           out.close();         } else {           out.flush();         }         out = null;       }     } {code}  ,io
Possible performance enhancement in Hadoop compress module,"There are several less performant implementation issues with the current Hadoop compression module. Generally, the opportunities all come from the fact that the granuarities of I/O operations from the CompressionStream and DecompressionStream are not controllable by the users, and thus users are forced to attach BufferedInputStream or BufferedOutputStream to both ends of the CompressionStream and DecompressionStream: - ZlibCompressor: always returns false from needInput() after setInput(), and thus lead to a native call deflateBytesDirect() for almost every write() operation from CompressorStream(). This becomes problematic when applications call write() on the CompressorStream with small write sizes (e.g. one byte at a time). It is better to follow similar code path in LzoCompressor and append to internal uncompressed data buffer. - CompressorStream: whenever the compressor produces some compressed data, it will directly issue write() calls to the down stream. Could be improved by keep appending to the byte[] until it is full (or half full) before writing to the down stream. Otherwise, applications have to use a BufferedOutputStream as the down stream in case the output sizes from CompressorStream is too small. This generally causes double buffering. - BlockCompressorStream: similar issue as described above. - BlockDecompressorStream: getCompressedData() reads only one compressed chunk at a time. Could be better to read a full buffer, and then obtain compressed chunk from buffer (similar to DecompressStream is doing, but admittedly a bit more complicated).  In generally, the following could be some guideline of Compressor/Decompressor and CompressorStream/DecompressorStream design/implementation that can give users some performance guarantee: - Compressor and Decompressor keep two DirectByteBuffer, the size of which should be tuned to be optimal with regard to the specific compression/decompression algorithm. Ensure always call Compressor.compress() will a full (or near full) uncompressed data DirectBuffer. - CompressorStream and DecompressorStream maintains a byte[] to read data from the down stream. The size of the byte[] should be user customizable (add a bufferSize parameter to CompressionCodec's createInputStream and createOutputStream interface). Ensure that I/O from the down stream at or near the granularity of the size of the byte[]. So applications can simply rely on the buffering inside CompressorStream and DecompressorStream (for the case of LZO: BlockCompressorStream and BlockDecompressorStream).  A more radical change would be to let the downward InputStream to directly deposit data to a ByteBuffer or the downard OutputStream to accept input data from ByteBuffer. We may call it ByteBufferInputStream and ByteBufferOutputStream. The CompressorStream and DecompressorStream may simply test whether the down stream indeed implements such interfaces and bypass its own byte[] buffer if true.",io
Hadoop-Patch build is failing ,"Hadoop-Patch build is failing with the following error:  test-patch.sh: line 165: -Xmaxwarns: command not found [exec] BUILD FAILED      [exec] Target ""&>"" does not exist in the project ""Hadoop"".       [exec]      [exec] Trunk findbugs is broken? ",build
[patch] native library build script uses unportable sed(1) regexp's,"The native library build script uses unportable sed(1) regular expressions, making it impossible to compile a library using non-GNU sed(1). In particular, any POSIX-conformant sed(1) implementation will fail. The following patch has been tested to work on both Linux (Ubuntu) and FreeBSD. ",build
"TaskTracker never stops cleanup threads, MiniMRCluster becomes unstable","If many unit tests start/stop unique MiniMRCluster instances, over time the number of threads in the test vm grow to large causing tests to hang and/or slow down.",test
New lines and leading spaces are not trimmed of a value when configuration is read,While configuration value is read the leading and trailing spaces and new line characters are taken into account.,conf
NPE in TestLimitTasksPerJobTaskScheduler,"The test is failed consistently.  It is also failed on Hudson. {noformat} Testsuite: org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler Tests run: 5, Failures: 0, Errors: 5, Time elapsed: 0.391 sec  Testcase: testMaxRunningTasksPerJob took 0.328 sec  Caused an ERROR null java.lang.NullPointerException   org.apache.hadoop.mapred.LimitTasksPerJobTaskScheduler.start(LimitTasksPerJobTaskScheduler.java:53)   org.apache.hadoop.mapred.TestJobQueueTaskScheduler.setUp(TestJobQueueTaskScheduler.java:197)  ... {noformat}",test
test-patch should have a mode where developers can turn on running core and contrib tests.,"There are scenarios, such as on a feature freeze date, *smile*, when developers rely on test-patch, rather than hudson to ensure their patches are not causing regressions. For tests though, developers still have to run core and contrib tests and grep on the output to detect failures, which can lead to human errors. Having a mode where test-patch can optionally be asked to run the core and contrib tests also and report on the consolidated status would greatly help.",test
TestLimitTasksPerJobTaskSchedule test is failing on linux,"TestLimitTasksPerJobTaskSchedule test is failing on linux giving the following message:     [junit] Running org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler     [junit] Tests run: 5, Failures: 0, Errors: 5, Time elapsed: 0.363 sec     [junit] Test org.apache.hadoop.mapred.TestLimitTasksPerJobTaskScheduler FAILED",test
"Job Restart tests take 10 minutes, can time out very easily","HADOOP-3245 added job restart and tests for it, but the tests take a long time  TestJobTrackerRestart 667.682  TestJobTrackerRestartWithLostTracker 322.223  Something needs to be done to speed them up to keep the test cycle viable.",test
"Remove the deprecated, unused class ShellCommand.",org.apache.hadoop.fs.ShellCommand was deprecated in 0.16 and is unused in current trunk.,fs
"dfs datanode metrics, bytes_read, bytes_written overflows due to incorrect type used.","bytes_read, and bytes_written metrics are using int (MetricsTimeVaryingInt) as counter.  This type is too small to store the bytes_read and bytes_written metrics.  Recommend to change this to long (metricsLongValue).",metrics
KFS: Allow KFS layer to interface with multiple KFS namenodes,"The KFS ""glue"" layer code in Hadoop, currently, only allows an application to interface with a single KFS namenode.  The KFS client side library has been modified to allow applications to interface with multiple KFS namenodes.  This jira issue is for incorporating the change into the KFS code.  ",fs
[HOD] Accounting option should be displayed in hod help options,"HADOOP-4145 enabled accounting checks to be plugged into HOD. With this, some sites may mandate the account string to be passed to HOD, via the -A or --resource_manager.pbs-account option. However, HOD allocate and script commands' help does not currently display this option. This issue is for enhancing the help messages for improved usability.",contrib/hod
TestStreamingBadRecords.testNarrowDown fails intermittently,TestStreamingBadRecords.testNarrowDown is failing sometimes because of the delay in the processed counter updation (processed counter updation happens by the separate thread). Due to this there is a broader skipped range to be narrow down. The no of attempts provided in the test case get exhausted before completing the task successfully. ,test
Serialization framework use SequenceFile/TFile/Other metadata to instantiate deserializer,"SequenceFile metadata is useful for storing additional information about the serialized data, for example, for RecordIO, whether the data is CSV or Binary.  For thrift, the same thing - Binary, JSON, ...  For Hive, this may be especially important, because it has a Dynamic generic serializer/deserializer that takes its DDL at runtime (as opposed to RecordIO and Thrift which require pre-compilation into a specific class whose name can be stored in the sequence file key or value class).   In this case, the class name is like Record.java in RecordIO - it doesn't tell you anything without the DDL.  One way to address this could be adding the sequence file metadata to the getDeserializer call in Serialization interface.  The api would then be something like getDeserializer(Class<?>, Map<Text, Text> metadata) or Properties metadata.  But, I am open to proposals.  This also means that saying a class implements Writable is not enough to necessarily deserialize it since it may do specific actions based on the metadata - e.g., RecordIO might determine whether to use CSV rather than the default Binary deserialization.  There's the other issue of the getSerializer returning the metadata to be written to the Sequence/T File.  ",contrib/serialization
"In ""ant test-patch"", runContribTestOnEclipseFiles should not be run on Hudson only","runContribTestOnEclipseFiles is ONLY executed on Hudson but it won't be executed by default.  So it won't be run when developers run ""ant test-patch"".","build,test"
KFS: Update the kfs jar file,Please update the kfs jar file in hadoop/lib to the one in this jira (kfs-0.2.2.jar).,fs
Declare hsqldb.jar in eclipse plugin,lib/hsqldb.jar is not declared in eclipse plugin.,contrib/eclipse-plugin
Fix warnings generated by FindBugs,"Findbugs generates several errors related to unused return values, thread synchronization and ambiguous method calls","conf,fs,record"
TestLeaseRecovery2.testBlockSynchronization failing.,Found this while running HADOOP-4173 through Hudson.  HadoopQA output: {code} org.apache.hadoop.hdfs.TestLeaseRecovery2.testBlockSynchronization Failing for the past 2 builds (Since Failed#3352 ) Took 0 seconds. Error Message  Timeout occurred. Please note the time in the report does not reflect the time until the timeout.  Stacktrace  junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout. {code} See http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3353/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery2/testBlockSynchronization/ and http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3352/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery2/testBlockSynchronization/,test
findbugs should run over the tools.jar also,"Currently the findbugs target doesn't include the tools.jar, as I discovered when I moved a class from tools.jar to hadoop.jar and got hit by a findbugs warning.",test
message generated when the client exception has a null message is not useful,"This was created by HADOOP-3844; if the exception doesn't have a meaningful message, the output isn't that informative : java.io.IOException: Call to localhost/127.0.0.1:8012 failed on local exception: null",ipc
DFSIO is failing on 500 nodes cluster,"On executing following command :  bin/hadoop jar ~/hadoop/hadoop-0.19.0-test.jar TestDFSIO -write -nrFiles 990 -fileSize 320       This error occurs: 08/09/24 06:15:03 INFO mapred.JobClient:  map 98% reduce 32% java.io.IOException: Job failed!   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1201)   org.apache.hadoop.fs.TestDFSIO.runIOTest(TestDFSIO.java:236)   org.apache.hadoop.fs.TestDFSIO.writeTest(TestDFSIO.java:218)   org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:354)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)   org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)   org.apache.hadoop.test.AllTestDriver.main(AllTestDriver.java:77)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.util.RunJar.main(RunJar.java:165)   org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)  On looking at hadoop logs, It seems that file names are clashing  2008-09-24 06:21:41,618 INFO org.apache.hadoop.mapred.JobTracker: Removed completed task 'attempt_200809240600_0005_m_000802_2_1222236048515' from 'tracker_xxxx/client x.x.x.x:xxxxx' 2008-09-24 06:21:41,627 INFO org.apache.hadoop.mapred.JobTracker: Adding task 'attempt_200809240600_0005_m_000802_4_1222236048515' to tip task_200809240600_0005_m_000802, for tracker 'tracker_xxxx/client x.x.x.x:xxxxx' 2008-09-24 06:21:41,627 INFO org.apache.hadoop.mapred.JobInProgress: Choosing rack-local task task_200809240600_0005_m_000802 2008-09-24 06:21:41,724 INFO org.apache.hadoop.mapred.TaskInProgress: Error from attempt_200809240600_0005_m_000900_2_1222236048515: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /benchmarks/TestDFSIO/io_data/test_io_20 for DFSClient_attempt_200809240600_0005_m_000900_2_1222236048515 on client client x.x.x.x, because this file is already being created by DFSClient_attempt_200809240600_0005_m_000900_0_1222236048515 on client x.x.x.x  ","io,test"
LineRecordReader.LineReader should use util.LineReader,"LineRecordReader.LineReader was moved to util and removed from the mapreduce package, but its implementation remains in mapred.LineRecordReader. It would be better if this used the implementation in util.",util
Bug in FSInputChecker makes it possible to read from an invalid buffer,"Bug in FSInputChecker makes it possible to read from an invalid buffer. The buffer in FSInputChecker becomes invalid when readChecksumChunk is used to read a chunk to a user buffer directly. Currently, it's not marked as invalid in this case and may be read subsequently.",fs
write the random number generator seed to log in the append-related tests,"There are quite a few append-related tests (see HADOOP-2658 for a list of related issues) failed occasionally.  Some of the tests use a random number generator.  We should write the random number generator seed to log and, hopefully, we could reproduce the failure.",test
Support for user configurable global filters on HttpServer,"HADOOP-3854 introduced a framework for adding filters to filter browser facing urls. Sometimes, there is a need to filter all urls. For example, at Yahoo, we need to open an SSL port on the HttpServer and only accept hsftp requests from clients who can authenticate themselves using client certificate and is authorized according to certain policy file. For this to happen, we need a method to add a user configurable ""global"" filter, which filters on all client requests. For our purposes, such a global filter will block all https requests except those accessing the hsftp interface (it will let all http requests go through, so accesses through the normal http ports are unaffected). Moreover, those hsftp requests will be subject to further authorization checking according to the policy file.",security
append() does not work for LocalFileSystem,append is supported by LocalFileSystem but it does not update crc when a file is appended.   When you enable checksum verification {{TestLocalFileSystem.testAppend}} fails. Since HADOOP-4277 is a blocker for 0.17  I am planning to disable this test in HADOOP-4277.,fs
Enable Java assertions when running tests,A suggestion to enable Java assertions in the project's build xml when running tests. I think this would improve the build quality. To enable assertions add the following snippets to the JUnit tasks in build.xml:  <assertions>      <enable /> </assertions>  -- For example:  <junit ... >      ...     <assertions>         <enable />     </assertions> </junit>   ,build
Forrest doc for skip bad records feature,Forrest documentation is required for skip records feature HADOOP-153,documentation
eclipse-plugin no longer compiles on trunk,{noformat} compile:      [echo] contrib: eclipse-plugin     [javac] Compiling 2 source files to /Users/chrisdo/work/commit/build/contrib/eclipse-plugin/classes     [javac] /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopServer.java:423: write(java.io.DataOutput) in org.apache.hadoop.conf.Configuration cannot be applied to (java.io.FileOutputStream)     [javac]     this.conf.write(fos);     [javac]              ^     [javac] /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java:166: write(java.io.DataOutput) in org.apache.hadoop.conf.Configuration cannot be applied to (java.io.FileOutputStream)     [javac]       conf.write(fos);     [javac]           ^     [javac] Note: /Users/chrisdo/work/commit/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/servers/RunOnHadoopWizard.java uses unchecked or unsafe operations.     [javac] Note: Recompile with -Xlint:unchecked for details.     [javac] 2 errors {noformat},contrib/eclipse-plugin
Checksum error during execution of  unit tests on linux environment.,Following unit tests are failing for 0.18.1_H4277_H4271  org.apache.hadoop.fs.TestLocalFileSystem.testAppend Error Message Checksum error: /xx/workspace/hadoop-0.18.1_H4277_H4271/build/test/data/append/f at 0 Stacktrace org.apache.hadoop.fs.ChecksumException: Checksum error: /xx/workspace/hadoop-0.18.1_H4277_H4271/build/test/data/append/f at 0   org.apache.hadoop.fs.FSInputChecker.verifySum(FSInputChecker.java:277)   org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:242)   org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)   org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)   java.io.DataInputStream.read(DataInputStream.java:132)   org.apache.hadoop.fs.TestLocalFileSystem.readFile(TestLocalFileSystem.java:43)   org.apache.hadoop.fs.TestLocalFileSystem.testAppend(TestLocalFileSystem.java:173)   org.apache.hadoop.dfs.TestDFSShell.testPut Error Message Checksum error: /xx/workspace/hadoop-0.18.1_H4277_H4271/build/test/data/f2 at 0  Stacktrace java.io.IOException: Checksum error: /xx/workspace/hadoop-0.18.1_H4277_H4271/build/test/data/f2 at 0    org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:187)   org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1173)   org.apache.hadoop.dfs.TestDFSShell.testPut(TestDFSShell.java:254) ,test
TestReplication fails quite often,TestReplication times out once every 5-6 attempts. I will attach log from a failed test.,test
Document the capacity scheduler in Forrest,"Document the capacity scheduler and related issues, some of which have touched the framework as well. The issues are all linked in HADOOP-3444.",documentation
Hadoop SocketInputStream.read() should return -1 in case of EOF.,{{org.apache.hadoop.net.SocketInputStream.read()}} should return -1 when underlying read returns -1. ,io
ChecksumFileSystem does not override all create(...) methods,"ChecksumFileSystem does not override the following method {code} public FSDataOutputStream create(Path f, FsPermission permission,       boolean overwrite, int bufferSize, short replication, long blockSize,       Progressable progress) throws IOException  {code} As a consequence, when creating a file with LocalFileSystem and the create method above, no checksum will be created.",fs
Some convenient methods in the FileSystem API should be final.,"In FileSystem, some methods are overloaded with different parameter list.  We should mark the convenient methods final.  For example, the create(...) method has 10 different signatures.  Subclasses of FileSystem should only override the one with the most number or parameters but not the others.  We should mark all the other 9 create(...) final.",fs
FsShell -ls fails for file systems without owners or groups,"FsShell.ls() fails (uses printf ""%-0d"") when all owners or groups are empty.  This happens with NativeS3FileSystem. ",scripts
Improve FsShell -du/-dus and FileSystem.getContentSummary efficiency,"FsShell.du has two inefficiencies:  * calling getContentSummary twice for each top-level item rather than calling it once and saving the result * calling getContentSummary for files rather than using the size it already has in FileStatus  getContentSummary has one:  * calling itself for files rather than using the length it already has in FileStatus  Every call to getContentSummary results in a call to getFileStatus, which may be expensive (e.g. NativeS3FileSystem has both network latency and actual monetary cost).  The simple solution:  * FsShell.du calls once per item and saves the ContentSummary * FsShell.du uses FileStatus.getLen for files * getContentSummary only calls itself for directories  Another solution, rather than adding special casing to callers, is to add a getContentSummary that takes a FileStatus.",fs
"Hadoop triggers a ""soft"" fd leak. "," Starting with Hadoop-0.17, most of the network I/O uses non-blocking NIO channels. Normal blocking reads and writes are handled by Hadoop and use our own cache of selectors. This cache suites well for Hadoop where I/O often occurs on many short lived threads. Number of fds consumed is proportional to number of threads currently blocked.   If blocking I/O is done using java.*, Sun's implementation uses internal per-thread selectors. These selectors are closed using {{sun.misc.Cleaner}}. Looks like this cleaning is kind of like finalizers and tied to GC. This is pretty ill suited if we have many threads that are short lived. Until GC happens, number of these selectors keeps growing. Each selector consumes 3 fds.  Though blocking read and write are handled by Hadoop, {{connect()}} is still the default implementation that uses per-thread selector.   Koji helped a lot in tracking this. Some sections from 'jmap' output and other info  Koji collected led to this suspicion and will include that in the next comment.  One solution might be to handle connect() also in Hadoop using our selectors. ",io
Adding service-level authorization to Hadoop,"Service-level authorization is the initial checking done by a Hadoop service to find out if a connecting client is a pre-defined user of that service. If not, the connection or service request will be declined. This feature allows services to limit access to a clearly defined group of users. For example, service-level authorization allows ""world-readable"" files on a HDFS cluster to be readable only by the pre-defined users of that cluster, not by anyone who can connect to the cluster. It also allows a M/R cluster to define its group of users so that only those users can submit jobs to it.  Here is an initial list of requirements I came up with.      1. Users of a cluster is defined by a flat list of usernames and groups. A client is a user of the cluster if and only if her username is listed in the flat list or one of her groups is explicitly listed in the flat list. Nested groups are not supported.      2. The flat list is stored in a conf file and pushed to every cluster node so that services can access them.      3. Services will monitor the modification of the conf file periodically (5 mins interval by default) and reload the list if needed.      4. Checking against the flat list is done as early as possible and before any other authorization checking. Both HDFS and M/R clusters will implement this feature.      5. This feature can be switched off and is off by default.  I'm aware of interests in pulling user data from LDAP. For this JIRA, I suggest we implement it using a conf file. Additional data sources may be supported via new JIRA's.",security
[HOD] Support specification of account through an environment variable.,None,contrib/hod
Separate TestDatanodeDeath.testDatanodeDeath() into 4 tests,TestDatanodeDeath fails occasionally as reported in HADOOP-4278.  TestDatanodeDeath.testDatanodeDeath() indeed contains 4 tests.  It would be easily for debugging to separate the tests.,test
Rename all the FSXxx classes to FsXxx,"There are two naming conventions in Hadoop, FSXxx and FsXxx.  We should rename all the FSXxx classes to FsXxx.  See also http://issues.apache.org/jira/browse/HADOOP-4044?focusedCommentId=12637296#action_12637296",fs
NPE from CreateEditsLog,"HADOOP-1869 added a call to setAccessTime(long) from the INode cstr, which relies on a non-null value from FSNamesystem::getFSNamesystem. {noformat} java.lang.NullPointerException         at org.apache.hadoop.hdfs.server.namenode.INode.setAccessTime(INode.java:301)         at org.apache.hadoop.hdfs.server.namenode.INode.<init>(INode.java:99)         at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.<init>(INodeDirectory.java:45)         at org.apache.hadoop.hdfs.CreateEditsLog.addFiles(CreateEditsLog.java:68)         at org.apache.hadoop.hdfs.CreateEditsLog.main(CreateEditsLog.java:214) {noformat}",test
"IPC.Client.handleConnectionFailure should bail out if the retry count is >= the maximum, not ==","Looking at the code in IPC.Client.handleConnectionFailure(), its clear that the connection setup exits if the client retrys equals the maximum specified        if (curRetries == maxRetries) {         throw ioe;       }  But there's nothing to stop anyone setting ipc.client.connect.max.retries=0 in the configuration, and if that happens, the code will spin until the (integer) retry count wraps around. The test should be curRetries >= maxRetries",ipc
Configuration.getProps() should be made protected for ease of overriding,"HADOOP-4293 does make Configuration cleaner and easier to work with, but it does contain assumptions about how configurations are represented (in a private Properties instance) that subclasses may wish to diverge from.  By making getProps() protected, people who subclass JobConf or Configuration can do their own binding from configuration data to Properties.   ",conf
Metric Averages are not averages,"Metrics averages are not averages; instead of updating the metric with the average number periodically, Hadoop metrics *increments* the metric.  I.e., each update we have value = old_value + current average.  Instead, we want each update to have value = current_average.  Patch will be attached momentarily.",metrics
Fix line formatting in hadoop-default.xml for hadoop.http.filter.initializers,"Line formatting in hadoop-default.xml for property hadoop.htttp.filter.initializers is not elegant, we should fix it before 0.19. ",conf
Race condition creating S3 buffer directory for NativeS3FileSystem,"The buffer directory is checked for existence, then created if it doesn't exist.  But the create can fail if the another process creates it in between.  We can fix this by checking for existence again if the create fails.  I've seen ""Cannot create S3 buffer directory"" occur, and this race is the most plausible explanation.",fs/s3
TestJobQueueInformation fails regularly,Got the same result from Linux and Windows: {noformat} Testcase: testJobQueues took 40.806 sec         FAILED task tracker dir /home/tsz/hadoop/latest/build/test/mapred/local/1_0/taskTrackerr does not exist. junit.framework.AssertionFailedError: task tracker dir /home/tsz/hadoop/latest/build/test/mapred/local/1_0/taskTracker does not exist.         at org.apache.hadoop.mapred.TestMiniMRWithDFS.checkTaskDirectories(TestMMiniMRWithDFS.java:140)         at org.apache.hadoop.mapred.TestMiniMRWithDFS.runWordCount(TestMiniMRWitthDFS.java:196)         at org.apache.hadoop.mapred.TestJobQueueInformation.testJobQueues(TestJoobQueueInformation.java:90) {noformat},test
Run Hadoop sort benchmark on Amazon EC2,"By running a benchmark on EC2 we can see how well Hadoop performs, how to tune it, and how performance changes between releases.",contrib/cloud
TestHDFSFileSystemContract fails on windows,"'ant -Dtestcase=TestHDFSFileSystemContract test-core' fails on Windows nightly build machine.   Not sure if this is Hadoop error or a configuration error particular to this machine. Basically the following assert fails :  {noformat}     Path workDir = path(getDefaultWorkingDirectory());     assertEquals(workDir, fs.getWorkingDirectory()); {noformat}  This is essentially testing that {{System.getProperty(""user.name"")}} is same as string returned by Cygwin's 'whoami'. But in this case, these are ""SYSTEM"" and ""hadoopqa"" respectively.",test
allow use of Torque per-job temporary directories,"Torque can be configured to create a per-job temporary directory that will automatically be cleaned up at the completion of a Torque job. At our site, we are strongly encouraged to use this directory and not /tmp. The name of the per-job temp dir is in the environment variable TMPDIR in the Torque job's environment. It would be very useful for HOD to be extended so that one could write in hodrc:  [ringmaster] temp-dir = $TMPDIR work-dirs = $TMPDIR/1  [hodring] temp-dir = $TMPDIR  such that $TMPDIR is evaluated inside the Torque job.",contrib/hod
Various TestMiniMR tests failing on Windows,On the windows nightly build many {{TestMiniMR*}} tests fail with a timeout. I will attach output from some of the tests.   The problem might actually be some mis configuration on the machine.. but it is easy to see that from the test output. Someone more familiar with the test needs look into I think.,test
Merge AccessControlException and AccessControlIOException into one exception class,Merge org.apache.hadoop.fs.permission.AccessControlException and org.apache.hadoop.security.AccessControlIOException into a single class since they are cut & paste from each other.,fs
"Add ""hdfs://"" to fs.default.name on quickstart.html","Trivial change, but if you use the suggested hadoop-site.xml from the current quickstart guide, you'll get an error like the below:  2008-10-12 23:32:42,560 WARN  fs.FileSystem (FileSystem.java:fixName(165)) - ""localhost:9000"" is a deprecated filesystem name. Use ""hdfs://localhost:9000/"" instead.  ",documentation
TestLeaseRecovery.testBlockSynchronization failed on trunk,TestLeaseRecovery.testBlockSynchronization failed on a hadoop patch build. Please refer to http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3450/testReport/org.apache.hadoop.hdfs/TestLeaseRecovery/testBlockSynchronization/ for details. I don't know if it is a random issue.,test
FsAction operations shouldn't create new Objects,"FsAction::and and related operations use {{values()[<expr>]}}, referencing a member var {{INDEX}}. This creates a new array object and uses member indices identical to {{ordinal()}}. It should either keep a reference to {{values()}} and use {{ordinal()}} explicitly or observe semantics consistent with {{INDEX}}.",fs
"Update documentation in forrest for Mapred, streaming and pipes",Update forrest documentation for jvm reuse and job tracker restart in mapred_tutorial. Also update forrest documentation for streaming for HADOOP-2302 and HADOOP-3341. Update the package summary for pipes for HADOOP-1480,documentation
S3 file systems should not create bucket,"Both S3 file systems (s3 and s3n) try to create the bucket at every initialization.  This is bad because  * Every S3 operation costs money.  These unnecessary calls are an unnecessary expense. * These calls can fail when called concurrently.  This makes the file system unusable in large jobs. * Any operation, such as a ""fs -ls"", creates a bucket.  This is counter-intuitive and undesirable.  The initialization code should assume the bucket exists:  * Creating a bucket is a very rare operation.  Accounts are limited to 100 buckets. * Any check at initialization for bucket existence is a waste of money.  Per Amazon: ""Because bucket operations work against a centralized, global resource space, it is not appropriate to make bucket create or delete calls on the high availability code path of your application. It is better to create or delete buckets in a separate initialization or setup routine that you run less often."" ",fs/s3
menu layout change for Hadoop documentation,http://hadoop.apache.org/core/docs/r0.18.1/  changing menu layout for the Hadoop documentation + some minor content changes. New menu will include: Overview Quick Start ========================== Cluster Setup Hadoop Quick Start Hadoop Cluster Setup Hadoop Map/Reduce Tutorial Hadoop Command Guide Hadoop FS Shell Guide Hadoop DistCp Guide Hadoop Native Libraries Hadoop Streaming Hadoop Archive Files ========================== HDFS User Guide HDFS Architecture HDFS Permissions Guide HDFS Quotas Admin Guide ========================== HOD User Guide HOD Config Guide HOD Admin Guide ====================== Scheduling New 0.19 topic ... ====================== API Docs API Changes Wiki FAQ Release Notes Change Log,documentation
Add new/missing commands in forrest,Following commands needs to be added to: - commands_manual: queue job  [-set-priority <job-id> <priority>] dfsadmin  [-setSpaceQuota <quota> <dirname>...<dirname>]                    [-clrSpaceQuota <dirname>...<dirname>]  - hdfs_shell: fs [-count[-q] <path>]     [-moveToLocal [-crc] <src> <localdst>],documentation
S3 object names with arbitrary slashes confuse NativeS3FileSystem,"Consider a bucket with the following object names:  * / * /foo * foo//bar  NativeS3FileSystem treats an object named ""/"" as a directory.  Doing an ""fs -lsr"" causes an infinite loop.  I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names.  Thoughts?",fs/s3
Add new/missing dfs commands in forrest,"The following DFS commands are missing Forrest documentation, as per HADOOP-4427. Splitting that as there are different teams working on the issues.  commands_manual: - dfsadmin [-setSpaceQuota <quota> <dirname>...<dirname>] - [-clrSpaceQuota <dirname>...<dirname>]  hdfs_shell: - fs [-count[-q] <path>] - [-moveToLocal [-crc] <src> <localdst>]",documentation
TestJobInProgressListener should also test for jobs killed in queued state,{{TestJobInProgressListener}} checks  if the listeners are informed as expected. This test case is missing one test where the job is killing in the queued state. ,test
Hod hung up when job is killed externally during allocation,Hod hung up during allocation when job is killed externally. Job is killed after ringmaster comes up.,contrib/hod
"""hadoop  jar"" runs wrong jar file in case of name clash","Imagine I run a command as follows:      hadoop jar test.jar  Let's imagine that my main module in test.jar is named Sort.  Let's also imagine that some jar file in my CLASSPATH contains a module also named Sort, which also defines main.  Hadoop will run THAT jar file, not the one I specified.",util
Support comments in 'slaves'  file,Support comments in 'slaves'  file. Comments  start with '#' (start of line or inline ) ,"conf,scripts"
Add a unit test for applications creating symlinks in wokring  directory,There should be a unit test for applications creating symlinks. The test should give the configuration for creating symlinks and verify the symlinks are created during the task's executation. ,test
Separate testClientTriggeredLeaseRecovery() out from TestFileCreation,TestFileCreation.testClientTriggeredLeaseRecovery() failed in some recent Hudson build.  We should separate testClientTriggeredLeaseRecovery() out from TestFileCreation for easily debugging.,test
SequenceFileOutputFormat is coupled to WritableComparable and Writable,For some reason SequenceFileOutputFormat calls asSubclass() for OutputKeyClass and OutputValueClass. This throws a ClassCastException when using non-Writable types with the new Serialization framework.,io
ant jar file not being included in tar distribution,The ant-<version>.jar is not being included in the distribution.,build
The (deprecated) method NetUtils.getServerAddress() doesn't validate configuration values enough,"Although HADOPP-2827 wants to kill this method, it is handy, but it doesn't have enough address validation, and can return a null value, which triggers NPEs a bit later on in the code",conf
ant compile-native shorthand,This issue introduces a shorthand for  {code} ant compile-core-native -Dcompile.native=true  {code}  ,build
Security features for Hadoop,This is a top-level tracking JIRA for security work we are doing in Hadoop. Please add reference to this when opening new security related JIRAs. ,security
Map and Reduce tasks should run as the user who submitted the job,"Currently the TaskTracker spawns the map/reduce tasks, resulting in them running as the user who started the TaskTracker.    For security and accounting purposes the tasks should be run as the job-owner.",security
"Job-specific data on HDFS (job.xml, input splits etc.) should have right access-control",None,security
ant jar when run for first time does not inclue version information,Ant jar when run for first time does not include version information.,build
FSDataOutputStream.getPos() == 0when appending to existing file and should be file length,does not seem getPos is set correctly when opening an existing file with non-zero length in append mode. ,fs
javax.management.MalformedObjectNameException: Invalid character ':' in value part of property,javax.management.MalformedObjectNameException: Invalid character ':' in value part of property  This can be reproduced by running any test using MiniDFSCluster.,metrics
config ipc.server.tcpnodelay is no loger being respected,"I was troubleshooting some slow IPC from hbase, and though it may be a Naggles algorithm issue. So I turned on tcpNoDelay on the client and server and this had no affect. Turns out that the ""ipc.server.tcpnodelay"" setting was no longer being read.",ipc
Fault in TestDistributedUpgrade,"A TestDistributedUpgrade subtest checks that the Name Node _does not_ start when a distributed upgrade is required. In 0.18, the subtest fails when the Name Node _does_ start. The fault is with the test, not HDFS. Not a problem in 0.19.",test
Deadlock in RPC Server,RPC server could get into a deadlock especially when clients or server are network starved. This is a deadlock between RPC responder thread trying to check if there are any connection to be purged and RPC handler trying to queue a response to be written by the responder.  This was first observed [this thread|http://www.nabble.com/TaskTrackers-disengaging-from-JobTracker-to20234317.html]. ,ipc
NNThroughputBenchmark throws NPE if fs.default.name is not set,"Don't specify fs.default.name in hadoop-site.xml, then run NNThroughputBenchmark: {noformat} $ ./bin/hadoop org.apache.hadoop.hdfs.NNThroughputBenchmark -op create -threads 10 -files 10000 -filesPerDir 10 -close 08/11/01 00:46:57 ERROR hdfs.NNThroughputBenchmark: java.lang.NullPointerException         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:134)         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:130)         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:150)         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:208)         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:194)         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:859)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.<init>(NNThroughputBenchmark.java:107)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:1122)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.main(NNThroughputBenchmark.java:1174)  Exception in thread ""main"" java.lang.NullPointerException         at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:134)         at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:130)         at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:150)         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:208)         at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:194)         at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:859)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.<init>(NNThroughputBenchmark.java:107)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.runBenchmark(NNThroughputBenchmark.java:1122)         at org.apache.hadoop.hdfs.NNThroughputBenchmark.main(NNThroughputBenchmark.java:1174) {noformat}",benchmarks
An independent HTTPS proxy for HDFS,"Currently, an HDFS cluster itself exposes an HTTP/HTTPS interface for reading files (i.e., HFTP/HSFTP). However, there is a need for an HTTPS proxy server running independently from the cluster and serving data to users who otherwise can't access the cluster directly. This proxy will authenticate its users via SSL certificates and implements the HSFTP interface for reading files.  ",security
create-hadoop-image doesn't fail with expired Java binary URL,"Part of creating a Hadoop EC2 image involves putting the URL for the Java binary into hadoop-ec2-env.sh.  Ths URL is time-sensitive; a working URL will eventually redirect to a HTML warning page.  create-hadoop-image-remote does not notice this, and will create, bundle, and register a non-working image, which launch-cluster will launch, but on which the hadoop commands will not work.  To fix, check the output status of the ""sh java.bin"" command in create-hadoop-image-remote, die with that status, and check for that status when create-hadoop-image-remote is run. ",contrib/cloud
unused and misleading configuration in hadoop-init,"src/contrib/ec2/bin/image/hadoop-init is appended to rc.local on all ec2 cluster boxes.  This shell script generates the hadoop-site.xml configuration file.  It starts with some default settings, which are used to populate the file.  These defaults are then overwritten by the user data (from hadoop-ec2-env.sh) passed to the EC2 instance by launch-hadoop-master and launch-hadoop-slaves.  This isn't a bug; setting variables in hadoop-ec2-env.sh does the right thing.  However, it's dead and misleading code (well, it misled me) and running a test Hadoop job to figure out what's going on takes a little effort.  Suggested change to hadoop-init:  Remove these lines:  {noformat} # set defaults MAX_TASKS=3 [ ""$INSTANCE_TYPE"" == ""m1.large"" ] && MAX_TASKS=6 [ ""$INSTANCE_TYPE"" == ""m1.xlarge"" ] && MAX_TASKS=12  MAX_MAP_TASKS=$MAX_TASKS MAX_REDUCE_TASKS=$MAX_TASKS {noformat}  Add a comment before the lines which access the user data:  {noformat} # get user data passed in by the ec2 instance launch wget -q -O - http://169.254.169.254/latest/user-data | tr ',' '\n' > /tmp/user-data source /tmp/user-data {noformat} ",contrib/cloud
typo in javadoc for map.input.file,"One user pinged me that inside the Mapper javadoc  {noformat}  83  *       public void configure(JobConf job) {  84  *         mapTaskId = job.get(""mapred.task.id"");  85  *         inputFile = job.get(""mapred.input.file"");  86  *       } {noformat}  inputFile should be *map*.input.file and not *mapred*.input.file.",documentation
Fix the PiEstimator output messages and code comments,There are bugs in the PiEstimator output messages and the code comments are insufficient for beginners learning map/reduce programs.,documentation
Installation on Solaris needs additional PATH setting,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this.  Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",fs
should run old version of unit tests to check back-compatibility,We should test back-compatibility by running unit tests from a prior release.,test
There is a cygpath error if log directory does not exist,{noformat} //d:\@sze\hadoop\latest\logs does not exist.  bash-3.2$ ./bin/hadoop fs -lsr / cygpath: cannot create short name of d:\@sze\hadoop\latest\logs ... {noformat},scripts
Documentation for Tool interface is a bit busted,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information.    1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes  2) int run() needs to actually return an int  3) JobConf.setInputPath() and JobConf.setOutputPath() are deprecated.  4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument    More generally, a working implementation of Tool in the docs would be handy.","documentation,util"
hadoop-default.xml should not be in conf/ directory,"It is error prone to keep hadoop-default.xml in the conf/ directory.  Folks copy configuration directories between releases, but this file is strongly tied to a particualr release and should not be either editted or used with other than the release it comes with.",conf
javadoc: warning - Multiple sources of package comments found for some packages,"ant javadoc {noformat}   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging""   [javadoc] javadoc: warning - Multiple sources of package comments found for package ""org.apache.commons.logging.impl"" {noformat} ",documentation
API link in forrest doc should point to the same version of hadoop.,"For example, in http://hadoop.apache.org/core/docs/r0.17.2/hdfs_user_guide.html, the setSafeMode() link is pointing to http ://hadoop.apache.org/core/docs/current/..., which is not an 0.17 doc.",documentation
Split the default configurations into 3 parts,"We need to split hadoop-default.xml into core-default.xml, hdfs-default.xml and mapreduce-default.xml. That will enable us to split the project into 3 parts that have the defaults distributed with each component.",conf
220 javac compiler warnings,"""ant test-patch"" on trunk for a zero size patch file.  Then, it said, {noformat}     [exec]     -1 javac.  The applied patch generated 220 javac compiler warnings (more than the trunk's current 1011 warnings). {noformat} ",build
Unhandled failures starting jobs with S3 as backing store,"I run Hadoop 0.18.1 on Amazon EC2, with S3 as the backing store.  When starting jobs, I sometimes get the following failure, which causes the job to be abandoned:  org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException   org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveBlock(Jets3tFileSystemStore.java:222)   sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)   org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)   $Proxy4.retrieveBlock(Unknown Source)   org.apache.hadoop.fs.s3.S3InputStream.blockSeekTo(S3InputStream.java:160)   org.apache.hadoop.fs.s3.S3InputStream.read(S3InputStream.java:119)   java.io.DataInputStream.read(DataInputStream.java:83)   org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:47)   org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)   org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:214)   org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:150)   org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1212)   org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1193)   org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:177)   org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1783)   sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)   org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)   org.apache.hadoop.ipc.Client.call(Client.java:715)   org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)   org.apache.hadoop.mapred.$Proxy5.submitJob(Unknown Source)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:788)   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)  The stack trace suggests that copying the job file fails, because the HDFS S3 filesystem can't find all of the expected block objects when it needs them.  Since S3 is an ""eventually consistent"" kind of a filesystem, and does not always provide an up-to-date view of the stored data, this execution path probably should be strengthened - at least to retry these failed operations, or wait for the expected block file if it hasn't shown up yet.   ",fs/s3
Add ability to split text files compressed with lzo,Right now any file compressed with lzop will be processed by one mapper. This is a shame since the lzo algorithm would be very suitable for large log files and similar common hadoop data sets. The compression rate is not the best out there but the decompression speed is amazing.  Since lzo writes compressed data in blocks it would be possible to make an input format that can split the files. ,io
Remove ChecksumDistriubtedFileSystem and InMemoryFileSystem,We should remove the obsolete Checksum FileSystems.,fs
"local.cache.size is set to 10 GB, while DEFAULT_CACHE_SIZE is set to 1 MB","In filecache.DistributedCache.java, the constant DEFAULT_CACHE_SIZE is set to 1 MB, while the value of local.cache.size in conf/hadoop-default.xml is 10 GB. I'm assuming these two values should be consistent, and that 10 GB is the appropriate value. ",filecache
FileSystem.CACHE should be ref-counted,"FileSystem.CACHE is not ref-counted, and could lead to resource leakage.",fs
Add a user to groups mapping service ,Currently the IPC client sends the UGI which contains the user/group information for the Server. However this represents the groups for the user on the client-end. The more pertinent mapping from user to groups is actually the one seen by the Server. Hence the client should only send the user and we should add a 'group mapping service' so that the Server can query it for the mapping.,security
Root cause of connection failure is being lost to code that uses it for delaying startup,"ipc.Client the root cause of a connection failure is being lost as the exception is wrapped, hence the outside code, the one that looks for that root cause, isn't working as expected. The results is you can't bring up a task tracker before job tracker, and probably the same for a datanode before a  namenode. The change that triggered this is not yet located, I had thought it was HADOOP-3844 but I no longer believe this is the case.",ipc
setQuietMode(true) causes configuration to fail when hadoop-site.xml is added via addResource,"When enabling quiet mode in the configuration and loading hadoop-site.xml by adding it as a resource instead of having it in the default location...  Configuration conf = new Configuration(); conf.setQuietMode(false); conf.addResource(new Path(""/somewhere/on/my/computer/hadoop-site.xml"")); FileSystem fs = FileSystem.get(conf);  A runtime exception is thrown to show that the default hadoop-site.xml could not be found. In non-quiet more it just returns. (Configuration:902). The information should be logged, not generate an exception. The caller does not catch this exception, so added hadoop-site.xml never actually gets parsed and added.",conf
Improve JavaDoc on JobConf.setCombinerClass to better document restrictions on combiners,"It is often not clear to developers that Combiners may be called 0, 1, or many times and that combiners input and output types must be the same and the combiner must not have side effects. Since there isn't a Combiner class, the best place to put this documentation is on JobConf.setCombinerClass.",documentation
"hadoop fs -help should list detailed help info for the following commands: test, text, tail, stat & touchz","The ""hadoop fs -help"" command should include descriptive text for the commands commands: test, text, tail, stat & touchz in the default help summary.    Seems like a matter of just adding them to the default ""else"" at the end of the printHelp method in the FsShell.java file.",fs
Current Ganglia metrics implementation is incompatible with Ganglia 3.1,Ganglia changed its wire protocol in the 3.1.x series; the current implementation only works for 3.0.x.,metrics
FileSystem.getFileBlockLocations() (aka default implementation for Local FileSystem) incorrect.,"The default implementation of FileSystem.getFileBlockLocations(FileStatus file, long start, long len) seems to be wrong. ",fs
"NativeS3FileSystem always tries to create a bucket, even when used for read-only workflows.","I'm running two types of workflow. The first is a set of map jobs that read from S3 using s3n://ID:SECRET@bucket/ as input and HDFS as output. The other is using distcp as a backup tool to copy the outputs of other jobs using distcp.   bq. hadoop distcp hdfs://host:50001/user/root/from/ s3n://ID:SECRET@backup-bucket/  Unfortunately, I'm getting too many failures of this kind:  {code} 08/11/19 16:08:27 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 PUT failed for '/' XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>OperationAborted</Code><Message>A conflicting conditional operation is currently in progress against this resource. Please try again.</Message><RequestId>324E696A4BCA8731</RequestId><HostId>{REMOVED}</HostId></Error>   org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.createBucket(Jets3tNativeFileSystemStore.java:74)   org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.initialize(Jets3tNativeFileSystemStore.java:63)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)   org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)   org.apache.hadoop.fs.s3native.$Proxy2.initialize(Unknown Source)   org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:215)   org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1339)   org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:56)   org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1351)   org.apache.hadoop.fs.FileSystem.get(FileSystem.java:213)   org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)   org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:158)   org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:210)   org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)   org.apache.hadoop.streaming.StreamJob.submitAndMonitorJob(StreamJob.java:925)   org.apache.hadoop.streaming.StreamJob.go(StreamJob.java:115)   org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:33)   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)   java.lang.reflect.Method.invoke(Method.java:597)   org.apache.hadoop.util.RunJar.main(RunJar.java:155)   org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)   org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)   org.apache.hadoop.mapred.JobShell.main(JobShell.java:68) Caused by: org.jets3t.service.S3ServiceException: S3 PUT failed for '/' XML Error Message: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>OperationAborted</Code><Message>A conflicting conditional operation is currently in progress against this resource. Please try again.</Message><RequestId>{REMOVED}</RequestId><HostId>{REMOVED}</HostId></Error>   org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:424)   org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestPut(RestS3Service.java:734)   org.jets3t.service.impl.rest.httpclient.RestS3Service.createObjectImpl(RestS3Service.java:1357)   org.jets3t.service.impl.rest.httpclient.RestS3Service.createBucketImpl(RestS3Service.java:1234)   org.jets3t.service.S3Service.createBucket(S3Service.java:1390)   org.jets3t.service.S3Service.createBucket(S3Service.java:1158)   org.jets3t.service.S3Service.createBucket(S3Service.java:1177)   org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.createBucket(Jets3tNativeFileSystemStore.java:69)  ... 29 more {code}  The issue is that Jets3tNativeFileSystemStore always tries to create a bucket during initialization. I'm sure that's a good thing for when s3n is used to write output, but for read operations, it'd be best if it checked for existence first. IMHO.",fs/s3
"In the javadoc of IndexedSortable.compare(...), the link is wrong.",Class java.util.Comparable does not exist.,"documentation,util"
TestGlobalFilter.testServletFilter fails,"{noformat}  junit.framework.AssertionFailedError: expected:<14> but was:<15>    org.apache.hadoop.http.TestGlobalFilter.testServletFilter(TestGlobalFilter.java:150)  {noformat}    For more details, see http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/666/ .",test
KFS::getBlockLocations() fails with files having multiple blocks,getBlockLocations() on KFS fail with the following stack trace for large files (with multiple blocks). {noformat}  java.lang.IllegalArgumentException: Offset 67108864 is outside of file  (0..67108863)         at  org.apache.hadoop.mapred.FileInputFormat.getBlockIndex(FileInputFormat.java:336)         at  org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:248)         at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)         at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)         at org.apache.hadoop.examples.WordCount.run(WordCount.java:149)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)         at org.apache.hadoop.examples.WordCount.main(WordCount.java:155)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at  org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)         at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:141)         at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:54)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {noformat}  blkStart was not updated properly.,fs
TestMapRed fails with 64bit JDK,2 test cases in TestMapRed fail when run with 64bit JDK.  They both run out of memory.  This is due to the default value of io.sort.mb being used (100mb).,test
"javadoc: ""the the"" => ""the""","Try searching ""the the"" in .java files.",documentation
[HOD] Grant read permissions for files/directories created by hod as part of provisioning hadoop clusters,"When HOD creates the log, work and temp directories for Hadoop, it creates them with permissions 0700. This is too restrictive for applications like Chukwa which need access to files in this directory for generating Hadoop metrics. This is a request to allow read permissions to other applications.",contrib/hod
Add support for dfsadmin commands for test TestCLI unit test,Curretly TestCLI assumes dfs command when describing tests in testConf.xml we need to add ability to run dfsadmin commands too.,test
librecordio does not scale to large records,librecordio does not deserialize records containing strings larger than 64k bytes. ,record
Fix quickstart.html to reflect that Hadoop works with Java 1.6.x now,We should fix http://hadoop.apache.org/core/docs/current/quickstart.html to reflect that Hadoop requires 1.6.x now.,documentation
The ls shell command documentation is out-dated,"Current ls output is  {noformat}  bash-3.2$ ./bin/hadoop fs -ls    Found 1 items  -rw-r--r--   3 tsz supergroup       1366 2008-11-24 16:58 /user/tsz/r.txt  {noformat}  but the doc says ""dirname <dir> modification_time modification_time permissions userid groupid"".  See http://hadoop.apache.org/core/docs/r0.18.2/hdfs_shell.html#ls",documentation
docs/api does not contain the hdfs directory after building,"When ant package is run, the build/docs/api folder is not reflecting the ""hdfs"" hierarchy of packages and classes.  org.apache.hadoop.hdfs is not getting created in the api folder. Instead, the build creates org.apache.hadoop.fs.   The same can be observed here as well : http://hadoop.apache.org/core/docs/r0.19.0/api/index.html  ",build
OOM in .TestSetupAndCleanupFailure,"The root cause may be my lifecycle changes, but I'm seeing an OOM in TestSetupAndCleanupFailure ",test
adding tests for quotas command line error  messages,adding tests for quotas command line error messages. Will add it to the TestCLI xml configuration,test
"documentation typos: ""the the""","Try searching ""the the"" in the documentation files.  See http://issues.apache.org/jira/browse/HADOOP-4704?focusedCommentId=12649589#action_12649589 for a list.",documentation
"Add a link to Hive under ""Related Projects"" on http://hadoop.apache.org/core/",We should link to Hive from http://hadoop.apache.org/core/ and make a Hive home page with forrest (see https://issues.apache.org/jira/browse/HIVE-81),documentation
saveVersion.sh could write a package-info.java that cannot be compiled when used with git,"When git is being used, the saveVersion.sh script generates a revision based on git log as follows: {{revision=`git log -1 --pretty=oneline`}} This revision is used to generate the HadoopVersionAnnotation in the package-info file. If the revision string contains quotes this will result in a compile time error.",build
EC2 scripts should configure Hadoop to use all available disks on large instances,"The Hadoop configuration on EC2 currently always uses a single disk, even when more are available (http://docs.amazonwebservices.com/AWSEC2/2007-08-29/DeveloperGuide/instance-storage.html). Performance is significantly boosted by using all the available disks, so we should configure Hadoop to use them automatically.",contrib/cloud
Reuse FileStatus in FsShell where possible,FsShell should reuse FileStatus objects instead of converting to a Path and making extra calls to the backend FS (which can be slow and expensive). ,fs
Chinese Translation of Hadoop-Related Documents,"Translate hadoop related documents, including javadoc, tutorial, programer guide, technical analyses etx, into chinese language.",documentation
gridmix2 code can be condensed,"The gridmix2 benchmark code, particularly GridMixRunner, contains a lot of duplication that can be condensed into a more manageable state.",benchmarks
Add a splitter for metrics contexts,It is currently not possible to configure multiple metrics contexts. It would be helpful if one could support more than one type of collector.,metrics
HDFS streams should not throw exceptions when closed twice,"When adding an {{InputStream}} via {{addResource(InputStream)}} to a {{Configuration}} instance, if the stream is a HDFS stream the {{loadResource(..)}} method fails with {{IOException}} indicating that the stream has already been closed. ","fs,fs/s3"
Fix Eclipse classpath following introduction of gridmix 2,Need to add src/benchmarks/gridmix2/src/java to classpath.,build
gridmix2 run script doesn't work on trunk,The order of arguments in the rungridmix_2 script is invalid after HADOOP-3986. ,benchmarks
"Windows installation fails with ""bin/hadoop: line 243: c:\Program: command not found""","Perhaps a space in the path name is confusing Cygwin.   The JAVA_HOME path is the default  ""C:\Program Files\Java\jdk1.6.0_11"".  Changing   JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName | sed -e ""s/ /_/g""` to   JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ""${JAVA}"" org.apache.hadoop.util.PlatformName | sed -e ""s/ /_/g""` appear to correct the problem.",scripts
Remove deprecated FileSystem methods,"Deprecated FileSystem methods like getReplication(Path src), delete(Path f), etc. should be removed.",fs
[HOD] HOD opens up directory permissions more than required,"In HADOOP-4705, we fixed hod to grant read access to all directories and files that HOD creates, and also set the umask such that files created by Hadoop also get read permissions to world. In shared clusters, this is opening up too much to the users. This issue is for resetting the permissions to be as constrained as possible, while still addressing the requirement raised in HADOOP-4705.",contrib/hod
TestTrackerBlacklistAcrossJobs fails randomly,"While fixing HADOOP-4786, I found that TestTrackerBlacklistAcrossJobs is failing randomly.",test
Chukwa build process generates files not tracked by svn,"After building chukwa, there are a couple artifacts: {noformat} ?      src/contrib/chukwa/conf/mdl.xml ?      src/contrib/chukwa/conf/chukwa-agents {noformat} These should be ignored, as in HADOOP-4571",build
separate branch for HadoopVersionAnnotation,"I think we should pull out the ""branch"" in the HadoopVersionAnnotation and display it on the Web/UI. ",build
Test target for chukwa build.xml needs to comply to hadoop build.xml test suites,"Chukwa build.xml file is not honoring the hadoop test target.  When calling ant -Dtest.include=mapred/Test* test, this command also executes chukwa test cases.  The build.xml needs to be polished to avoid testing the wrong test cases.",build
RPC Server can leave a lot of direct buffers ,RPC server unwittingly can soft-leak direct buffers. One observed case is that one of the namenodes at Yahoo took 40GB of virtual memory though it was configured for 24GB memory. Most of the memory outside Java heap expected to be direct buffers. This shown to be because of how RPC server reads and writes serialized data. The cause and proposed fix are in following comment.    ,ipc
RPC Server send buffer retains size of largest response ever sent ,"The stack-based ByteArrayOutputStream in Server.Hander is reset each time through the run loop.  This will set the BAOS 'size' back to zero but the allocated backing buffer is unaltered.  If during an Handlers' lifecycle, any particular RPC response was fat -- Megabytes, even -- the buffer expands during the write to accommodate the particular response but then never shrinks subsequently.  If a hosting Server has had more than one 'fat payload' occurrence, the resultant occupied heap can provoke memory woes (See https://issues.apache.org/jira/browse/HBASE-900?focusedCommentId=12654009#action_12654009 for an extreme example; occasional payloads of 20-50MB with 30 handlers robbed the heap of 700MB).",ipc
Avoid a buffer copy while replying to RPC requests.,"RPC server first serializes RPC response to a ByteArrayOutputStream and then creates a new array to write to socket. For most responses the RPC handler is able to write the entire response in-line. If we could use the same buffer used by ByteArrayOutputStream, we can avoid this copy.     As mentioned in HADOOP-4802, yet another copy could be avoided (in most cases) if we use a static direct buffer for the responses (not proposed for this jira).",ipc
S3FileSystem.renameRecursive(..) does not work correctly if src contains Java regular expression special characters,"In S3FileSystem, the variable srcPath is not supposed to be a regular expression but is used as a regular expression in the line below. {code}         Path newDst = new Path(oldSrcPath.replaceFirst(srcPath, dstPath)); {code}",fs
0.19.1 will not build under Solaris 5.10 (x86),"I checked out branch-0.19 from svn and attempted a build.  ant -Dcompile.native=true -Dnonspace.os=SunOS -Dmake.cmd=gmake clean tar       [exec] gmake[2]: Entering directory `/opt/hadoop/hadoop-0.19.1/branch-0.19/build/native/SunOS-x86-32/src/org/apache/hadoop/io/compress/lzo'      [exec] if /bin/sh ../../../../../../../libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I/opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hadoop/io/compress/lzo -I../../../../../../..  -I/opt/SDK/jdk//include -I/opt/SDK/jdk//include/solaris -I/opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src  -g -Wall -fPIC -O2 -m32 -g -O2 -MT LzoCompressor.lo -MD -MP -MF "".deps/LzoCompressor.Tpo"" -c -o LzoCompressor.lo /opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hadoop/io/compress/lzo/LzoCompressor.c; \      [exec]     then mv -f "".deps/LzoCompressor.Tpo"" "".deps/LzoCompressor.Plo""; else rm -f "".deps/LzoCompressor.Tpo""; exit 1; fi      [exec] mkdir .libs      [exec]  gcc -DHAVE_CONFIG_H -I. -I/opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hadoop/io/compress/lzo -I../../../../../../.. -I/opt/SDK/jdk//include -I/opt/SDK/jdk//include/solarop/hadoop-0.19.1/branch-0.19/src/native/src -g -Wall -fPIC -O2 -m32 -g -O2 -MT LzoCompressor.lo -MD -MP -MF .deps/LzoCompressor.Tpo -c /opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hads/lzo/LzoCompressor.c  -fPIC -DPIC -o .libs/LzoCompressor.o      [exec] /opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hadoop/io/compress/lzo/LzoCompressor.c: In function `Java_org_apache_hadoop_io_compress_lzo_LzoCompressor_initIDs':      [exec] /opt/hadoop/hadoop-0.19.1/branch-0.19/src/native/src/org/apache/hadoop/io/compress/lzo/LzoCompressor.c:137: error: syntax error before ',' token      [exec] gmake[2]: *** [LzoCompressor.lo] Error 1      [exec] gmake[2]: Leaving directory `/opt/hadoop/hadoop-0.19.1/branch-0.19/build/native/SunOS-x86-32/src/org/apache/hadoop/io/compress/lzo'      [exec] gmake[1]: *** [all-recursive] Error 1      [exec] gmake[1]: Leaving directory `/opt/hadoop/hadoop-0.19.1/branch-0.19/build/native/SunOS-x86-32'      [exec] gmake: *** [all] Error 2  Not sure what the cause or fix may be.  ",build
Usage description in the Quotas guide documentations are incorrect,Qutas guide shows the following usage: dfsadmin -setquota <N> <directory>...<directory> dfsadmin -clrquota <directory>...<director> dfsadmin -setspacequota <N> <directory>...<directory>  dfsadmin -clrspacequota <directory>...<director>  the correct commands are: -setQuota -clrQuota -setSpaceQuota -clrSpaceQuota,documentation
0.18 cannot be compiled in Java 5.,"Currently, 0.18 cannot be compiled in Java 5 since some codes are using Java 6 API.  Will create sub-tasks for individual components.",util
Should not use java.util.NavigableMap in 0.18,None,util
Update documentation for default configuration,Documentation needs to be updated as per the configuration changes in HADOOP-4631,documentation
Allow FileSystem shutdown hook to be disabled,"FileSystem sets a JVM shutdown hook so that it can clean up the FileSystem cache. This is great behavior when you are writing a client application, but when you're writing a server application, like the Collector or an HBase RegionServer, you need to control the shutdown of the application and HDFS much more closely. If you set your own shutdown hook, there's no guarantee that your hook will run before the HDFS one, preventing you from taking some shutdown actions.    The current workaround I've used is to snag the FileSystem shutdown hook via Java reflection, disable it, and then run it on my own schedule. I'd really appreciate not having to do take this hacky approach. It seems like the right way to go about this is to just to add a method to disable the hook directly on FileSystem. That way, server applications can elect to disable the automatic cleanup and just call FileSystem.closeAll themselves when the time is right.",fs
Minor typos in documentation and comments,Found a few minor typos in the documentation and source code comments.,documentation
Document deprecation of o.a.h.fs.permission.AccessControlException better,HADOOP-4348 deprecated org.apache.hadoop.fs.permission.AccessControlException - it would be good to document the deprecation better to point users to org.apache.hadoop.security.AccessControlException.,"documentation,fs"
HDFS arch doc outdated,"Just noticed that nobody's updated the Hadoop Distributed File System: Architecture and Design doc in a while. For instance, it still says: bq. HDFS does not yet implement user quotas or access permissions. ",documentation
Document service level authorization - HADOOP-4348,Document service level authorization (HADOOP-4348) via forrest.,"documentation,scripts"
Fix IPC Client to not use UGI,"Hadoop embraced JAAS via HADOOP-4348.  We need to fix IPC Client to use standard features of JAAS such as LoginContext, Subject etc. rather than UserGroupInformation in the IPC header, Client.Connection etc.",ipc
Change filesystem permissions to use JAAS rather than UGI,"Hadoop embraced JAAS via HADOOP-4348.  We need to fix HDFS to use JAAS concepts such as Subject, Principal, Permission etc. rather than UserGroupInformation for file-system permissions.",fs
Improvement to IPC,"I'd like to propose an improvement for consideration given my experience of working on HADOOP-4348:  Currently the first call doubles up as a 'connection setup' trigger. I'd like to propose adding a new 'init' call which is always called by the clients for connection setup. The advantages are: * We could fold in the getProtocolVersion call into the setup call, this ensures that the Server always checks for protocol versions, regardless of whether the (malicious?) client does an explicit call for getProtocolVersion or not. * We could authorize the connection here. * We could add to check to ensure that the Server instance actually implements the protocol used by the client to communicate, rather than fail on the first IPC call  The flip side being an extra round-trip.  Lets discuss.",ipc
Fix help message in MRAdmin,MRAdmin's help message wrongly uses 'refresh-auth-policy' instead of '-refreshServiceAcl' for the command specific help message.,"fs,security"
TestUlimit is failing after Hadoop-4620,"TestUlimit launches 3 map tasks and generates output for only 1 as input to other 2 map tasks are empty. With Hadoop-4620, even the map tasks with empty input are generating the output. The fix could be just setting the no of mapper to 1",test
to add appropriate reference to the dependent library files in the chukwa/build.xml file ,While going through the chukwa/build.xml 's package-hadoop target found that we are trying to copy jsp-api.jar from the hadoop/lib dir but actually the jsp-api. jar resides in chukwa/lib directory.   For more details see comment   https://issues.apache.org/jira/browse/HADOOP-4709?focusedCommentId=12654489#action_12654489  -Giri,build
Change XML format to 1.1 to add support for serializing additional characters,"Feature added by this Jira has a problem while setting up some of the invalid xml characters e.g. ctrl-A e.g. mapred.textoutputformat.separator = ""\u0001""    e,g,  String delim = ""\u0001"";  Conf.set(""mapred.textoutputformat.separator"", delim);    Job client serializes the jobconf with mapred.textoutputformat.separator set to ""\u0001"" (ctrl-A) and problem happens when it is de-serialized (read back) by job tracker, where it encounters invalid xml character.    The test for this feature public : testFormatWithCustomSeparator() does not serialize the jobconf after adding the separator as ctrl-A and hence does not detect the specific problem.    Here is an exception:    08/12/06 01:40:50 INFO mapred.FileInputFormat: Total input paths to process : 1  org.apache.hadoop.ipc.RemoteException: java.io.IOException:  java.lang.RuntimeException: org.xml.sax.SAXParseException: Character reference ""&#1"" is an invalid XML  character.  at  org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:961)  at  org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:864)  at  org.apache.hadoop.conf.Configuration.getProps(Configuration.java:832)  at org.apache.hadoop.conf.Configuration.get(Configuration.java:291)  at  org.apache.hadoop.mapred.JobConf.getJobPriority(JobConf.java:1163)  at  org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:179)  at  org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1783)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)    at org.apache.hadoop.ipc.Client.call(Client.java:715)  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)  at org.apache.hadoop.mapred.$Proxy1.submitJob(Unknown Source)  at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:788)  at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)  at",conf
#NAME?,"When submitting a hadoop job from Windows (Cygwin) to a Linux hadoop cluster (or vice versa), and when you specify multiple additional jar files via the -libjars flag, hadoop throws a ClassNotFoundException for any classes located in the additional jars specified via the -libjars flag.  This is caused by the fact that hadoop uses system.getProperty(""path.separator"") as the delimiter in the list of jar files passed via -libjars.  My suggested solution is to use a comma as the delimiter, rather than the path.separator.  I realize comma is, perhaps, a poor choice for a delimiter because it is valid in filenames on both Windows and Linux, but the -libjars flag uses it as the delimiter when listing the additional required jars.  So, I figured if it's already being used as a delimiter, then it's reasonable to use it internally as well. ",filecache
Split the hadoop script into 3 parts,"We need to split the bin/hadoop into 3 parts for core, mapred and hdfs. This will enable us to distribute the individual scripts with each component.",scripts
Ant test-patch goal (and test/bin/test-patch.sh script) fails without warnings if ANT_HOME environment variable is not set,"A call to ""ant test-patch"" fails if ANT_HOME is not set as an environment variable.  No errors if variable set with export or in /etc/environment",test
Remove bindings to lzo,It looks like the lzo bindings are infected by lzo's GPL and must be removed from Hadoop.,io
After introduction of ivy ant test-patch always returns -1 score,After dependency management is moved to ivy : ant test-patch always returns a -1 score. The reason reported by the target is modification of the Eclipse classpath. ,build
TestJobTrackerRestart fails on trunk,"HADOOP-1230 changed the definition of TaskReport.equals:  {noformat} @@ -172,7 +172,7 @@        return false;      if(o.getClass().equals(TaskReport.class)) {        TaskReport report = (TaskReport) o; -      return counters.contentEquals(report.getCounters()) +      return counters.equals(report.getCounters()) {noformat}  This results in: {noformat} Testcase: testJobTrackerRestart took 473.926 sec   FAILED Task reports for same attempt has changed junit.framework.AssertionFailedError: Task reports for same attempt has changed   at org.apache.hadoop.mapred.TestJobTrackerRestart.testTaskReports(TestJobTrackerRestart.java:514)   at org.apache.hadoop.mapred.TestJobTrackerRestart.testTaskEventsAndReportsWithRecovery(TestJobTrackerRestart.java:447)   at org.apache.hadoop.mapred.TestJobTrackerRestart.testJobTrackerRestart(TestJobTrackerRestart.java:599) {noformat}  ",test
Improvements to TestJobTrackerRestart,"TestJobTrackerRestart could use the following improvements: # Remove the one minute 'wait' - this is really bad for test-cases # It assumes that a particular ordering of job-scheduling based on current behaviour of JobQueueTaskScheduler assigning a single task per heartbeat (pre HADOOP-3136) and thus a particular job completion order, which is incorrect and won't work with all schedulers.",test
Example in WritableComparable javadoc doesn't compile,See http://www.nabble.com/API-Documentation-question---WritableComparable-tt20967409.html.,io
findbugs target and docs target which uses forrest is yet to be ported using IVY ,"findbugs ant target is yet to be ported to use IVY for dependency management.  The reason being that , ivy can be used for resolving the findbugs.jar file but  the test-patch ant target uses the findbbugs bin directory.    docs ant target  docs uses forrest and java5,  forrest artifacts are unavailable on the centralized repo to be used through ivy.  Thanks, Giri ",build
some dependencies are yet to be resolved using IVY ,"Though we are using ivy for resolving dependencies, not all the dependencies are resolved through IVy.  The reason is the unavailability of the appropriate version of the artifacts in the repository and the ambiguity of the version of the dependencies  At the moment beloe is the list of dependencies which are still resolved from the local lib directories.    under the lib folder commons-cli-2.0-SNAPSHOT.jar   - yet to be available in the centralized repo kfs-0.2.2.jar                                         - not available in the maven repo. hsqldb-1.8.0.10.jar                           - latest available version is   under the lib/jsp-2.1 folder  jsp-2.1.jar                                          - version # unknown   jsp-api-2.1.jar                                  - version # unknown  under src/test/lib/  folder   ftplet-api-1.0.0-SNAPSHOT.jar                               -  unavailable in the maven repo ftpserver-server-1.0.0-SNAPSHOT.jar                   -  unavailable in the maven repo ftpserver-core-1.0.0-SNAPSHOT.jar                      -  unavailable in the maven repo mina-core-2.0.0-M2-20080407.124109-12.jar    -  unavailable in the maven repo  under src/contrib/chukwa/lib  json.jar                                               -         version # unknown  under src/contrib/thriftfs/lib                                       libthrift.jar                                           -  unavailable in the maven repo.   Thanks, Giri",build
Upgrade to JUnit 4,"Amongst other things, JUnit 4 has better support for class-wide set up and tear down (via @BeforeClass and @AfterClass annotations), and more flexible assertions (http://junit.sourceforge.net/doc/ReleaseNotes4.4.html). It would be nice to be able to take advantage of these features in tests we write.  JUnit 4 can run tests written for JUnit 3.8.1 without any changes.",test
Support for a per-class cluster in ClusterMapReduceTestCase,"ClusterMapReduceTestCase currently creates a new mini cluster for each test case. For many tests this is unnecessary overhead as the test cases could run against the same cluster, one after another, without risking correctness. As a part of this issue we should change tests that subclass ClusterMapReduceTestCase to use a per-test cluster where appropriate.",test
static initializers for default config files duplicate code,The default config files are loaded by static initializers.  The code in these initializers is two lines that contains string literals.  This is fragile and duplicated code.,conf
TestMapReduceLocal fails,{noformat} Testcase: testWithLocal took 33.276 sec  Caused an ERROR Output directory /home/tsz/hadoop/latest/build/test/data/out already exists org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory /home/tsz/hadoop/latest/build/test/data/out already exists   org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:124)   org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:770)   org.apache.hadoop.mapreduce.Job.submit(Job.java:437)   org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:450)   org.apache.hadoop.mapreduce.TestMapReduceLocal.runSecondarySort(TestMapReduceLocal.java:144)   org.apache.hadoop.mapreduce.TestMapReduceLocal.testWithLocal(TestMapReduceLocal.java:89) {noformat},test
fs -lsr does not align correctly when the username lengths are different,"For example,  {noformat}  bash-3.2$ ./bin/hadoop fs -lsr /user  drwx------   - nicholas supergroup          0 2008-12-18 15:17 /user/nicholas  -rw-r--r--   3 nn_sze supergroup       1366 2008-12-18 15:17 /user/nicholas/a.txt  drwx------   - tsz      supergroup          0 2008-11-25 15:55 /user/tsz  -rw-------   3 tsz supergroup       1366 2008-11-25 15:53 /user/tsz/r.txt  {noformat}",fs
Fix bzip2 work with SequenceFile,"Somehow bzip2 does not work with SequenceFile:  {code}     String codec = ""org.apache.hadoop.io.compress.BZip2Codec"";     SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, new Path(output),          reader.getKeyClass(), reader.getValueClass(), CompressionType.BLOCK,          (CompressionCodec)Class.forName(codec).newInstance()); {code}  The stack trace is here: {noformat} java.lang.UnsupportedOperationException         at org.apache.hadoop.io.compress.BZip2Codec.getCompressorType(BZip2Codec.java:80)         at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:98)         at org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:914)         at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1198)         at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:401)         at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:329)         at org.apache.hadoop.mapred.TestSequenceFileBZip.main(TestSequenceFileBZip.java:43)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.hadoop.util.RunJar.main(RunJar.java:165)         at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)         at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68) {noformat} ",io
[HOD] Provide execute access to JT history directory path for group,"HADOOP-4782 was opened to discuss the right level of access required for Chukwa to be able to read JT history logs under HOD provisioned directories. This was in turn required because HADOOP-4705, which provided world readable rights for all directories provisioned by HOD was found to be very unsecure for shared clusters. As per discussions on these two jiras, we decided to NOT change HOD's default behavior (of not granting access) for Hadoop 0.20, and providing very restricted access for Hadoop 0.18.3 (only execute permissions for group on the directory path until the history directory on the JT node).  HADOOP-4782 tracked the reversal of the changes in HADOOP-4705 for Hadoop 0.20. This issue is being opened to make the restricted change in Hadoop 0.18.3. The patch submitted on HADOOP-4782 for Hadoop 0.18.3 can just be uploaded here. I am filing a new jira only because the nature of the fixes is different in spirit for the two versions.",contrib/hod
do not keep forrest output in subversion,"We currently re-generate PDF and HTML documentation whenever we commit a  documentation patch, which creates huge commit messages that few read. This was  originally done so that folks who check out the sources from subversion did not  need to install forrest in order to read the documentation.  Note, this issue only concerns the versioned documentation included in trunk and releases, not the website, whose forrest output should continue to be kept in subversion.",documentation
compile-core-test build fails (bad class file error: ExternalMapReduce.class),"I am trying to get my development environment up and running.  Following instructions on Hadoop Wiki + any relevant JIRA Bug Fixes.  I downloaded the latest trunk (ID 728242) via subversion client in to my eclipse environment.  Based on instructions posted, I set my ANT_HOME environment variable, ran the Ant build task for eclipse-files target succesfully.  After refreshing the project, I attempted a ""build project"", which generated the error below.    Trunk Build fails within ""compile-core-test"" section:  ======================================== CONSOLE OUTPUT: ======================================== compile-core-test:        [javac] Compiling 7 source files to /home/hadoop/workspace/Hadoop/build/test/classes        [javac] Note: Some input files use unchecked or unsafe operations.        [javac] Note: Recompile with -Xlint:unchecked for details.        [javac] Compiling 124 source files to /home/hadoop/workspace/Hadoop/build/test/classes        [javac] /home/hadoop/workspace/Hadoop/src/test/org/apache/hadoop/mapred/TestCommandLineJobSubmission.java:76: cannot access testshell.ExternalMapReduce        [javac] bad class file: /home/hadoop/workspace/Hadoop/src/test/testshell/ExternalMapReduce.class        [javac] class file contains wrong class: src.test.testshell.ExternalMapReduce        [javac] Please remove or make sure it appears in the correct subdirectory of the classpath.        [javac]                                new testshell.ExternalMapReduce(), args);        [javac]                                             ^        [javac] Note: Some input files use or override a deprecated API.        [javac] Note: Recompile with -Xlint:deprecation for details.        [javac] 1 error  BUILD FAILED /home/hadoop/workspace/Hadoop/build.xml:635: Compile failed; see the compiler error output for details.  ======================================== Disclaimer: I am new to the Apache software foundation in general and the Hadoop project specifically.   If I have submitted this ticket improperly, please annotate this ticket so I know to correct bad practice. ========================================","build,test"
Improvements to TestSafeMode,TestSafeMode  - needs a detailed description of the test case - should not use direct calls to the name-node rather call {{DistributedFileSystem}} methods. ,test
[HOD] Include ringmaster RPC port information in the notes attribute,"In large cluster deployments, due to node failures, it sometimes happens that HOD clusters get allocated, but not deallocated even after the idleness limit of the cluster (the time for which no jobs are run) exceeds. One of the main reasons for this is the ringmaster process which is responsible for tracking and cleaning an idle cluster (of which it is a part) itself goes down. To handle such scenarios it makes sense to centrally track the ringmaster nodes for suspicious clusters. But since the information about which port the ringmaster is bound to is not centrally available, this becomes impossible to monitor.  This issue is an enhancement request to include ringmaster RPC port information along with the JT and NN info as part of the resource manager's notes attribute so that it can be used by any monitoring processes built around it.",contrib/hod
[HOD] Cleanup idle HOD clusters whose ringmaster nodes might have gone down,"As mentioned in HADOOP-4937, sometimes in large cluster deployments, faulty nodes on which the ringmaster process comes up may go down after the cluster is successfully allocated. Such clusters fail to deallocate automatically even if the idleness limit of the cluster is exceeded. This is because the idleness is tracked by the ringmaster process which itself has gone down.  As large number of nodes can get held up due to this, such clusters should be detected and deallocated in some manner.",contrib/hod
Remove delete(Path f),"Remove the following:  {code}    /** Delete a file. */    /** @deprecated Use delete(Path, boolean) instead */ @Deprecated     public abstract boolean delete(Path f) throws IOException;  {code}",fs
"Remove getBlockSize(Path f), getLength(Path f) and getReplication(Path src)",Remove the following  - public long getBlockSize(Path f) throws IOException  - public long getLength(Path f) throws IOException  - public short getReplication(Path src) throws IOException,fs
"Remove getName() and getNamed(String name, Configuration conf)","Remove these two methods:  - public String getName()  - public static FileSystem getNamed(String name, Configuration conf)  ",fs
Allow Xinclude in hadoop config file,It would be easier to mange the configuration of hadoop by allowing include files in configuration file (file: hadoop-site.xml) ,conf
ant test-patch does not work,"ant test-patch is reporting ""Trunk compilation is broken?"" for any patch. {noformat} ...      [exec] /home/tsz/apache-ant-1.7.1/bin/ant -Dversion=PATCH-a.patch -Djavac.args=-Xlint -Xmaxwarns 1000  -DHadoopPatchProcess= clean tar > /home/tsz/tmp/trunkJavacWarnings.txt 2>&1      [exec] Trunk compilation is broken? {noformat}",scripts
CompressorStream and BlockCompressorStream should be public,"To simplify writing codecs, CompressionStream and BlockCompressionStream provide helper base classes. They should be made public so they can be used outside of Hadoop's package structure.",io
Improved files system interface for the application writer.,"Currently the FIleSystem interface serves two purposes:  - an application writer's interface for using the Hadoop file system  - a file system implementer's interface (e.g. hdfs, local file system, kfs, etc)    This Jira proposes that we provide a simpler interfaces for the application writer and leave the FilsSystem  interface for the implementer of a filesystem.    - Filesystem interface  has a  confusing set of methods for the application writer  - We could make it easier to take advantage of the URI file naming  ** Current approach is to get FileSystem instance by supplying the URI and then access that name space. It is consistent for the FileSystem instance to not accept URIs for other schemes, but we can do better.  ** The special copyFromLocalFIle can be generalized as a  copyFile where the src or target can be generalized to any URI, including the local one.  ** The proposed scheme (below) simplifies this.    - The client side config can be simplified.   ** New config() by default uses the default config. Since this is the common usage pattern, one should not need to always pass the config as a parameter when accessing the file system.    -   ** It does not handle multiple file systems too well. Today a site.xml is derived from a single Hadoop cluster. This does not make sense for multiple Hadoop clusters which may have different defaults.  ** Further one should need very little to configure the client side:  *** Default files system.  *** Block size   *** Replication factor  *** Scheme to class mapping  ** It should be possible to take Blocksize and replication factors defaults from the target file system, rather then the client size config.  I am not suggesting we don't allow setting client side defaults, but most clients do not care and would find it simpler to take the defaults for their systems  from the target file system.   ",fs
config property mapred.child.java.opts has maximum length that generates NoClassDefFoundError if exceeded,"There is an unexpected max length for the value of config property mapred.child.java.opts that, if exceeded, generates an opaque NoClassDefFoundError in child tasks.    The max length for the value is 146 chars.  A length of 147 chars will cause the exception.  For example, adding a single extra space between options will convert a working jvm opts clause into one that always generates NoClassDefFoundError when tasktrackers exec child tasks.  As laboriously diagnosed, conf/hadoop-site.xml  was used to set the property and runs were done on ""Amazon EC2 Ubuntu 8.04 hardy AMI"" (Debian version ""lenny/sid"") using java 1.6.0_07-b06.  Multiple slaves nodes were used and after conf changes, stop-all.sh and start-all.sh were run before each test.  The job config props as found on the slave did not appear to have a truncated or damaged value.  It made no difference whether @taskid@ appeared at the end or middle of the options and absence of @taskid@ did not eliminate the problem.  This bug wastes considerable time because the error looks like a classpath problem and even after the java opts property is suspected, a character quoting or unsupported option seems more likely than a length limit.    ",conf
"Specify node-rack mapping, dfsadmin -report to warn on mismatch","It would be helpful if the operator had some way to specify what rack a node is in, perhaps as an annotation in the dfs.hosts file? Subsequently, if a node reports that it is in a different rack than specified, dfsadmin -report can issue a warning on that fact.    An additional warning would also be nice: If a rack is configured to have X hosts, but in fact has some significant percentage (configurable?) fewer hosts than configured, dfsadmin -report should advert to that fact as well.  ",conf
DFSClient should log instead of printing into std err.,{{DFSClient.LeaseChecker.close()}} uses {{System.err.println()}} and {{Exception.printStackTrace()}} to output the exception.{{LOG.error()}} should be used instead.,test
TestFsck does not run in Eclipse.,"{{TestFsck.testFsckMove()}} falls into infinite loop if run from Eclipse because it uses incorrect default settings for {{""test.build.data""}} configuration variable.",test
Reduce task getting map output over HTTP should have right access control,None,security
Implement a native OS runtime for Hadoop,"It would be useful to implement a JNI-based runtime for Hadoop to get access to the native OS runtime. This would allow us to stop relying on exec'ing bash to get access to information such as user-groups, process limits etc. and for features such as chown/chgrp (org.apache.hadoop.util.Shell).",native
Junit tests that time out don't write any test progress related logs,"Some junit tests time out frequently possibly because of a bug. When such tests time out, the log4j appender isn't writing anything to the log files. It seems that all the log statements  are buffered in the memory till test completion. The logs get written to the log file only after the test goes to completion.  This is seriously limiting debugging in presence of a test time out. *If* possible, we should try to flush logs regularly so that we can find out the extent to which a test has progressed before timing out.",test
Create a mock MapReduce cluster simulator to test schedulers,"Currently the Hadoop schedulers use a FakeTaskTrackerManager to run tests which is both messy and doesn't really simulate trackers going up and down, tasks finishing at different times, faiulres, etc. It would be nice to have a simulated MapReduce cluster where tasks really do take different amounts of (simulated) time, trackers may be slow, tasks can be made to fail, etc. The existing TaskTrackerManager interface given to the schedulers, plus perhaps a mockable clock (e.g. the FairScheduler.Clock class) should be enough to do all this. The end result will be easier-to-write and more complex scheduler tests.",test
ganglia is now showing any graphs,"Ganglia is not showing any graphs since the rdd tool require installed fonts, though the fedora core image used as basis to build the hadoop image does not come with fonts.  To fix this just install dejavu-fonts as another package. The line in create-hadoop-image-remote.sh should look like this: yum -y install rsync lynx screen ganglia-gmetad ganglia-gmond ganglia-web dejavu-fonts httpd php  ",contrib/cloud
TestReplication#testPendingReplicationRetry leaves an opened fd unclosed,The unit test opens a block file to overwrite but does not close it; So subsequent test would fail because the data directory is not able to be removed.,test
Document HTTP/HTTPS methods to read directory and file data,"In HADOOP-1563, [~cutting] wrote: bq. The URI for this should be something like hftp://host:port/a/b/c, since, while HTTP will be used as the transport, this will not be a FileSystem for arbitrary HTTP urls.  Recently, we've been talking about implementing an HDFS proxy (HADOOP-4575) which would be a secure way to make HFTP/HSFTP available. In so doing, we may even remove HFTP/HSFTP from being offered on the HDFS itself (that's another discussion).  In the case of the HDFS proxy, does it make sense to do away with the artificial HFTP/HSFTP protocols, and instead simply offer standard HTTP and HTTPS? That would allow non-HDFS-specific clients, as well as using various standard HTTP infrastructure, such as load balancers, etc.  NB, to the best of my knowledge, HFTP is only documented on the [distcp|http://hadoop.apache.org/core/docs/current/distcp.html] page, and HSFTP is not documented at all? ",documentation
Support concatenated gzip files,"When running MapReduce with concatenated gzip files as input only the first part is read, which is confusing, to say the least. Concatenated gzip is described in http://www.gnu.org/software/gzip/manual/gzip.html#Advanced-usage and in http://www.ietf.org/rfc/rfc1952.txt. (See original report at http://www.nabble.com/Problem-with-Hadoop-and-concatenated-gzip-files-to21383097.html)  ",io
"[HOD] logcondense should delete all hod logs for a user, including jobtracker logs","Currently, logcondense.py does not delete jobtracker logs that it uploads to the DFS when the HOD cluster is deallocated. This will result in the hod-logs directory to slowly accumulate a whole bunch of jobtracker logs. Particularly for users who run a lot of user jobs, this could fill up the namespace.  Further these directories will cause the logcondense program to keep repeatedly looking at these directories stressing out the namenode. So, logcondense.py should optionally also delete the jobtracker logs.",contrib/hod
FileSystem.isDirectory() should not be deprecated.,We should remove FileSystem.isDirectory().,fs
"Hod should be modified to generate core-site.xml, mapred-site.xml and hdfs-site.xml","Hod is currently generating hadoop-site.xml but as per the modifications done in hadoop (ref: JIRA-4631), hod should also be modified to be in synch with this change. It should also generate core-site.xml, mapred-site.xml and hdfs-site.xml instead of hadoop-site.xml from 0.20.0 onwards.",contrib/hod
"'whoami', 'topologyscript' calls failing with java.io.IOException: error=12, Cannot allocate memory","We've seen primary/secondary namenodes fail when calling whoami or topologyscripts. (Discussed as part of HADOOP-4998)  Sample stack traces.  Primary Namenode {noformat} 2009-01-12 03:57:27,381 WARN org.apache.hadoop.net.ScriptBasedMapping: java.io.IOException: Cannot run program ""/path/topologyProgram"" (in directory ""/path""): java.io.IOException: error=12, Cannot allocate memory         at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)         at org.apache.hadoop.util.Shell.runCommand(Shell.java:149)         at org.apache.hadoop.util.Shell.run(Shell.java:134)         at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)         at org.apache.hadoop.net.ScriptBasedMapping.runResolveCommand(ScriptBasedMapping.java:122)         at org.apache.hadoop.net.ScriptBasedMapping.resolve(ScriptBasedMapping.java:73)         at org.apache.hadoop.dfs.FSNamesystem$ResolutionMonitor.run(FSNamesystem.java:1869)         at java.lang.Thread.run(Thread.java:619) Caused by: java.io.IOException: java.io.IOException: error=12, Cannot allocate memory         at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)         at java.lang.ProcessImpl.start(ProcessImpl.java:65)         at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)         ... 7 more  2009-01-12 03:57:27,381 ERROR org.apache.hadoop.fs.FSNamesystem: The resolve call returned null! Using /default-rack for some hosts 2009-01-12 03:57:27,381 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/55.5.55.55:50010  {noformat}  Secondary Namenode {noformat}  2008-10-09 02:00:58,288 ERROR org.apache.hadoop.dfs.NameNode.Secondary: java.io.IOException: javax.security.auth.login.LoginException: Login failed: Cannot run program ""whoami"": java.io.IOException: error=12, Cannot allocate memory         at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:250)         at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:275)         at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:257)         at org.apache.hadoop.dfs.FSNamesystem.setConfigurationParameters(FSNamesystem.java:370)         at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:359)         at org.apache.hadoop.dfs.SecondaryNameNode.doMerge(SecondaryNameNode.java:340)         at org.apache.hadoop.dfs.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:312)         at org.apache.hadoop.dfs.SecondaryNameNode.run(SecondaryNameNode.java:223)         at java.lang.Thread.run(Thread.java:619)          at org.apache.hadoop.dfs.FSNamesystem.setConfigurationParameters(FSNamesystem.java:372)         at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:359)         at org.apache.hadoop.dfs.SecondaryNameNode.doMerge(SecondaryNameNode.java:340)         at org.apache.hadoop.dfs.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:312)         at org.apache.hadoop.dfs.SecondaryNameNode.run(SecondaryNameNode.java:223)         at java.lang.Thread.run(Thread.java:619)  {noformat} ",util
Update chinese documentation for default configuration,The chinese documentation needs to be updated as per HADOOP-4828,documentation
ant binary should not compile docs,"ant binary now compiles docs. The compilation of binary itself takes around 6 minutes. Since the tar ball does not include docs, they need not be compiled.    The size of binary is 17MB on my system. I could see duplicate library copies in the tar contents.    For example :  -rw-rw-r-- /             26202 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/commons-logging-api-1.0.4.jar  -rw-rw-r-- /           2532573 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-core.jar  -rw-rw-r-- /             69850 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/hadoop-0.21.0-dev-tools.jar  -rw-rw-r-- /            516429 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/jetty-6.1.14.jar  -rw-rw-r-- /            121070 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/junit-3.8.1.jar  -rw-rw-r-- /            391834 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/log4j-1.2.15.jar  -rw-rw-r-- /             15345 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/slf4j-api-1.4.3.jar  -rw-rw-r-- /             15010 2009-01-16 11:53:48 hadoop-0.21.0-dev/contrib/hdfsproxy/lib/xmlenc-0.52.jar    ----------------------------------------------    -rw-rw-r-- /           2532573 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-core.jar  -rw-rw-r-- /             69850 2009-01-16 11:53:51 hadoop-0.21.0-dev/hadoop-0.21.0-dev-tools.jar  -rw-rw-r-- /            516429 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/jetty-6.1.14.jar  -rw-rw-r-- /             26202 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/commons-logging-api-1.0.4.jar  -rw-rw-r-- /            121070 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/junit-3.8.1.jar  -rw-rw-r-- /            391834 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/log4j-1.2.15.jar  -rw-rw-r-- /             15345 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/slf4j-api-1.4.3.jar  -rw-rw-r-- /             15010 2009-01-16 11:53:34 hadoop-0.21.0-dev/lib/xmlenc-0.52.jar  ",build
add a Hadoop-centric junit test result listener,"People are encountering different problems with hadoop's unit tests, defects currently being WONTFIX'd  # HADOOP-5001 : Junit tests that time out don't write any test progress related logs # HADOOP-4721 : OOM in .TestSetupAndCleanupFailure  There is a root cause here, the XmlResultFormatter of Ant buffers everything before writing out a DOM. Too much logged: OOM and no output. Timeout: kill and no output.  We could add a new logger class to hadoop and then push it back into Ant once we were happy, or keep it separate if we had specific dependencies (like on hadoop-dfs API) that they lacked.   Some ideas # stream XML to disk. We would have to put the test summary at the end; could use XSL to generate HTML and the classic XML content # stream XHTML to disk. Makes it readable as you go along; makes the XSL work afterwards harder. # push out results as records to a DFS. There's a problem here in that this needs to be a different DFS from that you are testing, yet it needs to be compatible with the client.   Item #3 would be interesting but doing it inside JUnit is too dangerous classpath and config wise. Better to have Ant do the copy afterwards. What is needed then is a way to easily append different tests to the same DFS file in a way that tools can analyse them all afterwards. The copy is easy -add a new Ant resource for that- but the choice of format is trickier.  Here's some work I did on this a couple of years back; I've not done much since then: http://people.apache.org/~stevel/slides/distributed_testing_with_smartfrog_slides.pdf  Is anyone else interested in exploring this?  ",test
Update the year for the copyright to 2009,The year should be updated to 2009 before any new release comes out.,documentation
testSequenceFileGzipCodec won't pass without native gzip codec,"Somehow, SequenceFile requires native gzip codec. We should remove it from the test cases since that may not pass on all platforms. ",test
JavaDoc errors in 0.18.3,There are JavaDoc errors in 0.18.3. These are not present in 0.19 and above thus went undetected by Hudson and others. ,util
Broken AMI/AKI for ec2 on hadoop,"c1.xlarge and m1.large instances fail to boot.   ec2-get-console-output show them stuck at ""Creating /dev"" step.",contrib/cloud
 HashFunction inadvertently destroys some randomness,"HashFunction.hash restricts initval for the next hash to the [0, maxValue) range of the hash indexes returned. This is suboptimal, particularly for larger nbHash and smaller maxValue.  Rather we should first set initval, then restrict the range for the result assignment.",util
Update TestCLI with additional test cases.,"Currently TestCLI contains few of the dfs commands and verifies some of the error messages for quota and refreshServiceAcl.. Here is a proposal to add additional test cases to TestCLI to cover an exhaustive list of Hadoop commands. Here is a list of action items for the same:  1) Complete the test cases for dfs commands which are not yet automated such as count, chmod, chown, chgrp etc  2) Verify help messages in fs, dfsadmin, mradmin  3) Add other Hadoop commands such as archives, dfsadmin, balancer, job, queue, version, jar, distcp, daemonlog etc to the command line test.  ",test
"Split TestCLI into HDFS, Mapred and Core tests","At present, TestCLI contains command line tests for both hdfs and mapred. Going forward, this test has to be broken up into separate hdfs, mapred and core tests.",test
Copying a file to local with Crc throws an exception,$ hadoop dfs -get -crc /user/aa/test.txt test.txt get: org.apache.hadoop.dfs.DistributedFileSystem cannot be cast to org.apache.hadoop.fs.ChecksumFileSystem  The problem seems to be caused by the line 251 in FsShell#copyToLocal: {noformat} 250;      if (copyCrc) { 251:        ChecksumFileSystem csfs = (ChecksumFileSystem) srcFS;                ...              } {noformat}  Copying crc files to local should not require the source file system to be ChecksumFileSystem.  ,fs
Trash URI semantics can be relaxed,"When using fully qualified URIs with FsShell, the authority element of the URI must match the default filesystem exactly, or else one may get an error message when the trash is enabled: {noformat} $ hadoop fs -rmr hdfs://namenode1/user/foo/bar rmr: Wrong FS: hdfs://namenode1/user/foo/bar, expected: hdfs://namenode1.foobar.com Usage: java FsShell [-rmr <path>] $ hadoop fs -rmr hdfs://namenode1.foobar.com/user/foo/bar $ {noformat}  It should be possible to use the FileSystem for the Path provided rather than the default FileSystem. 0.17 was less particular about this.",fs
include releaseaudit as part of  test-patch.sh script ,"Existing test-patch.sh script doesn't seem to execute releaseaudit target as part of patch testing. We need to call releaseaudit target from test-patch.sh script  Thanks, Giri",build
Branch 0.18 doesn't display version info,"""hadoop version"" doesn't display the version of hadoop for branch 0.18 rather it shows version unknown. Same behaviour is observed in svn as well as in git.",build
Configuration default resource handling needs to be able to remove default resources ,"There's a way to add default resources, but not remove them. This allows someone to push an invalid resource into the default list, and for the rest of the JVM's life, any Conf file loaded with quietMode set will fail.",conf
optimizing build.xml target dependencies,"Need to optimize build.xml  For ex: findbugs target depends on package target and package target depends on doc, jar, cn-docs , etc... Though findbugs is run on three of the jar files for which we have three different targets, jar, tools-jar , examples  Likewise different targets could be optimized.   Thanks, Giri",build
"Split build script for building core, hdfs and mapred separately",None,build
to optimize hudsonBuildHadoopNightly.sh script,None,build
"split the core, hdfs, and mapred jars from each other and publish them independently to the Maven repository","I think to support splitting the projects, we should publish the jars for 0.20.0 as independent jars to the Maven repository ",build
TestKillCompletedJob is failing intermittetnly when run as part of test-core,"TestKillCompletedJob fails most times when run as part of test-core, but succeeds when run by itself. {noformat} Testcase: testKillCompJob took 8.048 sec  Caused an ERROR Job failed! java.io.IOException: Job failed!   org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1343)   org.apache.hadoop.mapred.TestKillCompletedJob.launchWordCount(TestKillCompletedJob.java:78)   org.apache.hadoop.mapred.TestKillCompletedJob.testKillCompJob(TestKillCompletedJob.java:112) {noformat}",test
Upgrade Clover to 2.4.2 and enable Test Optimization in HADOOP,The current [Hadoop build|http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/clover/] on Hudson is using Clover 1.3.13.   I will attach a patch to the build.xml and the clover 2.4.2 jar to this issue.  Test Optimization works by only running tests for which code has changed. This can be used both in a CI environment and on a developers machine to make running tests faster.  Clover will also run tests which previously failed. Modified source code is detected at compile time and then used to select the tests that get run. ,build
"logcondense should delete hod logs for a user , whose username has any of the characters in the value passed to ""-l"" options ","Logcondense script is not able to delete the completed job directories of hadoop logs in hod-logs inside HDFS  for the the users , whose username has any of the characters , in the value passed to ""-l"" options  or in '/user' as set default . This happened because logcondense script use python 'lstrip' method , which returns copy of the string after removing leading characters in the value passed to ""-l"" options or in ""/user"" instead of just stripping value from the given string .",contrib/hod
A bunch of mapred unit tests are failing on Windows,"A bunch of unit tests are consistently failing when run on Windows. Below are a list of unit tests which are failing and the corresponding exceptions thrown:  Exception: ""java.net.ConnectException: Connection refused: no further information"" Failing tests: * TestMiniMRMapRedDebugScript - testMapDebugScript * TestNoDefaultsJobConf - testNoDefaults * TestQueueManager - testAllEnabledACLForJobSubmission * TestCompressedEmptyMapOutputs - testMapReduceSortWithCompressedEmptyMapOutputs * TestJobInProgressListener - testJobQueueChanges * TestKillCompletedJob - testKillCompJob * TestMiniMRClasspath - testClassPath * TestMiniMRDFSCaching - testWithDFS * TestMiniMRWithDFSWithDistinctUsers - testDistinctUsers * TestSetupAndCleanupFailure - testWithDFS * TestDBJob - testRun * TestMiniMRWithDFS - testWithDFS * TestJobStatusPersistency - testNonPersistency * TestSpecialCharactersInOutputPath - testJobWithDFS * TestUserDefinedCounters - testMapReduceJob * TestDelegatingInputFormat - testSplitting * TestEmptyJobWithDFS - testEmptyJobWithDFS * TestJavaSerialization - testMapReduceJob * TestClusterMapReduceTestCase - testMapReduce   Exception: java.lang.IllegalArgumentException: Pathname /<path> from <path> is not a valid DFS filename. Failing tests: * TestJobInProgress - testRunningTaskCount * TestJobQueueInformation - testJobQueues * TestJobTrackerRestart - testJobTrackerRestart   Exception: java.io.IOException: Bad connect ack with firstBadLink 127.0.0.1:<port number> Failing tests: * TestJobSysDirWithDFS - testWithDFS * TestJobInProgress - testPendingMapTaskCount * TestMiniMRDFSSort - testMapReduceSort   Exception: junit.framework.AssertionFailedError Failing tests: * TestMRServerPorts - testJobTrackerPorts * TestMRServerPorts - testTaskTrackerPorts * TestMiniMRTaskTempDir - testTaskTempDir   Exception: java.io.IOException: Job failed! Failing tests: * TestMiniMRLocalFS - testWithLocal ",test
TestLocalDirAllocator fails under AIX ,TestLocalDirAllocator fails when running under AIX for the same reasons as CYGWIN under Windows (as noted in the test source code comments).  AIX allows the writing of a file in a directory that is marked read-only. This breaks the test. If the test is changed to sense for AIX (as it does for windows) then the usefulness of this unit test is questionable other than exposing an interesting anomoly in the native file system. ,test
TestSocketIOWithTimeout fails under AIX - TIMEOUT error. ,This test expects an exception to occur when read/writing a closed socket.  Under AIX this does not occur and results in a loop.  ,test
libhdfs test conf uses deprecated fs.default.name value,src/c++/libhdfs/tests/conf/core-site.xml contains a deprecated layout for fs.default.name: {noformat}   <name>fs.default.name</name>   <value>localhost:23000</value> {noformat},test
Remove deprecated call method from RPC,"  public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) is deprecated and only called from the unit test.  It should be removed.",ipc
"Separate the core, hdfs and mapred junit tests","To support splitting of projects, the tests should be separated into different directories.",build
RPC call throws IllegalArgumentException complaining duplicate metrics registration,"Here is the error log:     INFO  ipc.Server (Server.java:run(968)) - IPC Server handler 7 on 51017, call addBlock(/file7, DFSClient_-2132593831) from 127.0.0.1:51030: error: java.io.IOException: java.lang.IllegalArgumentException: Duplicate metricsName:addBlock     java.io.IOException: java.lang.IllegalArgumentException: DuplicatemetricsName:addBlock          at org.apache.hadoop.metrics.util.MetricsRegistry.add(MetricsRegistry.java:56)          at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init>(MetricsTimeVaryingRate.java:89)          at org.apache.hadoop.metrics.util.MetricsTimeVaryingRate.<init>(MetricsTimeVaryingRate.java:99)          at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:522)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)",ipc
MapWritable#putAll does not store classes,MapWritable's putAll method does not call addToMap for keys and values as MapWritable#put does. So new classes will not be stored in class-id maps and will lead to problems during readFields.,io
"""ant binary"" wastes time building documentations that are not needed in the packaging",None,build
"hdfs_quota_admin_guide.html 'q' in setQuota, etc not properly capitalized","This is trivial, but unfortunately significant. The quota admin guide has entries like, ""dfsadmin -setquota <N> <directory>...<directory>"" and ""dfsadmin -clrquota <directory>...<director>"" (oh hey that's missing a 'y').  The dfsadmin command is case-sensitive: {quote} $ hadoop dfsadmin -setquota 200 /user/marco setquota: Unknown command {quote}  but setQuota works: {quote} $ hadoop dfsadmin -setQuota 200 /user/marco $  {quote} ",documentation
better messaging when tasktacker cannot access userlogs,"When the tasktracker cannot access the userlogs directory all the tasks fail but the only message in the logs is:  2009-01-28 16:00:37,024 WARN org.apache.hadoop.mapred.TaskRunner: attempt_200901280756_0001_m_000534_1 Child Error java.io.IOException: Task process exit with nonzero status of 1.         at org.apache.hadoop.mapred.TaskRunner.runChild(TaskRunner.java:462)         at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:403)  It should write to the log what the actual problem is. ",io
TestHeartbeatHandling uses MiniDFSCluster.getNamesystem() which does not exist in branch 0.20,"This breaks branch 0.20 build, which currently does not compile. This will probably require promoting HADOOP-5017 to branch 0.20 or simply using {{cluster.getNameNode().getNamesystem()}} in this test.",test
Subclasses of ClusterMapReduceTestCase can't easily add new configuration parameters,"Currently there is not a clean way for subclasses of ClusterMapReduceTestCase to add to the JobConf used to start the cluster daemons.  The startCluster() method does take a Properties object that is added to the JobConf used to the start the daemons.  However, startCluster() is called from JUnit inside the setUp() method, which sets this parameter to be null.  If you try to override setUp() in a subclass of ClusterMapReduceTestCase, then you won't be able to invoke the TestCase.setUp() ancestor without calling ClusterMapReduceTestCase's setUp() (which will pass in the null parameter).  On the other hand, if you just call startCluster() within your test method, then you would be starting up a cluster that was already started.",test
Chukwa : TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails , org.apache.hadoop.chukwa.datacollection.agent.TestAgentConfig.testInitAdaptors_vs_Checkpoint regularly fails in Hudson builds. I am not sure which branches it affects. I will attach one of the failure logs. ,test
TestDFSIO reports itself as TestFDSIO,"When TestDFSIO starts up, it reports itself as ""TestFSDIO"", which would seem to be a typo. ",benchmarks
Create a privacy policy for the Hadoop website,"It would be great to collect analytics about the visitors to the website and to do so, we need to create a privacy policy that tells visitors what we will collect.",documentation
Unit tests for TestProxyUgiManager and TestHdfsProxy consistently failing on trunk builds,"of the last 10 trunk builds, these unit tests have failed in about 50% of the builds.   Trunk builds have been failing unit tests consistently for as far back as I can see in hudson.  ",test
avoiding unnecessary byte[] allocation in SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes,SequenceFile.CompressedBytes and SequenceFile.UncompressedBytes are used by the SequenceFile's raw bytes reading/writing API. The current implementation does not reuse the internal byte[] and causes unnecessary buffer allocation and initializaiton (zeroing the buffer).,io
NPE in Shell.runCommand(),I have seen one of the task failures with following exception: java.lang.NullPointerException   java.lang.ProcessBuilder.start(ProcessBuilder.java:441)   org.apache.hadoop.util.Shell.runCommand(Shell.java:149)   org.apache.hadoop.util.Shell.run(Shell.java:134)   org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)   org.apache.hadoop.util.ProcessTree.isAlive(ProcessTree.java:244)   org.apache.hadoop.util.ProcessTree.sigKillInCurrentThread(ProcessTree.java:67)   org.apache.hadoop.util.ProcessTree.sigKill(ProcessTree.java:115)   org.apache.hadoop.util.ProcessTree.destroyProcessGroup(ProcessTree.java:164)   org.apache.hadoop.util.ProcessTree.destroy(ProcessTree.java:180)   org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.kill(JvmManager.java:377)   org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:249)   org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:113)   org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:76)   org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:411)  ,util
hudson trunk build failure due to autoheader failure in create-c++-configure-libhdfs task,"create-c++-configure-libhdfs:      [exec] autoheader: warning: missing template: HADOOP_CONF_DIR      [exec] autoheader: Use AC_DEFINE([HADOOP_CONF_DIR], [], [Description])      [exec] autoreconf: /usr/bin/autoheader failed with exit status: 1  See output at: http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/746/",build
"SAXParseException: ""id"" must not contain the '<' character","Found the following exception in recent Hudson builds (e.g. see [build #3821|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3821/console]): {noformat}      [exec]   [javadoc] JDiff: reading the comments in from file '/zonestorage/hudson/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/docs/jdiff/user_comments_for_hadoop_0.19.0_to_hadoop_742698_HADOOP-5205_PATCH-12399851.xml'...      [exec]   [javadoc] Fatal Error (102): parsing XML comments file:org.xml.sax.SAXParseException: The value of attribute ""id"" must not contain the '<' character.      [exec]   [javadoc] org.xml.sax.SAXParseException: The value of attribute ""id"" must not contain the '<' character. {noformat}",build
Update year to 2009 for javadoc,The year is still 2008 in the generated javadoc.,documentation
cygwin path translation not happening correctly after Hadoop-4868,None,scripts
BZip2CompressionOutputStream NullPointerException,BZip2CompressionOutputStream will throw a NullPointerException if the user creates a BZip2CompressionOutputStream and close it without writing out any data.,io
"Split the AllTestDriver for core, hdfs and mapred",The sub projects would have individual test jar. This would require separate driver class for each.,test
SequenceFile is using mapred property,"SequenceFile is using ""mapred.local.dir"". It should not depend on mapred as it is part of the core.",io
Add license headers to html and jsp files,License headers are missing in some html and jsp files.,documentation
duplicate variables in build.xml hadoop.version vs version let build fails at assert-hadoop-jar-exists,"<property name=""hadoop.jar"" location=""${build.dir}/hadoop-${hadoop.version}-core.jar"" /> where hadoop.version is defined in ${ivy.dir}/libraries.properties as hadoop.version=0.20.0.  Though <jar jarfile=""${build.dir}/${final.name}-core.jar"" where <property name=""final.name"" value=""${name}-${version}""/>. Means there is a hadoop-0.21.0-dev-core.jar builded though the assert-hadoop-jar-exists target looks for a  hadoop-0.2.0-core.jar.  ",build
"preparing HadoopPatchQueueAdmin.sh,test-patch.sh scripts to run builds on hudson slaves.",To modify hadoopPatchQueueAdmin.sh and test-patch.sh script to run patch builds on hudson slaves.,build
MBeanUtil should log errors using the logging API,MBeanUtil prints stack traces to standard error. It should use commons logging instead.,metrics
ivy publish and ivy integration does not cleanly work,"As far I understand the goal using ivy for hadoop is to be able intgrating hadoop easily in thirdparty builds that uses transient dependency tools like ivy or maven.  The way ivy is currently integrated a couple hick ups.  + the generated artifact files have names that can't be used for maven or ivy. e.g. hadoop-version-core but standard would be hadoop-core-version. This effects all those generated artifact files like hadoop-version-example etc. This is caused by the use of ${final.name}-core.jar + This conflicts with the use of ""${version}"" in ivy.xml <info organisation=""org.apache.hadoop"" module=""${ant.project.name}"" revision=""${version}"">. The result will be a error report by ivy that found artifact and defined artifact name are different.  ",build
'ant javadoc' does not check whether outputs are up to date and always rebuilds,Running 'ant javadoc' twice in a row calls the javadoc program both times; it doesn't check to see whether this is redundant work.,build
to remove duplicate calls to the cn-docs target.,"package target depends on docs, cn-docs and ....etc... and doc target intern calls cn-docs which results in call calling cn-docs target twice when ant package executed.  ",build
Xinclude setup results in a stack trace,"seen this in SVN_HEAD, and it was mentioned on the user list in the week. It explains why my health tests are failing on class.ForName(FSConstants)  gistration(127.0.0.1:8024, storageID=DS-1466307248-127.0.1.1-8024-1234537374021, infoPort=8022, ipcPort=50020):DataXceiver [sf-startdaemon-debug] java.lang.ExceptionInInitializerError [sf-startdaemon-debug]   org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:76) [sf-startdaemon-debug]   java.lang.Thread.run(Thread.java:619) [sf-startdaemon-debug] Caused by: java.lang.UnsupportedOperationException: This parser does not support specification ""null"" version ""null"" [sf-startdaemon-debug]   javax.xml.parsers.DocumentBuilderFactory.setXIncludeAware(DocumentBuilderFactory.java:590) ",conf
Fix for HADOOP-5079 HashFunction inadvertently destroys some randomness,"HADOOP-5079 did this ""HashFunction.hash restricts initval for the next hash to the [0, maxValue) range of the hash indexes returned. This is suboptimal, particularly for larger nbHash and smaller maxValue. Rather we should first set initval, then restrict the range for the result assignment.""  The patch committed on that issue introduced a new bug: ""My first patch contained a regression: you have to take the remainder before calling Math.abs, since Math.abs(Integer.MIN_VALUE) == Integer.MIN_VALUE still"" (Jonathan Ellis).",io
Some tests not run by default,"To be run by the 'test' target, test file names must start with ""Test"" and end in "".java"".  One example that violates this is src/test/org/apache/hadoop/fs/FileSystemContractBaseTest.java.  Is this on purpose to that it's not run automatically?  Are there other tests like this?","build,test"
Job with output hdfs:/user/<username>/outputpath (no authority) fails with Wrong FS,"Using namenode with default port of 8020.  When starting a job with output hdfs:/user/knoguchi/outputpath, my job fails with   Wrong FS: hdfs:/user/knoguchi/outputpath, expected: hdfs://aaa.bbb.cc ",fs
Documentation: Chinese (cn) doc structure placed in the middle of the English doc structure,The Chinese doc structure was plopped into the middle of the English doc structure. You need to figure out where you are going to put translations of the Hadoop core docs.  [-] src      [-] docs ------------------> English docs               [+] .svn               [+] build                     changes               [+] cn  -------------> Chinese docs               [+] src ,"build,documentation"
Builds failing - Cannot run program ant,"The last couple of builds have failed with the following error error message, taken from http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/754/  BUILD SUCCESSFUL Total time: 177 minutes 27 seconds [trunk] $ ant FATAL: command execution failed.Maybe you need to configure the job to choose one of your Ant installations? java.io.IOException: Cannot run program ""ant"" (in directory ""/home/hudson/hudson-slave/workspace/Hadoop-trunk/trunk""): java.io.IOException: error=2, No such file or directory <snip>",build
License header missing in TestJobInProgress.java,None,documentation
gridmix2 is not getting compiled to generate gridmix.jar,"Not able to compile gridmix2 to generate gridmix.jar. Compilation gets failed giving build failed message. It seems that problem is with mapper class and reduce class specified in CombinerJobCreator.java. Changed mapper class from ""MapClass.class"" to ""Mapper.class"" and reduce class  from ""Reduce.class"" to ""Reducer.class"" then it started working and gridmix.jar was generated.",benchmarks
ivy directory should be there in hadoop tar ball,Ivy directory should also be in hadoop tar ball. It is required when we are using hadoop (untarred from hadoop tar ball) and need to compile any of the compnent individually (e.g. gridmix2). ,build
Unit test fails out on trunk org.apache.hadoop.http.TestServletFilter.testServletFilter,From: http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/760/    Regression    org.apache.hadoop.http.TestServletFilter.testServletFilter    Failing for the past 1 build (Since #760 )  Took 1 min 10 sec.  Error Message    url[4]=/static/hadoop-logo.jpg expected:<8> but was:<9>,test
"""ant javadoc-dev"" does not work",{noformat}  bash-3.2$ ant javadoc-dev  Buildfile: build.xml    javadoc-dev:      [mkdir] Created dir: d:\@sze\hadoop\latest\build\docs\dev-api    [javadoc] Generating Javadoc    BUILD FAILED  d:\@sze\hadoop\latest\build.xml:936: Reference ivy-common.classpath not found.    Total time: 0 seconds  {noformat},build
Unit test org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk,"org.apache.hadoop.fs.TestCopyFiles.testMapCount fails on trunk quite often with ""Unexpected map count"" error message. See [http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/762/testReport/org.apache.hadoop.fs/TestCopyFiles/testMapCount/] for detailed output.",test
Fix null value handling in StringUtils#arrayToString() and #getStrings(),"StringUtils#arrayToString() converts String array to a String of comma separated elements. If the String array includes null values, these are recovered as ""null"" (literal) from getStrings() method, which eventually causes configuration issues. ",util
Not all core javadoc are checked by Hudson,"Since ""ant javadoc"" does not generate all core javadocs, some javadocs (e.g. HDFS javadocs) are not checked by Hudson.",build
Provide documentation for LazyOutput Feature,"HADOOP-4927 introduced support for the ""LazyOutput"" feature. Documentation needs to be updated for this.",documentation
TestMapReduceLocal is missing a close() that is causing it to fail while running the test on NFS,The readFile method in this test is not calling a close of the file after it is done reading. This causes some lingering .nfs* files that is preventing the directory from getting deleted properly causing the second program in this test to fail.,test
BZip2CompressionOutputStream sometimes corrupts data,We are using a BZip2CompressionOutputStream.java version with bugs.  See the following 2 issues for details: https://issues.apache.org/bugzilla/show_bug.cgi?id=24798 https://issues.apache.org/bugzilla/show_bug.cgi?id=41596 ,io
Trash documentation should describe its directory structure and configurations,"Trash documentation should mention the significance of ""Current"" and ""<time-stamp>"" directories which get generated inside Trash directory. The documentation should also incorporate modifications done in HADOOP: 4970.",documentation
bzip2 codec (CBZip2OutputStream) creates corrupted output file for some inputs,"Bzip2 codec generated corrupted output files in some test executions I performed. This bug is probably related to https://issues.apache.org/bugzilla/show_bug.cgi?id=41596.  * In my case, the problem seems to be at the BWT (Burrows-Wheeler Transform) implementation.",io
TestLocalMRNotification.testMR failed in Hudson,"TestLocalMRNotification.testMR failed in Hudson, from [build #3911|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3911/testReport/] to the latest, [build #3917|http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3917/testReport/].",test
reverse DNS doesnt resolve local loop address 127.0.1.1,"if the local IPAddr is loopback, then reverse DNS lookup fails. It may be better to recognise this state and return localhost in such a situation, rather than try to look up 1.1.0.127.in-addr.arpa",io
hadoop-daemon isn't compatible after HADOOP-4868,The CLI changed for hadoop-daemon.sh in an incompatible way. It now requires the sub-system name in the CLI. ,scripts
MapReduceBase has been deprecated but there are no references to classes that are replacing it,MapReduceBase has been deprecated but the JavaDoc has no references to classes that are replacements.,documentation
JobID is deprecated but there are no references to classes that are replacing it.,JobID is deprecated but there are no references to classes that are replacing it.,documentation
" ""quota commands"" shown as lower case",http://hadoop.apache.org/core/docs/r0.19.0/hdfs_quota_admin_guide.html  The command is shown as: dfsadmin -setquota dfsadmin -setspacequota  But it should read as in the CLI dfsadmin -setQuota dfsadmin -setSpaceQuota  I'm assuming the other quota commands should also be mixed case.,documentation
Create a ThrowableWritable for serializing exceptions robustly,"HADOOP-5201 and other issues would benefit from a stable representation of exceptions, one that can be sent over the network, maybe pushed out to web UIs and which we can be 100% sure that the far end will be able to handle if they have the hadoop-core JAR on their classpath.",ipc
add progress callback feature to the slow FileUtil operations with ability to cancel the work,"This is something only of relevance of people doing front ends to FS operations, and as they could take the code in FSUtil and add something with this feature, its a blocker to none of them.     Current FileUtil.copy can take a long time to move large files around, but there is no progress indicator to GUIs, or a way to cancel the operation mid-way, j interrupting the thread or closing the filesystem.    I propose a FileIOProgress interface to the copy ops, one that had a single method to notify listeners of bytes read and written, and the number of files handled.    {code}  interface FileIOProgress {   boolean progress(int files, long bytesRead, long bytesWritten);  }    The return value would be true to continue the operation, or false to stop the copy and leave the FS in whatever incomplete state it is in currently.     it could even be fancier: have  beginFileOperation and endFileOperation callbacks to pass in the name of the current file being worked on, though I don't have a personal need for that.    GUIs could show progress bars and cancel buttons, other tools could use the interface to pass any cancellation notice upstream.    The FileUtil.copy operations would call this interface (blocking) after every block copy, so the frequency of invocation would depend on block size and network/disk speeds. Which is also why I don't propose having any percentage done indicators; it's too hard to predict percentage of time done for distributed file IO with any degree of accuracy.",fs
"Hadoop Quota documentation missing ""user facing"" content","Right now, the HDFS Quota Admin guide is full of good stuff on how to set/unset both space and name quotas.  However, there's no documentation on how users should expect the system to behave. What will happen to their running jobs, how will the command line client react, etc? ",documentation
Provide scripting functionality to the synthetic load generator,"Currently the load generator accepts parameters at start time as to the read and write probabilities to apply against the namenode.  It would be useful to be able to provide it with a script of these values, so that they can be varied over time.  This would allow the namenode to be tested with varying loads over time so its behavior to changing loads can be examined.",test
Throw exception instead of writing to System.err when there is a CRC error on CBZip2InputStream,"From org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.java:      {code}  private static void reportCRCError() throws IOException {     // The clean way would be to throw an exception.     // throw new IOException(""crc error"");       // Just print a message, like the previous versions of this class did     System.err.println(""BZip2 CRC error"");  }  {code}",io
DataNodeCluster should not create blocks with generationStamp == 1,"In DataNodeCluster.main(..), injected blocks are created with generationStamp == 1, which is a reserved value but not a valid generation stamp.  As a consequence, NameNode may die when those blocks are reported.",test
"hdfsproxy includes duplicate jars in tarball, source in binary tarball","The binary tarball should not include hdfsproxy source. Similarly, hdfsproxy should not include its own copy of jars already in the distribution, particularly hadoop-\* jars.",build
TestJobHistory fails intermittently.,TestJobHistory fails intermittently with following error message: User log file file:/home/hudson/hudson-slave/workspace/Hadoop-Patch-vesta.apache.org/trunk/build/test/data/succeed/output1/_logs/history/localhost_1236132901323_job_200903040215_0001_hudson_test-job-succeed.recover does not exist  One of the hudson builds with failure : http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/41/testReport/org.apache.hadoop.mapred/TestJobHistory/testJobHistoryUserLogLocation/  I saw the failure in local machine also some times.,test
hadoop-port-range parameter should be optional.,"""__hadoopPortRange"" parameter has been made mandatory in hod. Deallocation is a problem if hadoop-port-range is specified only in command line and not included in hodrc. This parameter should be made optional.",contrib/hod
GenericOptionsParser should parse generic options even if they appear after Tool-specific options,"Currently, when GenericOptionsParser encounters an unrecognized option, it stops processing command-line arguments, and returns the rest to the specific Tool. This forces users to remember the order of arguments, and leads to errors such as following:    org.apache.commons.cli.UnrecognizedOptionException: Unrecognized option:  -Dmapred.reduce.tasks=4          at org.apache.commons.cli.Parser.processOption(Parser.java:368)          at org.apache.commons.cli.Parser.parse(Parser.java:185)          at org.apache.commons.cli.Parser.parse(Parser.java:70)          at  MyTool.run(MyTool.java.java:290)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at  MyTool.main(MyTool.java:19)    In Hadoop-streaming as well, -D parameters should appear before streaming-specific arguments, such as -mapper, -reducer etc.    If GenericOptionsParser were to scan the entire command-line, ignoring unrecognized (tool-specific) options, and returning all unrecognized options back to the tool, this problem would be solved.  ",util
Adding support for HDFS proxy,"Currently, HDFS doesn't authenticate users. The client simply tells HDFS who she is and HDFS will take her word for it. That makes it possible for HDFS proxy to access HDFS on behalf of a user - the proxy simply claims to be that user. Once we turn on authentication, the proxy can't do that without having user's credentials. We need a solution such that HDFS proxy can continue to access HDFS on behalf of users.",security
Misnamed function in ZlibCompressor.c,"All methods in ZlibCompressor.c are named like ""Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_METHODNAME""  There is one that is called: Java_org_apache_hadoop_io_compress_ZlibCompressor_setDictionary  Should be: Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_setDictionary",native
IO exception while executing hadoop fs -touchz  fileName  ,Stack trace while executing hadoop fs -touchz command .  [user@xyzhostname ~]$ hadoop fs -touchz test/new0LenFile2 09/03/05 23:31:21 WARN hdfs.DFSClient: Problem renewing lease for DFSClient_-661919204 java.io.IOException: Call to xxxxx-xxx.xxx.com/xxx.xxx.xxx.xxx:xxxx failed on local exception: java.nio.channels.ClosedByInterruptException  ,fs
"Wrong description of "" hadoop fs -test "" in FS Shell guide . ","Hadoop FS Shell Guide  documentation for -test command option -d  current description is.     "" -d check return 1 if the path is directory else return 0. ""   Where as it should be .        "" -d check to see if the path is Directory . Return 0 if true "" ",documentation
IPC client drops interrupted exceptions,The IPC client needless drops InterruptedException.,ipc
In hdfs /*/* globbing does not work,"With reference to Jira issue : 3497, we tried globbing /*/*. But it does not work. Output is given below.      bin/hadoop --config /tmp/cluster/ fs -lsr /  log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).  log4j:WARN Please initialize the log4j system properly.  drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a  drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a/b  drwxr-xr-x   - karthv supergroup          0 2009-03-06 08:00 /a/b/c        bin/hadoop --config /tmp/cluster/ fs -lsr /*/*    Actually it is ""bin/hadoop --config /tmp/cluster/ fs -lsr Forward slash followed by star followed by forward slash followed by star""    output:  lsr: Cannot access /bin/arch: No such file or directory.  lsr: Cannot access /bin/ash: No such file or directory.  lsr: Cannot access /bin/ash.static: No such file or directory.  lsr: Cannot access /bin/awk: No such file or directory.  lsr: Cannot access /bin/basename: No such file or directory.  lsr: Cannot access /bin/bash: No such file or directory.  lsr: Cannot access /bin/bsh: No such file or directory.  lsr: Cannot access /bin/cat: No such file or directory.  ....  ....  ...    It keeps giving a long list like this without showing the results.",documentation
It should be posible to specify metadata for the output file produced by SequenceFile.Sorter.sort,None,io
Failing contrib tests should not stop the build,"If one or more of the unit tests in HdfsProxy fail, none of the subsequent test suites are run. All of the unit tests should run, regardless of previous projects' status.",test
Merge FileSystem.create and FileSystem.append,"Currently, when a user wants to modify a file, the user first calls exists() to know if this file is already there. And then uses create() or append() according to whether the file exists or not.  the code looks like:  {code}  FSDataOutputStream out_1 = null;  if (fs.exists(path_1))     out_1 = fs.append(path_1);  else     out_1 = fs.create(path_1);  {code}  . On the performace side,It involes two RPCs. On the easy-of-use side, it is not very convient in contrast to the traditional open interface.    It will more complicate if there is a overwrite parameter specified. I donot know whether there is a bug about 'overwrite' in 0.19, some times it takes a long time for overwrite creates to reture. So i make the write file code with overwrite param works like:  {code}  boolean exists = fs.exists(name);  if (overwrite) {      if (exists)         fs.delete(name, true);       this.out = fs.create(name, overwrite, bufferSize, replication,          blockSize, progress);       this.currentRowID = 0;   } else {     if (!exists)   this.out = fs.create(name, overwrite, bufferSize,       replication, blockSize, progress);     else   this.out = fs.append(name, bufferSize, progress);  {code}    Some code statements there are really redundant and not needed, especialy with the delete(). But without deleting first, the overwrite takes a long time to reture.    BTW, i will create another issue about the overwrite problem. If it is not a bug at all or a duplicate, someone please close it.    ",fs
HOD refactoring to ease integration with scheduler/resource managers other than torque,"Situation: HOD currently uses the pbsdsh (a distributed shell that works via Torque's TM interface to start remote processes) command to start processes on all nodes in the job.  This call is provided as part of a torqueInterface class that is meant to abstract interactions with the torque resource managers (RMs).  However, this is not functionality typically provided by other RMs, and is instead typically performed by an distributed command available on the HPC system, mpiexec, ssh, or site-specific scripts.  The specificity of pbsdsh to Torque makes writing HOD interfaces to other RMs somewhat difficult as it forces the implementer to choose the remote start method on a somewhat faulty per-RM basis.    Proposal: Refactor the torqueInterface and nodePool classes so that the choice of remote start method is available as a configuration option in hodrc.  This involves fairly simple changes to remove the pbsdsh command from the Scheduler class and addition configuration step of starting the appropriate remote start wrapper.  The selection of the nodePool class will be altered to allow dynamic loading of classes, so that new interfaces people choose to write will not require altering HOD code.  Provide remote start classes for pbsdsh, mpiexec, ssh, as well as custom scripts (sites often provide mpiexec wrappers that ensure proper selection of network interfaces, etc).  Provide interface classes to SGE and Moab, as well as updated Torque class.",contrib/hod
TaskTracker metrics are disabled,"HADOOP-3772 changed TaskTracker to use an instrumentation class, but did not update the default metrics class to the new API. TT metrics are currently discarded.",metrics
"default ""hadoop-metrics.properties"" doesn't mention ""rpc"" context","The ""hadoop-metrics.properties"" file that's shipped in conf/ has configuration settings for the metrics contexts ""dfs"", ""mapred"", and ""jvm"".  The (trivial) patch I'm proposing is to include default configuration for the ""rpc"" context as well.  RPC metrics may be useful, and it's difficult for a user to intuit otherwise that rpc metrics even exist.  (I stumbled upon them after exploring with JConsole and JMX.)","documentation,metrics"
javadoc warning: can't find restoreFailedStorage() in ClientProtocol,ant javadoc-dev  {noformat}    [javadoc] /home/tsz/hadoop/latest/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java:399: warning - Tag @see: can't find restoreFailedStorage() in org.apache.hadoop.hdfs.protocol.ClientProtocol    [javadoc] /home/tsz/hadoop/latest/src/hdfs/org/apache/hadoop/hdfs/tools/DFSAdmin.java:412: warning - Tag @see: can't find restoreFailedStorage() in org.apache.hadoop.hdfs.protocol.ClientProtocol  {noformat},documentation
CSS Style Changes for Hadoop Doc Headers and Code,skinconf.xml file in /trunk/src/docs/src/documentation folder updated to include new CSS definitions for headers and code. Changes will apply to all hadoop docs. Improves readability.,documentation
Change Hadoop doc menu to sub-menus,"In directory Trunk\src\docs\src\documentation\content\xdocs ...    1. site.xml - changed doc menu to submenus    2. *.xml - for a few docs, changed titles to match submenus and made some minor edits ",documentation
Exposing Hadoop metrics via HTTP,"Implement a ""/metrics"" URL on the HTTP server of Hadoop daemons, to expose metrics data to users via their web browsers, in plain-text and JSON.",metrics
RunJar.unJar() should write the last modified time found in the jar entry to the uncompressed file,"For tools like jruby and jython, last modified times determine if a script gets recompiled.  Losing the correct last modified time causes some unfortunate recompilation race conditions when a job is running.  ",util
"calling new SequenceFile.Reader(...) leaves an InputStream open, if the given sequence file is broken",None,io
TestCLI fails,None,test
TestRecoveryManager fails wtih FileAlreadyExistsException,TestRecoveryManager always fails when I run core tests in a linux redhat machine. It does not fail on a Mac machine.    Testcase: testRecoveryManager took 55.842 sec          Caused an ERROR  Output directory file:XX/build/test/data/test-recovery-manager/output1 already exists  org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:XX/build/test/data/test-recovery-manager/output1 already exists          at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:111)          at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:772)          at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)          at org.apache.hadoop.mapred.TestRecoveryManager.testRecoveryManager(TestRecoveryManager.java:196)  ,test
"ReliabilityTest does not test lostTrackers, some times.","ReliabilityTest does not lose trackers, if tasktracker pid could not be obtained.  If command for the TaskTracker process is large, doing 'ps' and 'grep' does not return the pid of the process, thus it could not be suspended.  ",test
hadoop-env.sh still refers to java1.5,The example JAVA_HOME in conf/hadoop-env.sh still points to /usr/lib/j2sdk1.5-sun    better to have it set to point to wherever the sun java 6 RPM sticks Java ,conf
JobTracker metrics do not match job counters,"After one run of a 10 map, 3 reduce sort job, the mapred metrics report:  {noformat}  mapred.tasktracker: hostName=snip, sessionId=, mapTaskSlots=3, maps_running=0, \    reduceTaskSlots=3, reduces_running=0, tasks_completed=15, tasks_failed_ping=0, tasks_failed_timeout=0  mapred.jobtracker: hostName=snip, sessionId=, jobs_completed=1, jobs_submitted=1, \    maps_completed=10, maps_launched=12, reduces_completed=3, reduces_launched=3  {noformat}    After the second (w/ one manually killed reduce)  {noformat}  mapred.tasktracker: hostName=snip, sessionId=, mapTaskSlots=3, maps_running=0, \     reduceTaskSlots=3, reduces_running=0, tasks_completed=32, tasks_failed_ping=0, tasks_failed_timeout=0  mapred.jobtracker: hostName=snip, sessionId=, jobs_completed=2, jobs_submitted=2, \    maps_completed=20, maps_launched=24, reduces_completed=6, reduces_launched=8  {noformat}    The counters report the expected 10/3 map/reduce completions, the second job reporting a single failure.    # The {{maps_launched}} and {{reduces_launched}} counts are likely recording setup and cleanup tasks as well  # After being recorded among launched tasks, it looks like setup and cleanup are _not_ included among completed tasks  # {{tasks_completed}} on the TaskTracker should include only user tasks  # The {{reduces_launched}} count makes little sense to me. With three reduces launched per job and one failed, what's the other launched reduce counting?",metrics
Backup and checkpoint nodes should be documented,HDFS user guide should include description of backup and checkpoint nodes.,documentation
Add waiting/failed tasks to JobTracker metrics,"In addition to launched and completed tasks, it would be helpful if the number of waiting tasks and failed tasks were accumulated as metrics.",metrics
Typo in diskQuota help  documentation ,"Minor typo in setSpaceQuota help documentation displayed on CLI.   disk is misspelled .    -clrQuota <dirname>...<dirname>: Clear the quota for each directory <dirName>.    Best effort for the directory. with fault reported if    1. the directory does not exist or is a file, or    2. user is not an administrator.    It does not fault if the directory has no quota.  -setSpaceQuota <quota> <dirname>...<dirname>: Set the dik space quota <quota> for each directory <dirName>.    The directory quota is a long integer that puts a hard limit    on the number of names in the directory tree.    Quota can also be speciefied with a binary prefix for terabytes,    petabytes etc (e.g. 50t is 50TB, 5m is 5MB, 3p is 3PB).    Best effort for the directory, with faults reported if    1. N is not a positive integer, or    2. user is not an administrator, or    3. the directory does not exist or is a file, or    4. the directory would immediately exceed the new space quota.    Assigning to self.",documentation
Remove dependency of testcases on RESTART_COUNT,There are 2 usecases for this :  # RESTART_COUNT is not guaranteed to be flushed to fs.  # HADOOP-5394 plans to discontinue logging restart-count to job history file.,test
Ignore Eclipse helpers in Subversion and Git,The top-level directory {{.externalToolBuilders}} should be ignored by Subversion and Git.,build
Implement -setSpaceQuota and -clrSpaceQuota tests on directory using globbing  ,"TestCLI.java  for testing Command Line Interface is missing  tests for -setSpaceQuota on directory using globbing , and -clrSpaceQuota on directory using globbing . ",test
"On a fresh check out of 0.20 release branch, not able to run TestMapReduceLocal unit testcase, using ant","On a fresh check out of 0.20 release branch,   I did a run of TestMapReduceLocal with command ""ant test -Dtestcase=TestMapReduceLocal""  It failed. The logs under build/test/TEST-org.apache.hadoop.mapreduce.TestMapReduceLocal.txt says  that ~/build/test/data/out exists.    I removed the data directory itself and then tried again. it says the same error.    I tried by doing a ant clean first and then running this unit testcase. It gives the same error.  ",build
DataNodeCluster should create blocks with the same generation stamp as the blocks created in CreateEditsLog,"HADOOP-5384 makes DataNodeCluster to create blocks with generation stamp Block#GRANDFATHER_GENERATION_STAMP(0) so simuated datanodes do not crash NameNode any more. But there is still a problem. CreateEditLogs creates blocks with generation stamp GenerationStamp#FIRST_VALID_STAMP (1000). Because of the generation stamp mismatch, all injected blocks are marked as invalid when NameNode processes block reports.",test
A few improvements to DataNodeCluster,"DataNodeCluster is a great tool to simulate a large scale DFS cluster using a small set of machines. A few suggestions to improve this tool:  # DataNodeCluster uses MiniDFSCluster#startDataNode to start multiple instances of DataNode on one machine. MiniDFSCluster sets DataNode's address to be 127.0.0.1. We should allow to set its address to 0.0.0.0 so DataNodes in different machines could communicate.  # Currently the size of the blocks injected to DataNode and created in CreatedEditsLog is hardcoded as 10. It would be more convenient if this could be configurable. Also we need to make sure that both use the same block size.  # If the replication factor of blocks is larger than 1, currently a DataNode in DataNodeCluster will be injected blocks multiple times and therefore it sends block reports to NameNode multiple times. Initial block reports contain only a portion of its blocks and therefore may cause unnecessary block replications. It would be cleaner if only one block report with all its blocks is sent. ",test
Two minor problems in TestOverReplicatedBlocks,"- There is no apache license header.  - It uses a deprecated API, FSNamesystem.getFSNamesystem().",test
Build generates unchecked cast warnings from o.a.h.mapred.Task,HADOOP-5382 introduced unchecked cast warnings that should either be accommodated or suppressed.,build
Javadoc-dev ant target runs out of heap space,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:  {noformat}  [javadoc] Constructing Javadoc information...  [javadoc] Standard Doclet version 1.6.0_07  [javadoc] Building tree for all the packages and classes...  [javadoc] java.lang.OutOfMemoryError: Java heap space  [javadoc]   java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)  [javadoc]   java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)  [javadoc]   java.util.HashMap.put(HashMap.java:385)  [javadoc]   sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)  [javadoc]   sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)  [javadoc]   sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)  [javadoc]   sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)  [javadoc]   java.util.ResourceBundle.getObject(ResourceBundle.java:378)  [javadoc]   java.util.ResourceBundle.getObject(ResourceBundle.java:381)  [javadoc]   java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)  [javadoc]   sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)  [javadoc]   sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)  [javadoc]   java.util.TimeZone.getDisplayNames(TimeZone.java:399)  [javadoc]   java.util.TimeZone.getDisplayName(TimeZone.java:350)  [javadoc]   java.util.Date.toString(Date.java:1025)  [javadoc]   com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)  [javadoc]   com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)  [javadoc]   com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)  [javadoc]   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  [javadoc]   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  [javadoc]   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  [javadoc]   java.lang.reflect.Method.invoke(Method.java:597)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)  [javadoc]   sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  [javadoc]   sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  [javadoc]   sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  [javadoc]   java.lang.reflect.Method.invoke(Method.java:597)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)  [javadoc]   com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)  {noformat}",build
hadoop command uses large JVM heap size,"Command used to determine JAVA_PLATFORM in bin/hadoop command does not set the heap size. The command uses default 1GB heap size. The tasks invoking hadoop command end up  using large heap size in streaming jobs. If the maximum memory that can be used by a task is restricted, this could result in map/reduce job failures.  ",build
Add link to distributions wiki page from releases page,None,documentation
Feature Designs and Test Plans templates in HTML,This Jira is to address an existing email thread which requested a Feature Design template and Test Plan template in HTML format.      Past Design Doc Examples  http://issues.apache.org/jira/secure/attachment/12348296/DFSUpgradeProposal3.html    Past Test Plan Examples  https://issues.apache.org/jira/secure/attachment/12373559/PermissionsTestPlan2.pdf  https://issues.apache.org/jira/secure/attachment/12363605/BlockCrcFeatureTestPlan.pdf  https://issues.apache.org/jira/secure/attachment/12351299/TestPlan-HdfsUpgrade.html   ,documentation
hadoop commands seem extremely slow in 0.20 branch,"hadoop dfs get, rm, -mkdir- ,cp, mv, ls, etc   mydir/fileA mydir/fileB mydir/fileC ...    seem to be very slow in 0.20 branch.    ",fs
Hadoop Streaming - GzipCodec,Hadoop Streaming - How do I generate output files with gzip format    This:  mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCode    Should be this:  mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec    ,documentation
Make ObjectWritable support EnumSet,"This is a demand for Hadoop-5438.   Also another small improvement is that i saw that in the beginning of readObject, it tries to get the class from PRIMITIVE_NAMES and then conf. Maybe it is better to add a direct load after them if the delaredClass is still null. Like this:  {code}  String className = UTF8.readString(in);      Class<?> declaredClass = PRIMITIVE_NAMES.get(className);      if (declaredClass == null) {        try {          declaredClass = conf.getClassByName(className);        } catch (Exception e) {        }      }            if(declaredClass == null) {        try {          declaredClass = Class.forName(className);        } catch (ClassNotFoundException e) {          throw new RuntimeException(""readObject can't find class "" + className, e);        }      }  {code}",io
create_write operation of nnbench is failing.,"create_write operation of nnbench (org.apache.hadoop.hdfs.NNBench) is not able to create control file. It is failing and displaying following error message:    Exception in thread ""main"" java.io.IOException: Mkdirs failed to create /<specified-path>/output/control          at org.apache.hadoop.hdfs.NNBench.createControlFiles(NNBench.java:154)          at org.apache.hadoop.hdfs.NNBench.main(NNBench.java:577)  ",test
Implement a pure Java CRC32 calculator,"We've seen a reducer writing 200MB to HDFS with replication = 1 spending a long time in crc calculation. In particular, it was spending 5 seconds in crc calculation out of a total of 6 for the write. I suspect that it is the java-jni border that is causing us grief.","performance,util"
TestBinaryPartitioner javac warnings.,This is introduced by HADOOP-5528.  All of them about using unparametrized types.,test
Final parameters in Configuration doesnt get serialized,Here are the steps to reproduce the bug  # Mark a parameter as _final_ in hadoop-site.xml  # Load the conf in some job  # Change the final parameter  # Write the conf to a file    The final parameter gets overridden upon serialization.,conf
TestCapacityScheduler fails with NPE,"Observed on Hudson:  {noformat}  java.lang.NullPointerException    org.apache.hadoop.mapred.JobInProgress.terminateJob(JobInProgress.java:2117)    org.apache.hadoop.mapred.JobInProgress.terminate(JobInProgress.java:2153)    org.apache.hadoop.mapred.JobInProgress.kill(JobInProgress.java:2221)    org.apache.hadoop.mapred.TestCapacityScheduler$FakeTaskTrackerManager.killJob(TestCapacityScheduler.java:359)    org.apache.hadoop.mapred.CapacityTaskScheduler.killJobIfInvalidRequirements(CapacityTaskScheduler.java:1431)    org.apache.hadoop.mapred.CapacityTaskScheduler.jobAdded(CapacityTaskScheduler.java:1463)    org.apache.hadoop.mapred.JobQueuesManager.jobAdded(JobQueuesManager.java:183)    org.apache.hadoop.mapred.TestCapacityScheduler$FakeTaskTrackerManager.submitJob(TestCapacityScheduler.java:387)    org.apache.hadoop.mapred.TestCapacityScheduler.submitJob(TestCapacityScheduler.java:625)    org.apache.hadoop.mapred.TestCapacityScheduler.testHighMemoryJobWithInvalidRequirements(TestCapacityScheduler.java:1992)  {noformat}    This was introduced by HADOOP-5565. FakeJobInProgress doesn't pass a JobTracker reference to the subtype cstr, so calling kill() derefs the null JT field.",test
Some c++ scripts are not chmodded before ant execution,"Before executing a lot of the configure scripts, there are lines like:      <chmod file=""${c++.libhdfs.src}/configure"" perm=""ugo+x""/>    that ensure the configure script is executable (since they seem to not always make it into the distribution in this form).    These chmods are missing for Pipes and C++ Utils which makes the build fail.",build
change S3Exception to checked exception,Currently the S3 filesystems can throw unchecked exceptions (S3Exception) which are not declared in the interface of FileSystem. These aren't caught by the various callers and can cause unpredictable behavior. IOExceptions are caught by most users of FileSystem since it is declared in the interface and hence is handled better.    I propose we modify S3Exception to extend IOException.,fs/s3
Spec file and SRPM for building a Hadoop-0.19.1 RPM,"I like the idea of Cloudera's RPMs, in that packages are convenient and the boot scripts are very handy, but they are for a patched 0.18.3 and include other stuff.  Here I offer a spec file for 0.19.1 without extra cruft.  It is essentially the spec from Cloudera's RPM with suitable edits.",build
make chukwa log4j configuration more transparent from hadoop,"The current log4j appender retro fitting to hadoop is less than ideal. In theory, the log4j appender configuration should be changable by the environment scripts.  This ticket is to track any changes required to make hadoop log4j configuration more portable.",build
add a Pingable interface with a Ping() method for checking the health of things,"I'm filing this as part of the plan for getting HADOOP-3628 checked in: separate Ping() from everything else so that it can go in later/separately, and apply to more bits of the code than just the node root threads . The various helper classes that they use underneath could be marked as Pingable so that the health check operations could move the workload down.    Use cases  # Pingable Filesystem and MapReduce services: namenode, datanode, job-tracker, task-tracker, etc  # Eventually : Pingable pig/cascading,HBase services  # Implementation of the ping operation in the services by calling their in-JVM classes and aggregating the results  # in JVM-health checks (JMX operations etc)  # Over RMI/REST Smartfrog health checks (my use case)  # Thrift and other wire format health checks   # Public HTTP checks that return an error code with XML or JSON output.   # Command line tools (that could check the HTTP pages)  # Make it easy to test    The current HADOOP-3628 ping() operation includes the service state from the proposed service lifecycle, and a list of nested exceptions; it only works in -VM. To work in more use cases  # It needs to move to a serialized exception format - the ThrowableWritable of HADOOP-5348.  # We need to consider how best to return the far-end's state.     I'm going offline for two weeks; here is somewhere for people to add their thoughts and work for me when I get back.",util
Create target for 10 minute patch test build,I think we should create an ant target that performs a smoke test on the patched system to enable developers to have faster turn around time on developing patches than the 3 hour unit tests that we currently have.,test
distributed cache doesn't work with other distributed file systems,"Currently the DistributedCache does a check to see if the file to be included is an HDFS URI. If the URI isn't in HDFS, it returns the default filesystem. This prevents using other distributed file systems -- such as s3, s3n, or kfs  -- with distributed cache. When a user tries to use one of those filesystems, it reports an error that it can't find the path in HDFS.",filecache
Update junit eclipse classpath,"Since the junit library is changed, the corresponding eclipse classpath should be updated.",build
Allow ServicePlugins to hook callbacks into key service events,"HADOOP-5257 added the ability for NameNode and DataNode to start and stop ServicePlugin implementations at NN/DN start/stop. However, this is insufficient integration for some common use cases.    We should add some functionality for Plugins to subscribe to events generated by the service they're plugging into. Some potential hook points are:    NameNode:    - new datanode registered    - datanode has died    - exception caught    - etc?    DataNode:    - startup    - initial registration with NN complete (this is important for HADOOP-4707 to sync up datanode.dnRegistration.name with the NN-side registration)    - namenode reconnect    - some block transfer hooks?    - exception caught    I see two potential routes for implementation:    1) We make an enum for the types of hookpoints and have a general function in the ServicePlugin interface. Something like:    {code:java}  enum HookPoint {    DN_STARTUP,    DN_RECEIVED_NEW_BLOCK,    DN_CAUGHT_EXCEPTION,   ...  }    void runHook(HookPoint hp, Object value);  {code}    2) We make classes specific to each ""pluggable"" as was originally suggested in HADDOP-5257. Something like:    {code:java}  class DataNodePlugin {    void datanodeStarted() {}    void receivedNewBlock(block info, etc) {}    void caughtException(Exception e) {}    ...  }  {code}    I personally prefer option (2) since we can ensure plugin API compatibility at compile-time, and we avoid an ugly switch statement in a runHook() function.    Interested to hear what people's thoughts are here.",util
After HADOOP-4920 we need a place to checkin releasenotes.html,Since HADOOP-4920 we no longer checkin our built documentation.  This means there is no longer a place to put releasenotes.html since HowToRelease wiki dictated that releasenotes.html be checked in to docs directory as part of the release process.    I'll create a build.xml patch that assumes release notes are in src/docs/releasenotes.html and copies them to build/docs/releasenotes.html,build
TestJobHistory fails if /tmp/_logs is not writable to. Testcase should not depend on /tmp,"TestJobHistory sets /tmp as hadoop.job.history.user.location to check if the history file is created in that directory or not. If /tmp/_logs is already created by some other user, this test will fail because of not having write permission.",test
Not able to generate gridmix.jar on already compiled version of hadoop,"Not able to generate gridmix.jar for gridmix2 on compiled version of hadoop (which is untarred  from hadoop binary) as builld.xml tries to find build directory in hadoop-home but ""build"" directory is not present on this path.    Following error is displayed :  Buildfile: build.xml    init:      [mkdir] Created dir: <hadoop-home>/src/benchmarks/gridmix2/build      [mkdir] Created dir: <hadoop-home>/src/benchmarks/gridmix2/dist    compile:      [javac] Compiling 3 source files to <hadoop-home>/src/benchmarks/gridmix2/build    BUILD FAILED  <hadoop-home>/src/benchmarks/gridmix2/build.xml:28: <hadoop-home>/build not found.",benchmarks
TestReplicationPolicy.<init> fails on java.net.BindException,java.net.BindException: Address already in use    sun.nio.ch.Net.bind(Native Method)    sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)    sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)    org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)    org.apache.hadoop.http.HttpServer.start(HttpServer.java:422)    org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:330)    org.apache.hadoop.hdfs.server.namenode.NameNode.activate(NameNode.java:277)    org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:269)    org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:373)    org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:367)    org.apache.hadoop.hdfs.server.namenode.TestReplicationPolicy.<clinit>(TestReplicationPolicy.java:58)    java.lang.Class.forName0(Native Method)    java.lang.Class.forName(Class.java:169)    org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:335)    org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)    org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)    Looks that it was caused by another running unit test. They used the same http server port.,test
Counter for S3N Read Bytes does not work,Counter for S3N Read Bytes does not work on trunk. On 0.18 branch neither read nor write byte counters work. ,fs/s3
Validate data passed through TestReduceFetch,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",test
Eclipse templates fail out of the box; need updating,"The Hadoop templates, when run ""out of the box"" have Eclipse build ""problems"".  I've produced a small patch that fixes it.",build
Hadoop configurations should be read from a distributed system,"Rather than distributing the hadoop configuration files to every data node, compute node, etc, Hadoop should be able to read configuration information (dynamically!) from LDAP, ZooKeeper, whatever.",conf
Mixed up #includes in c++ files.,"src/c++/pipes/impl/HadoopPipes.cc, src/c++/utils/impl/SerialUtils.cc, and src/c++/utils/impl/StringUtils.cc fail to compile due to some mixed up #includes.    When running ant -Dcompile.c++=true clean compile-c++ on svn trunk, I get errors about undeclared functions in the above files, such as:    {quote}       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc: In function 'uint64_t HadoopUtils::getCurrentMillis()':       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc:74: error: 'strerror' was not declared in this scope       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc: In function 'std::string HadoopUtils::quoteString(const std::string&, const char*)':       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc:103: error: 'strchr' was not declared in this scope       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc: In function 'std::string HadoopUtils::unquoteString(const std::string&)':       [exec] /home/ricky/h/test/hadoop-0.18.3/src/c++/utils/impl/StringUtils.cc:144: error: 'strtol' was not declared in this scope  {quote}",build
ant clean does not clean the generated api docs,None,build
We often end up with null release information,Often in the web/ui we see the version as null. I suspect something goes wrong in the build script that deletes the generated attribute file.,build
hadoop-config.sh blows away CLASSPATH env,"The bin/hadoop-config.sh script blows away the user's CLASSPATH environment variable when setting up the hadoop-specific classpath. It's possible that this is intentional, but it would make developing contrib service plugins (eg HADOOP-4707) easier if hadoop-config appended rather than replaced CLASSPATH.    Attaching trivial patch to change this behavior.",scripts
Scheduler test code does not compile,HADOOP-5661 removed a deprecated constructor in ClusterStatus without updating callers in {{TestFairScheduler}} or {{TestCapacityScheduler}},test
Configuration should provide a way to write only properties that have been set,"The Configuration.write and .writeXml methods always output all properties, whether they came from a default source, a loaded resource file, or an ""overlay"" set call.  There should be a way to write only the properties that were set, leaving out the properties that came from a default source.    Why?  Suppose I build a configuration on a machine that is not associated with a grid, write it out to XML, then try to load it on a grid gateway.  The configuration would contain all of the defaults picked up from my non-grid machine, and would completely overwrite all the defaults on that grid.    I propose to add methods to write out only the overlay values in Object and XML formats.    I see two options for implementing this:  1) Either completely new methods could be crafted (writeOverlay(DataOutput) and writeOverlayXml(OutputStream), or   2) The existing write() and writeXml() methods could be adjusted to take an additional parameter indicating whether the full properties or overlay properties should be written.  (Of course, the existing write() and writeXml() methods would remain, defaulting to the current behavior.)    Option 1 has less impact to existing code.  Option 2 is a cleaner implementation with less code-duplication involved.  I would much prefer to do option 2.    Oh, and in case it's not clear, I'm offering to make this change and submit it.    Thoughts?    .  Topher",conf
Metric to show number of fs.exists (or number of getFileInfo) calls,"In HADOOP-5712, it would have helped our debugging if I had this metric.",metrics
Should conf/mapred-queue-acls.xml be added to the ignore list?,conf/mapred-queue-acls.xml is auto-generated by ant but it becomes an unknown file to svn.  {noformat}  $ svn status  $ ant  ...  BUILD SUCCESSFUL  Total time: 10 seconds  $ svn status  ?      conf/mapred-queue-acls.xml  {noformat},conf
HTTP metrics interface enable/disable must be configurable,"HADOOP-5469 added a convenient end-run around JMX authentication by revealing the same metrics over HTTP. That's cool, but we need to secure all accesses to our Hadoop cluster, so while this may be enabled by default, we need some configurable way to disable the unauthenticated port.  ","metrics,security"
Changelog is inconsistent with commits,Recent branching has confused some of the entries in the changelog. A quick audit of recent releases would be useful,documentation
IPC call can raise security exceptions when the remote node is running under a security manager,"I'm getting a security exception (java.lang.reflect.ReflectPermission suppressAccessChecks) in RPC.Server.call(), when calling a datanode brought up under a security manager, in method.setAccessible(true)    ",ipc
Add SFTP FileSystem,I have implemented a FileSystem that supports SFTP. It uses JSch (http://www.jcraft.com/jsch/) in order to manage SFTP.,fs
Add map/reduce slot capacity and lost map/reduce slot capacity to JobTracker metrics,It would be nice to have the actual map/reduce slot capacity and the lost map/reduce slot capacity (# of blacklisted nodes * map-slot-per-node or reduce-slot-per-node). This information can be used to calculate a JT view of slot utilization.,metrics
HDFS architecture documentation describes outdated placement policy,"The ""Replica Placement: The First Baby Steps"" section of HDFS architecture document states:    ""...  For the common case, when the replication factor is three, HDFS's placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance.  ...""    However, according to the ReplicationTargetChooser.chooseTarger()'s code the actual logic is to put the second replica on a different rack as well as the third replica. So you have two replicas located on a different nodes of remote rack and one (initial replica) on the local rack's node. Thus, the sentence should say something like this:    ""For the common case, when the replication factor is three, HDFS's placement policy is to put one replica on one node in the local rack, another on a node in a different (remote) rack, and the last on a different node in the same remote rack. This policy cuts the inter-rack write traffic which generally improves write performance.""  ",documentation
UGI checks in testcases are broken,"While running {{TestMiniMRWithDFSWithDistinctUsers}}, I used this patch to test the ugi checks   {code}  Index: src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java  ===================================================================  --- src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java (revision 768189)  +++ src/hdfs/org/apache/hadoop/hdfs/server/namenode/PermissionChecker.java (working copy)  @@ -40,6 +40,7 @@       if (LOG.isDebugEnabled()) {         LOG.debug(""ugi="" + ugi);       }  +    LOG.info(""ugi="" + ugi);          if (ugi != null) {         user = ugi.getUserName();  {code}  While initializing a job, the ugi information should point to jobtracker as jobtracker does a dfs read. But today we will see that the log shows _pi_ as the caller instead of the jobtracker.","security,test"
Split waiting tasks field in JobTracker metrics to individual tasks,"Currently, job tracker metrics reports waiting tasks as a single field in metrics. It would be better if we can split waiting tasks into maps and reduces. ",metrics
Configuration: Hierachical; documented; null-safe; thread-safe,"Patch to configuration to:    * Update documentation to match code  * Make configurations hierarchical  * Remove global REGISTRY of configurations  * Adding global default does not reload child configurations  * (Eventually) make properly thread-safe  * Allow well-behaved overriding with nulls  * Allow future satisfaction of ticket 5708 (with which I agree, but Doug does not).",conf
FsShell  forces creation of default filesystem object and incorrectly uses default filesystem in some places,"i was trying do fs -ls operations on s3n filesystem uris (which was not configured as my default filesystem - the default filesystem was not bought up). these operations failed since they always tried to open the default filesystem (error: """"Bad connection to FS. command aborted.""). but the default filesystem is not required in these cases at all.    the code also shows that the default filesystem is used in certain places (instead of the filesystem of the uri). for instance:    copy(String argv[], ...) { ...         Path dst = new Path(dest);         if (!fs.isDirectory(dst)) {     there are some other places as well.",fs
allow user to specify ami image for hadoop ec2 launch script,ec2 launch scripts don't allow user to specify an ami of their choice (other than by modifying the scripts of course). it would be preferable if the standard script allowed for an override via env variables - it would be easier to build wrapper scripts that launched other amis if this were allowed.,contrib/cloud
Add link to training from website,"Add a link to training videos from the getting started section (like Pig, see http://hadoop.apache.org/pig/).",documentation
Task process hanging on an RPC call,"On a random node on a cluster, I found one task process waiting on an RPC call. The process has been in that state for a few days at least.    ""main"" prio=10 tid=0x08069400 nid=0x6f52 in Object.wait() [0xf7e6c000..0xf7e6d1f8]     java.lang.Thread.State: WAITING (on object monitor)          at java.lang.Object.wait(Native Method)          - waiting on <0xf1215700> (a org.apache.hadoop.ipc.Client$Call)          at java.lang.Object.wait(Object.java:485)          at org.apache.hadoop.ipc.Client.call(Client.java:725)          - locked <0xf1215700> (a org.apache.hadoop.ipc.Client$Call)          at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)          at org.apache.hadoop.mapred.$Proxy0.statusUpdate(Unknown Source)          at org.apache.hadoop.mapred.Task.statusUpdate(Task.java:691)          at org.apache.hadoop.mapred.Task.taskCleanup(Task.java:795)          at org.apache.hadoop.mapred.Child.main(Child.java:176)",ipc
Create unit test for LinuxTaskController,Add unit tests to test {{LinuxTaskController}} functionality introduced by HADOOP-4490,"security,test"
Implement a 'refreshable' configuration system with right access-controls etc.,"We have various bits and pieces of code to refresh certain configuration files, various components to restrict access to who can actually refresh the configs etc.     I propose we start thinking about a simple system to support this as a first-class citizen in Hadoop Core...",conf
Allow HADOOP_ROOT_LOGGER to be configured via conf/hadoop-env.sh,Currently it's set in bin/hadoop-daemon.sh... we should allow it to be specified in conf/hadoop-env.sh,scripts
to resolve jsp-2.1 jars through IVY,None,build
Capacity scheduler test case is failing due to incorrect cleanup of svn workspace,"TestCapacitySchedulerConf.testDefaults is failing because of the commit of HADOOP-5726. This is because the patch changed capacity-scheduler.xml.template and that necessitates a copy to the capacity-scheduler.xml, which is not tracked under version control. Since the build process does not remove files not tracked by SVN, the file is not removed, and the move not done. Hence hudson builds are reporting a failure.    A discussion of this is in HADOOP-5721 (where the test failure was noticed).",build
issue creating buffer directory when using native S3FileSystem,"Note the following settings in hadoop-site.xml, which  means I've configured 40 reduce tasks to run across 20 nodes. I checked Jira, and found HADOOP-4377, which refers to this as a race condition that has been fixed, the bug was closed. I'm re-opening because this may be configuration specific, e.g. I am running multiple reduce tasks on the same node (no mention was made of that in the original bug)    <property>    <name>mapred.reduce.tasks</name>    <value>40</value>  </property>  <property>    <name>mapred.tasktracker.reduce.tasks.maximum</name>    <value>4</value>    <final>true</final>  </property>    One of the tasks fails trying to create the S3 buffer directory:      attempt_200905111058_0001_r_000019_0: 1589020 [main] WARN org.apache.hadoop.mapred.TaskTracker  - Error running child  attempt_200905111058_0001_r_000019_0: java.io.IOException: Cannot create S3 buffer directory: /mnt/tmp/hadoop-hadoop/s3  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.newBackupFile(NativeS3FileSystem.java:152)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream.<init>(NativeS3FileSystem.java:136)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.fs.s3native.NativeS3FileSystem.create(NativeS3FileSystem.java:278)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:503)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:403)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:117)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:293)  attempt_200905111058_0001_r_000019_0:   at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)",fs/s3
Allow native libraries to be placed anywhere,"Currently, Hadoop only includes native libraries from the build/native and lib/native directories.  I would like to be able to include them from arbitrary directories (such as /usr/lib, /usr/lib64, etc.) as RPMs on which I depend may place them anywhere.",conf
neither s3.block.size not fs.s3.block.size are honoured,"S3FileSystem does not override FileSystem.getDefaultBlockSize(), so the s3 default block size is actualy controlled by fs.local.block.size    As far as I can see, the s3 block size specific parameters (either with or without the fs. prefix) are read nowhere.",fs/s3
problem using top level s3 buckets as input/output directories,"When I specify top level s3 buckets as input or output directories, I get the following exception.    hadoop jar subject-map-reduce.jar s3n://infocloud-input s3n://infocloud-output    java.lang.IllegalArgumentException: Path must be absolute: s3n://infocloud-output          at org.apache.hadoop.fs.s3native.NativeS3FileSystem.pathToKey(NativeS3FileSystem.java:246)          at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:319)          at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:667)          at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:109)          at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:738)          at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1026)          at com.evri.infocloud.prototype.subjectmapreduce.SubjectMRDriver.run(SubjectMRDriver.java:63)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at com.evri.infocloud.prototype.subjectmapreduce.SubjectMRDriver.main(SubjectMRDriver.java:25)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.util.RunJar.main(RunJar.java:155)          at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)          at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)    The workaround is to specify input/output buckets with sub-directories:       hadoop jar subject-map-reduce.jar s3n://infocloud-input/input-subdir  s3n://infocloud-output/output-subdir    ",fs/s3
NativeS3FileSystem doesn't report progress when writing,"This results in timeouts since the whole file is uploaded in the close method. See http://www.mail-archive.com/core-user@hadoop.apache.org/msg09881.html.    One solution is to keep a reference to the Progressable passed in to the NativeS3FsOutputStream's constructor, and progress it during writes, and while copying the backup file to S3 in the close method.",fs/s3
Fix javac warnings in several dfs tests related to unncessary casts,There are quite a few unnecessary casts as reported in javac in the following files:  /src/test/OAH/hdfs/TestDataTransferProtocol.java  /src/test/OAH/hdfs/TestFSInputChecker.java  /src/test/OAH/hdfs/TestFileAppend.java  /src/test/OAH/hdfs/TestPread.java  /src/test/OAH/hdfs/server/namenode/TestNodeCount.java  ,test
"Handling javac ""deprecated"" warning for using UTF8",  o.a.h.io.UTF8 is deprecated but is still used in multiple places. FSEditLog.java has 40 UTF8 related warnings. I don't think it is feasible to avoid using UTF8 in FSEditLog.java.     Two options to get rid of these warnings :    1. use @SupressWarnings at each use of UTF or for enclosing class.    2. define a wrapper class {{DeprecatedUTF8}} that is not {{@deprecated}}.     I prefer the second option in this case since it keeps FSEditLog.java and other places clean and still makes it explicit that a deprecated class is used.    This is part of spring cleaning effort to remove warnings in javac. I will attach a patch for the second option.,build
Use absolute path for JobTracker's mapred.local.dir in MiniMRCluster,"MiniMRCluster's JobTracker sets the mapred.local.dir to a relative path: ""build/test/mapred/local"". While currently this does not cause any problem, if at some time in future, the JobTracker starts using LocalDirAllocator for localizing files, this could cause failures in contrib tests. This issue was faced when working with an internal patch that tried to introduce LocalDirAllocator into the JobTracker, and capacity scheduler test cases started failing.",test
"Bug in S3N handling of directory markers using an object with a trailing ""/"" causes jobs to fail","Some tools which upload to S3 and use a object terminated with a ""/"" as a directory marker, for instance ""s3n://mybucket/mydir/"". If asked to iterate that ""directory"" via listStatus(), then the current code will return an empty file """", which the InputFormatter happily assigns to a split, and which later causes a task to fail, and probably the job to fail. ",fs/s3
fixes to ec2 scripts to allow remote job submission,i would very much like the option of submitting jobs from a workstation outside ec2 to a hadoop cluster in ec2. This has been explored here:    http://www.nabble.com/public-IP-for-datanode-on-EC2-tt19336240.html    the net result of this is that we can make this work (along with using a socks proxy) with a couple of changes in the ec2 scripts:  a) use public 'hostname' for fs.default.name setting (instead of the private hostname being used currently)  b) mark hadoop.rpc.socket.factory.class.default as final variable in the generated hadoop-site.xml (that applies to server side)    #a has no downside as far as i can tell since public hostnames resolve to internal/private IP addresses within ec2 (so traffic is optimally routed).,contrib/cloud
Build successful despite test failure on test-core target,"{{ant -Dtestcase=TestFoo test-core}} will succeed, even if TestFoo fails. Running the same test with the run-test-mapred target failed the build, as expected.",test
JMX Metrics For TaskTracker,"Create useful metrics for the TaskTracker Component.   * TasksStarted: Counter  * TasksFailed: Counter  If graphed, having these metrics will help administrators determine graphically how task load is being spread. Also will help determine if certain nodes have external conditions that are causing a task to fail.",metrics
JMX Metrics For JobTracker,The Job Tracker Should have JMX Metrics for monitoring performance    * JobsSubmitted: counter  * JobsSuccessful: counter  * JobsFailed: counter    * TotalTasksCreated: counter  * TotalTasksSuccessful: counter  * TotalTasksFailed    These can be used to monitor the JobTracker performance.,metrics
Eliminate UTF8 and fix warnings in test/hdfs-with-mr package,Replace UTF8 with Text and fix java warnings not related to deprecated mapred api.  Warnings related to the deprecated map reduce api should probably be targeted in a separate unified approach.,test
s3n files are not getting split by default ,"running with stock ec2 scripts against hadoop-19 - i tried to run a job against a directory with 4 text files - each about 2G in size. These were not split (only 4 mappers were run).    The reason seems to have two parts - primarily that S3N files report a block size of 5G. This causes FileInputFormat.getSplits to fall back on goal size (which is totalsize/conf.get(""mapred.map.tasks"")).Goal Size in this case was 4G - hence the files were not split. This is not an issue with other file systems since the block size reported is much smaller and the splits get based on block size (not goal size).    can we make the S3N files report a more reasonable block size?",fs/s3
Cleaning NNBench* off javac warnings,These files have a number of javac 'class depricated' warnings     src/test/hdfs-with-mr/org/apache/hadoop/hdfs/NNBench.java    src/test/hdfs-with-mr/org/apache/hadoop/hdfs/NNBenchWithoutMR.java  It is possible to fix most of them plus make some readability improvements on the code.,test
"Fix javac warnings in TestHDFSServerPorts, TestCheckpoint, TestNameEditsConfig, TestStartup and TestStorageRestore","All of these warnings relate to use of the deprecated SecondaryNameNode.  Since the 2ndNN isn't going to be excised anytime soon, these should all be suppressed with as narrow of a suppression scope as possible.",test
GzipCodec should read compression level etc from configuration,"GzipCodec currently uses the default compression level. We should allow overriding the default value from Configuration.    {code}    static final class GzipZlibCompressor extends ZlibCompressor {      public GzipZlibCompressor() {        super(ZlibCompressor.CompressionLevel.DEFAULT_COMPRESSION,            ZlibCompressor.CompressionStrategy.DEFAULT_STRATEGY,            ZlibCompressor.CompressionHeader.GZIP_FORMAT, 64*1024);      }    }  {code}  ",io
Change some MiniMR based mapred test cases to use local file system instead of MiniDFS,"Some of the MiniMR test cases could be changed to use the local file system instead of using DFS without any loss of testing functionality. This possibly could lead to a decent amount of time saving, when aggregated across several tests. For example, TestJobDirCleanup on my local box took 60 seconds with DFS and 50 seconds with localFS. ",test
"Allow writing to output directories that exist, as long as they are empty","The current behavior in FileOutputFormat.checkOutputSpecs is to fail if the path specified by mapred.output.dir exists at the start of the job. This is to protect from accidentally overwriting existing data. There seems no harm then in slightly relaxing this check to allow the case for the output to exist if it is an empty directory.    At a minimum this would allow outputting to the root of S3N buckets, which is currently impossible (https://issues.apache.org/jira/browse/HADOOP-5805).",fs
"If dfs.http.address is default, SecondaryNameNode can't find NameNode","As detailed in this blog post:  http://www.cloudera.com/blog/2009/02/10/multi-host-secondarynamenode-configuration/  if dfs.http.address is not configured, and the 2NN is a different machine from the NN, the 2NN fails to connect.    In SecondaryNameNode.getInfoServer, the 2NN should notice a ""0.0.0.0"" dfs.http.address and, in that case, pull the hostname out of fs.default.name. This would fix the default configuration to work properly for most users.",fs
ChecksumFileSystem is ignoring 0 byte CRC files,"We saw an issue where the original file is not empty but the corresponding CRC file is empty (do not know why, could be because the process that wrote the file crashed in between). While reading, fs.open got a EOFException when trying to read the checksum version from the CRC file and crc validation was disabled for the file. Since the original intention was to have the CRC validations for this file, should we just fail here instead of ignoring the exception?  ",fs
Add more Metrics to Namenode to capture heap usage,"Recently we had GC issues, where Namenode used more heap than usual. There was no growth indicated by the data in current Metrics to justify the heap usage. Adding more stats such as:  - Counter to track blocks that are pending deletion  - BlocksMap hashmap capacity  - Counter to track excess number of blocks   ",metrics
Hadoop TOP Site - new subprojects,"Hadoop TOP Site (not Hadoop Core Site)    Changes:  1. What Is Hadoop?    Added new subprojects - Avro, HDFS, MapReduce (Chukwa already there)    Edited blurbs for all subprojects to 1 line    2. Who Uses Hadoop?    Cleaned up    3. News    Added new section about New Hadoop Subprojects    Cleaned up old sections    4. OTHER    Menu - added Hama under Related Projects    Applied new CSS header styles ....    Note: I use these URLS in the xml doc files (for the new subprojects)    HDFS: http://hadoop.apache.org/hdfs/    MapReduce: http://hadoop.apache.org/mapreduce/    ",documentation
Minor correction in HDFS Documentation ,Remove incorrect statement from HDFS Document ( hdfs_design.html )  which says -    * HDFS does not yet implement user quotas or access permissions.*  ,documentation
FileSystem.fixName() has unexpected behaviour,"{{FileSystem.fixName()}} tries to patch up fs.default.name values, but I'm not sure it helps that well.     Has it been warning about deprecated values for long enough for it to be turned off? ",fs
"Adding Eclipse ""launch"" tasks for Hadoop daemons","Eclipse has a notion of ""run configuration"", which encapsulates what's needed to run or debug an application.  I use this quite a bit to start various Hadoop daemons in debug mode, with breakpoints set, to inspect state and what not.    Next comment will include a patch.",build
Standardize fall-back value of test.build.data for testing directories,"Currently when the test.build.data property is not set, as happens when run through some configurations of Eclipse, the fall-back value varies wildly.  Most calls set to /tmp, which is not good as it is beyond the scope of the ant clean task and thus will not be deleted.  Others default to ""."" which can drop the test files right in the current directory.  Speaking with Konstanin, it seems the correct location should be ""build/test/data"" to ensure any files that are created are within the scope of Ant's clean command.    Attached is the current variation in this setting.",test
Testpatch isn't catching newly introduced javac warnings,"Testpatch doesn't seem to be catching newly introduced javac warnings, as detailed in the results of the experiment below.  ",test
EC2 scripts should exit on error,"For example, if an ec2-authorize command fails the script should stop so that the problem is easier to debug.",contrib/cloud
Hudson's release audit warnings link is broken,"For example, on HADOOP-5170 the link http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/392/artifact/trunk/current/releaseAuditDiffWarnings.txt gives a 404. This makes it hard to work out which file or files are causing a problem.",build
trunk eclipse-plugin build fails while trying to copy commons-cli jar from the lib dir,None,contrib/eclipse-plugin
OUtils#copyBytes methods should not close streams that are passed in as parameters,"The following methods in IOUtils close the streams that are passed in as parameters. Calling these methods can easily trigger findbug OBL: Method may fail to clean up stream or resource (OBL_UNSATISFIED_OBLIGATION). A good practice should be to close a stream in the same method where the stream is opened.     public static void copyBytes(InputStream in, OutputStream out, int buffSize, boolean close)   public static void copyBytes(InputStream in, OutputStream out, Configuration conf, boolean close)    These methods should be deprecated.",io
Modify TestJavaSerialization to use LocalJobRunner instead of MiniMR/DFS cluster,TestJavaSerialization currently uses MiniMR/DFS cluster. This test can be done with local job runner also. This reduces the run time of the test from 61 seconds to 6 seconds.,test
Hudson -1 wording change,The wording should be changed when Hudson -1 a patch for no unit test updates.  New wording to be be added in comments.,build
KosmosFileSystem.isDirectory() should not be deprecated.,FileSystem.isDirectory() was un-deprecated by HADOOP-5045.  We should do the same for KosmosFileSystem.,fs
Fix javac warnings in HDFS tests,There are javac warnings in the following tests:  {noformat}  src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java  src/test/hdfs/org/apache/hadoop/hdfs/TestSmallBlock.java  src/test/hdfs/org/apache/hadoop/hdfs/TestFileStatus.java  src/test/hdfs/org/apache/hadoop/hdfs/TestDFSShellGenericOptions.java  src/test/hdfs/org/apache/hadoop/hdfs/TestSeekBug.java  src/test/hdfs/org/apache/hadoop/hdfs/TestDFSStartupVersions.java  {noformat},test
TestFileOuputFormat can use LOCAL_MR instead of CLUSTER_MR,TestFileOutputFormat can use local MR instead of MiniMR. This brings down the execution time from 32 seconds to 3 seconds,test
org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface test fails on trunk,None,build
Use JDK 1.6 File APIs in DF.java wherever possible,JDK 1.6 has File APIs like File.getFreeSpace() which should be used instead of spawning a command process for getting the various disk/partition related attributes. This would avoid spikes in memory consumption by tasks when things like LocalDirAllocator is used for creating paths on the filesystem.,fs
Support availability zone in EC2 scripts,It would be convenient to be able to control which availability zone to launch instances in.,contrib/cloud
fs tests should not be placed in hdfs.,"The following tests are under the org.apache.hadoop.fs package but were moved to hdfs sub-directory by HADOOP-5135:  {noformat}  ./org/apache/hadoop/fs/ftp/TestFTPFileSystem.java  ./org/apache/hadoop/fs/loadGenerator/TestLoadGenerator.java  ./org/apache/hadoop/fs/permission/TestStickyBit.java  ./org/apache/hadoop/fs/TestGlobPaths.java  ./org/apache/hadoop/fs/TestUrlStreamHandler.java  {noformat}  - Some of them are not related to hdfs, e.g. TestFTPFileSystem. These files should be moved out from hdfs and should not use hdfs codes.  - Some of them are testing hdfs features, e.g. TestStickyBit. They should be defined under org.apache.hadoop.hdfs package.  ",test
unnecessary exception catch in NNBench,"NNBench.createControlFiles - catches Exception (even though only IOException is thrown) and then throws a new IOException with message from original. This seems unnecessary and lossfull, because we loose stack info form the original exception.    Suggestion - remove the catch part of try block.",test
create script to provide classpath for external tools,It would be useful for tools building on top of Hadoop to have a script that returns the class path that is needed to get all of the Hadoop jars and the needed libraries.,scripts
Namenode shouldn't read mapred-site.xml,"The name node seem to read mapred-site.xml and fails if it can't parse it.    2009-06-05 22:37:15,663 FATAL org.apache.hadoop.conf.Configuration: error parsing conf file: org.xml.sax.SAXParseException: Error attempting to parse XML file (href='/hadoop/conf/local/local-mapred-site.xml').  2009-06-05 22:37:15,664 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.RuntimeException: org.xml.sax.SAXParseException: Error attempting to parse XML file (href='/hadoop/conf/local/local-mapred-site.xml').    In our config,  local-mapred-site.xml is included only in mapred-site.xml which we don't push to the namenode.",conf
gridmix-env-2 should not have fixed values for HADOOP_VERSION and HADOOP_HOME,"""gridmix-env-2 "" of gridmix2  is having fix old values for HADOOP_VERSION and HADOOP_HOME which override the default environment settings. These lines should be either commented or no value should be specified to them (templates only) as is the case with ""gridmix-env"" of gridmix.",benchmarks
Many test jobs write to HDFS under /,"Many test jobs such as testmapredsort, TestDFSIO and nnbench try to write to HDFS under root.   If a user 'X' brings up the cluster and gives full access to user 'Y' under /user/Y, user Y's test jobs still will not run because they demand access to / which cannot be granted. Such jobs should be modified to write their temp outputs under /user/Y and not directly under /",test
BlockLocation deserialization is incorrect,The {{readFields}} method of {{BlockLocation}} does not correctly allocate new arrays for topologyPaths and hosts. Patch shortly.,fs
S3N listStatus incorrectly returns null instead of empty array when called on empty root,"Null means the directory does not exist, which is obviously not the case.",fs/s3
Remove @author tags from Java source files,"Excluding first 2 results there are total 28 @author tags in java source files .    {code}  hostname:Hadoop rphulari$ grep -r ""@author""  .  ./.svn/text-base/CHANGES.txt.svn-base: 69. HADOOP-1147.  Remove @author tags from Java source files.  ./CHANGES.txt: 69. HADOOP-1147.  Remove @author tags from Java source files.  ./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/IFSImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSInputStream.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSOutputStream.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/.svn/text-base/KosmosFileSystem.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/IFSImpl.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/KFSImpl.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/KFSInputStream.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/KFSOutputStream.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/core/org/apache/hadoop/fs/kfs/KosmosFileSystem.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:### Check for @author tags in the patch  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  echo ""    Checking there are no @author tags in the patch.""  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  authorTags=`$GREP -c -i '@author' $PATCH_DIR/patch`  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:  echo ""There appear to be $authorTags @author tags in the patch.""  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:    -1 @author.  The patch appears to contain $authorTags @author tags which the Hadoop community has agreed to not allow in code contributions.""  ./src/test/bin/.svn/text-base/test-patch.sh.svn-base:    +1 @author.  The patch does not contain any @author tags.""  ./src/test/bin/test-patch.sh:### Check for @author tags in the patch  ./src/test/bin/test-patch.sh:  echo ""    Checking there are no @author tags in the patch.""  ./src/test/bin/test-patch.sh:  authorTags=`$GREP -c -i '@author' $PATCH_DIR/patch`  ./src/test/bin/test-patch.sh:  echo ""There appear to be $authorTags @author tags in the patch.""  ./src/test/bin/test-patch.sh:    -1 @author.  The patch appears to contain $authorTags @author tags which the Hadoop community has agreed to not allow in code contributions.""  ./src/test/bin/test-patch.sh:    +1 @author.  The patch does not contain any @author tags.""  ./src/test/core/org/apache/hadoop/fs/kfs/.svn/text-base/KFSEmulationImpl.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/test/core/org/apache/hadoop/fs/kfs/.svn/text-base/TestKosmosFileSystem.java.svn-base: * @author: Sriram Rao (Kosmix Corp.)  ./src/test/core/org/apache/hadoop/fs/kfs/KFSEmulationImpl.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/test/core/org/apache/hadoop/fs/kfs/TestKosmosFileSystem.java: * @author: Sriram Rao (Kosmix Corp.)  ./src/test/hdfs/org/apache/hadoop/hdfs/.svn/text-base/TestModTime.java.svn-base: * @author Dhruba Borthakur  ./src/test/hdfs/org/apache/hadoop/hdfs/TestModTime.java: * @author Dhruba Borthakur  hostname:Hadoop rphulari$ grep -r ""@author""  . | wc -l        30        {code}",documentation
reference to java1.5 in conf/hadoop-env.sh,conf/hadoop-env.sh has a commented out line    # export JAVA_HOME=/usr/lib/j2sdk1.5-sun    that path should be moved up to java 1.6,conf
Factor out job submission code in testcases into UtilsForTests,I still see code blocks that handcraft wordcount. Ideally we should move it into UtilsForTests.,test
Use java.net.preferIPv4Stack to force IPv4,"This was mentioned on HADOOP-3427, there is a property,  java.net.preferIPv4Stack, which you set to true for the java net process to switch to IPv4 everywhere.     As Hadoop doesn't work on IPv6, this should be set to true in the startup scripts. Hopefully this will ensure that Jetty will also pick it up.",scripts
TestRunningTaskLimit doesnt work as expected,"I see the following code in TestRunningTaskLimit  {code}  JobConf jobConf = createWaitJobConf(mr, ""job1"", 20, 20);      jobConf.setRunningMapLimit(5);      jobConf.setRunningReduceLimit(3);            // Submit the job      RunningJob rJob = (new JobClient(jobConf)).submitJob(jobConf);            // Wait 20 seconds for it to start up      UtilsForTests.waitFor(20000);            // Check the number of running tasks      JobTracker jobTracker = mr.getJobTrackerRunner().getJobTracker();      JobInProgress jip = jobTracker.getJob(rJob.getID());      assertEquals(5, jip.runningMaps());      assertEquals(3, jip.runningReduces());  {code}    This check is timing based and might not work as expected. Instead we can run a job with > 5 maps (all waiting) and then wait for the job to reach a stable state and then test if exactly 5 maps were scheduled or not.",test
TestJobTrackerSafeMode might not work as expected,It failed on trunk for me. Looks like the mapred.tasktracker.expiry.interval is set to 5sec which is too less. I think we should carefully set it. ,test
Cannot build HDFS library using ant,"As latest revison of Scribe requires trunk source code from Hadoop, I have to build the HDFS library manually using ant.  I notice that the target name was changed to compile-c++-libhdfs ( it was compile-libhdfs before ).    Then I use the following command :   $ ant compile-c++-libhdfs -Dlibhdfs=1  $ ant package  $ ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1    I got this error message :   /root/hadoop_trunk/src/contrib/fuse-dfs/build.xml:37: libhdfs.so does not exist:  Seems that the libhdfs.so should be built after ""ant package"" but it's not.",scripts
TestDFSIO does not use configuration properly.,A part of TestDFSIO uses default conf instead of configuration passed in by MR framework. ,benchmarks
TestTaskTrackerMemoryManager fails with NPE,Here is the error  {code}  null  java.lang.NullPointerException          at org.apache.hadoop.mapred.TestTaskTrackerMemoryManager.testTasksBeyondLimits(TestTaskTrackerMemoryManager.java:256)  {code},test
Forrest documentation compilation is broken because of HADOOP-5913,"Forrest documentation is broken because of the commit of HADOOP-5913. The is causing test-patch to break with this error:    {noformat}       [exec] validate-xdocs:       [exec] ../src/docs/src/documentation/content/xdocs/commands_manual.xml:593:11: The element type ""td"" must be terminated by the matching end-tag ""</td>"".  {noformat}",documentation
test-patch target does not validate that forrest docs are built correctly,"The test-patch target does not explicitly check if the new patch breaks forrest documentation. It actually runs the 'tar' target while checking for javac warnings, but does not seem to validate if the target ran successfully or not.",build
Handling of  Trash with quota,"Currently with quota turned on, user cannot call '-rmr' on large directory that causes over quota.    {noformat}  [knoguchi src]$ hadoop dfs -rmr /tmp/net2  rmr: Failed to move to trash: hdfs://abc.def.com/tmp/net2  [knoguchi src]$ hadoop dfs -mv /tmp/net2 /user/knoguchi/.Trash/Current  mv: org.apache.hadoop.hdfs.protocol.QuotaExceededException: The quota of /user/knoguchi is exceeded: namespace  quota=37500 file count=37757, diskspace quota=-1 diskspace=1991250043353  {noformat}    Besides from error message being unfriendly, how should this be handled?  ",fs
GridMix is broke after upgrading random(text)writer to newer mapreduce apis,GridMix data generation scripts need to use the newer mapreduce api.,benchmarks
No space left on device,"Exception in thread ""main"" org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device          at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)          at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)          at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)          at java.io.FilterOutputStream.close(FilterOutputStream.java:140)          at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)          at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)          at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.close(ChecksumFileSystem.java:339)          at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)          at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)          at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:825)          at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)          at org.apache.nutch.indexer.Indexer.index(Indexer.java:72)          at org.apache.nutch.indexer.Indexer.run(Indexer.java:92)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at org.apache.nutch.indexer.Indexer.main(Indexer.java:101)  Caused by: java.io.IOException: No space left on device          at java.io.FileOutputStream.writeBytes(Native Method)          at java.io.FileOutputStream.write(FileOutputStream.java:260)          at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)",io
Fix Eclipse project and classpath files following project split,The Eclipse files need updating to use new paths following the split.,build
Multiple bugs w/ Hadoop archives,"Found and fixed several bugs involving Hadoop archives:    - In makeQualified(), the sloppy conversion from Path to URI and back mangles the path if it contains an escape-worthy character.    - It's possible that fileStatusInIndex() may have to read more than one segment of the index. The LineReader and count of bytes read need to be reset for each block.    - har:// connections cannot be indexed by (scheme, authority, username) -- the path is significant as well. Caching them in this way limits a hadoop client to opening one archive per filesystem. It seems to be safe not to cache them, since they wrap another connection that does the actual networking.  ",fs
Allow configuring the IPC module to send pings,"The IPC Client sets a socketTimeout for the time period specified by the pingInterval and then sends a ping every pingInterval. This means that if a RPC server does not respond to a RPC client, then the RPC client blocks forever. This is a problem for applications that wants to switch quickly from one un-responsive HDFS cluster  to a good one. ",ipc
CDPATH environment variable causes bin scripts to fail.,"Most of the scripts in bin/* assume that cd produces no output. But when using bash (and some other shells) cd will output the destination directory if the CDPATH environment variable is set. CDPATH is very useful, and it's unfortunate to have to unset it to use Hadoop.    The offending line (in start-all.sh, though most of the scripts exhibit the problem) is:      bin=`cd ""$bin""; pwd`    Adding this to the top of each affected script will fix the problem:      unset CDPATH",scripts
Configuration clone constructor does not clone all the members.,"Currently, Configuration(Configuration other) constructor does not clone all the members.  It clones only resources, properties, overlay and finalParameters. It needs to clone loadDefaults, classLoader, defaultResources, quietmode.  This resulted in bugs like HADOOP-4975",conf
Tests in oah.record are silently broken or are potentially broken,"Several of the tests in the oah.record package are silently broken or potentially so:  # In most of the tests, try blocks wrap io operations and are caught with by printing the stack trace, but have no test fail, which results in the exceptions being silently swallowed.  This is really bad and directly leads to...  # In FromCpp.java the test attempts to create files in /temp/ which fails, but because of item #1 this is not caught and the test passes, however...  # Neither FromCpp.java nor ToCpp.java are named starting with Test, so neither are being run during run-test-core  # When ToCpp.java is explicitly run, it's failing on an NPE.  ",test
Provide a way to automatically handle backward compatibility of deprecated keys,"There are cases when we have had to deprecate configuration keys. Use cases include, changing the names of variables to better match intent, splitting a single parameter into two - for maps, reduces etc.    In such cases, we typically provide a backwards compatible option for the old keys. The handling of such cases might typically be common enough to actually add support for it in a generic fashion in the Configuration class. Some initial discussion around this started in HADOOP-5919, but since the project split happened in between we decided to open this issue to fix it in common.",conf
Provide an option in ShellCommandExecutor to timeout commands that do not complete within a certain amount of time.,In MAPREDUCE-211 we came across a need to provide an option to timeout commands launched via the ShellCommandExecutor. The use case is for the health check script being developed in MAPREDUCE-211. We would like the TaskTracker thread to not be blocked by a problematic script or in instances where fork()+exec() has hung (which apparently has been observed in large clusters).,util
Handle large (several MB) text input lines in a reasonable amount of time,"problem:  =======  hadoop was timing out on a simple pass-through job (with the default 10 min timeout)    cause:  =====  i hunted this down to how Text lines are being processed inside org.apache.hadoop.util.LineReader.  i have a fix, a task that took more than 20 minutes and still failed to complete, completes with this fix in under 30 s.  i attach the patch (for trunk)    the problem traces:  ================    hadoop version: 0.19.0  userlogs on slave node:    2009-05-29 13:57:33,551 WARN org.apache.hadoop.mapred.TaskRunner: Parent died.  Exiting attempt_200905281652_0013_m_000006_1  [root@domU-12-31-38-01-7C-92 attempt_200905281652_0013_m_000006_1]#    tellingly, the last input line processed right before this WARN is 19K. (i log the full input line in the map function for debugging)    output on map-reduce task:    Task attempt_200905281652_0013_m_000006_2 failed to report status for 600 seconds. Killing!  09/05/29 14:08:01 INFO mapred.JobClient:  map 99% reduce 32%  09/05/29 14:18:05 INFO mapred.JobClient:  map 98% reduce 32%  java.io.IOException: Job failed!      at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1217)      at com.adxpose.data.mr.DailyHeatmapAggregator.run(DailyHeatmapAggregator.java:547)      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)      at com.adxpose.data.mr.DailyHeatmapAggregator.main(DailyHeatmapAggregator.java:553)      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)      at java.lang.reflect.Method.invoke(Method.java:597)      at org.apache.hadoop.util.RunJar.main(RunJar.java:165)      at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)      at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)",io
test-patch takes 45min!,The runtime of test-patch has increased to 45min!,"build,test"
Support in LocalDirAllocator to set permissions of the paths created.,Support is needed in LocalDirAllocator to set permissions of the paths created. This is needed for HADOOP-4491.,fs
to fix hudsonPatchQueueAdmin for different projects,To fix hudsonPatchQueueAdmin process for different hadoop projects.    ,build
bug in documentation: org.apache.hadoop.fs.FileStatus.getLen() ,"In javadoc method  org.apache.hadoop.fs.FileStatus.getLen()  writtend that this method ""return the length of this file, in blocks""  But method return size in bytes.",documentation
"Hadoop not stopping (windows XP, cygwin)","Hadoop start fine with the start-all.sh script, but when stopping it says there are no processes to stop. The java processes can be seen in the task manager and also through ""ps -ef"" on cygwin.",scripts
Add TestFTPFileSystem to Common,"TestFTPFileSystem was incorrectly moved to hdfs sub-directory by HADOOP-5135.  The test, which belongs to Common, is nothing to do with HDFS.",fs
64 javac compiler warnings,"Ran ""ant test-patch"" with a empty patch.  Got   {noformat}       [exec]     -1 javac.  The applied patch generated 64 javac compiler warnings (more than the trunk's current 124 warnings).   {noformat}",build
hdfs script does not work after project split.,"There are problems running hdfs script from common.  # Usage message for hdfs does not have ""fs"" option anymore.  # There seem to be an undocumented option ""dfs"", but using it throws NoClassDefFoundError or ClassNotFoundException.  # Same with other options e.g. name-node.    May I am missing something here. How do we call ls these days?  Do we need to move hdfs script to hdfs project.  ",scripts
patchJavacWarnings and trunkJavacWarnings are not consistent.,The values of patchJavacWarnings and trunkJavacWarnings are not consistent when running test-patch.sh with an empty patch over Common.  HDFS and MapReduce seem not having this problem.,build
Hadoop QA mails on Patch tests do not have links to test failures,"The recent hadoopqa results on test patches do not seem to have the links to the actual tests that failed.   For example, http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/356/testReport/",build
The real user name should be used by bin/hadoop fs (ie. FsShell) instead of the one in the configuration.,The real user name should be used by FsShell instead of the one in the configuration. This will make it a tiny bit harder for someone to pretend to be someone else to the file system.,fs
Serializer and Deserializer should extend java.io.Closeable,"This change wouldn't change behaviour or the API, but would make it possible to use such utilities as IOUtils#closeStream() on Serializers and Deserializers.",io
A sysproperty should not be set unless the property is set on the ant command line in build.xml.,"Patch for HADOOP-3315 contains an improper usage of setting a sysproperty. What it does now:  {code}        <sysproperty key=""io.compression.codec.lzo.class""            value=""${io.compression.codec.lzo.class}""/>  {code}  What should be:  {code}        <syspropertyset dynamic=""no"">           <propertyref name=""io.compression.codec.lzo.class""/>        </syspropertyset>  {code}",build
RPC client opens an extra connection for VersionedProtocol,"RPC client caches connections per protocol. However, since all of our real protocols are subclasses of VersionedProtocol, a bug in the implementation makes the client opens an extra connection just for the VersionedProtocol, which is not needed.",ipc
ReflectionUtils performance regression,HADOOP-4187 introduced extra calls to Class.forName in ReflectionUtils.setConf. This caused a fairly large performance regression. Attached is a microbenchmark that shows the following timings (ms) for 100M constructions of new instances:    Explicit construction (new Test): around ~1.6sec  Using Test.class.newInstance: around ~2.6sec  ReflectionUtils on 0.18.3: ~8.0sec  ReflectionUtils on 0.20.0: ~200sec    This illustrates the ~80x slowdown caused by HADOOP-4187.,conf
New Hadoop Common Site,"New Hadoop Common Site    Set up site, initial pass.   May need to add more content.   May need to update some links.",documentation
to fix project specific test-patch requirements ,only mapreduce project needs create-c++-configure target which needs to be executed as part to the test-core ant target.,build
No error message for deleting non-existant file or directory.,"If non-existant path or src is provided with rm/rmr option then no error message is displayed   command: hadoop dfs -rm <src>  (where src is non-existant)  dfs displays ""rm:  <src>""  while it should display ""No such file or directory"".",fs
eliminate the depracate warnings introduced by H-5438,Eliminate the deprecated warnings introduced by HADOOP-5438.,fs
DistributedCache.addArchiveToClassPath doesn't work in 0.18.x branch,"addArchiveToClassPath is a method of DistributedCache class. It should be called before running a task. It accepts path to a jar file on a DFS. After it  this method should put this jar file on sitribuuted cache and than add this file to classpath to each map/reduce process on job tracker.    This method didn't work.    Bug 1:    addArchiveToClassPath adds DFS-path to archive to mapred.job.classpath.archives property. It uses System.getProperty(""path.separator"") as delimiter of multiple path.    getFileClassPaths that is called from TaskRunner uses splits mapred.job.classpath.archives using System.getProperty(""path.separator"").    In unix systems System.getProperty(""path.separator"") equals to "":"". DFS-path urls is hdfs://host:port/path. It means that a result of split will be  [ hdfs,//host,port/path].    Suggested solution: use "","" instead of      Bug 2:    in TaskRunner there is an algorithm that looks for correspondence between DFS paths and local paths in distributed cache.   It compares       if (archives[i].getPath().equals(                                                 archiveClasspaths[j].toString())){    instead of        if (archives[i].toString().equals(                                                 archiveClasspaths[j].toString()))   ",fs
"hadoop 0.20 branch ""test-patch"" is broken","There were two problems found in src/test/bin/test-patch.sh while I am doing the backporting of TFile patch (HADOOP-3315):  - java5.home and forrest.home is not defined for the ant command in pre-build stage, which leads to the following error message (in file trunkJavacWarnings.txt):  {code}  java5.check:    BUILD FAILED  /home/htang/workspace/test-patch/branch-0.20/build.xml:891: 'java5.home' is not defined.  Forrest requires Java 5.  Please pass -Djava5.home=<base of Java 5 distribution> to Ant on the command-line.  {code}  - When referring 10-th argument from the command line, it should use ""${10}"" instead of ""$10"" (which is $1 with a zero appended).",build
FS shell commands returns incorrect exit code  when error occurs,HDFS documentation ( http://hadoop.apache.org/core/docs/current/hdfs_shell.html#du )  mentions that   {noformat}  Exit Code:    Returns 0 on success and -1 on error.   {noformat}     Current Fs shell behavior is buggy with this agreement.    {code}  statepick-lm:Hadoop rphulari$ bin/hadoop fs -ls foo  ls: Cannot access foo: No such file or directory.  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -lsr foo  lsr: Cannot access foo: No such file or directory.  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -du foo  du: Cannot access foo: No such file or directory.  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -dus foo  dus: Cannot access foo: No such file or directory.  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -cp foo f2  cp: File does not exist: foo  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -copyToLocal foo f2  copyToLocal: null  statepick-lm:Hadoop rphulari$ echo $?  255  statepick-lm:Hadoop rphulari$ bin/hadoop fs -copyFromLocal foo f2  copyFromLocal: File foo does not exist.  statepick-lm:Hadoop rphulari$ echo $?  255  {code}      In all above cases exit code on error should be -1 ,fs
Upgrade to JetS3t version 0.7.1,The JetS3t library is used for the S3 filesystems. We should upgrade to the latest version (0.7.1) which has support for Requester Pays buckets. ,fs/s3
Need to be able to instantiate a comparator instance from a comparator string without creating a TFile.Reader object,"Occasionally, we want have the same instance of comparator object represented by the TFile comparator string. We should be able to do that without requiring to first open up a tfile that was previously written use the same comparator.",io
The servlets should quote html characters,"We need to quote html characters that come from user generated data. Otherwise, all of the web ui's have cross site scripting attack, etc.",security
Hadoop scripts do not correctly put jars on the classpath,"The various Hadoop scripts (bin/hadoop, bin/hdfs, bin/mapred) do not properly identify the jars needed to run Hadoop. They try to include hadoop-*-hdfs.jar, etc, rather than the hadoop-hdfs-*.jar that is actually created by the 'ant jar' and 'ant package' targets.",scripts
RAgzip: multiple map tasks for a large gzipped file,It support to enable multiple map tasks for one large gzipped file.,"io,native"
MultiSortedTFileScanner,"Given a set of sorted Tfiles or a directory with several sorted Tfiles, it would be nice to have an helper class that help reading those Tfiles in a total sort order.  The MultiSortedTFileScanner class could take a directory as input or Tfile could be dynamically added/removed to/from the MultiSortedTFileScanner.",io
deprecate Record IO,"With the advent of Avro, I think we should deprecate Record IO.",record
Move map/reduce specific classes out of common,"Move Sort functions, process tree, and memory calculator classes out of Common and into Map/Reduce.",util
Deprecate the Daemon class,The Daemon class has very little value and should be deprecated and then removed.,util
Move CyclicIteration to HDFS,I think we should move CyclicIteration from Common utils to HDFS.,util
"Remove ""core"" from build.xml","build.xml still creating ""core"" directories and other stuffs such as  {code}    <property name=""test.core.build.classes"" value=""${test.build.dir}/core/classes""/>    ...      <property name=""jdiff.stable.javadoc""               value=""http://hadoop.apache.org/core/docs/r${jdiff.stable}/api/""/>  {code}",build
releaseaudit (rats) should not be run againt the entire release binary,None,build
Add get/setEnum to Configuration,It would be useful if Configuration had helper get/set methods for enumerated types.,conf
Progress class should provide an api if phases exist,Progress class needs to provide an api for client to know if there are phases.  This is needed for Task.setProgress() to decide whether to update progress of task or progress of current phase in the task.,util
Test-patch's method of checking for new tests is not correct,"As happened in HDFS-458, test-patch/hudson saw the previous patch(HDFS-492) add a new test, bringing the total number of tests to 49.  Then it tested 458, which had no new tests as it was a change to the build file.  492 had not yet been committed yet, so there were still only 48 tests in trunk.  But test-patch failed the patch, expecting 49 tests.  test-patch should check the correct number of tests based on trunk, not on whatever number it last saw.",test
Add metadata to Serializations,"The Serialization framework only allows a class to be passed as metadata. This assumes there is a one-to-one mapping between types and Serializations, which is overly restrictive. By permitting applications to pass arbitrary metadata to Serializations, they can get more control over which Serialization is used, and would also allow, for example, one to pass an Avro schema to an Avro Serialization.",contrib/serialization
Improve PureJavaCrc32,Got some ideas to improve CRC32 calculation.,"performance,util"
HADOOP_HEAPSIZE cannot be done per-server easily,The hadoop script forces a heap that cannot be easily overridden if one wants to push the same config everywhere.,conf
"Incorrect UserName at Solaris because it has no ""whoami"" command by default","Solaris enviroment has no __whoami__ command, so the __getUnixUserName()__ at UnixUserGroupInformation class fails because it's calling to Shell.USER_NAME_COMMAND which is defines as ""whoami"".  So it launched an Exception and set the default ""DrWho"" username ignoring all the FileSystem permissions.","security,util"
"""package"" task in build.xml should copy source with preservelastmodified","In the package task, it copies the source to dist.dir without using preservelastmodified=true. This can cause issues with the autotools configure process for the C++ builds. Namely, it will sometimes think the configure script is out of date with respect to its source files and try to re-bootstrap, which relies on particular versions of autotools on the building computer. This isn't something that should be required for those wanting to build from a distribution tarball.",build
bin/hadoop version not working,"Two problems found:  - ${build.src} not included in ant target ""compile-core-classes"", thus o.a.h.package-info.java is not compiled, which contains the version annotation.  - bin/hadoop-config.sh attempts to include jar files matching pattern hadoop-*-core.jar rather than hadoop-core-*.jar.",build
"src/native/packageNativeHadoop.sh only packages files with ""hadoop"" in the name","src/native/packageNativeHadoop.sh only packages files with ""hadoop"" in the name. This becomes too restrictive when a user wants to inject third-party native libraries into his/her own tar build.","build,scripts"
Incorret version compilation with es_ES.ISO8859-15 locale on Solaris 10,"When you compile _hadoop_ on Solaris 10 with locale _es_ES.ISO8859-15_ the _src/saveVersion.sh_ script generates incorrect date on _build/src/org/apache/hadoop/package-info.java_    The ploblem is that the _saveVersion.sh_ script unsets the *LC_CTYPE* to avoid the problem, but on Solaris the _date_ command uses the *LC_TIME* enviroment variable as you can see at the _man page_    {noformat}  Specifications of native language translations of month  and  weekday  names  are  supported.  The month and weekday names  used for a language are based on the locale specified by the  environment variable LC_TIME. See environ(5).  {noformat}    Here's an example about *date* on Solaris    {quote}  $ echo $LC_CTYPE  es_ES.ISO8859-15  $ echo $LC_TIME  es_ES.ISO8859-15  $ date  lunes  3 de agosto de 2009 11H10'25"" CEST  $ unset LC_TYPE  $ date  lunes  3 de agosto de 2009 11H10'31"" CEST  $ unset LC_TIME  $ date  Mon Aug  3 11:10:35 CEST 2009  {quote}    So the _saveVersion.sh_ script creates the _package-info.java_ file as     {code}  /*   * Generated by src/saveVersion.sh   */  @HadoopVersionAnnotation(version=""0.20.1-dev"", revision="""",                           user=""itily"", date=""lunes  3 de agosto de 2009 11H16'01"" CEST"", url=""http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.20.0"")  package org.apache.hadoop;  {code}    And if you run hadoop with _version_ argument it's says ""Unknow"", here's an example    {quote}  $ hadoop version  Hadoop Unknown  Subversion Unknown -r Unknown  Compiled by Unknown on Unknown  {quote}      To solve this issue is as simple as adding     *unset LC_TIME*    to _saveVersion.sh_ script, and the output is as _C_ locale as  {code}  /*   * Generated by src/saveVersion.sh   */  @HadoopVersionAnnotation(version=""0.20.1-dev"", revision="""",                           user=""itily"", date=""Mon Aug  3 11:19:41 CEST 2009"", url=""http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.20.0"")  package org.apache.hadoop;  {code}      ",build
Adding a couple private methods to AccessTokenHandler for testing purposes,"To support some test cases being added as part of HDFS-409, a couple private methods need to be added to AccessTokenHandler. One is for setting token lifetime and another is checking if a token has expired.",security
GenericOptionsParser should ignore symlinks from archive filenames,"http://hadoop.apache.org/common/docs/r0.20.0/streaming.html#Large+files+and+archives+in+Hadoop+Streaming example doesnt work as expected.   Stack trace :  {code}  trunk$  ./bin/hadoop jar build/contrib/streaming/hadoop-0.21.0-dev-streaming.jar  -D mapred.map.tasks=1 \           -D mapred.reduce.tasks=1 -D mapred.job.name=""Experiment"" \          -archives 'hdfs://namenode:port/user/me/cachedir.jar#testlink' -input input -output output -mapper ""xargs cat"" -reducer ""cat""  java.io.FileNotFoundException: File hdfs://namenode:port/user/me/cachedir.jar#testlink does not exist.    org.apache.hadoop.util.GenericOptionsParser.validateFiles(GenericOptionsParser.java:349)    org.apache.hadoop.util.GenericOptionsParser.processGeneralOptions(GenericOptionsParser.java:275)    org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:375)    org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)    org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:138)    org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:59)    org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)    org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:50)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.apache.hadoop.util.RunJar.main(RunJar.java:160)  {code}",util
Fixes for Eclipse template,"The Avro jar file is missing, and the entry for jets3t does not match the version downloaded by Ivy",build
improvement in fairscheduler,"I understand the hadoop fair scheduler does the scheduling at a task level. I think in the pool of jobs if we can queue up the jobs based on a backfilling strategy, the delay which long jobs might face in the current strategy, can be reduced fruther.  Has anybody tried such a strategy?  ",contrib/hod
Replace FSDataOutputStream#sync() by hflush(),This jira aims add hflush() API to FSDataOutputStream and deprecates sync() API.    Note that the change should not commit to the trunk before hdfs append branch is merged to hdfs trunk.,fs
Enhance checkstyle results by Hudson Hadoop QA to provide a diff,"Currently, when a patch is submitted to Hudson, it provides the checkstyle results indicating problems that happened throughout the project history. For e.g., http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-vesta.apache.org/463/artifact/trunk/build/test/checkstyle-errors.html.    It is not useful at all in this format. It would be of better use to provide the style problems resulting ONLY OUT OF THE CURRENT PATCH.    Another point: Should we also consider a +1, -1 by Hudson w.r.t code style ala findBugs warnings, test results etc.",build
Allow Super user access only from certian trusted IP Range- This is to avoid spoofing by others as super user and gain access to the cluster,"The current hadoop environment assumes  that everything is trusted environment and so doesn't have any security checks. This might lead to any one to spoof as super user and gain access to the cluster if the environment is open for every one. So, there is a need to restrict super user permission only from certain IP addresses.       ",ipc
TestHDFSTrash fails because of TestTrash in common,None,test
Shell.getUlimitMemoryCommand is tied to Map-Reduce,Currently org.apache.hadoop.util.Shell.getUlimitMemoryCommand relies on a MAPREDUCE specific configuration property for the memory limit. We should break the link.,util
Add service base class and tests to hadoop-common/util,"For a service base class, we need to add to common  # The Service class  # A mock Service class (it's not in test, its in src/, because it helps in functional tests downstream)  # Unit tests    This is the issue to cover this work",util
Use aspects to introduce new methods required for testing,"HADOOP-6176 introduces two new methods which are essentially required for testing purposes. However, this bring a permanent modifications to the production code.    The same result can be reached by using aspect development. Thus the modifications will be brought in by byte code instrumentation and will exist only during the testing cycles.","security,test"
Add the documentation for io.map.index.skip in core-default,io.map.index.skip is used in common. But is documented in MapReduce project. It should be documented in core-default.xml,io
glue together the different projects to make local builds easier,It's currently fairly tricky to get everything building/up to date in a single machine. We can and should improve this.,build
FileSystem::ListStatus should throw FileNotFoundException,"As discussed in HDFS-538, it would be better for FileSystem and its implementations to throw FileNotFoundException when the file is not found, rather than returning null.  This will bring it in line with getFileStatus and, I expect, default user expectations.",fs
harchive:   Har doesn't work on files having '%' character,If I have a harchive file test.har that contain   /a/b <dir>   /a/b/abc%cde <file>  /a/b/fgh <file>    {noformat}  $ hadoop dfs -cat test.har/_masterindex  1   0 2046275926 0 244   $ hadoop dfs -cat test.har/_index  / dir none 0 0 user   /user dir none 0 0 knoguchi   /user/knoguchi/a/b dir none 0 0 abc%cde fgh   /user/knoguchi dir none 0 0 a   /user/knoguchi/a dir none 0 0 b   /user/knoguchi/a/b/fgh file part-0 8 10   /user/knoguchi/a/b/abc%cde file part-0 0 8   $ hadoop dfs -lsr har:///user/knoguchi/test.har/  drw-r--r--   - knoguchi users          0 2009-08-18 18:52 /user/knoguchi/test.har/user  drw-r--r--   - knoguchi users          0 2009-08-18 18:52 /user/knoguchi/test.har/user/knoguchi  drw-r--r--   - knoguchi users          0 2009-08-18 18:52 /user/knoguchi/test.har/user/knoguchi/a  drw-r--r--   - knoguchi users          0 2009-08-18 18:52 /user/knoguchi/test.har/user/knoguchi/a/b  lsr: could not get get listing for 'har:/user/knoguchi/test.har/user/knoguchi/a/b' : File: har://hdfs-aaa.bbb.ccc.com:8020/user/knoguchi/test.har/user/knoguchi/a/b/abc%cde does not exist in har:///user/knoguchi/test.har  $  {noformat}  ,fs
Improve error message when moving to trash fails due to quota issue,"HADOOP-6080 provided an option for deleting files even when overquota, but the error message that's returned in this situation is unhelpful and doesn't suggest skipTrash as a remediation:  {noformat}$ hdfs -rmr /foo/bar/bat/boo    rmr: Failed to move to trash:  hdfs://cluster/foo/bar/bat/boo{noformat}  In this situation, the error message should say there was a quote problem and suggest -skipTrash.  ",fs
Implementing aspects development and fault injeciton framework for Hadoop,"Fault injection framework implementation in HDFS (HDFS-435) turns out to be a very useful feature both for error handling testing and for various simulations.    There's certain demand for this framework, thus it need to be pulled up from HDFS and brought into Common, so other sub-projects will be able to share it if needed.","build,test"
Proposed enhancements/tuning to hadoop-common/build.xml,Minor enhancements to the build.xml of hadoop-common for better ivy integration/reporting,build
libhdfs leaks object references,libhdfs leaks many objects during normal operation.  This becomes exacerbated by long-running processes (such as FUSE-DFS).,fs
Block loss in S3FS due to S3 inconsistency on file rename,"Under certain S3 consistency scenarios, Hadoop's S3FileSystem can 'truncate' files, especially when writing reduce outputs.  We've noticed this at tracksimple where we use the S3FS as the direct input and output of our MapReduce jobs.  The symptom of this problem is a file in the filesystem that is an exact multiple of the FS block size - exactly 32MB, 64MB, 96MB, etc. in length.    The issue appears to be caused by renaming a file that has recently been written, and getting a stale INode read from S3.  When a reducer is writing job output to the S3FS, the normal series of S3 key writes for a 3-block file looks something like this:    Task Output:  1) Write the first block (block_99)  2) Write an INode (/myjob/_temporary/_attempt_200907142159_0306_r_000133_0/part-00133.gz) containing [block_99]  3) Write the second block (block_81)  4) Rewrite the INode with new contents [block_99, block_81]  5) Write the last block (block_-101)  6) Rewrite the INode with the final contents [block_99, block_81, block_-101]    Copy Output to Final Location (ReduceTask#copyOutput):  1) Read the INode contents from /myjob/_temporary/_attempt_200907142159_0306_r_000133_0/part-00133.gz, which gives [block_99, block_81, block_-101]  2) Write the data from #1 to the final location, /myjob/part-00133.gz  3) Delete the old INode     The output file is truncated if S3 serves a stale copy of the temporary INode.  In copyOutput, step 1 above, it is possible for S3 to return a version of the temporary INode that contains just [block_99, block_81].  In this case, we write this new data to the final output location, and 'lose' block_-101 in the process.  Since we then delete the temporary INode, we've lost all references to the final block of this file and it's orphaned in the S3 bucket.    This type of consistency error is infrequent but not impossible. We've observed these failures about once a week for one of our large jobs which runs daily and has 200 reduce outputs; so we're seeing an error rate of something like 0.07% per reduce.    These kind of errors are generally difficult to handle in a system like S3.  We have a few ideas about how to fix this:  1) HACK! Sleep during S3OutputStream#close or #flush to wait for S3 to catch up and make these less likely.  2) Poll for updated MD5 or INode data in Jets3tFileSystemStore#storeINode until S3 says the INode contents are the same as our local copy.  This could be a config option - ""fs.s3.verifyInodeWrites"" or something like that.  3) Cache INode contents in-process, so we don't have to go back to S3 to ask for the current version of an INode.  4) Only write INodes once, when the output stream is closed.  This would basically make S3OutputStream#flush() a no-op.  5) Modify the S3FS to somehow version INodes (unclear how we would do this, need some design work).  6) Avoid using the S3FS for temporary task attempt files.  7) Avoid using the S3FS completely.    We wanted to get some guidance from the community before we went down any of these paths.  Has anyone seen this issue?  Any other suggested workarounds?  We at tracksimple are willing to invest some time in fixing this and (of course) contributing our fix back, but we wanted to get an 'ack' from others before we try anything crazy :-).    I've attached a test app if anyone wants to try and reproduce this themselves.  It takes a while to run (depending on the 'weather' in S3 right now), but should eventually detect a consistency 'error' that manifests itself as a truncated file.",fs/s3
chmod fails on Windows+cygwin,"When run on Windows + cygwin, chmod fails because it fails to call {{makeShellPath}} on the parameter.  The strack trace with 0.19.1 below:    {noformat}  org.apache.hadoop.util.Shell$ExitCodeException: chmod: changing permissions of `C:\\files\\hudson\\hadoop\\task-tracker\\taskTracker\\jobcache\\job_200908191713_0008\\job.xml': No such file or directory      org.apache.hadoop.util.Shell.runCommand(Shell.java:195)    org.apache.hadoop.util.Shell.run(Shell.java:134)    org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:286)    org.apache.hadoop.util.Shell.execCommand(Shell.java:338)    org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:540)    org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:532)    org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:274)    org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:364)    org.apache.hadoop.fs.FileSystem.create(FileSystem.java:487)    org.apache.hadoop.fs.FileSystem.create(FileSystem.java:468)    org.apache.hadoop.fs.FileSystem.create(FileSystem.java:375)    org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:208)    org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:142)    org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1214)    org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1195)    org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:750)    org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:1656)    org.apache.hadoop.mapred.TaskTracker.access$1200(TaskTracker.java:102)    org.apache.hadoop.mapred.TaskTracker$TaskLauncher.run(TaskTracker.java:1621)  {noformat}",fs
Remove commons dependency on commons-cli2,GenericOptionsParser depends on commons-cli2,util
Hadoop Doc Split: Common Docs,Hadoop Doc Split: Common Docs    Please note that I am unable to directly check all of the new links. Some links may break and will need to be updated.,documentation
HttpServer wraps InterruptedExceptions by IOExceptions if interrupted in startup,"Following on some discusson on mapred-dev, we should keep an eye on the fact that Jetty uses sleeps when starting up; jetty can be a big part of the delays of bringing up a node. When interrupted, the exception is wrapped by an IOException, the root cause is still there, just hidden.    If we want callers to distinguish InterruptedExceptions from IOEs, then this exception should be extracted. Some helper method to start an http daemon could do this -catch the IOE, and if there is a nested interrupted exception, rethrow it, otherwise rethrowing the original IOE  ",util
RPC Client operations cannot be interrupted,"RPC.waitForProxy swallows any attempts to interrupt it while waiting for a proxy; this makes it hard to shutdown a service that you are starting; you have to wait for the timeouts.     There are only 4-5 places in the code that use either of the two overloaded methods, removing the catch and changing the signature should not be too painful, unless anyone is using the method outside the hadoop codebase. ",ipc
Core doesn't have TestCommonCLI facility,"TestCLI is a base class, which cannot run FS type of commands.  We need a ""copy"" of TestHDFSCLI as TestCommonCLI to be able to test CLI stuff in common.    I suggest we create TestCommonCLI.java in hadoop-common",test
New improved FileSystem interface for those implementing new files systems.,"The FileContext API (HADOOP-4952) provides an improved interface for the application writer.  This lets us simplify the FileSystem API since it will no longer need to deal with notions of default filesystem [ / ],  wd, and config  defaults for blocksize, replication factor etc. Further it will not need the many overloaded methods for create() and open() since  the FileContext API provides that convenience.  The FileSystem API can be simplified and can now be restricted to those implementing new file systems.      This jira proposes that we create new file system API,  and deprecate FileSystem API after a few releases.",fs
Add a method to WritableUtils performing a bounded read of a String,MAPREDUCE-318 needs to sanity check vint-length-encoded Strings read from a byte stream. It would be appropriate to add this as a general utility method.,io
Source checksum computation in saveVersion.sh doesn't work on MacOS X,"HADOOP-5203, added a ""source checksum"" field computed with {{md5sum}}. MacOS X uses an equivalent tool called {{md5}}",build
Configuration does not lock parameters marked final if they have no value.,"A use-case: Recently, we stumbled on a bug that wanted us to disable the feature to run a debug script in map/reduce on tasks that fail, specified by mapred.{map|reduce}.task.debug.script. The best way of disabling it seemed to be to set it with no value in the cluster configuration and mark it final. This did not work, however, because the code in configuration explicitly checks if the value is not null before adding it to the list of final parameters.    Without an ability to do this, there might be a need to explicitly have a special way of disabling optional features.",conf
Configuration should allow storage of null values.,"Currently the configuration class does not allow null keys and values. Null keys don't make sense, but null values may have semantic meaning for some features. Not storing these values in configuration causes some arguable side effects. For instance, if a value is defined in defaults, but wants to be disabled in site configuration by setting it to null, there's no way to do this currently. Also, no track of keys with null values is recorded. Hence, tools like dump configuration (HADOOP-6184) would not display these properties.    Does this seem like a sensible use case ?",conf
Atempt to make a directory under an existing file on LocalFileSystem should throw an Exception.,"This task is sub task of HDFS-303    {quote}   ""1. When trying to make a directory under an existing file, HDFS throws an IOException while LocalFileSystem doesn't.   An IOException seems good here. ""  {quote}  Actually HDFS throws FileNotFoundException in this case (in FSDirectory.mkdirs() ). So I guess we should do the same.",fs
"Move process tree, and memory calculator classes out of Common into Map/Reduce.",None,util
Allow caching of filesystem instances to be disabled on a per-instance basis,"HAR filesystem instances should not be cached, so this JIRA seeks to provide a general mechanism for disabling the cache on a per-filesystem basis. (Carried over from HADOOP-6097.)",fs
Changes in common to rename the config keys as detailed in HDFS-531.,This jira tracks the code changes required in common to rename the config keys. The list of changed keys is attached to HDFS-531.,fs
Adding a new method for getting server default values from a FileSystem,This is the changes made to Common to support file creation using server default values for a number of configuration params. See HDFS-578 for details.,fs
clean-up Trash.moveToTrash() code. Should throw exception instead of returning false,"In case of errors it should throw an exception instead of returning false.  One valid case is when interval is 0 (trash disabled), but that could be moved to a separate method isEnabled().    ",fs
(very) minor typo in javadoc for org.apache.hadoop.util.Tool,"In the description, ""Here is how a typical Tool is implemented:"" there's some sample code for a class called ""MyApp"" that gives a simple implementation of Tool.  In its main method, it references a class ""Sort"" where I believe MyApp should appear:             int res = ToolRunner.run(new Configuration(), new Sort(), args);  should be           int res = ToolRunner.run(new Configuration(), new MyApp(), args);    I'm brand-new to hadoop (Day 1) and was initially confused how ToolRunner knew which class to run.",util
Command-line for append,"Once append is implemented in hdfs, it would be nice if users can append files from a command-line.   Either have a separate 'append' command or add '-append' option for 'put' and 'cp'    Like,    {noformat}  % cat mylocalfile  | hadoop dfs -put -append - /user/knoguchi/myfile    ('-' for stdin)  % hadoop dfs -cp -append myhdfsfile1 myhdfsfile2    {noformat}",fs
Rename operation is not consistent between different implementations of FileSystem,The rename operation has many scenarios that are not consistently implemented across file systems.  ,fs
NPE in handling deprecated configuration keys.,I run TestFileCreation in Eclipse and get a NullPointerException. Debugging shows that {{Configuration.populateDeprecationMapping()}} sets {{Configuration.properties}} to null.  This code was introduced in HADOOP-6105.,conf
Improvements to FileContext metrics output formatting,"The output of FileContext has two big issues: 1) it doesn't include a timestamp, 2) it doesn't differentiate between tags and metrics in formatting. This patch is to improve the output format to be more useful.",metrics
Provide a way to load deprecated keys from other sub-projects into configuration,"In HADOOP-6105, we provided a method of adding deprecated keys from other sub-projects like HDFS and Map/Reduce using a key called hadoop.conf.extra.classes. The expectation was that this key had class names that will be statically loaded by the Configuration class and the classes could add deprecated keys in static blocks of their classes. We discovered as in HADOOP-6243, that the classes added could have side-effects if they, for e.g. load resources in configuration. To unblock builds, we are disabling this feature in fixing HADOOP-6243. However, I am opening this JIRA to follow-up on a fix for the original intention.",conf
Update umask code to use key deprecation facilities from HADOOP-6105,"When the new octal/symbolic umask JIRA (HADOOP-6234) went through, the config key deprecation patch (HADOOP-6105) wasn't ready.  Now that both are committed, the home-brewed key deprecation system from 6234 should be updated to use the new system.",fs
move the hdfs and mapred scripts to their respective subprojects,"The scripts for mapred and hdfs are currently in common, but they need to be moved to their respective subprojects.",scripts
Circus: Proposal and Preliminary Code for a Hadoop System Testing Framework,"This issue contains a proposal and preliminary source code for Circus, a Hadoop system testing framework.  At a high level, Circus will help Hadoop users and QA engineers to run system tests on a configurable Hadoop cluster, or distribution of Hadoop.  See the comment below for the proposal itself.",test
test-patch.sh doesn't clean up conf/*.xml files after the trunk run.,"test-patch.sh doesn't clean up conf/*.xml files after trunk run. This is a problem as after applying patch , the *.xml files are not updated by the template files.",build
Provide method to determine if a deprecated key was set in the config file,"HADOOP-6105 provided a method to deprecate config keys and transparently refer to the new key. However, it didn't provide a method to see if the deprecated key had been used in the config file.  This is useful when, if the deprecated key had been used, its value needs to be converted before use, for instance when we changed the umask format.  A method like ""boolean wasDeprecatedKeySet()"" would be great.  Patch shortly.",conf
Add a Ceph FileSystem interface.,"The experimental distributed filesystem Ceph does not have a single point of failure, uses a distributed metadata cluster instead of a single in-memory server, and might be of use to some Hadoop users.    http://ceph.com/docs/wip-hadoop-doc/cephfs/hadoop/",fs
s3n fails with SocketTimeoutException,"If a user's map function is CPU intensive and doesn't read from the input very quickly, compounded by the buffering of input, then S3 might think the connection has been lost and will close the connection. Then when the user attempts to read from the input again, they'll receive a SocketTimeoutException and the task will fail.",fs/s3
Update comments in test-patch.sh to clarify changes done in HADOOP-6250,It will be useful to add comments in test-patch.sh for changes done as part of HADOOP-6250 to explain them.,build
Two TestFileSystem classes are confusing hadoop-hdfs-hdfwithmr,"I propose to rename hadoop-common/src/test/core/org/apache/hadoop/fs/TestFileSystem.java -> src/test/core/org/apache/hadoop/fs/TestFileSystemCaching.java.  Otherwise, it conflicts with hadoop-hdfs/src/test/hdfs-with-mr/org/apache/hadoop/fs/TestFileSystem.java.","build,fs,test"
Unit tests for FileSystemContextUtil.,This Jira is to add the unit tests associated with HADOOP-4952.,fs
Junit tests for FileContextURI,FileContextURI unit tests. ,test
build-contrib.xml unnecessarily enforces that contrib projects be located in contrib/ dir,build-contrib.xml currently sets hadoop.root to ${basedir}/../../../. This path is relative to the contrib project which is assumed to be inside src/contrib/. We occasionally work on contrib projects in other repositories until they're ready to contribute. We can use the <dirname> ant task to do this more correctly.,build
Add ivy jar to .gitignore,"The ivy/ivy-2.0.0-rc2.jar is fetched automatically by ant, so it should be ignored by version control. Since many people use git, we should add it to the .gitignore file",build
Missing synchronization for defaultResources in Configuration.addResource,"Configuration.defaultResources is a simple ArrayList. In two places in Configuration it is accessed without appropriate synchronization, which we've seen to occasionally result in ConcurrentModificationExceptions.",conf
FileContext needs to provide deleteOnExit functionality,FileSystem provided an API to the applications {{deleteOnExit(Path f)}} used for registering a path to be deleted on JVM shutdown or when calling {{FileSystem.close()}} methods. Equivalent functionality is required in FileContext.,fs
Fix FileContext to allow both recursive and non recursive create and mkdir,Modify FileContext to allow recursive and non-recursive create and mkdir (see HADOOP-4952),fs
TestLocalFSFileContextMainOperations tests wrongly expect a certain order to be returned.,The test to be more forgiving on the return values of list status calls.,test
test-patch script should -1 patches that increase the number of Thread.sleep() calls in tests,"Calls to Thread.sleep() in tests are likely dubious.  There are rare occasions where these are required, but in general they lead to flaky unit tests that fail sporadically.  We used to be much better at code reviewing these out of patches, but have lost ground on that in the last few years (and our flaky unit tests show that!).     I propose we have the test-patch script ensure that a patch does not add to the number of Thread.sleep() calls in *tests* by giving a -1 to patches that do increase it.  I suggest this is a simple grep for lines containing ""Thread.sleep("" and a ""+"" character in the first column.",test
Update HowToCommit with clear list of Committer responsibilities,"http://wiki.apache.org/hadoop/HowToCommit should be updated to clearly spell out these committer responsibilities:    1) For non-trivial patches, code reviews must be done by another *committer*.  2) Code reviews follow the checklist (http://wiki.apache.org/hadoop/CodeReviewChecklist) so that, among other things, the patch is checked for robust documentation and unit tests   3) A -1 from Hadoop QA for lack of unit tests *must* be explained satisfactorily by the contributor else the patch *is not* committed. In this case, the manual tests performed by the contributor must be explained.  4) A -1 from Hadoop QA for findbugs, javadoc, or javac warnings means the patch *must not* be committed until these are fixed (else every subsequent patch will get -1 on these)  5) A -1 from Hadoop QA for core *and* contrib unit tests means the patch *must not* be committed until these are fixed (else every subsequent patch will get -1 on these)    Others responsibilities?",documentation
Uniform way of setting default param for ToolRunner (like passing -Ddfs.umask),Sometimes our users want to overwrite the dfs.umask setting we have on our cluster  (but continue to use the configdir we setup).  They would need to explicitly insert     hadoop dfs -Ddfs.umask=23 -put ...  or   hadoop jar myjar.jar org.MyMain -Ddfs.umask=23  ...    for all the hadoop related calls.    It would be nice if this  can be done by setting an environment like     export HADOOP_TOOL_OPTS=-Ddfs.umask=23    and cover all cases.,util
test-patch should verify that FindBugs version used for verification is correct one,There's a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson.    test-patch script has to verify if the version of FindBugs is correct.,"build,test"
The exception meessage in FileUtil$HardLink.getLinkCount(..) is not clear,"When a file is not found, FileUtil$HardLink.getLinkCount(..) shows the following error message.  {noformat}  java.lang.NumberFormatException: For input string: """"    java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)    java.lang.Integer.parseInt(Integer.java:470)    java.lang.Integer.parseInt(Integer.java:499)    org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:663)    org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.detachBlock(ReplicaInfo.java:209)   ...  stat: cannot stat `/home/tsz/hadoop/hdfs/h265/build/test/data/dfs/data/data3/current/finalized/blk_-94418387820168072_1001.meta':   No such file or directory on file:/home/tsz/hadoop/hdfs/h265/build/test/data/dfs/data/data3/current/finalized/blk_-94418387820168072_1001.meta  {noformat}  It looks like that there was a uncaught NumberFormatException.",fs
Any hadoop commands crashing jvm (SIGBUS)  when /tmp (tmpfs) is full,"{noformat}  [knoguchi@ ~]$ df /tmp  Filesystem           1K-blocks      Used Available Use% Mounted on  tmpfs                   524288    524288         0 100% /tmp  [knoguchi@ ~]$ hadoop dfs -ls   #  # An unexpected error has been detected by Java Runtime Environment:  #  #  SIGBUS (0x7) at pc=0x00824077, pid=19185, tid=4160617360  #  # Java VM: Java HotSpot(TM) Server VM (10.0-b22 mixed mode linux-x86)  # Problematic frame:  # C  [libc.so.6+0x6e077]  memset+0x37  #  # An error report file with more information is saved as:  # /homes/knoguchi/hs_err_pid19185.log  #  # If you would like to submit a bug report, please visit:  #   http://java.sun.com/webapps/bugreport/crash.jsp  #  Aborted  [knoguchi@ ~]$   {noformat}    This does not happen when /tmp is not in tmpfs.  ",scripts
The Glob methods in FileContext doe not deal with URIs correctly,The glob methods in FileContext were copied from FileSystem where they dealt with the a single filesystem.  While they were extended to fit in FileContext they don't seem to deal with URI pathnames correctly.    For example the following two lines in FileContext may be the source of the problem - the scheme and authority seem to be   ignored beyond these points.     line 1013: String filename = pathPattern.toUri().getPath();  line 1035:  String filename = pathPattern.toUri().getPath();    ,fs
Support units for configuration knobs,We should add support for units in our Configuration system so that we can specify values to be *1GB* or *1MB* rather than forcing every component which consumes such values to be aware of these.,conf
Improve Configuration to support default and minimum values ,"HADOOP-6105 provided a way to automatically handle deprecation of configs - I'd like to take this further to support default values, minimum values etc.",conf
Confusing warn message from Configuration,"Starting a cluster without setting mapreduce.task.attempt.id and then  {noformat}  $ ./bin/hadoop fs -put README.txt r.txt  09/09/29 22:28:10 WARN conf.Configuration: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id  09/09/29 22:28:10 INFO hdfs.DFSClient: Done flushing  09/09/29 22:28:10 INFO hdfs.DFSClient: Closing the streams...  {noformat}",conf
Native Libraries Guide - Update ,Native Libraries Guide - Update    Updated/edited guide for Hadoop 0.21 release.   Some passages/statements were not clear (user confusion).    Note: Setting priority to Blocker to make sure these changes get into Hadoop 0.21.0 release (and to pave the way for future updates/additions to this guide),documentation
FsShell -text should work on filesystems other than the default,FsShell currently recognizes only the default FileSystem for the \-text command. It should take arbitrary paths,fs
Build process from tarball is flawed,"Currently, the natively compiled components have a requirement of building from within ant.  To make matters worse, after talking this over with Lee, it appears that the ant build assumes to be executed from a svn-checkout and not from the tarballs that are generated.  We need to    a) start packaging up the svn repo trees as src tarballs  b) separate out the compiled bits out from underneath ant completely  c) be able to create real builds from the tarball    As an admin, I shouldn't have to reach for svn to make Hadoop work optimally on non-Linux.",native
Hadoop's support for zlib library lacks support to perform flushes (Z_SYNC_FLUSH and Z_FULL_FLUSH),"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary.  The Hadoop wrapper for the zlib library does not support either of these two methods.    Adding support should be fairly trivial.  An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library.  I can submit a patch for this if desired.    It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4206909",io
Use JAAS LoginContext for our login,Currently we use a custom login module in UnixUserGroupInformation for acquiring user-credentials (via config or exec'ing 'whoami'). We should switch to using standard JAAS components such as LoginContext and possibly implement a custom UnixLoginContext for our current requirements. In future we can use this for Kerberos etc. ,security
Increase maxwarns for javac in test-patch,"Increase maxwarns for javac in test-patch to a higher number like 10,000. See comment https://issues.apache.org/jira/browse/MAPREDUCE-769?focusedCommentId=12763375&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12763375",build
Use java.io.File.set{Readable|Writable|Executable} where possible in RawLocalFileSystem ,Using java.io.File.set{Readable|Writable|Executable} where possible in RawLocalFileSystem when g & o perms are same saves a lot of 'fork' system-calls.,fs
Unify build property names to facilitate cross-projects modifications,"Current build files of {{common}}, {{mapreduce}}, {{hdfs}} have their own unique ways of naming properties which otherwise have very same meaning. I.e. the locations of java source code are called {{core.src.dir}}, {{mapred.src.dir}}, and {{hdfs.src.dir}} respectively.    It makes modifications needed for all three projects to be very unique thus increasing the number of difference between builds of the projects.",build
Support reading on un-closed SequenceFile,"When a SequenceFile.Reader is constructed, it calls fs.getFileStatus(file).getLen().  However, fs.getFileStatus(file).getLen() does not return the hflushed length for un-closed file since the Namenode does not know the hflushed length.  DFSClient have to ask a datanode for the length last block which is being written; see also HDFS-570.",io
make number of IPC accepts configurable,"We were recently seeing issues in our environments where HDFS clients would experience RST's from the NN when trying to do RPC to get file info, which would cause the task to fatal out. After some debugging we identified this to be that the IPC server listen queue -- ipc.server.listen.queue.size -- was far too low, we had been using the default value of 128 and found we needed to bump it up to 10240 before resets went away (although this value is a bit suspect, as I will explain later in the issue).    When a large map job starts, lots of clients very quickly start to issue RPC requests to the namenode, which creates this listen queue filling up problem, because clients are opening connections faster than Hadoop's RPC server can process them. We went back to our 0.17 cluster and instrumented that with tcpdump and found that we had been sending RST's for a long time there, but the retry handling was implemented differently back in 0.17 so a single TCP failure wasn't task-fatal.    In our environment we have our TCP stack set to explicitly send resets when the listen queue gets overflowed (syctl net.ipv4.tcp_abort_on_overflow = 1), default linux behavior is to start dropping SYN packets and let the client retransmit. Other people may be experiencing this issue and not noticing it because they are using the default behavior, which is to let the NN drop packets on the floor and let clients retransmit.    So we've identified (at least) 3 improvements that can be made here:  1) In src/core/org/apache/hadoop/ipc/Server.java, Listener.doAccept() is currently hardcoded to do 10 accept()'s at a time, then it will start to read. We feel that it would be better to allow the server to be configured to support more than 10 accept's at one time using a configurable parameter. We can still leave 10 as the default.  2) Increase the default value of ipc.server.listen.queue.size from 128, or at least document that people with larger clusters starting thousands of mappers at once should increase this value. I wonder if a lot of people running larger clusters are dropping packets and don't realize it because TCP is covering them up. One one hand, yay TCP, on the other hand, those are needless delays and retries because the server can handle more connections.  3) Document that ipc.server.listen.queue.size may be limited to the value of SOMAXCONN (linux sysctl net.core.somaxconn ; default 4096 on our systems). The Java docs are not completely clear about this, and it's difficult to test because you can't query the backlog of a listening socket. We were under some time pressure in our case and tried 1024 which was not enough, and 10240 which worked, so we stuck with that.  ",ipc
Enable asserts for tests by default,What do people think of making the tests run with java asserts enabled by default?,build
Add support for unix domain sockets to JNI libs,For HDFS-347 we need to use unix domain sockets. This JIRA is to include a library in common which adds a o.a.h.net.unix package based on the code from Android (apache 2 license),native
Configuration sends too much data to log4j,"Configuration objects send a DEBUG-level log message every time they're instantiated, which include a full stack trace. This is more appropriate for TRACE-level logging, as it renders other debug logs very hard to read.",conf
Expose flush APIs to application users,"Earlier this year, Yahoo, Facebook, and Hbase developers had a roundtable discussion where we agreed to support three types of flush in HDFS (API1, 2, and 3) and the append project aims to implement API2. Here is a proposal to expose these APIs to application users.  1. Three flush APIs  * API1: flushes out from the address space of client into the socket to the data nodes.   On the return of the call there is no guarantee that that data is out of the underlying node and no guarantee of having reached a DN.  New readers will eventually see this data if there are no failures.  * API2: flushes out to all replicas of the block. The data is in the buffers of the DNs but not on the DN's OS buffers.  New readers will see the data after the call has returned.   * API3: flushes out to all replicas and all replicas have done posix fsync equivalent - ie the OS has flushed it to the disk device (but the disk may have it in its cache).    2. Support flush APIs in FS  * FSDataOutputStream#flush supports API1  * FSDataOutputStream implements Syncable interface defined below. If its wrapped output stream (i.e. each file system's stream) is Syncable, FSDataOutputStream#hflush() and hsync() call its wrapped output stream's hflush & hsync.  {noformat}    public interface Syncable {      public void hflush() throws IOException;  // support API2      public void hsync() throws IOException;   // support API3    }  {noformat}  * In each file system, if only hflush() is implemented, hsync() by default calls hflush().  If only hsync() is implemented, hflush() by default calls flush().",fs
"""bin/hadoop fs -help count""  fails to show help about only ""count"" command. ","Currently ""hadoop fs -help count"" fails to show help about only count command.   Instead it displays following output    {noformat}  [rphulari@statepick-lm]> bin/hadoop fs -help count  hadoop fs is the command to execute fs commands. The full syntax is:     hadoop fs [-fs <local | file system URI>] [-conf <configuration file>]   [-D <property=value>] [-ls <path>] [-lsr <path>] [-df [<path>]] [-du <path>]   [-dus <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm [-skipTrash] <src>]   [-rmr [-skipTrash] <src>] [-put <localsrc> ... <dst>] [-copyFromLocal <localsrc> ... <dst>]   [-moveFromLocal <localsrc> ... <dst>] [-get [-ignoreCrc] [-crc] <src> <localdst>   [-getmerge <src> <localdst> [addnl]] [-cat <src>]   [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>] [-moveToLocal <src> <localdst>]   [-mkdir <path>] [-report] [-setrep [-R] [-w] <rep> <path/file>]   [-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>]   [-tail [-f] <path>] [-text <path>]    ..  ..  ..    {noformat}    Expected output of ""bin/hadoop fs -help count "" should be   {noformat}  [rphulari@statepick-lm]> bin/hadoop  fs -help count  -count[-q] <path>: Count the number of directories, files and bytes under the paths    that match the specified file pattern.  The output columns are:    DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or    QUOTA REMAINING_QUATA SPACE_QUOTA REMAINING_SPACE_QUOTA           DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME  {noformat}  ",fs
GzipCodec should not represent BuiltInZlibInflater as decompressorType,It is possible to pollute CodecPool in such a way that Hadoop cannot read gzip-compressed data.  ,io
Serialize the 'final'ness of Configuration keys,Currently Configuration.writeXml doesn't serialize the 'final' attribute. There are some scenarios where it is useful:  # TaskTracker should mark the tasks' 'localized' parameters as 'final'   # GUI tools,conf
Upgrade to Avro 1.2.0,"Avro 1.2 has been released.  The API's Hadoop Common uses have been simplified, and it should be upgraded.","io,ipc"
Capacity reporting incorrect on Solaris,"When trying to get Hadoop up and running on Solaris on a ZFS filesystem, I encountered a problem where the capacity reported was zero:    Configured Capacity: 0 (0 KB)    It looks like the problem is with the 'df' output:    $ df -k /data/hadoop   Filesystem           1024-blocks        Used   Available Capacity  Mounted on  /                              0     7186354    20490274    26%    /    The following patch (applied to trunk) fixes the problem.  Though the real problem is with 'df', I suspect the patch is harmless enough to include?    Index: src/java/org/apache/hadoop/fs/DF.java  ===================================================================  --- src/java/org/apache/hadoop/fs/DF.java (revision 826471)  +++ src/java/org/apache/hadoop/fs/DF.java (working copy)  @@ -181,7 +181,11 @@           this.percentUsed = Integer.parseInt(tokens.nextToken());           this.mount = tokens.nextToken();           break;  -   }  +    }  +  +    if (this.capacity == 0)  + this.capacity = this.used + this.available;  +         }        public static void main(String[] args) throws Exception {  ",fs
Hadoop Common - Site logo,"Hadoop Common - Site Logo    Update the logo (see attached jpg).  Image has elephant + common.    With this update, Site logo and Documentation logo will be the same.",documentation
Serialization should provide comparators,The Serialization interface should permit one to create raw comparators.,io
FileSystem API should allow progress callbacks in rename and delete,"Some operations (e.g., rename and delete) can take very long running times on some filesystem implementations (e.g., S3). The API should provide the ability to include progress callbacks during these operations.",fs
Hundson runs should check for AspectJ warnings and report failure if any is present,When a code modifications break AspectJ bindings the following warning appears in the ant's build output:  {noformat}       [echo] Start weaving aspects in place       [iajc] warning at /Users/cos/work/branch-0.21/src/test/aop/org/apache/hadoop/hdfs/server/datanode/FSDatasetAspects.aj:55::0 advice defined in org.apache.hadoop.hdfs.server.datanode.FSDatasetAspects has not been applied [Xlint:adviceDidNotMatch]       [echo] Weaving of aspects is finished  {noformat}    Build process (in particular under Hudson) needs to check the output for such warnings and fail the build if any is present,build
Fix build error for one of the FileContext Tests,The build fails in Hudson  org.apache.hadoop.fs.TestLocalFSFileContextMainOperations.testWorkingDirectory  (from TestLocalFSFileContextMainOperations)  Failing for the past 5 builds (Since Failed#272 )  Took 88 ms.  add description  Error Message    chmod: changing permissions of `/tmp/existingDir': Operation not permitted     Stacktrace    org.apache.hadoop.util.Shell$ExitCodeException: chmod: changing permissions of `/tmp/existingDir': Operation not permitted      org.apache.hadoop.util.Shell.runCommand(Shell.java:243)    org.apache.hadoop.util.Shell.run(Shell.java:170)    org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:363)    org.apache.hadoop.util.Shell.execCommand(Shell.java:449)    org.apache.hadoop.util.Shell.execCommand(Shell.java:432)    org.apache.hadoop.fs.RawLocalFileSystem.execCommand(RawLocalFileSystem.java:545)    org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:537)    org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:347)    org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:184)    org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:769)    org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:539)    org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testWorkingDirectory(FileContextMainOperationsBaseTest.java:170)  ,test
Hadoop 0.20 Docs - backport changes for streaming and m/r tutorial docs,"Doc changes added to the Hadoop/mapreduce/trunk (for Hadoop 0.21 release) need to be backported to Hadoop-0.20 branch (for Hadoop 0.20.2 release).    Doc files affected:   > streaming.xml  > mapred_tutorial.xml      Changes include:  1. During the execution of a streaming job, the names of the ""mapred"" parameters are transformed. The dots ( . ) become underscores ( _ ). -- (affects streaming doc and m/r tutorial doc)    2. For -files and -archives options, Hadoop now creates symlink with same name as file (user-defined symlinks, #mysymlink, currently not supported) -- (affects streaming doc)    3. Streaming supports streaming command options and generic command options. Generic options must be placed before streaming options, otherwise command fails. -- (affects streaming doc)  ",documentation
Add build-fi directory to the ignore list,The build-fi directory should be added to the git and svn ignore lists.,build
Integrating IBM General Parallel File System implementation of Hadoop Filesystem interface,GPFS is a high performance parallel file system for GNU/Linux clusters. This patch contains the implementation of the Hadoop Filesystem Interface. There is dependency on the availability of GPFS on the host where the JNI implementation can be built.  The patch consists of fs/gpfs classes and the JNI module in c++/libgpfs.   http://www-03.ibm.com/systems/clusters/software/gpfs/index.html,fs
Large-scale Automated Test Framework,"Hadoop would benefit from having a large-scale, automated, test-framework. This jira is meant to be a master-jira to track relevant work.    ----    The proposal is a junit-based, large-scale test framework which would run against _real_ clusters.    There are several pieces we need to achieve this goal:    # A set of utilities we can use in junit-based tests to work with real, large-scale hadoop clusters. E.g. utilities to bring up to deploy, start & stop clusters, bring down tasktrackers, datanodes, entire racks of both etc.  # Enhanced control-ability and inspect-ability of the various components in the system e.g. daemons such as namenode, jobtracker should expose their data-structures for query/manipulation etc. Tests would be much more relevant if we could for e.g. query for specific states of the jobtracker, scheduler etc. Clearly these apis should _not_ be part of the production clusters - hence the proposal is to use aspectj to weave these new apis to debug-deployments.    ----    Related note: we should break up our tests into at least 3 categories:  # src/test/unit -> Real unit tests using mock objects (e.g. HDFS-669 & MAPREDUCE-1050).  # src/test/integration -> Current junit tests with Mini* clusters etc.  # src/test/system -> HADOOP-6332 and it's children",test
"Generic options parser eats up IOException during parsing of -files, -libjars and -archives ","In Generic options parser,  any IOException during parsing of -files, -libjars and -archives is just logged. Instead it should be thrown out.",util
GenericOptionsParser does not understand uri for -files -libjars and -archives option,"If we give an uri for -files, -libjars and -archives option , in GenericOptionsParser. It gives FileNotFoundException.",util
Update Map/Reduce Tutorial to use the classes Job and Configuration instead of JobConf,In the Map/Reduce tutorial of Hadoop Common 0.20.1 the deprecated class {{JobConf}} is still used in the examples. Please update the tutorial to reflect the API changes.,documentation
SequenceFile writer does not properly flush stream with external DataOutputStream,"When using the SequenceFile.createWriter(..,FSDataOutputStream, ...) method to create a Writer, data is not flushed when the encapsulating SequenceFile is closed.    Example test case skeleton:  {code}  public void testWhyFail() throws IOException {        // There a was a failure case using :      Configuration conf = ... ;      Path path = new Path(""file:///tmp/testfile"");      FileSystem hdfs = path.getFileSystem(conf);        // writing      FSDataOutputStream dos = hdfs.create(path);      hdfs.deleteOnExit(path);        // it is specifically with this writer.      Writer writer = SequenceFile.createWriter(conf, dos,          WriteableEventKey.class, WriteableEvent.class,          SequenceFile.CompressionType.NONE, new DefaultCodec());        Writable value = ...;      Writable key = ...;        writer.append(key, value);      writer.sync();      writer.close();        // Test fails unless I close the underlying FSDataOutputStream handle with the line below.      //    dos.close();             // WTF: nothing written by this writer!      FileStatus stats = hdfs.getFileStatus(path);      assertTrue(stats.getLen() > 0);      // it should have written something but it failed.    }  {code}",io
Change build/test/clean targets to prevent tar target from including clover instrumented code,"Currently the clover targets in build.xml cause the code generated in the build root to contain clover instrumented code.   Should the tar target be called, this instrumented code is packaged up and made part of what would be delivered to the grid for use.   Installing cloverized code on real clusters is generally a very bad idea unless it's done with significant upfront thought.       I propose that we alter the targets so that when clover is enabled, the compile target does two passes.   The first would generate the uninstrumented code in the standard build root.  The second pass would then generate clover instrumented code in a build-clover build root.  That way, the tar target would only pick up uninstrumented code.   I strongly suggest a 2 pass compile, and not a different target, because you never want the two sets of objects to be out of sync.  (For instance, you might want to run the clover instrumented unit tests, and then package the uninstrumented code to be delivered to the next step in your QA/Release process.)    The test targets would also need to be altered.  I'd propose that the test results still be placed in their current location in the build root, regardless of whether the tests were run with instrumented or uninstrumented code.   This means that when clover is enabled, that the test target would execute against the objects in build-clover, but report results in build.  (This would allow currently existing test infrastructure to continue to report results without modification.)    The clean target(s) would also need to be enhanced to clean out both build roots.    The only drawback to this approach I can see is that if you wanted to produce instrumented code to be delivered to a real grid, you'd have to create the package from build-clover instead of build manually, or we'd have to add a ""tar-with-clover"" target that did it for you.",build
rm and rmr fail to correctly move the user's files to the trash prior to deleting when they are over quota.  ,"With trash turned on, if a user is over his quota and does a rm (or rmr), the file is deleted without a copy being placed in the trash.    ",fs
Hudson giving a +1 though no tests are included.,"An example [here|https://issues.apache.org/jira/browse/MAPREDUCE-1160?focusedCommentId=12771147&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12771147]. We should revert to -1 overall, otherwise, there's a real chance developers will miss giving a justification.",build
"Create a script to squash a common, hdfs, and mapreduce tarball into a single hadoop tarball","It would be convenient for the transition if we had a script to take a set of common, hdfs, and mapreduce tarballs and merge them into a single tarball. This is intended just to help users who don't want to transition to split projects for deployment immediately.",build
Stack trace of any runtime exceptions should be recorded in the server logs. ,Hadoop RPC catches any server side exception and throws an IOException. Runtime excpetions should be recorded in the server logs before being thrown as IOException.,ipc
Refactor Trash::moveToTrash() and its accompanying poor tests,"We've had several issues relating to the Trash and its access via the shell, none of which were picked up by the unit tests.  The current moveToTrash method has 8 different ways it can terminate and sometimes uses a false value and sometimes uses an exception to indicate failure.  This method should be refactored to improve readability and testability, and new tests written to exercise all possible code paths.",fs
Add support for specifying unpack pattern regex to RunJar.unJar,The changes in Common necessary for MAPREDUCE-967:  - Adds support for Pattern types to Configuration (plus unit test)  - Adds support to specify a Pattern to RunJar.unJar to decide which files get unpacked,"conf,util"
ant test  command should not pick up hadoop-site.xml from conf directory,ant test  command currently picks up hadoop-site.xml from conf directory. This results in test failures if hadoop-site.xml has a different configuration.,build
Implement FastLZCodec for fastlz/lzo algorithm,"Per  [HADOOP-4874|http://issues.apache.org/jira/browse/HADOOP-4874], FastLZ is a good (speed, license) alternative to LZO. ",io
Documenting Hadoop metrics,"Metrics should be part of public API, and should be clearly documented similar to HADOOP-5073, so that we can reliably build tools on top of them.","documentation,metrics"
CSV Grammer should have } as disallowed in buffer,"The CSV Writer outputs stuff like :    v{s{#5052494d415259,#505249...4e3e}}    Which by the grammar should still have the }} being part of the buffer. If you add ""}"" as a disallowed character, then this is parsable.    Grammar is under CSV here : http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/record/package-summary.html",record
"Make FileSystemContractBaseTest, LocalFileSystem, and FileSystem javadoc compliant ","These do not all seem to play nicely together. ie, plugging the LocalFileSystem into FileSystemContractBaseTest (via a subclass that just overrides setUp to set fs=LocalFileSystem) results in 5 failures and 19 errors out of 28 tests.  Additionally, the unit tests expect a few undocumented behaviors (ie, it expects to get an IOException on mkdirs(subdir/of/a/file) rather than just returning false) and some contradictory behaviors (it expects that delete(directory, false) should succeed on an empty directory) compared to FileSystem's javadoc.    I suspect that most of the fixes should come from changing the unit tests to be more sensible rather than modifying [Local]FileSystem, but however it's done these should all agree on expected behavior!",fs
Add a Cache for AbstractFileSystem in the new FileContext/AbstractFileSystem framework.,"The new filesystem framework, FileContext and AbstractFileSystem does not implement a cache for AbstractFileSystem.  This Jira proposes to add a cache to the new framework just like with the old FileSystem.",fs
Move Seekable and PositionedReadable interfaces from o.a.h.fs to o.a.h.io package,"Currently Seekable and PositionedReadable live in o.a.h.fs package. They really do not belong there, I propose we move them to the o.a.h.io package.","fs,io"
distributed cache doesn't work with HDFS and another file system,"This is a continuation of http://issues.apache.org/jira/browse/HADOOP-5635 (JIRA wouldn't let me edit that one). I found another issue with DistributedCache using something besides HDFS. In my case I have TWO active file systems, with HDFS being the default file system.    My fix includes two additional changes (from HADOOP-5635) to get it to work with another filesystem scheme (plus the changes from the original patch). I've tested this an it works with my code on HDFS with another filesystem. I have similar changes to mapreduce.filecacheTaskDistributedCacheManager and TrackerDistributedCacheManager (0.22.0).    Basically, URI.getPath() is called instead of URI.toString(). toString returns the scheme plus path which is important in finding the file to copy (getting the file system). Otherwise it searches the default file system (in this case HDFS) for the file.  ",filecache
Move Access Token implementation from Common to HDFS,"Access Token is HDFS specific and should be part of HDFS code base. Also, rename AccessToken to BlockAccessToken (and AccessKey to BlockAccessKey) to be more precise.",security
add Grid Engine support to HOD,"At [CRS4, Distributed Computing Group|http://dc.crs4.it], we developed and tested a simple patch that adds Grid Engine support to HOD. Since porting HOD to SGE is listed in the [Project Suggestions page|http://wiki.apache.org/hadoop/ProjectSuggestions], we decided to share the patch with the community. After patching, HOD passes all unit tests at our site, and we tested the SGE driver on a production cluster consisting of about 400 nodes. The patch should preserve Torque functionality, although we were not able to test this on production.    *NOTE*: the patch is for the official hadoop-0.20.1 HOD release. It should be easy, however, to generate patches for other versions.",contrib/hod
Deprecate FileSystem,"Here's a jira to track deprecating FileSystem. There are two high-level tasks:    1. Moving clients in common and mapreduce onto FileContext. I think this is currently blocked on HADOOP-6356 and HADOOP-6361. Anything else preventing them from being moved over? Any clients we could move over today?    2. Moving file system implementations onto AbstractFileSystem. Currently Hdfs and FilterFS (and local and checksum) extend AFS. RawLocalFileSystem uses the delegator. S3, ftp and others need to be moved over. Don't think there's anything blocking this stuff.    3. Stabilize FileContext",fs
Contrib project ivy dependencies are not included in binary target,Only Hadoop's own library dependencies are promoted to ${build.dir}/lib; any libraries required by contribs are not redistributed.,build
Misleading information in documentation - Directories don't use host file system space and don't count against the space quota.,Need to remove misleading information from quota documentation.    {noformat}  Directories don't use host file system space and don't count against the space quota.   {noformat},documentation
MurmurHash does not yield the same results as the reference C++ implementation when size % 4 >= 2,"Last rounds of MurmurHash are done in reverse order. data[length - 3], data[length - 2] and data[length - 1] in the block processing the remaining bytes should be data[len_m +2], data[len_m + 1], data[len_m].",util
JUnit tests should never depend on anything in conf,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete conf/mapred-queues.xml out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,test
Update documentation for FsShell du command,"HADOOP-4861 added new syntax for the FsShell du command, but neglected to update documentation. This JIRA is to add documentation for it to both branch-21 and trunk.",documentation
slaves file to have a header specifying the format of conf/slaves file ,"When we open the file conf/slaves - it is not immediately obvious what the format of the file is ( a comma-separated list or one per each line). The docs confirm it is 1 per line.     Specifying the information by means of a comment in the template so that it is easy to modify the same, and self-explanatory. ",conf
ChecksumFileSystem.getContentSummary throws NPE when directory contains inaccessible directories,"When getContentSummary is called on a path that contains an unreadable directory, it throws NPE, since RawLocalFileSystem.listStatus(Path) returns null when File.list() returns null.",fs
Broken symlinks in directory break RawLocalFileSystem.listStatus,"I have a broken unix symlink in my home directory. listStatus on that directory fails with a FileNotFoundException, which is unexpected.",fs
WritableUtils::*VLong utilities should be available for byte arrays,"Particularly when working with raw bytes in Writables, it is often useful to have versions of the vint utility functions for byte arrays.",util
publish hadoop jars to apache mvn repo.,use maven ant task to publish hadoop 20 jars to the apache maven repo,build
dfs does not support -rmdir (was HDFS-639),"From HDFS-639  > Given we have a mkdir, we should have a rmdir. Using rmr is not  > a reasonable substitute when you only want to delete empty  > directories.",fs
FsShell -getmerge source file pattern is broken,"The FsShell -getmerge command doesn't work if the ""source file pattern"" matches files. See below. If the current behavior is intended then we should update the help documentation and java docs to match, but it would be nice if the user could specify a set of files in a directory rather than just directories.    {code}  $ hadoop fs -help getmerge  -getmerge <src> <localdst>:  Get all the files in the directories that     match the source file pattern and merge and sort them to only    one file on local fs. <src> is kept.  $ hadoop fs -ls  Found 3 items  -rw-r--r--   1 eli supergroup          2 2009-11-23 17:39 /user/eli/1.txt  -rw-r--r--   1 eli supergroup          2 2009-11-23 17:39 /user/eli/2.txt  -rw-r--r--   1 eli supergroup          2 2009-11-23 17:39 /user/eli/3.txt  $ hadoop fs -getmerge /user/eli/*.txt sorted.txt  $ cat sorted.txt  cat: sorted.txt: No such file or directory  $ hadoop fs -getmerge /user/eli/* sorted.txt  $ cat sorted.txt  cat: sorted.txt: No such file or directory  $ hadoop fs -getmerge /user/* sorted.txt  $ cat sorted.txt   1  2  3  {code}",fs
Add support for LZF compression,"(note: related to [HADOOP-4874])    As per Doug's earlier comments, LZF does indeed look like a good compressor candidate for fast compression/decompression, good enough compression rate.  From my testing it seems at least twice as fast at compression, and somewhat faster for decompressing than gzip.  Code from [http://h2database.googlecode.com/svn/trunk/h2/src/main/org/h2/compress/] is applicable, and I have tested it with json data.    I hope to have more to spend on this in near future, but if someone else gets to this first that'd be good too.    ",io
Classpath should not be part of command line arguments,"Because bin/hadoop and bin/hdfs put the entire CLASSPATH in the command line arguments, it exceeds 4096 bytes, which is the maximum size that ps (or /proc) can work with. This makes looking for the processes difficult, since the output gets truncated for all components at the same point (e.g. NameNode, SecondaryNameNode, DataNode).     The mapred sub-project does not have this problem, because it calls ""export CLASSPATH"" before the final exec. bin/hadoop and bin/hdfs should do the same thing",scripts
Allow master to run on larger EC2 instance,"The namenode often needs more memory, so it would be useful to allow it to be run on a different instance size.",contrib/cloud
Helper class for FileContext tests,"    A helper class for FileContext tests should contain common methods which can be used in many unit tests, so that every unit test doesn't have to re-implement these functionality.  Examples of such methods:    createFile(FileContext fc, Path path, int numBlocks, int blockSize)  //To create a file with number of blocks and block-size passed.    getTestRootPath(FileContext fc)",test
Provide a description in the exception when an error is encountered parsing umask,"Currently when there is a problem parsing a umask, the exception text is just the offending umask with no other clue, which can be quite confusing as demonstrated in HDFS-763.  This message should include the nature of the problem and whether or not the umask parsing attempt was using old-style or new-style values.",fs
topology script called with host names instead of IP addresses,"According to the 'Hadoop Rack Awareness' section on http://hadoop.apache.org/common/docs/r0.20.0/cluster_setup.html the script/program topology.script.file.name can assume that names are passed in as IP addresses.    This seems to work for hdfs, but the rack-awareness of the JobTracker does not always work, depending on the hostnames of the cluster, because of a bug in the method normalizeHostName of org.apache.hadoop.net.NetUtils.java:    It returns the host name as-is when it decides that the string passed-in is already an IP but actually it checks whether the string starts with a valid hex character!  There is no need to check at all, because InetAddress.getByName is smart enough to not do a dns lookup when the name is an IP address.",util
Build is broken after HADOOP-6395 patch has been applied,After new version of AspectJ jar files were introduces by HADOOP-6395 patch Common and HDFS builds are broken  {noformat}  compile-fault-inject:    [taskdef] Could not load definitions from resource org/aspectj/tools/ant/taskdefs/aspectjTaskdefs.properties. It could not be found.       [echo] Start weaving aspects in place    BUILD FAILED  /Users/xxx/work/H0.22/common/src/test/aop/build/aop.xml:78: The following error occurred while executing this line:  /Users/xxx/work/H0.22/common/src/test/aop/build/aop.xml:56: Problem: failed to create task or type iajc  {noformat}  ,build
Shuffle deadlocks on wrong number of maps,"The new shuffle assumes that the number of maps is correct. The new JobSubmitter sets the old value. Something misfires in the middle causing:    09/12/01 00:00:15 WARN conf.Configuration: mapred.job.split.file is deprecated. Instead, use mapreduce.job.splitfile  09/12/01 00:00:15 WARN conf.Configuration: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps    But my reduces got stuck at 2 maps / 12 when there were only 2 maps in the job.  ",conf
"Specify a policy to define tests placement into categories: unit, functional, system, etc.","A clear guidelines are needed to define which tests are true unit tests, which are functional, systems, et cetera.  Otherwise it isn't obvious to everybody why certain decisions (i.e. why this test is a unit test?) are made.",test
Log errors getting Unix UGI,"For various reasons, the calls out to `whoami` and `id` can fail when trying to get the unix UGI information. Currently it silently ignores failures and uses the default DrWho/Tardis ugi. This is extremely confusing for users - we should log the exception at warn level when the shell execs fail.",security
testConf.xsl is not well-formed XML,"File {{/org/apache/hadoop/cli/testConf.xsl}} is not valid XML, as the <?xml?> directive comes after the comment. XML requires this to be the first thing in a file, so it can be used to determine the encoding.",test
Deprecate EC2 bash scripts,"With the addition of python-based EC2 scripts introduced in HADOOP-6108, the bash scripts in src/contrib/ec2 should be deprecated.",contrib/cloud
Rename the generated artifacts to common instead of core,"In the project split we renamed Core to Common, but failed to change the artifact names. We should do it before 0.21.0 is released.",build
Update Eclipse configuration to match changes to Ivy configuration,"The .eclipse_templates/.classpath file doesn't match the Ivy configuration, so I've updated it to use the right version of commons-logging",build
hadoop-core.pom contains hardcoded and nonexistent commons-cli version and jetty groupId/artifactId mixup,"hadoop-core.pom in trunk contains  a) hardcoded non-existing commons-cli version ""2.0-20070823""  b) jetty groupId/artifactId mixup : the given artifactId is the groupId, the groupId is the artifactId  ",build
Have a way to automatically update Eclipse .classpath file when new libs are added to the classpath through Ivy,"Currently Eclipse configuration (namely .classpath) isn't synchronized automatically when lib versions are changed. This causes great inconvenience so people have to change their project settings manually, etc.     It'd be great if these configs could be updated automatically every time such a change takes place, e.g. whenever ivy is pulling in new version of a jar. ",build
Rename TestCLI class to prevent JUnit from trying to run this class as a test,"TestCLI is a helper class which implements some common functionality for CLI based test (e.g {{TestDHFSCLI}}.  It doesn't include any tests by itself. However, JUnit tries to run it as a test because it has Test prefix in its name.",test
Remove deprecated file src/test/hadoop-site.xml,"hadoop-site.xml is deprecated. core-site.xml has to be used instead. However, hadoop-site.xml has been left in src/test folder which causes a lot of confusion and warning messages in the test runs.",test
Add ability to retrigger hudson with special token in comment,"It would be great if you could put some magic token (eg @HudsonTrigger) in the body of any JIRA comment in order to get the QA bot to rerun on the latest patch. Currently you have to toggle Patch Available status, which generates a lot of needless email.",yetus
Move TestReflectionUtils to Common,The common half of MAPREDUCE-1209,test
Adding a common token interface for both job token and delegation token,Both job token and delegation token will be used by RPC.,security
Alternative Java Distributions in the Hadoop Documention,"The documentation on the Hadoop Site, Wiki and Download pages specify to use the Sun distribution of Java. While many people use and test with Sun Java, it is not always the appropriate choice, most notably when there is not a version available for the target OS. To help users In such situations we should provide them with information about the available alternatives.",documentation
FileSystem should have mkdir and create file apis which do not create parent path.,FileSystem should have mkdir and create file apis which do not create parent path. The usecase is illustrated at [this|https://issues.apache.org/jira/browse/HDFS-98?focusedCommentId=12595989&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12595989],fs
Change RPC layer to support SASL based mutual authentication,"The authentication mechanism to use will be SASL DIGEST-MD5 (see RFC-2222 and RFC-2831) or SASL GSSAPI/Kerberos. Since J2SE 5, Sun provides a SASL implementation by default. Both our delegation token and job token can be used as credentials for SASL DIGEST-MD5 authentication.",security
String-to-String Maps should be embeddable in Configuration,"Per MAPREDUCE-1126, we need to be able to take a map of (key, value) pairs and embed that inside a Configuration object.",conf
permit RPC protocols to be implemented by Avro,"To more easily permit Hadoop to evolve to use Avro RPC, I propose to change RPC to use different implementations for clients and servers based on the configuration.  This is not intended as an end-user configuration: only a single RPC implementation will be supported in a given release, but rather a tool to permit us to more easily develop and test new RPC implementations.  As such, the configuration parameters used would not be documented.",ipc
Add unit tests framework (Mockito),Common tests are functional tests or end to end. It makes sense to have Mockito framework for the convenience of true unit tests development.   ,build
Server listener binds to wrong IP address for domain name given,"High-level:   In my configuration files, I specifiy:  {code:xml}   <property>      <name>fs.default.name</name>      <value>hdfs://{{internal-dns}}:8020</value>   </property>  {code}    The name node server binds to address {{external-ip}}:8020, while all the data nodes try to connect to {{internal-ip}}:8020, and nothing works.    Low-level: I've traced this down as far as {{org.apache.hadoop.ipc.Server}}, private class {{Listener}}.    {code:java}    /** Listens on the socket. Creates jobs for the handler threads*/    private class Listener extends Thread {            private ServerSocketChannel acceptChannel = null; //the accept channel      private Selector selector = null; //the selector that we use for the server      private InetSocketAddress address; //the address we bind at      private Random rand = new Random();      private long lastCleanupRunTime = 0; //the last time when a cleanup connec-                                           //-tion (for idle connections) ran      private long cleanupInterval = 10000; //the minimum interval between                                             //two cleanup runs      private int backlogLength = conf.getInt(""ipc.server.listen.queue.size"", 128);            public Listener() throws IOException {        address = new InetSocketAddress(bindAddress, port);        LOG.info(""in Server/Listener. bindAddress="" + bindAddress + "", address=""+address);    //Added by EA for testing        // Create a new server socket and set to non blocking mode        acceptChannel = ServerSocketChannel.open();        acceptChannel.configureBlocking(false);          // Bind the server socket to the local host and port        bind(acceptChannel.socket(), address, backlogLength);        port = acceptChannel.socket().getLocalPort(); //Could be an ephemeral port  {code}    The problem  seems to be in the line {code}address = new InetSocketAddress(bindAddress, port);{code}.  bindAddress  contains the DNS name of the *internal* interface, but address comes out with the IP address of the *external* interface.    ",ipc
Need indication if FindBugs' warnings level isn't 0,"It's be great to have some indication (also reflected by Hudson) if the level of FindBugs warnings during a particular build wasn't 0 (zero). I.e. build might be marked unstable, etc.",build
Enhance Startup Scripts to parametrize the path to a Plugin Directory Path,Update the hadoop scripts to use a configurable plugin directory for java plugins.   Allow the path to be set in hadoop-env.sh so that plugins can be pulled from a location outside of the hadoop installation directory.,scripts
Make RPC.waitForProxy with timeout public,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive.    The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",ipc
Prevent remote CSS attacks in Hostname and UTF-7.,There are currently vulnerabilities for CSS in Hadoop's Web UI that allow CSS attacks.,security
Serialization classes accept invalid metadata,The {{SerializationBase.accept()}} methods of several serialization implementations use incorrect metadata when determining whether they are the correct serializer for the user's metadata.,io
Enhance FSDataOutputStream to allow retrieving the current number of replicas of current block,"The current HDFS implementation has the limitation that it does not replicate the last partial block of a file when it is being written into until the file is closed. There are some long running applications (e.g. HBase) which writes transactions logs into HDFS. If datanode(s) in the write pipeline dies off, the application has no knowledge of it until all the datanode(s) fail and the application gets an IO error.    These applictions would benefit a lot if they can determine the number of live replicas of the current block to which it is writing data. For example, the application can decide that when one of the datanode in the write pipeline fails it will close the file and start writing to  a new file.",fs
Contrib tests are not being run,The test target in src/contrib/build.xml references contrib modules that are no longer there post project split. This was discovered in HADOOP-6426.,build
Hadoop wrapper script shouldn't ignore an existing JAVA_LIBRARY_PATH,Currently the hadoop wrapper script assumes its the only place that uses JAVA_LIBRARY_PATH and initializes it to a blank line.    JAVA_LIBRARY_PATH=''    This prevents anyone from setting this outside of the hadoop wrapper (say hadoop-config.sh) for their own native libraries.    The fix is pretty simple. Don't initialize it to '' and append the native libs like normal. ,scripts
Set Hadoop User/Group by System properties or environment variables,"Hadoop User/Group can be set by System properties or environment variables.  For example, in environment variables,  export HADOOP_USER=test  export HADOOP_GROUP=user    or in your MapReduce,  System.setProperty(""hadoop.user.name"", ""test"");  System.setProperty(""hadoop.group.name"", ""user"");",security
webapps aren't located correctly post-split,"Post-split, when one builds common it creates an empty build/webapps dir. If you then try to launch the NN for example using HADOOP_HDFS_HOME=/path/to/hdfs hdfs namenode, HttpServer.getWebAppsPath locates the empty common webapps dir, and the NN web UI fails to load.",util
"contrib/cloud failing, target ""compile"" does not exist","I'm not seeing this mentioned in hudson or other bugreports, which confuses me. With the addition of a src/contrib/cloud/build.xml from HADOOP-6426, contrib/build.xml won't build no more:   hadoop-common/src/contrib/build.xml:30: The following error occurred while executing this line:  Target ""compile"" does not exist in the project ""hadoop-cloud"".     What is odd is this: the final patch of HADOOP-6426 does include the stub <target> files needed, yet they aren't in SVN_HEAD. Which implies that a different version may have gone in than intended. ",build
Uninformative error message for some sort of file permission problem from the hadoop command,"It would be better if this error message would include the pathname for which permission was denied.      benson@cn0:/u1/benson/hadoop-test$ /u1/hadoop/bin/hadoop jar /u1/hadoop/hadoop-0.20.1-examples.jar wordcount /users/benson/gutenberg /users/benson/gutenberg-output  Exception in thread ""main"" java.io.IOException: Permission denied    java.io.UnixFileSystem.createFileExclusively(Native Method)    java.io.File.checkAndCreate(File.java:1716)    java.io.File.createTempFile(File.java:1804)    org.apache.hadoop.util.RunJar.main(RunJar.java:115)    ",util
Performance improvement for liststatus on directories in hadoop archives.,A liststatus call on a directory in hadoop archives leads to ( 2* number of files in directory) open calls to the namenode. This is very sub optimal and needs to be fixed to make it performant enough to be used on a daily basis. ,fs
compile-core-test failing with java.lang.NoSuchMethodError: org.objectweb.asm.ClassWriter.<init>,"I'm filing this for something for the search engines to index, so when others hit the problem, the solution is here.     hadoop-common's tests arent' compiling on one machine,  java.lang.NoSuchMethodError: org.objectweb.asm.ClassWriter.<init>",build
Add a command-line diagnostics entry point,"We could extend the conf/ servlet idea with a command line option to dump out the local settings/jvm state for:  # End users to debug problems themselves  # Attachment to bug report  # Something for Hadoop to collect and mine  # An entry point we can use in tests such as HADOOP-6453    Ant's {{ant -diagnostics}} is the basic idea and starting point; that code  would be the foundation: [Diagnostics.java|http://svn.apache.org/viewvc/ant/core/trunk/src/main/org/apache/tools/ant/Diagnostics.java?view=markup]      I'm keeping this separate from the more complex challenge of per-service diagnostics, HADOOP-6473, which could be added later. Here I propose starting with the common basic things you need to look at:  * JVM properties  * Env variables  * XML and XSL Factories and features  * Logging option -and if set to log4j, the log4j options  * Local Hadoop configuration  * java.io.tmpdir: writeability. Ant looks for clock drift, which is overkill here but matters for a build tool  * local networking state: local hostname, what that hostname resolves to, proxy settings.  * Maybe have a list of classes to look for and try and load (HDFS, Hive, MapReduce entry points), and log the JARs that host them, plus any errors if loading fails. By looking for all the standard filesystems, the diagnostics would pick up on missing dependency JARs.    The output could be changed to generate and an-memory tree of (name,value) pairs put together; this could then be printed as XML or JSON.      Testing? TestCLI, obviously.",util
Improvements to the hadoop-config script,Two improvements to the hadoop-config.sh script    1) dont set the HADOOP_HOME env if its already set    2) automatically detect JAVA_HOME if not already set    These changes have been stolen from the Cloudera distro.  ,scripts
TestUTF8 assertions could fail with better text ,"My HADOOP-6220 patch failed on Hudson with an error TestUTF8, but not one that is in any way useful:  {code}  Error Message: null    Stacktrace:    junit.framework.AssertionFailedError: null    org.apache.hadoop.io.TestUTF8.testIO(TestUTF8.java:80)  {code}  The tests should use better assertions for easier diagnostics",test
GenericOptionsParser constructor that takes Options and String[] ignores options,"This constructor:      public GenericOptionsParser(Options opts, String[] args) {      this(new Configuration(), new Options(), args);    }    should do a this(new Configuration(), opts, args)",util
OSGI headers in jar manifest,"When using hadoop inside an OSGI environment one needs to change the META-INF/MANIFEST.MF file to include OSGI headers (version and symbolic name). It would be convenient to do this in the default build.xml.     There are no runtime dependencies.    An easy way of doing this is to use the bnd ant task: http://www.aqute.biz/Code/Bnd    <target name=""build"">      <taskdef resource=""aQute/bnd/ant/taskdef.properties""        classpath=""bnd.jar""/>      <bnd         classpath=""src""         eclipse=""true""         failok=""false""         exceptions=""true""         files=""test.bnd""/>    </target>",build
Trash fails on Windows,"Using local file system on windows Trash tries to move file ""file:/C:/tmp/testTrash/foo"" to ""file:/C:/Documents and Settings/shv/.Trash/Current/C:/tmp/testTrash/foo"" with ""C:"" in the middle.",fs
fix common classes to work with Avro 1.3 reflection,A few minor changes are required to get some common classes to work correctly with Avro 1.3 reflection.,ipc
Support ranges in hosts file,It would be nice to have a range support for hosts file. For example host files could look like  {code:title=hosts.include|borderStyle=solid}  abc1-10.com  abc21-30.com  {code}  {code:title=hosts.exclude|borderStyle=solid}  abc1-5.com  {code}  ,util
Measure memory usage of Configuration object resource tracking,HADOOP-6408 modified Configuration to track the resource from which each setting was set. There's some concern that this may increase the JT memory footprint too much. This JIRA is to measure and record the memory usage for Configuration before and after HADOOP-6408.,conf
"Findbug report: LI_LAZY_INIT_STATIC, OBL_UNSATISFIED_OBLIGATION","From findbug report:    Method org.apache.hadoop.io.compress.CompressionCodecFactory.main(String[]) may fail to clean up java.io.OutputStream    Incorrect lazy initialization of static field org.apache.hadoop.fs.FileContext.localFsSingleton in org.apache.hadoop.fs.FileContext.getLocalFSFileContext()    Incorrect lazy initialization of static field org.apache.hadoop.util.ReflectionUtils.serialFactory in org.apache.hadoop.util.ReflectionUtils.getFactory(Configuration)    Given that these have simple fixes I think one bug is enough.    This is from findbug 1.3.9 running on current trunk:    Path: .  URL: http://svn.apache.org/repos/asf/hadoop/common/trunk  Repository Root: http://svn.apache.org/repos/asf  Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68  Revision: 898558  Node Kind: directory  Schedule: normal  Last Changed Author: tomwhite  Last Changed Rev: 897023  Last Changed Date: 2010-01-07 13:43:38 -0800 (Thu, 07 Jan 2010)  ","fs,io,util"
Path.normalize should use StringUtils.replace in favor of String.replace,"in our environment, we are seeing that the JobClient is going out of memory because Path.normalizePath(String) is called several tens of thousands of times, and each time it calls ""String.replace"" twice.    java.lang.String.replace compiles a regex to do the job which is very costly.  We should use org.apache.commons.lang.StringUtils.replace which is much faster and consumes almost no extra memory.  ",fs
HOD clusters terminate after one hour regardless of idleness-limit setting,"HOD submits its Torque jobs with walltime=1hour parameter.  That causes Torque to terminate the cluster regardless what the idleness-limit setting is.      Both the ""idleness-limit"" and ""walltime"" settings should be documented.",contrib/hod
MapFile.Reader does not seek to first entry for multi-valued key,"When a MapFile contains a key with multiple entries and one of these entries other than the first happens to be stored in the index, then the Reader's seek() and get*() methods will generally not return the first entry, making it impossible to retrieve all of the key's entries using next().    One easy solution would be to modify the Writer's append() method to only index an entry if it's the first entry belonging to its key, e.g.:          public synchronized void append(WritableComparable key, Writable val)        throws IOException {          boolean equalsLastKey = (size != 0 && comparator.compare(lastKey, key) == 0);        checkKey(key);          boolean largeEnoughInterval = size % indexInterval == 0;        if (largeEnoughInterval && !equalsLastKey) {            // add an index entry          position.set(data.getLength());           // point to current eof          index.append(key, position);        }          data.append(key, val);                      // append key/value to data        if (!largeEnoughInterval || !equalsLastKey)            size++;      }      (The size variable should then be renamed to something more accurate.)      ",io
Introduce wrapper around FSDataInputStream providing Avro SeekableInput interface,Reading data from avro files requires using Avro's SeekableInput interface; we need to be able to wrap FSDataInputStream in this interface.,fs
IPC client  bug may cause rpc call hang,I can reproduce some rpc call  hang bug when connection thread of ipc client receives response for outstanding call.     The stacks when hang occurs (TaskTracker):      Waiting on org.apache.hadoop.ipc.Client$Call@1c3cbb4b    Stack:      java.lang.Object.wait(Native Method)      java.lang.Object.wait(Object.java:485)      org.apache.hadoop.ipc.Client.call(Client.java:691)      org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)      org.apache.hadoop.mapred.$Proxy4.heartbeat(Unknown Source)      org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1250)      org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1082)      org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:1785)      org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:2796),ipc
Only the S3 test extends FileSystemContractBaseTest; the other FS tests do not,"TestChecksumFileSystem etc does not extend FileSystemContractBaseTest  In fact, ChecksumFileSystem doesn't pass the ContractBaseTest",test
RawLocalFileSystem.mkdirs does not throw IOException when it should?,"public boolean mkdirs(Path f) throws IOException {      Path parent = f.getParent();      File p2f = pathToFile(f);      return (parent == null || mkdirs(parent)) &&        (p2f.mkdir() || p2f.isDirectory());    }  none of the methods called throw IOException           yet FileSystemContract BaseTest.testMkdirsFailsForSubdirectoryOfExistingFile fails      createFile(path(""/test/hadoop/file""));         Path testSubDir = path(""/test/hadoop/file/subdir"");      try {        fs.mkdirs(testSubDir);    //<---------returns false instead of throwing exception        fail(""Should throw IOException."");      } catch (IOException e) {        // expected      }  --------------------------------------  further UNIX mkdir operates on a path argument.   why does hadoop mkdir create directories in the path one at a time???  NOTE that a permission or other failure leaves a garbage partially-completed path in the current hadoop impl.",fs
DistributedFileSystem#listStatus is very slow when listing a directory with a size of 1300,"When listing a directory of around 1300 children, it takes hundreds of milliseconds. It turns out the slowdowness is caused by the change made by HADOOP-4187. The return value of listStatus is an array of FileStatus. When deserializing each element of the array, ReflectionUtils#newInstance(Class<T>, Configuration) is called and then calls setConf, which calls setJobConf. SetJobConf checks if JobConf is on the class path by calling Configuration#getClassByName. Even though Configuration#getClassByName tries to optimize the lookup using a cached map, but since JobConf is not in the class path, so it is not in the cache. Every checkup ends up calling Class.ForName which is very expensive. Deserializing an array of 1300 entries requires calling of Class#ForName 1300 times!",util
contrib projects should pull in the ivy-fetched libs from the root project,"On branch-20 currently, I get an error just running ""ant contrib -Dtestcase=TestHdfsProxy"". In a full ""ant test"" build sometimes this doesn't appear to be an issue. The problem is that the contrib projects don't automatically pull in the dependencies of the ""Hadoop"" ivy project. Thus, they each have to declare all of the common dependencies like commons-cli, etc. Some are missing and this causes test failures.",build
Invalid example in the documentation of org.apache.hadoop.util.Tool,The example with the class {{MyApp}} doesn't work.  The {{run}} method needs to return an int ({{return 0;}} needs to be added at the end of the method) and {{main}} does a {{new Sort()}} instead of {{new MyApp()}},documentation
sed in build.xml fails,"I'm not sure whether this is a Solaris thing or an ant 1.7.1 thing, but it definitely doesn't do what it is supposed to.  Instead of getting SunOS-x86-32 (or whatever) I get -x86-32.    This patch replaces the sed call with tr. ",build
Failing tests prevent the rest of test targets from execution.,"Build system of 0.20 has the problem which has been fixed in after split (i.e. past 0.20) subprojects but never been properly backported.   During an execution of   {noformat}    % ant test  {noformat}  if a single test case gets failed say in {{test-core}} then {{test-contrib}} won't be executed at all.    This happens because of Ant dependencies mechanism as {{test}} target depends on {{test-core, test-contrib}}",build
Hadoop Common Docs - delete 3 doc files that do not belong under Common,Delete these 3 files from Hadoop Common TRUNK    \src\docs\src\documentation\content\xdocs\streaming.xml  (this file already in (and belongs in) Hadoop MAPREDUCE TRUNK)  \src\docs\src\documentation\content\xdocs\libhdfs.xml        (this file already in (and belongs in) Hadoop HDFS TRUNK)  \src\docs\src\documentation\content\xdocs\hdfs_permissions_guide.xml  (this file already in (and belongs in) Hadoop HDFS TRUNK),documentation
Incorrect values for metrics with CompositeContext,"In our clusters, when we use CompositeContext with two contexts, second context gets wrong values.  This problem is consistent on 500 (and above) node cluster.",metrics
FsPermission.getDefault choice of 0777 is bad in all cases,  /** Get the default permission. */  227   public static FsPermission getDefault() {  228     return new FsPermission((short)00777);  229   }    setting any data file to execute-others is bad security under all circumstances  should be 664 or 644,fs
Complex Writable classes are not thread safe,"While SequenceFile methods are properly ""synchronized"", the complex Writable classes are not thread safe.    e.g. ArrayWritable, interleaved in... calls by different threads will scramble input      public void readFields(DataInput in) throws IOException {      values = new Writable[in.readInt()];          // construct values      for (int i = 0; i < values.length; i++) {        Writable value = WritableFactories.newInstance(valueClass);        value.readFields(in);                       // read a value        values[i] = value;                          // store it in values      }    }    Please add synchronized prefixes.  Not needed for simple types.",io
SequenceFile.Sorter  design issue and class-check bug,"SequenceFile.Writer takes key/value classes as creation arguments and checks for validity on every append.  Reader does not take class arguments on creation because they are derived from the input file.  Sorter takes key/value classes as creation arguments??  no point.  should be derived from input.    In any case, SortPass does not compare Sorter key/value classes with input file classes.  No error is given for the following:       private static void writeTest4(FileSystem fs, int count, int seed, Path file,        SequenceFile.CompressionType compressionType, CompressionCodec codec, Configuration conf)       throws IOException {       fs.delete(file, true);       LOG.info(""creating "" + count + "" records with "" + compressionType +                "" compression"");       SequenceFile.Writer writer =          SequenceFile.createWriter(fs, conf, file,            StringWritable.class, FloatWritable.class, compressionType, codec);       FloatWritable x=new FloatWritable();       StringWritable y=new StringWritable();       for (int i = count-1; i >= 0; i--) {         x.set(i);  y.set(""""+i);         writer.append(y, x);       }       writer.close();     }       private static void sortTest(FileSystem fs, int count, int megabytes,        int factor, boolean fast, Path file, Configuration conf)     throws IOException {      fs.delete(new Path(file+"".sorted""), true);      SequenceFile.Sorter sorter = newSorter(fs, fast, megabytes, factor, conf);      LOG.debug(""sorting "" + count + "" records"");      sorter.sort(file, file.suffix("".sorted""));      LOG.info(""done sorting "" + count + "" debug"");     }          private static SequenceFile.Sorter newSorter(FileSystem fs,        boolean fast,       int megabytes, int factor, Configuration conf) {      SequenceFile.Sorter sorter =        fast       ? new SequenceFile.Sorter(fs, new IntWritable.Comparator(),         FloatWritable.class, IntWritable.class, conf)      : new SequenceFile.Sorter(fs, FloatWritable.class, IntWritable.class, conf);       sorter.setMemory(megabytes * 1024*1024);       sorter.setFactor(factor);       return sorter;     }  ---------------------Note String/Float  does not match Float/Int  Macintosh-2:datanode bobcook$ od -c file            0000000    S   E   Q 006 016   S   t   r   i   n   g   W   r   i   t   a  0000020    b   l   e  \r   F   l   o   a   t   W   r   i   t   a   b   l  0000040    e  \0  \0  \0  \0  \0  \0 203   `   n   E   J   z 272   d 352  0000060    w 177 373  \n 364   M 276  \0  \0  \0  \n  \0  \0  \0 006  \0  0000100   \0  \0 001  \0   4   @ 200  \0  \0  \0  \0  \0  \n  \0  \0  \0  0000120  006  \0  \0  \0 001  \0   3   @   @  \0  \0  \0  \0  \0  \n  \0  0000140   \0  \0 006  \0  \0  \0 001  \0   2   @  \0  \0  \0  \0  \0  \0  0000160   \n  \0  \0  \0 006  \0  \0  \0 001  \0   1   ? 200  \0  \0  \0  0000200   \0  \0  \n  \0  \0  \0 006  \0  \0  \0 001  \0   0  \0  \0  \0  *  0000220  Macintosh-2:datanode bobcook$ od -c file.sorted  0000000    S   E   Q 006  \r   F   l   o   a   t   W   r   i   t   a   b  0000020    l   e  \v   I   n   t   W   r   i   t   a   b   l   e  \0  \0  0000040   \0  \0  \0  \0   6 364 343  \r 256   h   U 222 365   T   7   l  0000060  357   i   ~   }  \0  \0  \0  \n  \0  \0  \0 006  \0  \0  \0 001  0000100   \0   4   @ 200  \0  \0  \0  \0  \0  \n  \0  \0  \0 006  \0  \0  0000120   \0 001  \0   3   @   @  \0  \0  \0  \0  \0  \n  \0  \0  \0 006  0000140   \0  \0  \0 001  \0   2   @  \0  \0  \0  \0  \0  \0  \n  \0  \0  0000160   \0 006  \0  \0  \0 001  \0   1   ? 200  \0  \0  \0  \0  \0  \n  0000200   \0  \0  \0 006  \0  \0  \0 001  \0   0  \0  \0  \0  \0          0000216  NOTE OUTPUT FILE IS TOTALLY TOASTED, but no error was generated!  PS: Your evaluation of my previous bug reports was enlightening.",io
Failed to run pseudo-distributed cluster on Win XP,"Failed to run pseudo-distributed cluster on Win XP, while standalone mode works fine.   Steps to reproduce:  1. Install cygwin+ssh on WinXp, download and unpack hadoop  2. Set up java_home in hadoop-env.sh  3. Adjust config according to sample attached files  4. Run following command:  bin/hadoop namenode -format  && bin/start-all.sh && bin/hadoop fs -put conf input && bin/hadoop jar hadoop-0.20.1-examples.jar grep input output 'dfs[a-z.]'    Job fails, sample log is attached",fs
Distch is not documented?,"HADOOP-4661 introduced distch, but I can find no documentation on how to actually use it.  Is it really non existent or am I just missing it?",documentation
Kerberos login in UGI should honor KRB5CCNAME,UGI should honor the environment variable KRB5CCNAME as the location of the ticket cache.,security
HarFileSystem cannot handle har:///,{noformat}  -bash-3.1$ hadoop jar examples.jar wordcount -Dmapred.job.queue.name=${JOBQ}  har:///user/tsz/t10_4.har/t10_4 t10_4_har_wc  java.io.IOException: No FileSystem for scheme: namenode_hostname          at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1375)          at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)          at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1390)          at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:196)          at org.apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:104)          at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1378)          at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:193)          at org.apache.hadoop.fs.Path.getFileSystem(Path.java:175)          at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:203)          ...  {noformat},fs
HarFileSystem throws NPE for har://hdfs-/foo,"{noformat}  -bash-3.1$ hadoop distcp -Dmapred.job.queue.name=${JOBQ}  har://hdfs-/user/tsz/t10_4.har/t10_4 t10_4_distcp t10_4_distcp  10/01/28 23:20:45 INFO tools.DistCp: srcPaths=[har://hdfs-/user/tsz/t10_4.har/t10_4]  10/01/28 23:20:45 INFO tools.DistCp: destPath=t10_4_distcp  With failures, global counters are inaccurate; consider running with -i  Copy failed: java.lang.NullPointerException          at org.apache.hadoop.fs.HarFileSystem.decodeHarURI(HarFileSystem.java:184)          at org.apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:95)          ...  {noformat}  ",fs
FsPermission:SetUMask not updated to use new-style umask setting.,"FsPermission:  {code}  221   /** Set the user file creation mask (umask) */  222   public static void setUMask(Configuration conf, FsPermission umask) {                                      223     conf.setInt(UMASK_LABEL, umask.toShort());  224   }  {code}  Needs to be updated to not use a decimal value. This is a bug introduced by HADOOP-6234.  ",fs
TestUTF8 fails,TestUTF8 is actually flaky. It generates 10 random strings to run the test on. If you change this number to 100000 it fails every time.,io
archive does not work with distcp -update,"The following distcp command  works.  {noformat}  hadoop distcp -Dmapred.job.queue.name=q har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101 t101_distcp  {noformat}  However, it does not work for -update.  {noformat}  -bash-3.1$ hadoop distcp -Dmapred.job.queue.name=q -update har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101 t101_distcp  10/01/29 20:06:53 INFO tools.DistCp: srcPaths=[har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101]  10/01/29 20:06:53 INFO tools.DistCp: destPath=t101  java.lang.IllegalArgumentException: Wrong FS: har://hdfs-nn_hostname:8020/user/tsz/t101.har/t101/text-00000000, expected: hdfs://nn_hostname          at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:310)          at org.apache.hadoop.hdfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:99)          at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)          at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:463)          at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:46)          at org.apache.hadoop.fs.FilterFileSystem.getFileChecksum(FilterFileSystem.java:250)          at org.apache.hadoop.tools.DistCp.sameFile(DistCp.java:1204)          at org.apache.hadoop.tools.DistCp.setup(DistCp.java:1084)          ...  {noformat}    ",fs
Contrib tests are failing Clover'ed build,"When {{test-contrib}} target is executed on a build instrumented with Clover all tests there are failing. Apparently {{clover.jar}} isn't included into contrib tests classpath.    Also, {{HdfsProxy}} test is failing because {{commons-cli}} jar isn't pulled by Ivy.",build
Need mapping from long principal names to local OS user names,"We need a configurable mapping from full user names (eg. omalley@APACHE.ORG) to local user names (eg. omalley). For many organizations it is sufficient to just use the prefix, however, in the case of shared clusters there may be duplicated prefixes. A configurable mapping will let administrators resolve the issue.",security
UserGroupInformation::createUserForTesting clobbers already defined group mappings,"In UserGroupInformation::createUserForTesting the follow code creates a new groups instance, obliterating any groups that have been previously defined in the static groups field.  {code}    if (!(groups instanceof TestingGroups)) {        groups = new TestingGroups();      }  {code}  This becomes a problem in tests that start a Mini{DFS,MR}Cluster and then create a testing user.  The user that started the user (generally the real user running the test) immediately has their groups wiped out and is prevented from accessing files/folders/queues they should be able to.  Before the UserGroupInformation.createRemoteUserForTesting, calls to userA.getGroups may return {""a"", ""b"", ""c""} and immediately after the new fake user is created, the same call will return an empty array.",security
add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory,"Add FileUtil.fullyDeleteContents(dir) api to delete contents of a directory, not directory itself. This will be useful if we want to clear the contents of cwd.",fs
Path objects are heavy,"Compared with java.lang.String, org.apache.hadoop.fs.Path is much heavier since it contains URI.  The size of a Path is roughly 3 times of a String.  See some numbers in the comments.    A major impact of decreasing Path size is allowing ls, archive, etc. on directories with many files.",fs
LocalDirAllocator should use whitespace trimming configuration getters,"This is the other half of MAPREDUCE-1441. If a user specifies mapred.local.dir with whitespace around directory names, it should be trimmed. Same goes for any LocalDirAllocator-based configuration.","conf,fs"
Ability to measure the throughput of individual components at node level,"Currently rely on JVMMetrics, DFSMetrics, IPCMetrics, MapRedMetrics to quickly check performance. It would be helpful to have metrics which would measure the individual component metrics. Some of them are listed below    1. Throughput of MapOutputServlet in a tasktracker (Currently there are statistics which report on server busy, successful output etc. However, understanding the data throughput of this servlet is little hard. Some additional metrics like the following would be helpful in determining the parallel copies in shuffle phase.    - Total amount of bytes served  - Total amount of time spent in reading & streaming  - Data throughput of MapOutputServlet in MB/second  - Number of concurrent requests now, peak requests    2. Another metric could be on DFSClient's DataStreamer. The amount of data streamed per second by a specific node would be helpful.  ",metrics
FileUtil.fullyDelete(dir) behavior is not defined when we pass a symlink as the argument,"FileUtil.fullyDelete(dir) deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so.   Or it should be changed not to delete the contents of the sym-linked directory.",fs
"Set hadoop.security.authentication to ""simple"" by default","The default value of ""hadoop.security.authentication"" is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",security
Add a -Dno-docs option to build.xml,"""ant tar"" took a long time to generate all the forrest docs and javadocs.  These docs are not always necessary.",build
Allow authentication-enabled RPC clients to connect to authentication-disabled RPC servers ,"This is useful when one has multiple clusters (of the same release version), some have authentication turned on and some off, and one needs to move data between them.","ipc,security"
Cached FileSystem objects can lead to wrong token being used in setting up connections,"The FileSystem class caches the filesystem objects that it creates for users. For some cases, e.g., if the filesystem object is actually a DistributedFileSystem, it also has an associated RPC client and hence an UGI for the respective user. This could lead to issues to do with using the right credentials when connecting with the namenode. The credentials in the UGI is never updated (even if the user in question now has new credentials) and in case the cached UGI's credentials have expired, this would lead to authentication error whenever there is a re-authentication (in the process of re-establishing connection to the namenode).",security
BloomMapFile can return false negatives,"BloomMapFile can return false negatives when using keys of varying sizes.  If the amount of data written by the write() method of your key class differs between instance of your key, your BloomMapFile may return false negatives.  ",io
Move the Delegation Token feature to common since both HDFS and MapReduce needs it,Move the Delegation Token feature to common since both HDFS and MapReduce needs it.,security
Replace org.mortbay.log.Log imports with commons logging,"Some IDEs auto-import {{org.mortbay.log.Log}}, contrary to the standard pattern using commons logging.","fs,io"
Document the various compression levels supported for GzipCodec,HADOOP-5879 made compression levels configurable for GzipCodec. It would be nice to document them.,"documentation,io"
KEYTAB_KERBEROS_OPTIONS in UserGroupInformation should have options for automatic renewal of keytab based tickets,"KEYTAB_KERBEROS_OPTIONS in UserGroupInformation should have options for automatic renewal of keytab based tickets. Specifically, renewTGT=true (and useTicketCache=true), as documented in http://java.sun.com/javase/6/docs/jre/api/security/jaas/spec/com/sun/security/auth/module/Krb5LoginModule.html",security
The RPC client should try to re-login when it detects that the TGT expired,"Currently while making RPC calls, the client will throw an exception if the client is not able to use the TGT (expired or timedout). This could be improved - it could catch the exception and try doing a re-login.",security
FileContextSymlinkBaseTest should use FileContextTestHelper,"FileContextSymlinkBaseTest should use FileContextTestHelper, both for utility methods and defining the root test directory instead of using /tmp. ",test
Add more tests to FileContextSymlinkBaseTest that cover intermediate symlinks in paths,"Intermediate symlinks in paths are covered by the tests that use /linkToDir/file, /linkToDir/subDir, etc  eg testCreateVia* in FileContextSymlinkBaseTest. I'll add additional tests to cover other basic operations on paths like /dir/linkToSomeDir/file beyond create() and open().","fs,test"
AbstractDelegationTokenSecretManager.stopThreads() will NPE if called before startThreads(),"Looking at the code for starting/stopping SecretManagers, it seems to me that {{AbstractDelegationTokenSecretManager.stopThreads()}} assumes that {{tokenRemoverThread}} is never null. That assumption is only valid if {{AbstractDelegationTokenSecretManager.startThreads()}} was called first.    the call to {{tokenRemoverThread.interrupt()}} should be guarded with a check for {{tokenRemoverThread!=null}}    I haven't encountered this in the field yet, but it should be trivial to replicate in a test and then fix.",security
Hadoop daemons should not start up if the ownership/permissions on the directories used at runtime are misconfigured,"The Hadoop daemons (like datanode, namenode) should refuse to start up if the ownership/permissions on directories they use at runtime are misconfigured or they are not as expected. For example, the local directory where the filesystem image is stored should be owned by the user running the namenode process and should be only readable by that user. We can provide this feature in common and HDFS and MapReduce can use the same.",security
The error messages for authentication and authorization failures can be improved,The error messages in case of authentication and authorization failures can be improved and made more structured.,security
Authorization for default servlets,"We have the following default servlets: /logs, /static, /stacks, /logLevel, /metrics, /conf. Barring ""/static"", rest of the servlets provide information that is only for administrators. In the context of security for the web-servlets, we need protected access to these pages.",security
FsShell#cat should avoid calling unecessary getFileStatus before opening a file to read,"Since FileSystem#open throws a FileNotFoundException when the file to be read does not exist, there is no need to check if the file is a directory or not before open. In case of HDFS, this could reduce one getFileInfo RPC to NameNode.",fs
RPC#stopProxy throws NullPointerExcption if getProxyEngine(proxy) returns null,{noformat}    public static void stopProxy(Object proxy) {      if (proxy!=null) {        getProxyEngine(proxy).stopProxy(proxy);      }    }  {noformat}  The method should check if getProxyEngine(proxy) returns null or not before stopProxy is called.,ipc
RPC responses may be out-of-order with respect to SASL,"SASL enforces its own message ordering. When RPC server sends its responses back, response A may be wrapped by SASL before response B but is put on response queue after response B. This results in RPC client receiving wrapped response B ahead of A. When the received messages are unwrapped by SASL, SASL complaints the messages are out of order.","ipc,security"
Configuration should trim whitespace around a lot of value types,"I've seen multiple users make an error where they've listed some whitespace around a class name (eg for configuring a scheduler). This results in a ClassNotFoundException which is very hard to debug, as you don't notice the whitespace in the exception! We should simply trim the whitespace in Configuration.getClass and Configuration.getClasses to avoid this class of user error.    Similarly, we should trim in getInt, getLong, etc - anywhere that whitespace doesn't have semantic meaning we should be a little less strict on input.",conf
A utility for reading and writing tokens into a URL safe string.,"We need to include HDFS delegation tokens in the URLs while browsing the file system. Therefore, we need a url-safe way to encode and decode them.",security
Add authenticated TokenIdentifiers to UGI so that they can be used for authorization,"When token is used for authentication over RPC, information other than username may be needed for access authorization. This information is typically specified in TokenIdentifier. This is especially true for block tokens used for client-to-datanode accesses, where authorization is based on access permissions specified in TokenIdentifier, and not on username. Block tokens used to be called access tokens and one can think of them as capability tokens. See HADOOP-4359 for more info.","ipc,security"
"Token class should have a toString, equals and hashcode method","The Token.toString would be helpful in logging. The equals/hashcode would be useful in UserGroupInformation.equals (currently the reference equality checks are done), and in other places.",security
Capture metrics for authentication/authorization at the RPC layer,Define metrics for authentication/authorization for the RPC layer.,"ipc,security"
Add FileStatus#isDirectory and isFile,"Per Sanjay's suggestion in HADOOP-6421 let's deprecate FileStatus#isDir() and add isDirectory() and isFile() to compliment isSymlink. Currently clients assume !isDir() implies a file, which is no longer true with symlinks. I'll file a separate jira to change the various uses of !isDir() to be isFile() or isFile() or isSymlink() as appropriate.",fs
Log authentication and authorization failures and successes,This jira will cover RPC authentication and SL authorizations logging.  ,security
CompressionCodecFactory throws IllegalArgumentException,WordCount does not run. :(  {noformat}  java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.          at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:96)          at org.apache.hadoop.io.compress.CompressionCodecFactory.<init>(CompressionCodecFactory.java:134)          at org.apache.hadoop.mapreduce.lib.input.TextInputFormat.isSplitable(TextInputFormat.java:46)          at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:247)          at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:886)          at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:780)          at org.apache.hadoop.mapreduce.Job.submit(Job.java:444)          at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:459)          at org.apache.hadoop.examples.WordCount.main(WordCount.java:67)          ...  {noformat},io
Better error messages for RPC clients when authentication fails,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.","ipc,security"
Add a username check for hadoop sub-commands,We experienced a case that sometimes we accidentally started HDFS or MAPREDUCE with root user. Then the directory permission will be modified and we have to chown them. It will be nice if there can be a username checking in the hadoop-daemon.sh script so that we always start with the desired username.,scripts
HarFileSystem cannot handle paths with the space character,"Since HarFileSystem is using "" "" as a separator in the index files, it won't work if there are "" "" in the path.",fs
TextRecordInputStream doesn't close SequenceFile.Reader,"Using hadoop fs -text on a glob with many sequence files can fail with too many open file handles.    The cause seems to be that TextRecordInputStream doesn't override close(), so printToStdout's call to close doesn't release the SequenceFile.Reader.",fs
Should add version to the serialization of DelegationToken,"Now that we are adding the serialized form of delegation tokens into the http interfaces, we should include some version information.",security
ClassLoader (Configuration#setClassLoader) in new Job API (0.20) does not work,"new Job/Configuration API's setClassLoader (Configuration#setClassLoader) gets overwritten w/ {{Thread.currentThread().getContextClassLoader()}} when invoking Job#submit.     Upon class to Job#submit, JobClient#submitJobInternal invokes {{JobContext context = new JobContext(job, jobId);}}, which in the constructor for org.apache.hadoop.mapreduce.JobContext, wraps Job w/ new JobConf and therefore overwrites set classLoader member @ Configuration via a init block w/ {{classLoader =  Thread.currentThread().getContextClassLoader();}}  ",conf
Provide workaround for issue with Kerberos not resolving cross-realm principal,"Java's SSL-Kerberos implementation does not correctly obtain the principal for cross-realm principles when clients initiate connections to servers, resulting in the client being unable to authenticate the server.  We need a work-around until this bug gets fixed.",security
Add different variants of non caching HTTP headers,"I'm suffering from proxy servers that are caching some of the HttpResponses that Hadoop generates in servlets/JSP pages. While the web ui is up to date, some of my build files are failing to pull stuff down because that is going via proxy -it sees an error page rather than the data    # Every servlet should set a short expires header and disable caching, especially in proxies.   # JSP pages should do it to  # It's essential that error responses do it.    Maybe this could be done in a filter. Otherwise something like  {code}      /**       * Turn off caching and say that the response expires now       * @param response the response       */      protected void disableCaching(HttpServletResponse response) {          response.addDateHeader(""Expires"", System.currentTimeMillis());          response.addHeader(""Cache-Control"", ""no-cache"");          response.addHeader(""Pragma"", ""no-cache"");      }  {code}    Before anyone rushes to do this, we should consult some HTTP experts in Yahoo! or Facebook to get the options right. It may be best to have, say, a 1s lifespan on everything.",io
register /conf servlet as /hadoop-site.xml,"I've been using the /conf stuff, works fairly well, but I need to save it to the local filesystem before use.     and there, the fact the servlet is called /conf is trouble. If it was (also) registered as hadoop-site.xml then the save-as dialogs would all be prepared with the target name of the configuration.    This is very minor, but if we want to drive client systems by the dynamically served up xml conf (which is an improvement), then we should make it as easy as possible",conf
Deadlock in DFSClient#getBlockLocations even with the security disabled,"Here is the stack trace:  ""IPC Client (47) connection to XX"" daemon  prio=10 tid=0x00002aaae0369c00 nid=0x655b waiting for monitor entry [0x000000004181f000..0x000000004181fb80]     java.lang.Thread.State: BLOCKED (on object monitor)  at org.apache.hadoop.io.UTF8.readChars(UTF8.java:210)  - waiting to lock <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)  at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)  at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:179)  at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)  at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:638)  at org.apache.hadoop.ipc.Client$Connection.run(Client.java:573)    ""IPC Client (47) connection to /0.0.0.0:50030 from job_201002262308_0007""  daemon prio=10 tid=0x00002aaae0272800 nid=0x6556 waiting for monitor entry [0x000000004131a000..0x000000004131ad00]     java.lang.Thread.State: BLOCKED (on object monitor)  at org.apache.hadoop.io.UTF8.readChars(UTF8.java:210)   - waiting to lock <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)  at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)  at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:179)  at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)  at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:638)  at org.apache.hadoop.ipc.Client$Connection.run(Client.java:573)    ""main"" prio=10 tid=0x0000000046c17800 nid=0x6544 in Object.wait() [0x0000000040207000..0x0000000040209ec0]     java.lang.Thread.State: WAITING (on object monitor)  at java.lang.Object.wait(Native Method)   - waiting on <0x00002aaacee6bc38> (a org.apache.hadoop.ipc.Client$Call)  at java.lang.Object.wait(Object.java:485)  at org.apache.hadoop.ipc.Client.call(Client.java:854) - locked <0x00002aaacee6bc38> (a org.apache.hadoop.ipc.Client$Call)  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:223)  at $Proxy2.getBlockLocations(Unknown Source)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)  at $Proxy2.getBlockLocations(Unknown Source)  at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:333)  at org.apache.hadoop.hdfs.DFSClient.access$2(DFSClient.java:330)  at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.getBlockAt(DFSClient.java:1606)   - locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)  at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1704)  - locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)  at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1856)  - locked <0x00002aaacecb8258> (a org.apache.hadoop.hdfs.DFSClient$DFSInputStream)  at java.io.DataInputStream.readFully(DataInputStream.java:178)  at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)  at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)  at org.apache.hadoop.io.UTF8.readChars(UTF8.java:211)  - locked <0x00002aaab3eaee50> (a org.apache.hadoop.io.DataOutputBuffer)  at org.apache.hadoop.io.UTF8.readString(UTF8.java:203)  at org.apache.hadoop.mapred.FileSplit.readFields(FileSplit.java:90)  at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)  at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:1)  at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:341)  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:357)  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)  at org.apache.hadoop.mapred.Child$4.run(Child.java:211)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:700)  at org.apache.hadoop.mapred.Child.main(Child.java:205)",io
Hadoop conf/ servlet improvements,"I'm playing with the conf/ servlet, trying to do a workflow that  # pulls down the conf servlet from a well known URL (this is trickier when your VMs are dynamic, but possible)  # saves it locally, using {{<get>}} task  #  {{<get>}} some info on the machines in the allocated cluster, like their external hostnames  # SCP in the configuration files, JAR files needed to submit work,   # submit work via SSH  I have to SSH as the VMs have different internal/external addresses; HDFS gets upset.    Some issues I've found so far  # It's good to set expires headers on everything; HADOOP-6607 covers that  # Having sorted conf values makes it easier to locate properties, otherwise you have to save it to a text editor and search around  # the <!-- Loaded from Unknown --> option makes things noisy  # Saving as a java.util.Properties would let me pull these things into a build file or other tool very easily. This is easy to test too.  # Have a comment at the top listing when the conf was generated, and the hostname. Maybe even make them conf values    More tricky is the conf options that are dynamic, things like   {code}  <property><!--Loaded from Unknown--><name>dfs.datanode.address</name><value>0.0.0.0:0</value></property>  {code}  These show what the node was started with, not what it actually got. I am doing a workaround there with my code (setting the actual values in the conf file with {{live.dfs.datanode.address}}, etc, and extracting them that way. I don't want to lose the original values, but do want the real ones  ",conf
Fix javac warnings introduced by HADOOP-6584,"Some javac warnings slipped through in HADOOP-6584:  {noformat}    [exec]   [javadoc] /Users/jhoman/work/git/hadoop-common/src/core/org/apache/hadoop/http/HttpServer.java:45  4: warning - @param argument ""needClientAuth"" is not a parameter name.       [exec]   [javadoc] /Users/jhoman/work/git/hadoop-common/src/core/org/apache/hadoop/security/Krb5AndCertsSs  lSocketConnector.java:183: warning - Tag @link: reference not found: Krb5SslSocketConnector       [exec]   [javadoc] /Users/jhoman/work/git/hadoop-common/src/core/org/apache/hadoop/security/Krb5AndCertsSs  lSocketConnector.java:183: warning - Tag @link: reference not found: Krb5SslSocketConnector       [exec]   [javadoc] /Users/jhoman/work/git/hadoop-common/src/core/org/apache/hadoop/security/Krb5AndCertsSs  lSocketConnector.java:183: warning - Tag @link: reference not found: Krb5SslSocketConnector       [exec]   [javadoc] Building index for all the packages and classes...       [exec]   [javadoc] /Users/jhoman/work/git/hadoop-common/src/core/org/apache/hadoop/security/Krb5AndCertsSs  lSocketConnector.java:183: warning - Tag @link: reference not found: Krb5SslSocketConnector       [exec]   [javadoc] Building index for all classes...       [exec]   [javadoc] Generating /Users/jhoman/work/git/hadoop-common/build/docs/api/stylesheet.css...       [exec]   [javadoc] 5 warnings{noformat}  ",security
Protocols RefreshUserToGroupMappingsProtocol and RefreshAuthorizationPolicyProtocol will fail with security enabled,None,security
RPC server should check for version mismatch first,Currently AuthMethod is checked before Version.,"ipc,security"
RunJar should provide more diags when it can't create a temp file,"When you see a stack trace about permissions, it is better if the trace included the file/directory at fault:  {code}  Exception in thread ""main"" java.io.IOException: Permission denied    java.io.UnixFileSystem.createFileExclusively(Native Method)    java.io.File.checkAndCreate(File.java:1704)    java.io.File.createTempFile(File.java:1792)    org.apache.hadoop.util.RunJar.main(RunJar.java:147)  {code}    As it is, you need to go into the code, discover that it's {{${hadoop.tmp.dir}/hadoop-unjar}}, but you need to know the value of hadoop.tmp.dir to really find out what the problem is.",util
Improve documentation for rack awareness,The current documentation for rack awareness (http://hadoop.apache.org/common/docs/r0.20.0/cluster_setup.html#Hadoop+Rack+Awareness) should be augmented to include a sample script.,documentation
Version 0.20.2 is not published on the maven repositories,"Dear Hadoop Project,    The Lucene/Mahout is anxious to make a release that depends on 0.20.2. Can you please go ahead and publish the artifacts? We would be happy to help if that would accelerate the process.    Regards,    Benson Margulies, Committer, Mahout",build
fs -ls does not work if a path name contains the ^ character,"Using a wildcard, the file is found.  {noformat}  -bash-3.1$ hadoop fs -ls k20d2f4/bin-2\?04+1_AF650AE776488A4D  Found 1 items  -rw-------   3 tsz users         17 2010-03-05 19:43 /user/tsz/k20d2f4/bin-2^04+1_AF650AE776488A4D  {noformat}  Replace the wildcard with ^, the file is not found.  {noformat}  -bash-3.1$ hadoop fs -ls k20d2f4/bin-2^04+1_AF650AE776488A4D  ls: Cannot access k20d2f4/bin-2^04+1_AF650AE776488A4D: No such file or directory.  {noformat}",fs
Token should not print the password in toString.,The toString method in Token should not print out the password.,security
Add StringUtils.split for non-escaped single-character separator,"This is for HDFS-1028 but useful generally. String.split(""/"") for example is way slower than an implementation that is specific to only single-character separators.",util
Hadoop conf/ servlet is serving up an invalid JSON file,"Seeing this on a machine hosting both the NN and JT. The NN's conf/ servlet works as both XML and JSON, but the JT's conf is only coming back as XML, when I ask for JSON I see  {code}  {""properties"":[  {code}    To make sure nothing was interfering I connected to the box, did a direct telnet  {code}  masterc0-vif0:~ # telnet masterc0-vif0 50030  Trying 10.144.175.2...  Connected to masterc0-vif0.  Escape character is '^]'.  GET /conf?format=json HTTP/1.0    HTTP/1.1 200 OK  Content-Type: text/javascript; charset=utf-8  Server: Jetty(6.1.14)    {""properties"":[Connection closed by foreign host.  masterc0-vif0:~ #   {code}",conf
NPE in TestIPC with kerberos,"Running TestIPC with {{hadoop.security.authentication=kerberos}}, but without any actual Kerberos around results in NPE.",ipc
Make Cloudera commit logs and Yahoo! change logs into web-readable formats,"Here are some quick hacks to help people read the various ""release notes"" from the various distributions.",documentation
versions of dependencies should be specified in a single place,"Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on.  The versions of these libraries are also present in ivy/libraries.properties, so that, when a library is updated, it must be updated in two places, which is error-prone.  We should instead only specify library versions in a single place.",build
FileUtil.fullyDelete() should continue to delete other files despite failure at any level.,"Ravi commented about this on HADOOP-6536. Paraphrasing...    Currently FileUtil.fullyDelete(myDir) comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir. This is because we return from method if the recursive call ""if(!fullyDelete()) {return false;}"" fails at any level of recursion.    Shouldn't it continue with deletion of other files/dirs continuing in the for loop instead of returning false here ?    I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf').","fs,util"
AccessControlList uses full-principal names to verify acls causing queue-acls to fail,"ACL configuration so far was using short user-names. With the changed {{UserGroupInformation}}, short names are different from the long fully-qualified names. {{AccessControlList}} continues to use {{UserGroupInformation.getUserName()}} for verifying access control. Because of this, queue acls fail for a user ""user@domain.org"" even though the short name ""user"" is part of the acl configuration.",security
Install or deploy source jars to maven repo,Publishing source jars to the local m2 cache or a public maven repo is extremely handy for Hadoop users that want to be able to inspect the Hadoop source from within their IDE.,build
Benchmark overhead of RPC session establishment ,Measure the latency of RPC session establishments through three mechanisms:   # simple - no auth   # kerberos authentication  # delegation token  authentication,benchmarks
FileSystem.get(..) may be blocked for a long time,"When FileSystem cache is enabled, FileSystem.get(..) will call FileSystem.Cache.get(..), which is a synchronized method.  If the lookup fails, a new instance will be initialized.   Depends on the FileSystem subclass implementation, the initialization may take a long time.  In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time.    In particular, the DistributedFileSystem initialization may take a long time since there are retries.  It is even worst if the socket timeout is set to a large value.",fs
FileSystem.get() does RPC retries within a static synchronized block,"If using FileSystem.get() in a multithreaded environment, and one get() locks because the NN URI is too slow or not responding and retries are in progress, all other get() (for the diffferent users, NN) are blocked.    the synchronized block in in the static instance of Cache inner class.  ",fs
Bugs on listStatus for HarFileSystem,"Two bugs on listStatus for HarFileSystem:    1) consider the following directory tree inside a hadoop archive  /foo  /foo/bar1  /fooo  /fooo/bar2    In this case, listStatus(new Path(""/foo"")) will include /fooo/bar2 because fileStatusesInIndex is testing  a prefix.    2) HADOOP-6591 didn't take into consideration method fileStatusesInIndex(), and archives v2 return empty results for listStatus()  ",fs
Move HarfileSystem out of Hadoop Common.,"Move HarFileSystem out of common so that we can get out of the cumbersome task of making updates in 2 places and also HarFileSystem - a user level file system, belongs in tools.",fs
Credentials should ignore null tokens,"When hftp goes to a non-secure cluster, getDelegationToken returns null. Credentials.addToken needs to ignore null tokens to avoid a NPE in serialization.",security
Exclude  Public elements in  generated Javadoc,"Packages, classes and methods that are marked with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",documentation
Switch RPC to use Avro,This is an umbrella issue for moving HDFS and MapReduce RPC to use Avro.,ipc
add wrapper for Avro data,To permit passing Avro data through MapReduce we can add a wrapper class and serialization for it.  ,io
hadoop zlib compression does not fully utilize the buffer,org.apache.hadoop.io.compress.ZlibCompressonr does not fully utilize its buffer.     Its needesInput() return false when there is any data in its buffer (64K by default). The performance will greately degrade since an JNI call will be invoded at each time the write() method of CompressonStream is called. ,io
BlockDecompressorStream get EOF exception when decompressing the file compressed from empty file,"An empty file can be compressed using BlockDecompressorStream, which is for block-based compressiong algorithm such as LZO. However, when decompressing the compressed file, BlockDecompressorStream get EOF exception.    Here is a typical exception stack:    java.io.EOFException  at org.apache.hadoop.io.compress.BlockDecompressorStream.rawReadInt(BlockDecompressorStream.java:125)  at org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:96)  at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:82)  at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:74)  at java.io.InputStream.read(InputStream.java:85)  at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:134)  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:39)  at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:186)  at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:170)  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)  at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:18)  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)  at org.apache.hadoop.mapred.Child.main(Child.java:196)  ",io
fs.inmemory.size.mb not listed in conf. Cluster setup page gives wrong advice.,"http://hadoop.apache.org/common/docs/current/cluster_setup.html    fs.inmemory.size.mb does not appear in any xml file  {noformat}  grep ""fs.inmemory.size.mb"" ./mapred/mapred-default.xml   [edward@ec src]$ grep ""fs.inmemory.size.mb"" ./hdfs/hdfs-default.xml   [edward@ec src]$ grep ""fs.inmemory.size.mb"" ./core/core-default.xml   {noformat}    http://hadoop.apache.org/common/docs/current/cluster_setup.html  Documentation error:  Real-World Cluster Configurations  {noformat}  conf/core-site.xml   io.sort.factor   100   More streams merged at once while sorting files.  conf/core-site.xml  io.sort.mb  200  Higher memory-limit while sorting data.  {noformat}    core --- io.sort.factor     -- should be mapred  core --- io.sort.mb     -- should be mapred  ",documentation
DFSadmin commands setQuota and setSpaceQuota allowed when NameNode is in safemode.  ,Currently DFSadmin commands setSpaceQuota and setQuota are allowed when Name node is in safe mode.  setQuota and setSpaceQuota operations causes changes in name node name system. These operations should be restricted when name node is in safe mode.    ,fs
Introduce common logging mechanism to mark begin and end of test cases execution,It is pretty hard to diagnose a test problem (especially in Hudson) when all you have is a very long log file for all your tests output in one place.    ZOOKEEPER-724 seems to have a nice solution for this problem.,test
RPC.waitForProxy should retry through NoRouteToHostException,"RPC.waitForProxy already loops through ConnectExceptions, but NoRouteToHostException is not a subclass of ConnectException. In the case that the NN is on a VIP, the No Route To Host error is reasonably common during a failover, so we should retry through it just the same as the other connection errors.",ipc
Apply audience and stability annotations to classes in common,Mark private implementation classes with the InterfaceAudience.Private or InterfaceAudience.LimitedPrivate annotation to exclude them from user Javadoc and JDiff.,documentation
zlib.compress.level  ignored for DefaultCodec initialization ,"HADOOP-5879 added a compression level for codecs, but DefaultCodec seems to ignore this conf value when initialized.  This is only when codec is first created.  reinit() probably sets it right. ",io
UserGroupInformation doesn't support use in hash tables,"The UserGroupInformation objects are mutable, but they are used as keys in hash tables. This leads to serious problems in the FileSystem cache and RPC connection cache. We need to change the hashCode to be the identity hash code of the Subject and change equals to use == between the Subjects.",security
To use maven for hadoop common builds,"We are now able to publish hadoop artifacts to the maven repo successfully [ Hadoop-6382]  Drawbacks with the current approach:  * Use ivy for dependency management with ivy.xml  * Use maven-ant-task for artifact publishing to the maven repository  * pom files are not generated dynamically     To address this I propose we use maven to build hadoop-common, which would help us to manage dependencies, publish artifacts and have one single xml file(POM) for dependency management and artifact publishing.    I would like to have a branch created to work on mavenizing  hadoop common.  ",build
BytesWritable.write(buf) use much more CPU in writeInt() then write(buf),BytesWritable.write() use nearly 4 times of CPU in wirteInt() as write buffer. It may be optimized.,io
nightly builds have incorrect VersionInfo,"Snapshots of Hadoop trunk downloaded from Ivy have VersionInfo.getVersion() returning ""Unknown""",build
Highlight Evolving elements in Javadoc,"It would be nice if we could mark Evolving (and Unstable) program elements in bold as the first word of the Javadoc comment (like ""Deprecated"").  ",documentation
"Remove FileContext#isFile, isDirectory and exists","# Add a method  Iterator<FileStatus> listStatus(Path), which allows HDFS client not to have the whole listing in the memory, benefit more from the iterative listing added in HDFS-985. Move the current FileStatus[] listStatus(Path) to be a utility method.  # Remove methods isFile(Path), isDirectory(Path), and exists.  All these methods are implemented by calling getFileStatus(Path).But most users are not aware of this. They would write code as below:   {code}    FileContext fc = ..;    if (fc.exists(path)) {      if (fc.isFile(path)) {       ...      } else {      ...      }    }  {code}  The above code adds unnecessary getFileInfo RPC to NameNode. In our production clusters, we often see that the number of getFileStatus calls is multiple times of the open calls. If we remove isFile, isDirectory, and exists from FileContext, users have to explicitly call getFileStatus first, it is more likely that they will write more efficient code as follow:  {code}    FileContext fc = ...;    FileStatus fstatus = fc.getFileStatus(path);    if (fstatus.isFile() {      ...    } else {      ...    }  {code}",fs
Fill AWS credentials when configuring Hadoop on EC2 instances,"There's a function ""configure_hadoop"" in the hadoop-ec2-init-remote.sh script used to configure EC2 nodes for Hadoop. The function actually uses AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables, but they are never passed to it. It can be fixed in service.py by passing those variables.",contrib/cloud
NetUtils:normalizeHostName does not process hostnames starting with [a-f] correctly,"  public static String normalizeHostName(String name) {      if (Character.digit(name.charAt(0), 16) != -1) {        return name;    This code is attempting to short-circuit the hostname->ip resolution on the assumption that if name starts with a digit, it's already an ip address.  This is of questionable value, but because it checks for a hex digit, it will fail on names starting with [a-f].  Such names will not be converted to an ip address, but be returned unchanged.",io
the first optimization: ZlibCompressor does not fully utilize the buffer,Thanks for Hong Tang's advice.    Sub task created for the first optimization. HADOOP-6662 closed. ,io
FileSystem.delete(...) implementations should not throw FileNotFoundException,"S3FileSystem.delete(Path path, boolean recursive) may fail and throw a FileNotFoundException if a directory is being deleted while at the same time some of its files are deleted in the background.    This is definitely not the expected behavior of a delete method. If one of the to-be-deleted files is found missing, the method should not fail and simply continue. This is true for the general contract of FileSystem.delete, and also for its various implementations: RawLocalFileSystem (and specifically FileUtil.fullyDelete) exhibits the same problem.    The fix is to silently catch and ignore FileNotFoundExceptions in delete loops. This can very easily be unit-tested, at least for RawLocalFileSystem.      The reason this issue bothers me is that the cleanup part of a long (Mahout) MR job inconsistently fails for me, and I think this is the root problem. The log shows:  {code}  java.io.FileNotFoundException: s3://S3-BUCKET/tmp/0008E25BF7554CA9/2521362836721872/DistributedMatrix.times.outputVector/_temporary/_attempt_201004061215_0092_r_000002_0/part-00002: No such file or directory.    org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:334)    org.apache.hadoop.fs.s3.S3FileSystem.listStatus(S3FileSystem.java:193)    org.apache.hadoop.fs.s3.S3FileSystem.delete(S3FileSystem.java:303)    org.apache.hadoop.fs.s3.S3FileSystem.delete(S3FileSystem.java:312)    org.apache.hadoop.mapred.FileOutputCommitter.cleanupJob(FileOutputCommitter.java:64)    org.apache.hadoop.mapred.OutputCommitter.cleanupJob(OutputCommitter.java:135)    org.apache.hadoop.mapred.Task.runJobCleanupTask(Task.java:826)    org.apache.hadoop.mapred.MapTask.run(MapTask.java:292)    org.apache.hadoop.mapred.Child.main(Child.java:170)  {code}  (similar errors are displayed for ReduceTask.run)","fs,fs/s3"
Add directory renaming test to FileContextMainOperationsBaseTest,I noticed FileContextMainOperationsBaseTest does not have a test that renames an empty directory to an empty directory (and shows that this fails without the overwrite option). ,"fs,test"
TestFileSystemCaching sometimes hang,TestFileSystemCaching#testCacheEnabledWithInitializeForeverFS() sometimes hangs if InitializeForeverFileSystem initializes first.,test
Add FileContext#listStatus that returns an iterator,"Add a method  Iterator<FileStatus> listStatus(Path), which allows HDFS client not to have the whole listing in the memory, benefit more from the iterative listing added in HDFS-985. Move the current FileStatus[] listStatus(Path) to be a utility method.",fs
Job Tracker should be able to refresh classpath without restarting,"When I tried to run HBase export in cluster mode, I copied zookeeper-3.2.1.jar to hadoop lib directory.  But initial attempts failed with java.lang.ClassNotFoundException because job tracker wasn't restarted.    It is desirable for job tracker to pick up new classpath without restarting which is sometimes hard to do in production environment.",conf
script to that would let us checkout code from different repos,"To write a shell script that would let us checkout code from two different repository , where we cant use svn:externals.",build
Revert the io.serialization package to 0.20.2's api,"I have a lot of concern about the usability of the new generic serialization framework. Toward that end, I've filed a jira for improving it. There is resistance to pushing a new API into 0.21 at the last moment, so we should back out the changes rather than introducing a new api in 0.21 and deprecating it in 0.22.",io
Make MapFile interface consistent with SequenceFile,The constructor for the SequenceFile.Reader/Writer classes accept a Path object as the specification of the file to read.  MapFile related classes instead want a String object that represents the MapFile directory.    I propose to make them consistent by replacing the String parameter with a Path one in MapFile related classes.  ,io
Hadoop commands guide should include examples,"Currently, The Hadoop command guide (http://hadoop.apache.org/common/docs/r0.20.0/commands_manual.html) just lists all the available command line options, with a description. It should include examples for each command for more clarity.",documentation
" Incorrect exit codes for ""dfs -chown"", ""dfs -chgrp""",ravi@localhost:~$ hadoop dfs -chgrp abcd /; echo $?  chgrp: changing ownership of  'hdfs://localhost/':org.apache.hadoop.security.AccessControlException: Permission denied  0    ravi@localhost:~$ hadoop dfs -chown  abcd /; echo $?  chown: changing ownership of  'hdfs://localhost/':org.apache.hadoop.security.AccessControlException: Permission denied  0    ravi@localhost:~$ hadoop dfs -chmod 755 /DOESNTEXIST; echo $?  chmod: could not get status for '/DOESNTEXIST': File does not exist: /DOESNTEXIST  0    -    Exit codes for both of the above invocations should be non-zero to indicate that the command failed.,fs
"Incorrect exit codes for ""dfs -chown"", ""dfs -chgrp""  when input is given in wildcard format.","Currently incorrect exit codes  are given for ""dfs -chown"", ""dfs -chgrp""  when input is given in wildcard format.    This bug is due to missing update of errors count in {{FsShell.java}}.    {code:title=FsShell.java|borderStyle=solid}  int runCmdHandler(CmdHandler handler, String[] args,                                     int startIndex, boolean recursive)                                      throws IOException {      int errors = 0;            for (int i=startIndex; i<args.length; i++) {        Path srcPath = new Path(args[i]);        FileSystem srcFs = srcPath.getFileSystem(getConf());        Path[] paths = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);        for(Path path : paths) {          try {            FileStatus file = srcFs.getFileStatus(path);            if (file == null) {              System.err.println(handler.getName() +                                  "": could not get status for '"" + path + ""'"");              errors++;            } else {              errors += runCmdHandler(handler, file, srcFs, recursive);            }          } catch (IOException e) {            String msg = (e.getMessage() != null ? e.getLocalizedMessage() :              (e.getCause().getMessage() != null ?                   e.getCause().getLocalizedMessage() : ""null""));            System.err.println(handler.getName() + "": could not get status for '""                                          + path + ""': "" + msg.split(""\n"")[0]);            errors++;          }        }      }   {code}    If there are no files on HDFS matching to wildcard input then  {{srcFs.globStatus(srcpath)}} returns 0.   {{ Path[] paths = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);}}    Resulting no increment in {{errors}} and command exits with 0 even though file/directory does not exist.",fs
add support for Parascale filesystem,"Parascale has developed an org.apache.hadoop.fs implementation that allows users to use Hadoop on Parascale storage clusters.  We'd like to contribute this work to the community. Should this be placed under contrib, or integrated into the org.apache.hadoop.fs space?",fs
jiracli fails to upload test-patch comments to jira,     [exec] ======================================================================       [exec]     Adding comment to Jira.       [exec] ======================================================================       [exec] ======================================================================       [exec]        [exec]        [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl       [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl       [exec] Failed to connect to: http://issues.apache.org/jira/rpc/soap/jirasoapservice-v2?wsdl       [exec]   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current  ,build
Relogin behavior for RPC clients could be improved,"Currently, the relogin in the RPC client happens on only a SaslException. But we have seen cases where other exceptions are thrown (like IllegalStateException when the client's ticket is invalid). This jira is to fix that behavior.",security
"BlockDecompressorStream.resetState() has a bug, which often causes the block decompressing not to work.",BlockDecompressorStream.resetState() has a bug.    The method of resetState() doesn't reinit the two member variables(int originalBlockSize; int noUncompressedBytes) of Class BlockDecompressorStream.      This bug will cause the block decompressing not to work.,io
New file format for very large records,"A file format that handles multi-gigabyte records efficiently, with lazy disk access",io
Re-instate deprecated FileSystem methods that were removed after 0.20,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.  ",fs
Configuration should support list of values,"Configuration supports 2 operations namely _set()_ and _get()_. It would be nice to have an inbuild support for lists where there can be multiple values (i.e list of values) assigned to one key. A workaround could be  {code}  // Assume Key be the parameter key and newValue be the value to be added/appended  Configuration c = new Configuration();  String value = c.get(Key);  value = value + "" "" + newValue  c.set(Key, value);  {code}    One common usecase is that in a production enviroment, some user facing params (e.g mapred.child.java.opts) are set to default values (say for performance reasons). Users themselves might want to *add* to this list. Doing a set would overwrite the previous values. The above mentioned hack is doable via code but not via command line. Hence we need the framework to support lists.  ",conf
"librecordio support for xerces 3, eliminate compiler warnings and the (optional) ability to compile in the source directory","I don't know if this code is current supported, but since it is in the tree here are some fixes:    1. support for xerces 3.X as well as 2.X      the patch checks XERCES_VERSION_MAJOR and I have tested on 3.X but before     committing, someone should retest on 2.X    2. gcc 4.4.3 on 64-bit complains about using %lld with int64_t.  Casting      to 'long long int' solves the issue    3. since there is currently no ant target, check if LIBRECORDIO_BUILD_DIR      is undefined and if so assume '.' to support compiling in the source directory      This should not effect ""normal"" compilation if/when an ant target is created.    patch attached",record
The RPC server Listener thread is a scalability bottleneck,The Hadoop RPC Server implementation has a single Listener thread that reads data from the socket and puts them into a call queue. This means that this single thread can pull RPC requests off the network only as fast as a single CPU can execute. This is a scalability bottlneck in our cluster.,ipc
"AccessControlList.toString() returns empty string when we set acl to ""*""","AccessControlList.toString() returns empty string when we set the acl to ""\*"" and also when we set it to empty(i.e. "" ""). This is causing wrong values for ACLs shown on jobdetails.jsp and jobdetailshistory.jsp web pages when acls are set to ""\*"".    I think AccessControlList.toString() needs to be changed to return ""\*"" when we set the acl to ""\*"".","security,util"
Log levels in o.a.h.security.Groups too high,The info logs in Groups.java for every getGroups call is causing my unrelated HDFS unit test to run out of memory since it logs so darn much.,security
Client does not close connection when an exception happens during SASL negotiation,setupSaslConnection in the RPC client might fail to successfully set up a sasl connection (e.g. if the principal is wrongly configured). It throws an exception back to the caller (setupIOstreams). setupIOstreams marks the connection as closed but it doesn't really close the socket. ,ipc
NetUtils.connect should check that it hasn't connected a socket to itself,"I had no idea this was possible, but it turns out that a TCP connection will be established in the rare case that the local side of the socket binds to the ephemeral port that you later try to connect to. This can present itself in very very rare occasion when an RPC client is trying to connect to a daemon running on the same node, but that daemon is down. To see what I'm talking about, run ""while true ; do telnet localhost 60020 ; done"" on a multicore box and wait several minutes.    This can be easily detected in NetUtils.connect by making sure the local address/port is not equal to the remote address/port.",util
unchecked exceptions thrown in IPC Connection orphan clients,"If the server sends back some malformed data, for example,  receiveResponse() can end up with an incorrect call ID. Then, when it tries to find it in the calls map, it will end up with null and throw NPE in receiveResponse. This isn't caught anywhere, so the original IPC client ends up hanging forever instead of catching an exception. Another example is if the writable implementation itself throws an unchecked exception or OOME.    We should catch Throwable in Connection.run() and shut down the connection if we catch one.",ipc
IPC doesn't properly handle IOEs thrown by socket factory,"If the socket factory throws an IOE inside setupIOStreams, then handleConnectionFailure will be called with socket still null, and thus generate an NPE on socket.close(). This ends up orphaning clients, etc.",ipc
Evaluate HtmlUnit for unit and regression testing webpages,"HtmlUnit (http://htmlunit.sourceforge.net/) looks like it may be a good tool to help unit testing and evaluating our various webpages throughout the project.  Currently this is done only occasionally in the code (usually falls to being a manual test during release cycles), and when it is done, usually the code to parse the webpage, etc. is re-written each time.  The framework is Apache licensed, so including it won't be an issue.  If it's found to be useful, new JIRAs for HDFS and MR should be opened.",test
can't control maxRetries in case of SocketTimeoutException,"One can set _ipc.client.connect.max.retries_ for _org.apache.hadoop.ipc.Client_.  This comes to effect on IOExceptions but not on SocketTimeoutException.  Client$Connection:307:  {code:java}            } catch (SocketTimeoutException toe) {              /* The max number of retries is 45,               * which amounts to 20s*45 = 15 minutes retries.               */              handleConnectionFailure(timeoutFailures++, 45, toe);            } catch (IOException ie) {              handleConnectionFailure(ioFailures++, maxRetries, ie);            }  {code}",ipc
Remove UnresolvedLinkException from public FileContext APIs,"HADOOP-6537 added UnresolvedLinkException to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, UnresolvedLinkException exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",fs
serializer.JavaSerialization should be added to io.serializations by default,"org.apache.hadoop.io.serializer.JavaSerialization isn't included in io.serializations by default.    When a class which implements the Serializable interface is used, user would see the following without serializer.JavaSerialization:    java.lang.NullPointerException     at  org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(SerializationFactory.java:73)     at  org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:759)     at  org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:487)     at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:575)     at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)     at org.apache.hadoop.mapred.Child.main(Child.java:170)",conf
Bug in FileContext#copy and provide base class for FileContext tests,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation.     On further investigation with the help of Sanjay we found that there is bug in FileContext#checkDest.    {noformat}    FileStatus dstFs = getFileStatus(dst);      try {        if (dstFs.isDir()) {          if (null == srcNa  {noformat}       *FileStatus dstFs = getFileStatus(dst);* should be in try...catch block.    {noformat}      try {         FileStatus dstFs = getFileStatus(dst);         if (dstFs.isDir()) {            if (null == srcNa  {noformat}      ","fs,test"
Octal umask with less than 3 digits is not handled,"Umasks with less than three digits should be accepted as valid umask. Umask with single digit, say 7, should be handled as 007. Similarly umask with two digits, say 77, should be handled as 077.",fs
Create a test for FileSystem API compatibility between releases,We should have an automated test for checking that programs written against an old version of the FileSystem API still run with a newer version. ,fs
Jets3tNativeFileSystemStore wrongly calls S3Service.createBucket during initialisation,"Reason: If a bucket is created with CreateBucketConfiguration specified, s3service.createBucket will fail.   Symptoms: If a bucket has CreateBucketConfiguration, Jets3tNativeFileSystemStore will fail with BucketAlreadyOwnedByYou Error.    A detailed descrioption from a blog (http://john.keyes.ie/boto-create_bucket-bucketalreadyownedbyyou-error/)    {quote}This evening I encountered a problem with it though. When the bucket did not exist, the method behaved as expected. When the bucket did exist though I received the following error response:    {code}<?xml version=""1.0"" encoding=""UTF-8""?>  <Error>      <Code>BucketAlreadyOwnedByYou</Code>      <Message>          Your previous request to create the named           bucket succeeded and you already own it.      </Message>      ...  </Error>{code}    This problem only manifests itself with buckets that are hosted in the EU. If the bucket is created in the US then the create_bucket method behaves as described.        For buckets created with a <CreateBucketConfiguration>, you will receive an error if you attempt to recreate the same bucket.    To create a bucket in the EU, the bucket is created with a CreateBucketConfiguration specified. I now use the following code to avoid the problem and it works for both US and EU buckets.    {code}  def get_bucket():      try:          bucket = conn.get_bucket('xxx', validate=True)      except S3ResponseError, e:          if e.code == ""NoSuchBucket"":              bucket = conn.create_bucket('xxx', location='EU')          else:              raise e      return bucket  {code}  {quote}    ",fs/s3
0.20.1 tarball on archive.apache.org has bogus md5 checksum file,"The MD5 checksum file for the 0.20.1 tarball contains an MD5 checksum along with  various SHA checksums and an RMD160 checksum:    {code}  % curl http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz.md5  hadoop-0.20.1.tar.gz:    MD5 = 71 9E 16 9B 77 60 C1 68  44 1B 49 F4 05 85 5B 72  hadoop-0.20.1.tar.gz:   SHA1 = 712A EE9C 279F 1031 1F83  657B 2B82 7ACA 0374                                 6613  hadoop-0.20.1.tar.gz: RMD160 = 4331 4350 27E9 E16D 055C  F23F FFEF 1564 E206                                 B144  hadoop-0.20.1.tar.gz: SHA224 = A5E4CBE9 EBBE5FE1 2020F3F1 BFBC6D3C C77A8E9B                                 E6C6062C A6484BDB  hadoop-0.20.1.tar.gz: SHA256 = 35024227 C51AFA34 6A2F3722 A652C4FB 4FEBFF35                                 E73BB364 3172F986 7233E2A4  hadoop-0.20.1.tar.gz: SHA384 = 54B5CB24 4708765A D3B930D8 73DB8A01 839B78DA                                 78A4A8B5 20ED0A88 5759E60F 92D3A90E 9AFB690A                                 C1EDB71F 6FB357AA  hadoop-0.20.1.tar.gz: SHA512 = 4D9DDEC5 27190749 87304D88 C2F9546E 6D9E62B0                                 EF8330C2 2D70BC2D 52CAFDD1 CD639E0E 8853177B                                 4E4F21EE EBEF316C 4DF3561A 10995B1D 95ED9047                                 16AC3013  {code}    The convention that most tools expect (including Ivy) is that a file named  xyz.tar.gz.md5 was generated by running the following command:    % md5sum xyz.tar.gz > xyz.tar.gz.md5    All of the other Hadoop releases on archive.apache.org follow this convention, with  the exception of hadoop-0.20.1, e.g:    {code}  % curl http://archive.apache.org/dist/hadoop/core/hadoop-0.20.0/hadoop-0.20.0.tar.gz.md5  6c751617e6429f23ec9b7ea7a02a73c2  hadoop-0.20.0.tar.gz  {code}    Can someone with access to the Apache archive repository please fix the 0.20.1  md5 checksum file?    Thanks.",build
Move commands_manual.xml from mapreduce into common,"Long term, we want to split the commands manual separately into the sub-projects (MAPEDUCE-1079). But for 0.21 atleast we can move it into common. Having it in mapreduce is definitely inconvenient.",documentation
Refactor Hadoop Command line interface.,"Currently Hadoop fs commands are scattered between FsShell , FsPermissions  and DfsAdmin .  It will be great if refactor hadoop CLI for better debugging, performance and code maintenance.    Any thoughts? ",fs
"adding some java doc to Server.RpcMetrics, UGI",None,ipc
Implement a functionality for suspend and resume a process.,Adding  two methods in DaemonProtocolAspect.aj for suspend and resume the process.    public int DaemonProtocol.resumeProcess(String pid) throws IOException;  public int DaemonProtocol.suspendProcess(String pid) throws IOException;    ,test
Remove hadoop.cluster.administrators,Remove hadoop.cluster.administrators in favor of having separate configuration property name for MapReduce and HDFS projects.  See more details on MAPREDUCE-1542.,security
Add mapred.tasktracker.tasks.maximum to limit the total number of map and reduce tasks,We have mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum now.    It is desirable to have mapred.tasktracker.tasks.maximum which limits the total number of map and reduce tasks. Meaning its value may be lower than (mapred.tasktracker.map.tasks.maximum + mapred.tasktracker.reduce.tasks.maximum)  This would be useful in a cluster whose nodes are multi-core.,conf
UserGroupInformation incompatibility: getCurrentUGI() and setCurrentUser() missing,getCurrentUGI() and setCurrentUser() are missing from the new 0.21 branch,security
Remote cluster control functionality needs JavaDocs improvement,"Herriot has remote cluster control API. The functionality works fairly well, however, JavaDocs are missed here and there. This has to be fixed.",test
MetricsServlet does not show shuffleInput metrics,Thanks to Philip and HADOOP-5469.patch.  It's very useful.  I just found out that it does not show shuffleInput tasktracker metrics.  Should be really straightforward to add.    Alex K  ,metrics
DefaultCodec.createOutputStream() leaks memory,"DefaultCodec.createOutputStream() creates a new Compressor instance in each OutputStream. Even if the OutputStream is closed, this leaks memory.",io
Clean up and add documentation for configuration keys in CommonConfigurationKeys.java,"Configuration keys in CommonConfigurationKeys.java should be cleaned up and documented (javadoc comments, appropriate *-default.xml descriptions).",fs
NullPointerException for hadoop clients launched from streaming tasks,"TaskRunner sets HADOOP_ROOT_LOGGER to info,TLA while launching the child tasks. TLA implicitly assumes that that task-id information will be made available via the 'hadoop.tasklog.taskid' parameter. 'hadoop.tasklog.taskid' is passed to the child task by the TaskRunner via HADOOP_CLIENT_OPTS. When the streaming task launches a hadoop client (say hadoop job -list), the HADOOP_ROOT_LOGGER of the hadoop client is set to 'info,TLA' but hadoop.tasklog.taskid is not set resulting into NPE.",scripts
Contrib Streaming.jar for hadoop-0.20.2 ,"I downloaded release-0.20.2/ from SVN hadoop/common/tags/. I was able to build successfully and use hadoop, but I noticed there is no hadoop-0.20.2-streaming.jar in build/contrib/streaming/ Is there a way to build this separately? I looked everywhere. The packaged download has it fine.    Thanks.",build
Add number of reader threads and queue length as configuration parameters in RPC.getServer,"In HDFS-599 we are introducing multiple RPC servers running inside of the same process on different ports. Since one might want to configure these servers differently we need a good abstraction to pass configuration values to servers as parameters, not through Configuration.",ipc
Duplicate commons-net in published POM,"The POM file that was published to the maven central repository for 0.20.2 contains two dependency entries for commons-net.    http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.pom    I believe Maven flags this as a warning and moves on, but it causes other maven-based tools such as M2Eclipse to choke.  It would be nice to have this fixed and a corrected POM republished.    Looks like the same problem exists on trunk as well in what I presume is the source template for the POM.  (http://svn.apache.org/viewvc/hadoop/common/trunk/ivy/hadoop-core-template.xml?view=markup)",build
Spill can fail with bad call to Random,"java.lang.IllegalArgumentException: n must be positive          at java.util.Random.nextInt(Random.java:250)          at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:243)          at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:289)          at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:124)          at org.apache.hadoop.mapred.MapOutputFile.getSpillFileForWrite(MapOutputFile.java:107)          at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1221)          at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1129)          at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:549)          at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:623)          at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)          at org.apache.hadoop.mapred.Child.main(Child.java:159)    confChanged assumes that the list of dirs it creates (LocalDirAllocator.java:215) has at least one element in it by the end of the function. If, for each local dir, either the conditional on line 221 is false, or the call to DiskChecker.checkDir() throws an exception, this assumption will not hold. In this case, dirIndexRandomizer.nextInt() is called on the number of elements in dirs, which is 0. Since dirIndexRandomizer (195) is an instance of Random(), it needs a positive (non-zero) argument to nextInt().",fs
Patch for running Hadoop on Windows without Cygwin,Proposed patch from Codeminders adds a possibility to run Hadoop on Windows without Cygwin. ,"build,conf,scripts"
RPC client can response more efficiently when sendParam() got IOException,"Under current RPC client implementation, when Client.Connection.sendParam() encounters IOException it just marks the exception and wait receiveResponse() thread to handle the exception. However, receiveResponse() may block ipc.ping.interval on socket read().     This means that RPC client may wait ipc.ping.interval(typically 1 miniute) when exception in sendParam().    It maybe not so reasonable in some situation. For example using ""hadoop fs -put "" a small file, DFSClient.close() will interupt renewLease() thread and it may wait 1 minute.",ipc
[Herriot] Exception exclusion functionality is not working correctly.,"Exception exclusion functionality is not working correctly because of that tests are failing by not matching the error count.  I debugged the issue and found that the problem with shell command which is generating in the getNumberOfMatchesInLogFile function.    Currently building the shell command in the following way.     if(list != null){    for(int i =0; i < list.length; ++i)    {      filePattern.append("" | grep -v "" + list[i] );    }  }      String[] cmd =          new String[] {              ""bash"",              ""-c"",              ""grep -c ""                  + pattern + "" "" + filePattern                  + "" | awk -F: '{s+=$2} END {print s}'"" };        However, The above commnad won't work correctly because you are counting the exceptions in the file before excluding the known exceptions.  In this case it gives the mismatch error counts everytime.The shell command should be in the following way to work correctly.    if (list != null) {    int index = 0;    for (String excludeExp : list) {      filePattern.append((++index < list.length)? ""| grep -v "" :               ""| grep -vc "" + list[i] );      }  }  String[] cmd =     new String[] {         ""bash"",         ""-c"",         ""grep ""             + pattern + "" "" + filePattern             + "" | awk -F: '{s+=$2} END {print s}'"" };  ",test
Herriot's artifact id for Maven deployment should be set to hadoop-core-instrumented,None,build
Utilities for system tests specific.,"Common utilities for system tests.  1. A method for restarting the daemon with new configuration.  public static void restartClusterWithNewConfig(Hashtable<String,Long> props, String confFile) throws Exception;    2. A method for restarting the daemon with default configuration.  public void restart() throws Exception;    3. A method for waiting until daemon is stop.  public void waitForClusterToStop() throws Exception;    4. A method for waiting until daemon is start.  public void waitForClusterToStart() throws Exception;",test
Ivy folder contains redundant files,{{ivy/}} contains few redundant files which are normally generated by the build. .pom files need to be removed from the source tree.,build
Namenode is not able to recover from disk full condition,"We ran an internal flow which resulted in:  Exception in thread ""main"" java.lang.RuntimeException: initialization of flow executor failed    After that we freed disk space on the Namenode server, but restarting Namenode failed.  Here is from Namenode log:    2010-05-19 17:15:15,514 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Namenode up at: sjc1-qa-certiq1.sjc1.ciq.com/10.201.8.247:9000  2010-05-19 17:15:15,516 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null  2010-05-19 17:15:15,518 INFO org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext  2010-05-19 17:15:15,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hadoop,hadoop  2010-05-19 17:15:15,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=supergroup  2010-05-19 17:15:15,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=true  2010-05-19 17:15:15,588 INFO org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics: Initializing FSNamesystemMetrics using context object:org.apache.hadoop.metrics.spi.NullContext  2010-05-19 17:15:15,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemStatusMBean  2010-05-19 17:15:15,637 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files = 1874  2010-05-19 17:15:16,202 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files under construction = 2  2010-05-19 17:15:16,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 259450 loaded in 0 seconds.  2010-05-19 17:15:16,599 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.NumberFormatException: For input string: """"      at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)      at java.lang.Long.parseLong(Long.java:431)      at java.lang.Long.parseLong(Long.java:468)      at org.apache.hadoop.hdfs.server.namenode.FSEditLog.readLong(FSEditLog.java:1273)      at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:656)      at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:999)      at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:812)      at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:364)      at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:88)      at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:312)      at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:293)      at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:224)      at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:306)      at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1004)      at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1013)    2010-05-19 17:15:16,599 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:",fs
UserGroupInformation.createProxyUser's javadoc is broken,None,security
Support for Ceph kernel client,None,fs
Move Hadoop cloud scripts to Whirr,"Whirr is a new Apache Incubator project for running cloud services. See http://incubator.apache.org/projects/whirr.html and http://wiki.apache.org/incubator/WhirrProposal.    The cloud scripts in src/contrib/cloud should move to Whirr. This should be done in 0.21 too, so the scripts do not appear in that release (since having them in Hadoop for a single release would be confusing).",contrib/cloud
Rename core-* configuration files to common-*,"To reflect the name of the project, the {{core-site.xml}} and {{core-default.xml}} configuration files should be renamed to {{common-}} and the references in documentation (including javadoc) should be updated. Compatibility should be maintained, with a warning.",conf
Create Wiki document about Herriot Test Framework and test development guide,None,documentation
test-patch needs to verify Herriot integrity,Whenever a new patch is submitted for verification {{test-patch}} process has to make sure that none of Herriot bindings were broken.,build
Factor out glob pattern code from FileContext and Filesystem,Refactor the glob pattern code out of FileContext and FileSystem into a package private GlobFilter and the reusable GlobPattern class (InterfaceAudience.Private)    Also fix the handling of ^ outside character class ([...]) reported in HADOOP-6618 and make the glob pattern code less restrictive (not throwing on some valid glob patterns.) and more POSIX standard compliant (support [!...]).,fs
set-version should precede Herriot artifacts installation target,{{set-version}} should be preceding the invocation of {{-mvn-system-install}}. Otherwise it will be failing when called on a clean workspace. This problem only affects local installation of artifacts.,build
Instrumented (Herriot) build uses too wide mask to include aspect files.,"AspectJ compiler needs to use narrower mask to look for sources and aspect files.   Otherwise, {{injectfaults}} might fail if called after {{inject-system-faults}}.  This is very minor issue and won't appear until we'll be having actually FI tests in the Common.",test
CLONE -fs -ls does not work if a path name contains the ^ character,"Using a wildcard, the file is found.  {noformat}  -bash-3.1$ hadoop fs -ls k20d2f4/bin-2\?04+1_AF650AE776488A4D  Found 1 items  -rw-------   3 tsz users         17 2010-03-05 19:43 /user/tsz/k20d2f4/bin-2^04+1_AF650AE776488A4D  {noformat}  Replace the wildcard with ^, the file is not found.  {noformat}  -bash-3.1$ hadoop fs -ls k20d2f4/bin-2^04+1_AF650AE776488A4D  ls: Cannot access k20d2f4/bin-2^04+1_AF650AE776488A4D: No such file or directory.  {noformat}",fs
Move configuration and script files post split,None,"conf,scripts"
FsShell 'hadoop fs -text' does not work with other  file systems ,"FsShell 'hadoop fs -text' can only work with file system which set by fs.default.name.  I use Gfarm file system from Hadoop.  https://gfarm.svn.sourceforge.net/svnroot/gfarm/gfarm_hadoop/trunk/  If i set fs.default.name to hdfs, the error ""Wrong FS"" occurred when i submit 'hadoop fs -text' to file on gfarm file system.     $ hadoop fs -text gfarmfs:///home/mikami/random/part-00000   text: Wrong FS: gfarmfs://null/home/mikami/random/part-00000, expected: hdfs://hostname:9000    if i set fs.default.name to gfarmfs:///, i can get correct result.    this command's result shouldn't depend on fs.default.name.",fs
FileStatus allows null srcPath but crashes if that's done,"FileStatus allows constructor invocation with a null srcPath but many methods like write, readFields, compareTo, equals, and hashCode depend on this property.",fs
Align Ivy version for all Hadoop subprojects.,HDFS-1177 and MAPREDUCE-1830 are upgrading Ivy to 2.1.0. Common still has Ivy version at 2.1.0-rc1,build
GzipCodec/CompressionOutputStream resetState() fails to reset gzip header and CRC,"    CompressionCodec gzip = new GzipCodec();      Path fnHDFS = new Path(workDir, ""concat"" + gzip.getDefaultExtension());      OutputStream out = localFs.create(fnHDFS);      Compressor gzCmp = gzip.createCompressor();      CompressionOutputStream gzOStm = gzip.createOutputStream(out, gzCmp);        gzOStm.write(""first gzip concat\n member\nwith three lines\n"".getBytes());      gzOStm.finish();      gzOStm.resetState();      gzOStm.write(""2nd gzip concat member\n"".getBytes());      gzOStm.finish();      gzOStm.resetState();      gzOStm.write(""gzip concat\nmember #3\n"".getBytes());      gzOStm.close();    This should create a 3-member concatenated gzip file (i.e., equivalent to concatenation of three individual gzip files).  However, resetState() simply resets the underlying deflate engine; it does not reset the running CRC-32, nor does it set a flag to write a new gzip header.  As a result, the output stream is corrupted--i.e., only the first member is readable.    I also have hex dumps, but they don't work well with proportional fonts.",io
"Remove FS_CLIENT_BUFFER_DIR_KEY = ""fs.client.buffer.dir"" from CommonConfigurationKeys.java (not used, deprecated)","In CommonConfigurationKeys.java:    public static final String  FS_CLIENT_BUFFER_DIR_KEY = ""fs.client.buffer.dir"";    The variable FS_CLIENT_BUFFER_DIR_KEY and string ""fs.client.buffer.dir"" are not used anywhere (Checked Hadoop Common, Hdfs and Mapred projects), it seems they should be removed.","conf,fs"
Add native gzip read/write coverage to TestCodec ,"Looking at ZlibCompressor I noticed that the finished member is never modified, and is therefore always false. This means ZlibCompressor#finished will always return false so CompressorStream#close loops indefinitely in finish:    {code}         while (!compressor.finished()) {          compress();        }  {code}    I modifed TestCodec#testGzipCodecWrite to also cover writing using the native lib and confirmed the hang with jstack. The fix is simple, ZlibCompressor should record when it's been finished.",io
allow FileSystem.copyFromLocalFile() to execute under specified username,"When the user calling FileSystem.copyFromLocalFile() doesn't have permission to write to certain hdfs path:  Thread [main] (Suspended (exception AccessControlException))         DFSClient.mkdirs(String, FsPermission) line: 905         DistributedFileSystem.mkdirs(Path, FsPermission) line: 262         DistributedFileSystem(FileSystem).mkdirs(Path) line: 1162         FileUtil.copy(FileSystem, Path, FileSystem, Path, boolean, boolean, Configuration) line: 194         DistributedFileSystem(FileSystem).copyFromLocalFile(boolean, boolean, Path, Path) line: 1231         DistributedFileSystem(FileSystem).copyFromLocalFile(boolean, Path, Path) line: 1207         DistributedFileSystem(FileSystem).copyFromLocalFile(Path, Path) line: 1179         GridM2mInstallation.copyInputFiles(FlowConfigurations$FlowConf) line: 380       Passwordless ssh has been setup for current user, tyu, on localhost and user hadoop on hdfs    FileSystem.copyFromLocalFile() should be able to execute using a different username than effective user on localhost so that AccessControlException can be avoided.",util
"Log directly from jetty to commons logging, bypassing SLF4J","Jetty may default to logging through SLF4J, but don't actually need it; you can provide your own logger which goes straight to commons-logging, I've done something similar in the past:    [[http://smartfrog.svn.sourceforge.net/viewvc/smartfrog/trunk/core/components/jetty/src/org/smartfrog/services/jetty/log/]]    You just need to point jetty at the relevant logger by setting up the relevant JVM property, such as {{-Dorg.mortbay.log.class=org.smartfrog.services.jetty.log.JettyLogger}}. Doing something similar in Avro/Hadoop-common would eliminate SLF4J from the dependency graph, route via commons-logging and then, usually, to Log4J, so having only one log to manage. All JVM startup scripts would need to set the relevant property, it is harmless on JVMs that don't start up Jetty6+",io
Document steps to enable {File|Ganglia}Context for kerberos metrics,"There was apatch adding kerberos metrics which included the following line in conf/hadoop-metrics.properties:    ugi.class=org.apache.hadoop.metrics.spi.NullContext    However to maintain consistency with dfs,jvm and mapred metrics and for documentation purpose we should include comments in the file to include steps to enable FileContext and GangliaContext as well. The following lines have to be included in conf/hadoop-metrics.properties:    # Configuration of the ""ugi"" context for file  #ugi.class=org.apache.hadoop.metrics.file.FileContext  #ugi.period=10  #ugi.fileName=/tmp/ugimetrics.log    # Configuration of the ""ugi"" context for ganglia  # ugi.class=org.apache.hadoop.metrics.ganglia.GangliaContext  # ugi.period=10  # ugi.servers=localhost:8649  ",conf
RPC allows clients to create objects with arbitrary size on the server,"When o.a.h.ipc.Server receives a rpc method call, it reads the parameters by initializing an o.a.h.ipc.RPC.Invocation object, which read the parameter values by calling ObjectWritable.readObject(..).  However, ObjectWritable.readObject(..) does not limit the object size and may create objects with arbitrary size.  As a consequence, any rpc client may create large objects in the server by passing large parameter objects.    For example, a user application may creates large objects in the namenode by calling DistributedFileSystem.setOwner(p, username, groupname) if username or groupname are large strings.  In such case, it could easily bring down the namenode.",ipc
Extract a subset of tests for smoke (DOA) validation,"Currently, there's no way to quickly find out if a new build of Hadoop is reasonably viable and can be used for future development/testing cycle.    Similarly to 'commit' testing it'd be great to have a wider subset of existing tests to be executed inside of 30 minutes. The main goal of such a subset is to have a reasonable guarantees that different parts of Hadoop's functionality aren't broken and the system is not DOA.    Full test run takes about 3+ hours and it is a too long wait time in many cases. ",test
"Add a new newInstance method in FileSystem that takes a ""user"" as argument","In order to implement HDFS-1000 for trunk, I need to have a newInstance API in FileSystem that takes a ""user"" as an argument.",fs
enhance FsShell.dus() to include sum of totalSize,FsShell.dus() prints out totalSize for each file found.  It would be desirable to print sum of totalSize's at the end.,util
"SequenceFile.Reader can't read gzip format compressed sequence file, which produce by a mapreduce job, without native compression library","An hadoop job output a gzip compressed sequence file(whether record compressed or block compressed).The client program use SequenceFile.Reader to read this sequence file,when reading the client program shows the following exceptions:    2090 [main] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  2091 [main] INFO org.apache.hadoop.io.compress.CodecPool - Got brand-new decompressor  Exception in thread ""main"" java.io.EOFException    java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:207)    java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)    java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)    java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)    java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)    org.apache.hadoop.io.compress.GzipCodec$GzipInputStream$ResetableGZIPInputStream.<init>(GzipCodec.java:92)    org.apache.hadoop.io.compress.GzipCodec$GzipInputStream.<init>(GzipCodec.java:101)    org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:170)    org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:180)    org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1520)    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)    com.shiningware.intelligenceonline.taobao.mapreduce.HtmlContentSeqOutputView.main(HtmlContentSeqOutputView.java:28)    I studied the code in org.apache.hadoop.io.SequenceFile.Reader.init method and read:        // Initialize... *not* if this we are constructing a temporary Reader        if (!tempReader) {          valBuffer = new DataInputBuffer();          if (decompress) {            valDecompressor = CodecPool.getDecompressor(codec);            valInFilter = codec.createInputStream(valBuffer, valDecompressor);            valIn = new DataInputStream(valInFilter);          } else {            valIn = valBuffer;          }  the problem seems to be caused by ""valBuffer = new DataInputBuffer();"" ,because GzipCodec.createInputStream creates an instance of GzipInputStream whose constructor creates an instance of ResetableGZIPInputStream class.When ResetableGZIPInputStream's constructor calls it base class java.util.zip.GZIPInputStream's constructor ,it trys to read the empty ""valBuffer = new DataInputBuffer();"" and get no content,so it throws an EOFException.",io
Provide a JNI-based implementation of GroupMappingServiceProvider,"The default implementation of GroupMappingServiceProvider does a fork of a unix command to get the groups of a user. Since the group resolution happens in the servers, this might be costly. This jira aims at providing a JNI-based implementation for GroupMappingServiceProvider.",security
[Herriot] Shell command for getting the new exceptions in the logs returning exitcode 1 after executing successfully.,"I have found an corner case issue while executing the shell command for getting the new exceptions in the logs. The problem could see whenever the log files are empty and trying to get the new exceptions count, it returns the output is 0 ,however it returns the exitcode is 1 instead of 0. grep -vc in the command is not  giving the expected exitcode because its not handling the multiple in options.        ",test
"RunJar fails executing thousands JARs within single JVM with error ""Too many open files""","According to Sun JVM (up to 7) bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4167874 - The JarFile objects created by sun.net.www.protocol.jar.JarFileFactory never get garbage collected, even if the classloader that loaded them goes away.    So, if linux-user has limitation on maximum number of open file descriptors (for example: ulimit -n shows 1024) and performs RunJar.main(...) over thousands of JARs that include other nested JARs (also loaded by ClassLoader) within single JVM, RunJar.main(...) throws following exception: java.lang.RuntimeException: java.io.FileNotFoundException: /some-file.txt (Too many open files)",util
Document changes to memory monitoring,Modify the cluster_setup guide with information about memory monitoring and admin configuration.,documentation
Revert FileSystem create method that takes CreateFlags,"As discussed in HDFS-609 and HADOOP-5438 we should back out the FileSystem create() method that takes a set of CreateFlag objects, until the interface has been agreed upon and fully tested.",fs
Herrior uses old way of accessing logs directories,Between 0.20 and 0.21 the way of passing log directories location has been changed from using environment variables to passing system properties. Herrior code needs to address it as well.,test
Deprecated scripts shouldn't be used by Herriot,{{hadoop-daemon.sh}} has been phased out in 0.21 and its usage for HDFS and MR was replaced by {{hdfs}} and {{mapred}} scripts.  Herriot shouldn't be using the deprecated script.,test
Provide a web server plugin that uses a static user for the web UI,We need a simple plugin that uses a static user for clusters with security that don't want to authenticate users on the web UI.,security
TFile.append compares initial key against null lastKey  ,"The following code in TFile.KeyReigster.close:                   byte[] lastKey = lastKeyBufferOS.getBuffer();              int lastLen = lastKeyBufferOS.size();              if (tfileMeta.getComparator().compare(key, 0, len, lastKey, 0,                  lastLen) < 0) {                throw new IOException(""Keys are not added in sorted order"");              }    compares the initial  key (passed in via  TFile.Writer.append) against a technically NULL lastKey. lastKey is not initialized until after the first call to TFile.Writer.append. The underlying RawComparator interface used for comparisons does not stipulate the proper behavior when either length 1  or length 2 is zero. In the case of LongWritable, its WritableComparator implementation does an unsafe read on the passed in byte arrays b1 and b2. Since TFile pre-allocates the buffer used for storing lastKey, this passes a valid buffer with zero count to LongWritable's comparator, which ignores length and thus produces incorrect results.   ",io
[Herriot]: Generic method for adding/modifying the attributes for new configuration.,"HADOP-6772 deals with   Common utilities for system tests.  1. A method for restarting the daemon with new configuration.  c throws Exception;     2. A method for restarting the daemon with default configuration.  public void restart() throws Exception;    3. A method for waiting until daemon is stop.  public void waitForClusterToStop() throws Exception;    In this  some variables are made of String, instead of Long. Those needs ot be changed too. So, can this method   ""public static void restartClusterWithNewConfig(Hashtable<String,Long> props, String confFile) throws Exception;""  be generalized to accepts string too. All otehr methods should work as usual.    ",test
Support for LZMA compression,"Add support for LZMA (http://www.7-zip.org/sdk.html) compression, which generally achieves higher compression ratios than both gzip and bzip2.",io
Investigate Eclipse API Tools for enforcing or reporting on API compatibility ,Some links:  * http://eclipsesource.com/blogs/2010/06/14/api_tools_top_eclipse_helios_feature_8/  * http://wiki.eclipse.org/PDE/API_Tools,"build,documentation"
[Herriot] Implement a functionality for getting the user list for creating proxy users.,"Develop a new method for getting the user list.     Method signature is   public ArrayList<String> getHadoopMultiUsersList() throws IOException;  Add new attribute in system-test.xml  file for getting userlist path.    For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",test
Support non-recursive create() in FileSystem & SequenceFile.Writer,"The proposed solution for HBASE-2312 requires the sequence file to handle a non-recursive create.  This is already supported by HDFS, but needs to have an equivalent FileSystem & SequenceFile.Writer API.","fs,io"
"In Quick Start, mention requirement that HADOOP_HOME needs to be set","Ref:  http://hadoop.apache.org/common/docs/r0.20.0/quickstart.html#Local  Ref: http://wiki.apache.org/hadoop/QuickStart    As a person new to Hadoop, I was walking through the Quick Start pages referenced above.  I got stuck on the first example and had to go digging through the scripts to figure out why it wasn't working for me.    It turns out that I didn't have the HADOOP_HOME environment variable set.  It would have saved a lot of time if the instructions mentioned setting this variable.      Thanks.",documentation
"""hadoop fs -text"" does not give a useful text representation of MapWritable objects","If a sequence file contains MapWritable objects, running ""hadoop fs -text"" on the file prints the following for each MapWritable:    org.apache.hadoop.io.MapWritable@4f8235ed    To be more useful, it should print out the contents of the map instead. This can be done by adding a toString method to MapWritable, i.e. something like:    public String toString() {      return (new TreeMap<Writable, Writable>(instance)).toString();  }  ",io
Update docs to reflect new jar names,"HADOOP-6382 changed the way jars are named, the docs need to be updated to reflect this.",documentation
Scripts for building Hadoop 0.22.0 release,None,build
Problem staging 0.21.0 artifacts to Apache Nexus Maven Repository,"I get this error:  {noformat}  -mvn-system-deploy:  [artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime  [artifact:deploy] Deploying to https://repository.apache.org/content/repositories/snapshots  [artifact:deploy] Uploading: org/apache/hadoop/hadoop-common-instrumented/0.21.0/hadoop-common-instrumented-0.21.0.jar to apache.snapshots.https  [artifact:deploy] Uploaded 1331K  [artifact:deploy] An error has occurred while processing the Maven artifact tasks.  [artifact:deploy]  Diagnosis:  [artifact:deploy]   [artifact:deploy] Error deploying artifact 'org.apache.hadoop:hadoop-common-instrumented:jar': Error deploying artifact: Failed to transfer file: https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common-instrumented/0.21.0/hadoop-common-instrumented-0.21.0.jar. Return code is: 400  {noformat}    Note that 400 is ""Bad Request"", not an authentication error (401).",build
"Add more documents about command ""daemonlog""","In the current document, it does not explain the argument ""name"" means. People without java knowledge would think it as process name, such as NameNode or DataNode, but actually it is class name. So I suggest to add more document about the ""daemonlog"" to explain more about the arguments.    PS: I only find the document on official site (Programming Guide -- Commands Guide), but did not found the document in trunk. Anybody know where's the document of Commands Guide ?",documentation
FsShell have resource leak,"When FsShell exectutes -text command it's using TextRecordInputStream. This class doesn't close inbuf and outbuf (with underlying socket in case of HDFS fs) opened in constructor. It's ok in ""classic"" ""command per JVM"" case when -text command is being used from command-line. But when FsShell is being used in same JVM several times (for example via http://martiansoftware.com/nailgun/index.html project) it cause socket leak.",fs
[Herriot] Cleanup of temp. configurations is needed upon restart of a cluster,"1. New configuration directory is not cleaning up after resetting to default configuration directory in a pushconfig functionality. Because of this reason, it's giving  permission denied problem for a folder, if  other user tried running the tests in the same cluster with pushconfig functionality. I could see this issue while running the tests on a cluster with security enabled and different user.    I have added the functionality for above issue and attaching the patch    2.  Throwing IOException and it says token is expired while running  the tests. I could see this issue in a secure cluster.    This issue has been resolved by setting the following attribute in the configuration.     mapreduce.job.complete.cancel.delegation.tokens=false     adding/updating this attribute in the push configuration functionality while creating the new configuration.  ",test
Have LocalDirAllocator.AllocatorPerContext.getLocalPathForWrite fail more meaningfully,"A stack trace makes it way to me, of a reduce failing  {code}  Caused by: org\.apache\.hadoop\.util\.DiskChecker$DiskErrorException: Could not find any valid local directory for file:/mnt/data/dfs/data/mapred/local/taskTracker/jobcache/job_201007011427_0001/attempt_201007011427_0001_r_000000_1/output/map_96\.out        at org\.apache\.hadoop\.fs\.LocalDirAllocator$AllocatorPerContext\.getLocalPathForWrite(LocalDirAllocator\.java:343)          at org\.apache\.hadoop\.fs\.LocalDirAllocator\.getLocalPathForWrite(LocalDirAllocator\.java:124)          at org\.apache\.hadoop\.mapred\.ReduceTask$ReduceCopier$LocalFSMerger\.run(ReduceTask\.java:2434)  {code}    We're probably running out of HDD space, if not its configuration problems. Either way, some more hints in the exception would be handy.  # Include the size of the output file looked for if known  # Include the list of dirs examined and their reason for rejection (not found or if not enough room, available space).  This would make it easier to diagnose problems after the event, with nothing but emailed logs for diagnostics.",fs
Allow sequence file to be created with different bytes per checksum,This jira allows a sequence file to be created with a configured bytes per checksum. So we could experiment how this parameter could effect the performance of random reads of a sequence file.,io
Fix '$bin' path duplication in setup scripts,"I have my bash environment setup to echo absolute pathnames when a relative one is specified in 'cd'. This caused problems with all the Hadoop bash scripts because the script accidentally sets the $bin variable twice in this setup. (e.g. would set $bin=""/path/bin/hadoop\n/path/bin/hadoop"").    This jira is for common scripts.  I filed a separate jira for HDFS scripts, which share the same pattern.",scripts
apparent bug in concatenated-bzip2 support (decoding),"The following simplified code (manually picked out of testMoreBzip2() in https://issues.apache.org/jira/secure/attachment/12448272/HADOOP-6835.v4.trunk-hadoop-mapreduce.patch) triggers a ""java.io.IOException: bad block header"" in org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock( CBZip2InputStream.java:527):    {noformat}      JobConf jobConf = new JobConf(defaultConf);        CompressionCodec bzip2 = new BZip2Codec();      ReflectionUtils.setConf(bzip2, jobConf);      localFs.delete(workDir, true);        // copy multiple-member test file to HDFS      String fn2 = ""testCompressThenConcat.txt"" + bzip2.getDefaultExtension();      Path fnLocal2 = new Path(System.getProperty(""test.concat.data"",""/tmp""),fn2);      Path fnHDFS2  = new Path(workDir, fn2);      localFs.copyFromLocalFile(fnLocal2, fnHDFS2);        FileInputFormat.setInputPaths(jobConf, workDir);        final FileInputStream in2 = new FileInputStream(fnLocal2.toString());      CompressionInputStream cin2 = bzip2.createInputStream(in2);      LineReader in = new LineReader(cin2);      Text out = new Text();        int numBytes, totalBytes=0, lineNum=0;      while ((numBytes = in.readLine(out)) > 0) {        ++lineNum;        totalBytes += numBytes;      }      in.close();  {noformat}    The specified file is also included in the H-6835 patch linked above, and some additional debug output is included in the commented-out test loop above.  (Only in the linked, ""v4"" version of the patch, however--I'm about to remove the debug stuff for checkin.)    It's possible I've done something completely boneheaded here, but the file, at least, checks out in a subsequent set of subtests and with stock bzip2 itself.  Only the code above is problematic; it reads through the first concatenated chunk (17 lines of text) just fine but chokes on the header of the second one.  Altogether, the test file contains 84 lines of text and 4 concatenated bzip2 files.    (It's possible this is a mapreduce issue rather than common, but note that the identical gzip test works fine.  Possibly it's related to the stream-vs-decompressor dichotomy, though; intentionally not supported?)",io
Cannot Configure 'progress' with CreateOpts API,"Noticed that CreateOpts class specifies a Progress class, but does not allow you to specify a Progressable object.  Need to add static function to set this API element.  Trying to get FileContext on parity with old FileSystem API.",fs
Add ability to get groups for ACLs from 'getent netgroup',Add ability to specify netgroups in ACLs (see class AccessControlList.java). Membership of users in netgroups will be determined by running 'getent negroups $groupName'. Netgroups will be differentiated from regular unix groups by having '@' as a first character.,security
SequenceFile and MapFile need cleanup to remove redundant constructors,"Currently there are 2 public SequenceFile.Reader constructors, 3 public SequenceFile.Writer constructors, 9 public SequenceFile.createWriter, 2 public MapFile.Reader constructors, and 8 public MapFile.Writer constructors. All of with various historical combinations of parameters that don't cover the entire space.    All of this makes it *very* difficult to add new optional parameters to SequenceFile and MapFile.     I'd like change to the style of FileContext.create with option parameters. I'll implement one public SequenceFile.Reader constructor and one public SequenceFile.createWriter and implement all of the current variants based on those two. I'll do the same for MapFile.Reader and MapFile.Writer including passing parameters down to the underlying SequenceFile.",io
FsShell should report raw disk usage including replication factor,"Currently FsShell report HDFS usage with ""hadoop fs -dus <path>"" command.  Since replication level is per file level, it would be nice to add raw disk usage including the replication factor (maybe ""hadoop fs -dus -raw <path>""?).  This will allow to assess resource usage more accurately.  -- Alex K  ",fs
Enable rotateable JVM garbage collection logs for Hadoop daemons,"The purpose of this enhancement is to make it easier to collect garbage collection logs and insure that they persist across restarts in the same way that the standard output files of Hadoop daemon JVM's currently does.    Garbage collection logs are a vital debugging tool for administrators and developers. In our production environments, at some point or another, every single type of Hadoop daemon has OOM'ed or experienced other significant issues related to GC and/or lack of heap memory. For the longest time, we have put in garbage collection logs in our HADOOP_NAMENODE_OPTS, HADOOP_JOBTRACKER_OPTS, etc. by using options like ""-XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$HADOOP_LOG_DIR/jobtracker.gc.log"".    Unfortunately, these logs don't survive a restart of the node, so if a node OOM's and then is restarted automatically, or manually by someone who is unaware, we lose the GC logs forever. We also have to manually add GC log options to each daemon. This patch:  1) Creates a single, optional, off by default, parameter for specifying GC logging.  2) If that parameter is set, automatically enables GC logging for all daemons in the cluster. The parameter is flexible enough to allow for the different ways various vendor's JVM's require garbage collection logging to be specified.   3) If GC logging is on, insures that the GC log files for each daemon are rotated with up to 5 copies kept, same as the .out files currently.    We are currently running a variation of this patch in our 0.20 install. This patch actually includes changes to common, mapred, and hdfs, so it obviously cannot be applied as-is, but is included here for review and comments.",scripts
 'compile-fault-inject' should never be called directly.,Similar to HDFS-1299 a  helper target  'compile-fault-inject' should never be called directly.,build
Add api to add user/group to AccessControlList,Add api addUser(String user) and addGroup(String group) to add user/group to AccessControlList,security
Provide a JNI-based implementation of ShellBasedUnixGroupsNetgroupMapping (implementation of GroupMappingServiceProvider),"The netgroups implementation of GroupMappingServiceProvider (see ShellBasedUnixGroupsNetgroupMapping.java) does a fork of a unix command to get the netgroups of a user. Since the group resolution happens in the servers, this might be costly. This jira aims at providing a JNI-based implementation for GroupMappingServiceProvider.    Note that this is similar to what https://issues.apache.org/jira/browse/HADOOP-6818 does for implementation of GroupMappingServiceProvider that  supports only unix groups.",security
there will be ant error if ant ran without network connected,"If you run `ant` without network connected, there will be an error below. And even if you connect your network, the error will exist.    ivy-init-antlib:    [typedef] java.util.zip.ZipException: error in opening zip file    [typedef]     at java.util.zip.ZipFile.open(Native Method)    [typedef]     at java.util.zip.ZipFile.<init>(ZipFile.java:114)    [typedef]     at java.util.zip.ZipFile.<init>(ZipFile.java:131)    [typedef]     at org.apache.tools.ant.AntClassLoader.getResourceURL(AntClassLoader.java:1028)    [typedef]     at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.findNextResource(AntClassLoader.java:147)    [typedef]     at org.apache.tools.ant.AntClassLoader$ResourceEnumeration.<init>(AntClassLoader.java:109)    [typedef]     at org.apache.tools.ant.AntClassLoader.findResources(AntClassLoader.java:975)    [typedef]     at java.lang.ClassLoader.getResources(ClassLoader.java:1016)    [typedef]     at org.apache.tools.ant.taskdefs.Definer.resourceToURLs(Definer.java:364)    [typedef]     at org.apache.tools.ant.taskdefs.Definer.execute(Definer.java:256)    [typedef]     at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:288)    [typedef]     at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)    [typedef]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    [typedef]     at java.lang.reflect.Method.invoke(Method.java:597)    [typedef]     at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)    [typedef]     at org.apache.tools.ant.Task.perform(Task.java:348)    [typedef]     at org.apache.tools.ant.Target.execute(Target.java:357)    [typedef]     at org.apache.tools.ant.Target.performTasks(Target.java:385)    [typedef]     at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1337)    [typedef]     at org.apache.tools.ant.Project.executeTarget(Project.java:1306)    [typedef]     at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)    [typedef]     at org.apache.tools.ant.Project.executeTargets(Project.java:1189)    [typedef]     at org.apache.tools.ant.Main.runBuild(Main.java:758)    [typedef]     at org.apache.tools.ant.Main.startAnt(Main.java:217)    [typedef]     at org.apache.tools.ant.launch.Launcher.run(Launcher.java:257)    [typedef]     at org.apache.tools.ant.launch.Launcher.main(Launcher.java:104)    [typedef] Could not load definitions from resource org/apache/ivy/ant/antlib.xml. It could not be found.    BUILD FAILED  /opt/hadoop-0.20.2/build.xml:1644: You need Apache Ivy 2.0 or later from http://ant.apache.org/        It could not be loaded from http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.0.0-rc2/ivy-2.0.0-rc2.jar  ",build
Text class should provide method to return byte array of getLength(),"People would use the following code to convert Text to String:  String valueString = new String(valueText.getBytes(), ""UTF-8"");    However, if Text is reused, the above call would return String of monotonically increasing length.  From 'Hadoop and XML' discussion thread:  The problem I am seeing is between the Map phase and the  Reduce phase, the XML is getting munged.  For Example:   </PrivateRate>   </PrivateRateSet>te>    Text should provide method to return byte array of getLength() length.",util
Functionality to create file or folder on a remote daemon side,"Functionality for creating either files or folders in task attempt folder while job is running. The functionality covers the following methods.    1. public void DaemonProtocol.createFile(String path, String fileName, boolean local) throws IOException;   It uses to create a file with full permissions.    2.   public void DaemonProtocol.createFile(String path, String fileName, FsPermission permission, boolean local) throws IOException;   It uses to create a file with given permissions.    3.   public void DaemonProtocol.createFolder(String path, String folderName, boolean local) throws IOException;  It uses to create a file with full permissions.    4.   public void DaemonProtocol.createFolder(String path, String folderName, FsPermission permission, boolean local) throws IOException;  It uses to create a folder with given permissions.  ",test
Add FileSystem#listLocatedStatus to list a directory's content together with each file's block locations,"This jira implements the new FileSystem API as proposed in HDFS-202. The new API aims to eliminate individual ""getFileBlockLocations"" calls to NN for each file in the input directory of a job. Instead, a file's block locations are returned together with FileStatus when listing a directory, thus improving getSplits performance.",fs
"When the value of a configuration key is set to its unresolved form, it causes the IllegalStateException in Configuration.get() stating that substitution depth is too large.","When a configuration value is set to its unresolved expression string, it leads to recursive substitution attempts in {{Configuration.substituteVars(String)}} method until the max substitution check kicks in and raises an IllegalStateException indicating that the substitution depth is too large. For example, the configuration key ""{{foobar}}"" with a value set to ""{{$\{foobar\}}}"" will cause this behavior.     While this is not a usual use case, it can happen in build environments where a property value is not specified and yet being passed into the test mechanism leading to failures due to this limitation.  ",conf
ChecksumFs#listStatus should filter out .crc files,Currently ChecksumFs#listStatus listing not only regular files but also .crc files. Crc files should be filtered out.,fs
JvmMetrics can't distinguish between jvms with same processName,"JvmMetrics has three tags: hostName, processName, and sessionId. For processes such as tasktracker/jobtracker/namenode/datanode which there is only one of on each host, these tags are fine. But for process names such as ""MAP"" and ""REDUCE"", since there might be multiple jvms running map/reduce tasks, we might end up with multiple set of metrics which all have the same tags, and no way to find out which jvm they actually correspond to. (In addition, since there is jvm reuse, those process names might not correspond to the actual task being ran)    A quick fix is to change this line in Child.java  JvmMetrics.init(task.getPhase().toString(), job.getSessionId());  to this  JvmMetrics.init(jvmId.toString(), job.getSessionId());  so that we are using the jvm id for the process name instead.",metrics
Common part of HDFS-1178,This is the Common part of HDFS-1178.,ipc
Use NetgroupCache.java in ShellBasedUnixGroupsNetgroupMapping.java,Code to handle netgroups cache is factored out of JniBasedUnixGroupsNetgroupMapping class and implemented in a separate class NetgroupCache.    ShellBasedUnixGroupsNetgroupMapping should also use the same NetgroupCache class instead of implementing the same cache.,security
Provide SSH based (Jsch) remote execution API for system tests,http://mvnrepository.com/  com.jcraft   jsch   0.1.42 version needs to be included in the build. This is  needed to facilitate implementation of some system (Herriot) testcases .    Please include this in ivy.    jsch is originally located in http://www.jcraft.com/jsch/,"build,test"
Documentation: Chinese (cn) doc is old,"Documentation: Chinese (cn) doc is old.  http://hadoop.apache.org/common/docs/r0.20.2/quickstart.html  JavaTM 1.6.x, preferably from Sun, must be installed.   Configuration    Use the following:  conf/core-site.xml:  <configuration>    <property>      <name>fs.default.name</name>      <value>hdfs://localhost:9000</value>    </property>  </configuration>      conf/hdfs-site.xml:  <configuration>    <property>      <name>dfs.replication</name>      <value>1</value>    </property>  </configuration>      conf/mapred-site.xml:  <configuration>    <property>      <name>mapred.job.tracker</name>      <value>localhost:9001</value>    </property>  </configuration>    http://hadoop.apache.org/common/docs/r0.20.2/cn/quickstart.html  JavaTM1.5.x??????????Sun?????Java???   ??    ????? conf/hadoop-site.xml:  <configuration>    <property>      <name>fs.default.name</name>      <value>localhost:9000</value>    </property>    <property>      <name>mapred.job.tracker</name>      <value>localhost:9001</value>    </property>    <property>      <name>dfs.replication</name>      <value>1</value>    </property>  </configuration>",documentation
Text.toString violates its abstraction,"I stumbled upon this when encoding a google protocol buffer in base64, and storing it in a Text object for serialization. Compare the following two lines:    byte [] decoded = b64.decode(val.getBytes())  //this does not return the same bytes as below and the result, after decoding the base64 successfully, is a very mangled protocol buffer    byte [] decoded = b64.decode(val.toString().getBytes());  //YES, toString() FIXES IT    Elsewhere in my code I also have:   Text curline = new Text(values.next().toString());  byte [] raw = base64.decode(curline.getBytes());  //This does work.    It looks like the Text object must be toString'd (just once, somewhere, even if its later repacked in a Text) before it will have the proper byte representation. I would classify this as a leaky abstraction and ask that the reason please be isolated and the api fixed somehow so that other developers dont have to spend 3 days figuring out when Text.getBytes isn't returning the right bytes even though Text.toString prints exactly the right string representation and Text.toString.getBytes does return the right bytes.",io
Fix java doc warnings in Groups and RefreshUserMappingsProtocol,There are a couple java docs warnings in Groups and RefreshUserMappingsProtocol.,security
LocalFileSystem Needs createNonRecursive API,"While running sanity check tests for HBASE-2312, I noticed that HDFS-617 did not include createNonRecursive() support for the LocalFileSystem.  This is a problem for HBase, which allows the user to run over the LocalFS instead of HDFS for local cluster testing.  I think this only affects 0.20-append, but may affect the trunk based upon how exactly FileContext handles non-recursive creates.",fs
Need a separate metrics per garbage collector,"In addition to current GC metrics which are the sum of all the collectors, Need separate metrics for monitoring young generation and old generation collections per collector w.r.t collection count and collection time. ",metrics
Being able to close all cached FileSystem objects for a given UGI,This is the Common part of MAPREDUCE-1900. It adds a utility method to FileSystem that closes all cached filesystems for a given UGI.,"fs,security"
Make RPC to have an option to timeout,"Currently Hadoop RPC does not timeout when the RPC server is alive. What it currently does is that a RPC client sends a ping to the server whenever a socket timeout happens. If the server is still alive, it continues to wait instead of throwing a SocketTimeoutException. This is to avoid a client to retry when a server is busy and thus making the server even busier. This works great if the RPC server is NameNode.    But Hadoop RPC is also used for some of client to DataNode communications, for example, for getting a replica's length. When a client comes across a problematic DataNode, it gets stuck and can not switch to a different DataNode. In this case, it would be better that the client receives a timeout exception.    I plan to add a new configuration ipc.client.max.pings that specifies the max number of pings that a client could try. If a response can not be received after the specified max number of pings, a SocketTimeoutException is thrown. If this configuration property is not set, a client maintains the current semantics, waiting forever.",ipc
Improve listFiles API introduced by HADOOP-6870,"This jira is mainly for addressing Suresh's review comments for HADOOP-6870:       1. General comment: I have concerns about recursive listing. This could be abused by the applications, creating a lot of requests into HDFS.     2. Any deletion of files/directories while reursing through directories results in RuntimeException and application has a partial result. Should we ignore if a directory was in stack and was not found later when iterating through it?     3. FileSystem.java            * listFile() - method javadoc could be better organized - first write about if path is directory and two cases recursive=true and false. Then if path is file and two cases recursive=true or false.            * listFile() - document throwing RuntimeException, UnsupportedOperationException and the possible cause. IOException is no longer thrown.     4. TestListFiles.java            * testDirectory() - comments test empty directory and test directory with 1 file should be moved up to relevant sections of the test.",fs
Common component of HDFS-1150 (Verify datanodes' identities to clients in secure clusters),HDFS-1150 will have changes to the start-up scripts and HttpServer.  These are handled here.,security
FileContext needs a method to test for existence of a file or directory.,"FileContext is missing a method to ask if a file or directory exists. Currently the only mechanism is to use getFileStatus, which throws an exception if the file doesn't exist, which I don't think is an appropriate use of that method.",fs
Native Libraries do not load if a different platform signature is returned from org.apache.hadoop.util.PlatformName,"bin/hadoop-config.sh has an environment variable called JAVA_PLATFORM which is set to to the results returned by org.apache.hadoop.util.PlatformName . These results are sometimes unique to the JRE being used. Although the value returned for 64 Bit Sun/Oracle Java and 64 Bit IBM Java is the same, it is different for the corresponding 32 Bit JREs.     The issue is that the value returned is used in creating the path to the native libraries on disk, i.e ${HADOOP_COMMON_HOME}/lib/native/${JAVA_PLATFORM}    Since the path on disk is fixed with the Sun JRE value /lib/native/Linux-i386-32 it therefore fails when it attempts to load the native libraries with the value returned with 32 Bit IBM Java, /lib/native/Linux-x86-32",native
Remove the Util separation in FileContext,"I think from a usability point of view, it would be nicer to write:    {code}  fc.exist(path);  {code}    instead of:    {code}  fc.utils().exist(path);  {code}    We should use the javadoc to mark the fact that the util functions are not atomic.",fs
A baby step towards inter-version RPC communications,"Currently RPC communications in Hadoop is very strict. If a client has a different version from that of the server, a VersionMismatched exception is thrown and the client can not connect to the server. This force us to update both client and server all at once if a RPC protocol is changed. But sometime different versions do not mean the client & server are not compatible. It would be nice if we could relax this restriction and allows us to support inter-version communications.    My idea is that DfsClient catches VersionMismatched exception when it connects to NameNode. It then checks if the client & the server is compatible. If yes, it sets the NameNode version in the dfs client and allows the client to continue talking to NameNode. Otherwise, rethrow the VersionMismatch exception.",ipc
FileSystem.copyToLocal creates files with 777 permissions,"FileSystem.copyToLocal ends up calling through to FileUtil.copy, which calls create() on the target file system without passing any permission object. Therefore, the file ends up getting created locally with 777 permissions, which is dangerous -- even if the caller then fixes up permissions afterwards, it exposes a window in which an attacker can open the file.","fs,security"
RawLocalFileSystem#setWorkingDir() does not work for relative names,RawLocalFileSystem#setWorkingDir() does not work for relative names,fs
RPC server's SASL_PROPS shouldn't be re-initialized every time an RPC client is created,"SaslRpcServer.SASL_PROPS is a SASL server property and should stay constant after initialization. In the initial implementation, we assumed all SASL clients will use the same constant value. If different clients might use different values depending on the conf in the getProxy() call (as current code implies), each client should have its own copy. In any case, a client shouldn't re-initialize server's copy.","ipc,security"
Better logging messages when a delegation token is invalid,"From our production logs, we see some logging messages of ""token is expired or doesn't exist"". It would be helpful to know whose token it was.",security
FileContext copy() utility doesn't work with recursive copying of directories.,None,fs
Rpc client doesn't use the per-connection conf to figure out server's Kerberos principal,"Currently, RPC client caches the conf that was passed in to its constructor and uses that same conf (or values obtained from it) for every connection it sets up. This is not sufficient for security since each connection needs to figure out server's Kerberos principal on a per-connection basis. It's not reasonable to expect the first conf used by a user to contain all the Kerberos principals that her future connections will ever need. Or worse, if her first conf contains an incorrect principal name, it will prevent the user from connecting to the server even if she later on passes in a correct conf on retry (by calling RPC.getProxy()).","ipc,security"
Guard against NPE when calling UGI.isLoginKeytabBased(),NPE can happen when isLoginKeytabBased() is called before a login is performed. See MAPREDUCE-1992 for an example.,security
Circular initialization between UserGroupInformation and KerberosName,"If the first call to UGI is UGI.setConfiguration(conf), it will try to initialize UGI class. During this initialization, the code calls KerberosName.setConfiguration(). KerberosName's static initializer will in turn call UGI.isSecurityEnabled(). Since UGI hasn't been completely initialized yet, isSecurityEnabled() will re-initialize UGI with a DEFAULT conf. As a result, the original conf used in UGI.setConfiguration(conf) will be overwritten by the DEFAULT conf.",security
IPC Client Connection close does not protect against partially Connection objects,"The purpose of this ticket is to cross-reference hbase-2042, opened 12/12/2009. close() is used to release Connection resources. It is invoked even though the Connection object may not have been completely initialized. For example, the file descriptor ulimit may have been exceeded in which case no socket would have been acquired. The result would be an NPE in close. The implied contract for close() seems to be that it will never fail.     There is a proposed patch in the hbase-2042 ticket.",ipc
Add daemon restart option,"With the addition of AvatarNode, we are working on a rolling restart option for upgrading a cluster.  It seems that a generic restart option would be useful for this purpose and perhaps other administrative tasks.  Add a 'restart' option to hadoop-daemon.sh and also modify 'slaves.sh' to run operations in parallel or serially.  ",scripts
Implement append operation for S3FileSystem,"Currently org.apache.hadoop.fs.s3.S3FileSystem#append throws an IOException(""Not supported"");  S3FileSystem should be able to support appending, possibly via common block storage logic.",fs/s3
Make metrics naming consistent,"While working HADOOP-6728, I noticed that our metrics naming style is all over the place:    * Capitalized camel case: e.g., ""FilesCreated"" in namenode metrics and some rpc metrics  * uncapitalized camel case: e.g, ""threadsBlocked"" in jvm metrics and some rpc metrics  * lowercased underscored: e.g., ""bytes_written"" in datanode metrics and mapreduce metrics    Let's make them consistent. How about uncapitalized camel case? My main reason for the camel case: some backends have limits on the name length and underscore is wasteful.    Once we have a consistent naming style we can do:  @Metric(""Number of INodes created"") MutableCounterLong filesCreated;    instead of the more redundant:  @Metric({""FilesCreated"", ""Number of INodes created""}) MutableCounterLong filesCreated;",metrics
Metrics2: metrics framework,"The jira tracks the new metrics framework only changes, i.e., it doesn't track the instrumentation changes (and compatibility issues) to existing code.",metrics
COMMON part of MAPREDUCE-1664,MAPREDUCE-1664 changes the behavior of queue acls and job acls. This needs documentation changes to cluster_setup.xml and a small change in AccessControlList.java,"documentation,security"
Native Libraries do not load if a different platform signature is returned from org.apache.hadoop.util.PlatformName,"The bin/hadoop script has an environment variable called JAVA_PLATFORM which is set to to the results returned by org.apache.hadoop.util.PlatformName . These results are sometimes unique to the JRE being used. Although the value returned for 64 Bit Sun/Oracle Java and 64 Bit IBM Java is the same, it is different for the corresponding 32 Bit JREs.  The issue is that the value returned is used in creating the path to the native libraries on disk, i.e ${HADOOP_COMMON_HOME}/lib/native/${JAVA_PLATFORM}    Since the path on disk is fixed with the Sun JRE value /lib/native/Linux-i386-32 it therefore fails when it attempts to load the native libraries with the value returned with 32 Bit IBM Java, /lib/native/Linux-x86-32",native
BZip2Codec incorrectly implements read(),HADOOP-4012 added an implementation of read() in BZip2InputStream that doesn't work correctly when reading bytes > 0x80. This causes EOFExceptions when working with BZip2 compressed data inside of sequence files in some datasets.,io
SocketInputStream incorrectly implements read(),"SocketInputStream's read() implementation doesn't upcast to int correctly, so it can't read bytes > 0x80. This is the same bug as HADOOP-6925, but in a different spot.",io
Fix BooleanWritable comparator in 0.20,The RawComparator for BooleanWritable was fixed as part of HADOOP-5699 in 0.21 and trunk. The fix should be pushed back into 0.20.,io
RPC should have a way to pass Security information other than protocol annotations,Currently Hadoop RPC allows protocol annotations as the only way to pass security information. This becomes a problem if protocols are generated and not hand written. For example protocols generated via Avro and passed over Avro tunnel (AvroRpcEngine.java) can't pass the security information.,"ipc,security"
AvroRpcEngine doesn't work with generated Avro code,AvroRpcEngine uses 'reflect' based java implementation. There should be a way to have it work with 'specific' (generated code from avro idl).,ipc
TestListFiles is flaky,"TestListFiles assumes a particular order of the files returned by the directory iterator. There's no such guarantee made by the underlying API, so the test fails on some hosts.",test
test for ByteWritable comparator,"The test for the ByteWritable comparator bug (see HADOOP-6928), which is already fixed in 0.21.",record
IPC Server readAndProcess threw NullPointerException,"{noformat}  2010-09-01 01:22:28,995 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50300:    readAndProcess threw exception java.lang.NullPointerException. Count of bytes read: 0  java.lang.NullPointerException          at org.apache.hadoop.ipc.Server$Call.toString(Server.java:268)          at java.lang.String.valueOf(String.java:2827)          at java.lang.StringBuilder.append(StringBuilder.java:115)          at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:705)          at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:720)          at org.apache.hadoop.ipc.Server$Connection.doSaslReply(Server.java:988)          at org.apache.hadoop.ipc.Server$Connection.saslReadAndProcess(Server.java:935)          at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1093)          at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:462)          at org.apache.hadoop.ipc.Server$Listener.run(Server.java:371)  {noformat}",ipc
broken links in http://wiki.apache.org/hadoop/FAQ#A12//s,http://wiki.apache.org/hadoop/FAQ#A12//s   has links to :    http://hadoop.apache.org/core/docs/current/hadoop-default.html#dfs.replication.min  http://hadoop.apache.org/common/docs/current/hadoop-default.html#dfs.safemode.threshold.pct    both of which are 404 as of time of filing this issue.,documentation
Invalid Entry for JSP JARs in Hadoop CLASSPATH,"~/src/Hadoop/stage/hadoop-0.22.0-SNAPSHOT > bin/hadoop classpath | sed 's/:/\n/g' |grep \\*  /home/ranjit/src/Hadoop/stage/hadoop-0.22.0-SNAPSHOT/bin/../lib/jsp-2.1/*.jar    The actual JSP JARs (jsp-2.1-6.1.14.jar  jsp-api-2.1-6.1.14.jar) are just under the ""lib"" folder - there  is no ""jsp-2.1"" sub-folder.    The offending code is from bin/hadoop-config.sh:        156 for f in $HADOOP_COMMON_HOME/lib/jsp-2.1/*.jar; do      157   CLASSPATH=${CLASSPATH}:$f;      158 done  ",build
ConnectionId.getRemotePrincipal() should check if security is enabled,"When security is not enabled, getRemotePrincipal() should return null, which means the Kerberos principal of the remote server is ignored. This bug was caught by TestCLI on Yahoo 20S branch.","ipc,security"
RawLocalFileSystem's markSupported method misnamed markSupport,It should be named markSupported to override the method defined in InputStream. Since it doesn't change the default no harm is done.,fs
Ability for having user's classes take precedence over the system classes for tasks' classpath,Fix bin/hadoop script to facilitate mapred-1938,scripts
The GroupMappingServiceProvider interface should be public,"The GroupMappingServiceProvider interface is presently package-protected. It seems likely that many organizations will be implementing their own versions of this to suit their particular setup. It would be nice if this interface were made public, and annotated with ""@InterfaceAudience.Private"" and ""@InterfaceStability.Evolving"".",security
[Herriot] Implement a functionality for getting proxy users definitions like groups and hosts.,"Gridmix should require a proxy user's file for impersonating various jobs. So, implement couple of methods for getting the proxy users list and a proxy users file (it's a combination of proxy users and groups) based on cluster configuration.    The proxy users list should require for map reduce jobs and proxy users file should require for gridmix jobs.    The following are methods signature,  public ProxyUserDefinitions getHadoopProxyUsers() - get the list of proxy users list based on cluster configuration.  ",test
"SecurityUtils' TGT fetching does not fall back to ""login"" user","In SecurityUtil.getTgtFromSubject and SecurityUtil.fetchServiceTicket, the current JAAS Subject is fetched directly from the AccessController, rather than using UserGroupInformation.getCurrentUser().getSubject(). This means that if it is not run in the confines of a doAs() block, it will fail since the current JAAS subject is null, even though SecurityUtil.login(...) may have been called.    In practice, one place this shows up is using the secondary namenode's ""-checkpoint force"" option in secured 0.20, since it's done inside the main thread with no surrounding doAs().",security
Kerberos relogin should set refreshKrb5Config to true,"In working on securing a daemon that uses two different principals from different threads, I found that I wasn't able to login from a second keytab after I'd logged in from the first. This is because we don't set the refreshKrb5Config in the Configuration for the Krb5LoginModule - hence it won't switch over to the correct keytab file if it's different than the first.",security
There's no way to remove a property from a Configuration,"The Configuration class has a lot of methods for setting properties, but it's impossible to remove a property once it's set. Trying to set it to null results in a NullPointerException, since java.util.Properties doesn't allow null values. When building a Configuration programmatically, this can be a problem. ",conf
"Reduces RPC packet size for primitive arrays, especially long[], which is used at block reporting",Current implementation of oah.io.ObjectWritable marshals primitive array types as general object array ; array type string + array length + (element type string + value)*n    It would not be needed to specify each element types for primitive arrays.,io
Suggest that HADOOP_CLASSPATH should be preserved in hadoop-env.sh.template,"HADOOP_CLASSPATH tends to be used to add to bin/hadoop's classpath.  Because of the way the comment is written, administrator's who customize hadoop-env.sh often inadvertently disable user's abilities to use it, by not including the present value of the variable.    I propose we change the commented out suggestion code to include the present value.    {noformat}   # Extra Java CLASSPATH elements.  Optional.  -# export HADOOP_CLASSPATH=  +# export HADOOP_CLASSPATH=""<extra_entries>:$HADOOP_CLASSPATH""  {noformat}",scripts
Distinct minicluster services (e.g. NN and JT) overwrite each other's service policies,"Because the protocol -> ACL mapping in ServiceAuthorizationManager is static, services which are run in the same JVM have the potential to clobber the other's service authorization ACLs whenever ServiceAuthorizationManager.refresh() is called. This causes authorization failures if one tries to launch a 2NN connected to a minicluster with hadoop.security.authorization enabled. Seems like each service should have its own instance of a ServiceAuthorizationManager, instead of using static methods.",security
Support sending priority RPC,There are certain class of RPCs that need priority delivery. This applies especially to heartbeat RPCs that distributed systems (like HDFS) uses. Ability to deliver heartbeat RPCs earlier than other data-movement RPCs can improve the scalability of large scale distributed systems.,ipc
"start-{dfs,mapred}.sh scripts fail if HADOOP_HOME is not set","If the HADOOP_HOME environment variable is not set then the start and stop scripts for HDFS and MapReduce fail with ""Hadoop common not found."". The start-all.sh and stop-all.sh scripts are not affected.",scripts
Sources JARs are not correctly published to the Maven repository,"When you try to ""close"" the staging repository to make it visible to the public the operation fails (see https://issues.apache.org/jira/browse/HDFS-1292?focusedCommentId=12909953&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12909953 for the type of error).",build
ant-eclipse-download,"ant-eclipse-download: Always getting to   http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2.  Can't it be moved to ivy settings to pick from ""fs""? Will definitly save build time for projects.",build
compile-core-test,"Currently compile-core-test only compiles test/core classes. classes defined in test/system are required by Mapreduce project's test classes. It requires to make a minor change in common build.xml as given below:    Changed    <javac       encoding=""${build.encoding}""       srcdir=""${test.src.dir}""       includes=""**/org/apache/hadoop/**/*.java""       destdir=""${test.core.build.classes}""       debug=""${javac.debug}""       optimize=""${javac.optimize}""       target=""${javac.version}""       source=""${javac.version}""       deprecation=""${javac.deprecation}"">        <compilerarg line=""${javac.args} ${javac.args.warnings}"" />        <classpath refid=""test.classpath""/>       </javac>    Original:   <javac       encoding=""${build.encoding}""       srcdir=""${test.src.dir}/core""       includes=""org/apache/hadoop/**/*.java""       destdir=""${test.core.build.classes}""       debug=""${javac.debug}""       optimize=""${javac.optimize}""       target=""${javac.version}""       source=""${javac.version}""       deprecation=""${javac.deprecation}"">        <compilerarg line=""${javac.args} ${javac.args.warnings}"" />        <classpath refid=""test.classpath""/>       </javac>    change is to       srcdir=""${test.src.dir}""       includes=""**/org/apache/hadoop/**/*.java""    from original          srcdir=""${test.src.dir}/core""       includes=""org/apache/hadoop/**/*.java""    Please suggest.    ",build
Return bindAdrress directly rather than get it from ServerSocketChannel in Listener's getAddress() method,"I am trying to make Pseudo-Distributed hadoop can been accessed by outside, I change the fs.default.name to 0.0.0.0.:9000 , but still failed. Finally I found that the problem is caused by the Listener's getAddress() in Server.java.  It return Inet6Address format (hdfs://0:0:0:0:0:0:0:0:9000) rather than Inet4Address, I write a blog for the details (http://zjffdu.appspot.com/?p=17001)    And since the class member ""address"" is binded to ""ServerSocketChannel"", then it will be easier to return the address directly rather than get it from ServerSocketChannel.     ",ipc
"FileSystem.mkdirs(Path, FSPermission) should use the permission for all of the created directories","Currently, FileSystem.mkdirs only applies the permissions to the last level if it was created. It should be applied to *all* levels that are created.","fs,security"
Fix FileUtil.getDU. It should not include the size of the directory or follow symbolic links,"The getDU method should not include the size of the directory. The Java interface says that the value is undefined and in Linux/Sun it gets the 4096 for the inode. Clearly this isn't useful.  It also recursively calls itself. In case the directory has a symbolic link forming a cycle, getDU keeps spinning in the cycle. In our case, we saw this in the org.apache.hadoop.mapred.JobLocalizer.downloadPrivateCacheObjects call. This prevented other tasks on the same node from committing, causing the TT to become effectively useless (because the JT thinks it already has enough tasks running)",fs
Allow compact property description in xml,"We should allow users to use the more compact form of xml elements. For example, we could allow:  {noformat}  <property name=""mapred.local.dir"" value=""/disk/0/mapred,/disk/1/mapred""/>  {noformat}  The old format would also be supported.",conf
Split Herriot related classes into a separate jar,In order to make cluster deployment more transparent let's separate system-test framework related classes into a separate jar file which might be simply added during test clusters deployment. This new jar file won't include any of the Common classes but only Herriot (test framework) related files and classes. ,"build,test"
SecurityAuth.audit should be generated under /build,"SecurityAuth.audit is generated under currently root project directory whenever I run anything, and is not being cleaned up by the clean target. It should be created under build directory instead.",build
Clover build doesn't generate per-test coverage,Because of the way the structure of Hadoop's builds is done Clover can't properly detect test classes among the sources. As the result clover reports are incomplete and do not provide viral per-test coverage info.,"build,test"
UGI should not try to renew non-renewable kerberos tickets,"When UserGroupInformation logs in a user from the ticket cache, it spawns a thread which renews the TGT ticket before it expires (HADOOP-6656). This happens even for non-renewable tickets, which results in error messages being printed to the console.    We should only try to renew the ticket if it has the ""Renewable"" flag.",security
Herriot daemon clients should vend statistics,The HDFS web user interface serves useful information through dfshealth.jsp and dfsnodelist.jsp.    The Herriot interface to Hadoop cluster daemons would benefit from the addition of some way to channel metics information.  ,test
Add JNI support for secure IO operations,"In support of MAPREDUCE-2096, we need to add some JNI functionality. In particular, we need the ability to use fstat() on an open file stream, and to use open() with O_EXCL, O_NOFOLLOW, and without O_CREAT.","io,native,security"
System test framework needs to black list unresponsive cluster nodes after a timeout ,"Sometimes one or more nodes in a cluster deployed for system testing purposes might become unresponsive (hw failure, Hadoop daemon crashes, etc.). In the current implementation, Herriot will be trying to connect to such a node(s) forever or until a timeout will occur. Instead, an unresponsive node should be places into a blacklist and the framework has to move on.    A cluster should be declared unusable if NN or JT are placed on the blacklist, or if a certain percentage of DNs (TTs) were blacklisted. ",test
Add abstraction layer to isolate cluster deployment mechanisms,"Certain types of system tests might require to perform a fresh deployment of a test cluster (e.g. upgrade tests, and similar).  This can be achieved by having an external way of deploying clusters and then running the tests. However, this won't work if re-deployment is needed in a middle of such test execution. In this case, Herriot needs to be able to explicitly call a deployment mechanism to carry on the process.    However, there are many possible ways of implementing cluster deployment and Herriot couldn't possibly be aware about all of them nor should be able to satisfy all their different interfaces. Thus an abstract interface should isolate plug-gable concrete implementations.  ",test
Provide a way to run system tests outside of source tree and Ant build environment,"In the current implementation the only way to execute Herriot (system) tests is to run certain Ant build target. As the result every test run causes full recompilation and re-weaving of AOP code. It takes about 2 minutes for 0.20 code and 1m20s for 0.21+    However, on multiple runs of the same test the waste of time might be significant. It'd be very helpful to be able to just run the tests our of test jar file or something without being force to recompile the code again and again.",build
"RPC ""relogin"" behavior should not be limited to global ""login user""","HADOOP-6706 added an automatic relogin and retry to the RPC connection mechanism in order to avoid ""Request is a replay"" errors when multiple daemons try to use the same Kerberos principal. It only does so, however, if the connecting user is the ""login user"". This is problematic for applications which do not use the global ""login user"" functionality but still need to take advantage of the retries.","ipc,security"
NPE from SequenceFile::Writer.CompressionCodecOption,The deprecated HADOOP-6856 constructors can create a compressed writers with a null-wrapped {{CompressionCodecOption}},io
SequenceFile.Reader should distinguish between Network IOE and Parsing IOE,"The SequenceFile.Reader api should give the user an easy way to distinguish between a Network/Low-level IOE and a Parsing IOE.  The use case appeared recently in the HBase project:    Originally, if a RegionServer got an IOE from HDFS while opening a region file, it would abort the open and let the HMaster reassign the region.  The assumption being that this is a network failure that will likely disappear at a later time or different partition of the network.  However, if HBase gets parsing exceptions, we want to log the problem and continue opening the region anyways, because parsing is an idempotent problem and retries won't fix this issue.    Although this problem was found in HBase, it seems to be a generic problem of being able to more easily identify idempotent vs transient errors.",io
Use JUnit Rule to optionally fail test cases that run more than 10 seconds,"Using JUnit Rules annotations we can fail tests cases that take longer than 10 seconds (for instance) to run.  This provides a regression check against test cases taking longer than they had previously due to unintended code changes, as well as provides a membership criteria for unit tests versus integration tests in HDFS and MR.",test
Add support for reading multiple hadoop delegation token files,"It would be nice if there were a way to specify multiple delegation token files via the HADOOP_TOKEN_FILE_LOCATION environment variable and the ""mapreduce.job.credentials.binary"" configuration value. I suggest a colon-separated list of paths, each of which is read as a separate delegation token file.",security
Move TestSequenceFile from MapReduce to Common,"{{TestSequenceFile}} escaped from Common and ended up in the MapReduce tree, causing several regressions in {{SequenceFile}} to go unnoticed",io
Update website for recent subproject departures,A number of subprojects have left Hadoop yet the website's not been fully updated to reflect that.,documentation
Broken link on cluster setup page of docs,The link on http://hadoop.apache.org/common/docs/current/cluster_setup.html#Configuring+the+Hadoop+Daemons to core-default.xml is presently:    {quote}  http://hadoop.apache.org/common/docs/current/common-default.html  {quote}    but it should be:    {quote}  http://hadoop.apache.org/common/docs/current/core-default.html  {quote},documentation
Allow wildcards to be used in ProxyUsers configurations,"There are some cases where the full tightness of the ProxyUsers configuration is not required or available -- for example, not all users of oozie may share a common ""oozie-users"" group, and the operators would prefer to allow oozie on a given host to act proxy for any user. We should allow the operator to specify a wildcard for hosts or groups in the proxyuser configurations.",security
Allow CodecFactory to return a codec object given a codec' class name,"CodecFactory specify the list of codec that are supported by Hadoop. However, it returns a codec only by a file's name. I would like to make getCodec method to alternatively take a codec's class name.    This is required by  HDFS-1435, where  1. it allows an HDFS admin to configure which codec to use to save an image.   2. It stores the codec class name in its on-disk image instead of a file's suffix.    When saving and reading an image, I'd like to get an codec from CodecFactory by its class name. ",io
'hadoop' script should set LANG or LC_COLLATE explicitly for classpath order,"The 'hadoop' script builds the classpath in pieces, including the following bit for the bulk of it:    {noformat}  # add libs to CLASSPATH  for f in $HADOOP_HOME/lib/*.jar; do    CLASSPATH=${CLASSPATH}:$f;  done  {noformat}    The ordering of ""*.jar"", i.e., the collation order, depends on either LANG or LC_COLLATE on Linux systems.  In the absence of either one, the script will default to whatever the user's environment specifies; for Red Hat, the default is ""en_US"", which is a case-insensitive (and punctuation-insensitive?) ordering.  If LANG is set to ""C"" instead, the ordering changes to the ASCII/UTF-8 byte ordering.    The key issue here is that $HADOOP_HOME/lib contains both upper- and lowercase jar names (e.g., ""SimonTool.jar"" and ""commons-logging-1.1.1.jar"", to pick a completely random pair), which will have an inverted order depending on which setting is used.    'hadoop' should explicitly set LANG and/or LC_COLLATE to whatever setting it's implicitly assuming.",scripts
Add alternative search-provider to site,"Use search-hadoop.com service to make available search in sources, MLs, wiki, etc.  This was initially proposed on user mailing list. The search service was already added in site's skin (common for all Hadoop related projects) before so this issue is about enabling it for Common.",documentation
Allow configuration changes without restarting configured nodes,"Currently, changing the configuration on a node (e.g., the name node) requires that we restart the node. We propose a change that would allow us to make configuration changes without restarting. Nodes that support configuration changes at run time should implement the following interface:    interface ChangeableConfigured extends Configured {     void changeConfiguration(Configuration newConf) throws ConfigurationChangeException;  }    The contract of changeConfiguration is as follows:  The node will compare newConf to the existing configuration. For each configuration property that is set to a different value than in the current configuration, the node will either adjust its behaviour to conform to the new configuration or throw a ConfigurationChangeException if this change is not possible at run time. If a configuration property is set in the current configuration but is unset in newConf, the node should use its default value for this property. After a successful invocation of changeConfiguration, the behaviour of the configured node should be indistinguishable from the behaviour of a node that was configured with newConf at creation.    It should be easy to change existing nodes to implement this interface. We can start by throwing the exception for all changes and then gradually start supporting more and more changes at run time. (We might even consider replacing Configured with ChangeableConfigured entirely, but I think the proposal above afford greater flexibility).       ",conf
Fix hadoop patch testing using jira_cli tool,None,build
Update test-patch.sh to remove callback to Hudson master,None,test
hadoop fs -getmerge does not work using codebase from trunk.,"Running the codebase from trunk, the hadoop fs -getmerge command does not work.  As implemented in prior versions (i.e. 0.20.2), I could run hadoop fs -getmerge pointed at a directory containing multiple files.  It would merge all files into a single file on the local file system.  Running the same command using the codebase from trunk, it looks like nothing happens.  ",fs
update the hudson-test-patch target to work with the latest test-patch script.,The hudson-test-patch target has to be updated to work with the current test-patch.sh script. Since the callback login in the test-patch.sh is removed. by hadoop-7005,build
Enable test-patch.sh to have a configured number of acceptable findbugs and javadoc warnings,test-patch.sh should be able to accept a properties file containing an acceptable number of findbugs and javadoc warnings.,test
MD5Hash provides a public factory method that creates an instance of MessageDigest,MD5Hash has a private way of creating a MessageDigest object that's thread local. I'd like to have such a method which is public so that checksuming fsimage (HDFS-903) could use it.,io
Typo in FileSystem.java,"For the Javadoc for getLocal method, ""Get the local file syste"" should be ""Get the local file system."".",fs
IPC.Client may have some connections for each given host/port,"ipc.Client class holds connection pool, that connection would be reused for a given host/port.  and WritableRpcEngine holds Client pool, that client would be reused for a given SocketFactory.  so, RPC.getProxy method returns proxy object, that only hold one connection or socket connected to a host/port, if or not, we create many proxy objects.    if large requests are sent to RPC Server through proxy, the request would be wait long time to be sent.    i think, for each host/port, ipc.Client would hold many connections.",ipc
Generalize CLITest structure and interfaces to facilitate upstream adoption (e.g. for web testing),There's at least one use case where TestCLI infrastructure is helpful for testing projects outside of core Hadoop (e.g. Owl web testing). In order to make this acceptance easier for upstream project TestCLI needs to be refactored.,test
Optimize locking codepath in LocalDirAllocator.getLocalPathToRead() and reduce creating many Path objects,"LocalDirAllocator.getLocalPathToRead() is called from TaskTracker to retrieve temporary map output location. This method is synchronized and amount of time spent on this method directly affects the performance of TaskTracker's ability to serve faster.  This JIRA is created to reduce the additional fat in getLocalPathToRead() and reduce creating the number of Path obejcts which are expensive.    1. getLocalPathToRead() internally calls RawLocalFileSystem.exists() in order to check the existence of a local file. RawLocalFileSystem.exists() internally creates a FileStatus object which needs to be populated with lastModified, length, directory details etc. This might not be necessary for just checking  file existence.    2. Creating a Path object out of a string requires fair amount of processing. It might be worth to store ""localDirsPath"" in LocalDirAllocator instead of storing ""localDirs"". This would avoid the number of times Path() object is created in LocalDirAllocator.    Any other codepath using LocalDirAllocator would also benefit from the above 2 optimizations.    The attached patch addresses the above 2 issues.",fs
ipc.Client.Connection.receiveResponse throws ClassNotFoundException when large requests are sent to RPC.Server,"i use solr with katta integration, pls see https://issues.apache.org/jira/browse/SOLR-1395  when i use:  {noformat}  ab -n 100 -c 10 http://localhost:8080/solr/select?q=solr&...  {noformat}    i found solr throws NPE, so, i added some code like below:  {code:title=ipc.Client.Connection|borderStyle=solid}  public void receiveResponse() {        ...        try {             ...             ...        } catch (IOException e) {          markClosed(e);        } catch (RunTimeException t) {            LOG.error(""Unexpected error reading responses on connection "", t);            throw t;        }      }  {code}    then, i catch the ClassNotFoundException:  {noformat}  java.lang.RuntimeException: readObject can't find class om-SL510?(response `?$name...    org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:185)    org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)    org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:511)  Caused by: java.lang.ClassNotFoundException: om-SL510?(response `?$name...    java.lang.Class.forName0(Native Method)    java.lang.Class.forName(Class.java:247)    org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:762)    org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:183)  {noformat}",ipc
Refactor build targets to enable faster cross project dev cycles.,The current build always generates fault injection artifacts and pushes them to Maven. Most developers have no need for these artifacts and no users need them. ,build
"establish a ""Powered by Hadoop"" logo","We should agree on a Powered By Hadoop logo, as suggested in:    http://www.apache.org/foundation/marks/pmcs#poweredby  ",documentation
MapWritable NullPointerException,We have encountered a NullPointerException when we use MapWritable with custom Writable objects.    The root cause is the counter newClasses in AbstractMapWritable is allowed to get out of sync with the id to class mapping tables when addToMap(Class) is called.  We have a patch to AbstractMapWritable.addToMap(Class) that handles the case when newClasses gets out of sync with the id to class mapping tables and adds a serialization optimization to minimize the number of class names to write out per MapWritable object.,io
Create a test method for adding file systems during tests.,It allows a (mocked) filesystem object to be added to cache for testing purposes. This is used by HDFS-1187.,test
Update docs on task-controller permissions after MAPREDUCE-2103,"MAPREDUCE-2103 corrects some permissions checks in the task-controller. For whatever reason, the task controller is documented in the common project, so we need to update docs here.",documentation
Adding new target to build.xml to run tests without compiling,"While testing Apache Harmony Select (lightweight version of Harmony) with Hadoop Common we had to first build with Harmony and then test using Harmony Select using the test-core target. This was done in an effort to investigate any issues with Harmony Select in running common. However, the test-core target also compiles the classes which we are unable to do with Harmony Select. A new target is proposed that only runs the tests without compiling them.","build,test"
ant eclipse does not include requisite ant.jar in the classpath,"RccTask imports classes from ant.jar    Importing the project into Eclipse results in these classes not being found, because ant.jar is not in the classpath.    Note that this patch requires that ANT_HOME be set, but this is consistent with the documentation as per    http://wiki.apache.org/hadoop/EclipseEnvironment    ",build
hadoop pom file are missing couple of common deps.,as part of hadoop metrics fix commons-configuration and commons-math are added as dependencies to hadoop. But the ivy/hadoop-core-pom-template.xml file is not update to reflect the changes. ,build
FsShell does not properly check permissions of files in a directory when doing rmr,"In POSIX file semantics, the ability to remove an entry a file is determined by whether the user has write permissions on the directory containing the file.  However, to delete recursively (rm -r) the user must have write permissions in all directories being removed.  Thus if you have a directory structure like /a/b/c and a user has write permissions on a but not on b, then he is not allowed to do 'rm -r b'.  This is because he does not have permissions to remove c, so the rm of b fails, even though he has permission to remove b.    However, 'hadoop fs -rmr b' removes both b and c in this case.  It should instead fail and return an error message saying the user does not have permission to remove c.  'hadoop fs -rmr c' correctly fails.",fs
Assert type constraints in the FileStatus constructor,"A FileStatus may represent a file, directory or symlink.  This is indicated using the isdir and symlink members, let's add an assert that validates the contstraints on these members (eg a directory may not have the symlink member set).  We could also verify this by having more than one constructor but we don't statically know the type of the file status when we create it.",fs
getUserToGroupsMappingService is not thread safe,"test-patch on trunk reports the following findbugs warning:    {noformat}  Incorrect lazy initialization of static field org.apache.hadoop.security.Groups.GROUPS in org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Configuration)  Bug type LI_LAZY_INIT_STATIC (click for details)   In class org.apache.hadoop.security.Groups  In method org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Configuration)  On field org.apache.hadoop.security.Groups.GROUPS  At Groups.java:[lines 138-142]  {noformat}    The relevant code:    {noformat}    public static Groups getUserToGroupsMappingService(Configuration conf) {      if(GROUPS == null) {        if(LOG.isDebugEnabled()) {          LOG.debug("" Creating new Groups object"");        }        GROUPS = new Groups(conf);      }      return GROUPS;    }  {noformat}    This is problematic if getUserToGroupsMappingService can accessed simultaneously by multiple threads.    ",security
"Add TestPath tests to cover dot, dot dot, and slash normalization","Add tests for the current path normalization for dot, dot dot, and slash in TestPath (from HDFS-836).",fs
Document incompatible API changes between releases,We can use JDiff to generate a list of incompatible changes for each release. See https://issues.apache.org/jira/browse/HADOOP-6668?focusedCommentId=12860017&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12860017,documentation
spellcheck for configuration,"Hadoop does fairly limited correctness checks of its configuration. I propose a ""configuration spellcheck"" that can automatically catch errors, and particularly can catch cases where users mis-type the name of an option.    The system works as follows:    - Use program analysis to extract the set of options supported by each Hadoop version, annotated when possible with their types into a 'dictionary file'.  - Distribute these extracted sets, per version.  - A script that reads a dictionary file, reads the Hadoop config from a specified directory, and reports deviations. In particular, the system can report when an option is set that Hadoop will never read or when an invalid value is specified.",conf
saveVersion script includes an additional \r while running whoami under windows,"I built common under windows occasionally, and found it failed because the 'user' in build/src/o/a/h/package-info.java is ""myhostmyname^M"".    It seems because the whoami of windows give a string with '\n\r' rather than '\n' only. thus I add an additional tr for it to eliminate the problem.    Since only windows would generate '\n\r' output, I think it won't harm to any other platform.",build
hadoop shell producing unexpected output to stdout,"Using ""hadoop fs -dus foobar/*"" I found unexpected output to stdout as the first line prints some debugging information about kerberos tokens.  This is breaking scripts which used to work with 0.20.x.    Retrieving token from: https://1.2.3.4:50470/getDelegationToken  hftp://blah.yahoo.net:50070/foo/bang     685256  hftp://blah.yahoo.net:50070/foo/bar      685256  hftp://blah.yahoo.net:50070/foo/baz      685256  [...]    Which appears to be coming from this line     ./hadoop-0.21.0/hdfs/src/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java:151:      System.out.println(""Retrieving token from: "" +     I also see a few other cases here which should be changed to use a logging service, changed to System.err, or removed:    grep -n -2 System.out ./hadoop-0.21.0/hdfs/src/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java    90-            91-          out.flush();  92:          System.out.println(""Succesfully wrote token of size "" +   93-              out.size() + "" bytes to ""+ args[0]);  94-        } catch (IOException ioe) {  95:          System.out.println(""Exception encountered:\n"" +  96-              StringUtils.stringifyException(ioe));  97-        } finally {  --  149-      150-    try {  151:      System.out.println(""Retrieving token from: "" +   152-          nnAddr + DelegationTokenServlet.PATH_SPEC + ugiPostfix);  153-      URL remoteURL = new URL(nnAddr + DelegationTokenServlet.PATH_SPEC + ugiPostfix);  --  161-      ts.write(file);  162-      file.flush();  163:      System.out.println(""Successfully wrote token of "" + file.size()   164-          + "" bytes  to "" + filename);  165-    } catch (Exception e) {    ","security,util"
Broken link to GenericOptionsParser in Javadoc,"In both Tool's and ToolRunner's Javadoc, there is the following link  pointing to GenericOptionsParser:    <a href=""{@docRoot}/org/apache/hadoop/util/GenericOptionsParser.html#GenericOptions"">    Since @InterfaceAudience.Private is specified for GenericOptionsParser, GenericOptionsParser does not appear in Javadoc. So the links to it are broken.     And in both Tool's and ToolRunner's Javadoc, there is ""@see GenericOptionsParser"" which shows as plain text instead of links because GenericOptionsParser is missed in Javadoc.",util
Update test-patch.sh to include failed test names and move test-patch.properties,"As Jakob suggested, it would be helpful if the Jira messages left by Hudson included the list of failed tests.    Also, test-patch.properties must be moved out of the src/test/bin dir because it is project specific and the entire bin dir is svn included into other projects (hdfs and mapreduce)",test
TestDU fails on systems with local file systems with extended attributes,"The test reports that the file takes an extra 4k on disk:    {noformat}  Testcase: testDU took 5.74 sec          FAILED  expected:<32768> but was:<36864>  junit.framework.AssertionFailedError: expected:<32768> but was:<36864>          at org.apache.hadoop.fs.TestDU.testDU(TestDU.java:79)  {noformat}    This is because du reports 32k for the file and 4k because the file system it lives on uses extended attributes.    {noformat}  common-branch-0.20 $ dd if=/dev/zero of=data bs=4096 count=8  8+0 records in  8+0 records out  32768 bytes (33 kB) copied, 9.6e-05 seconds, 341 MB/s  common-branch-0.20 $ du data  36 data  common-branch-0.20 $ du --apparent-size data  32 data  {noformat}    We should modify the test to allow for some extra on-disk slack. The on-disk usage could also be smaller if the file data is all zeros or compression is enabled. The test currently handles the former by writing random data, we're punting on the latter.",fs
1 Findbugs warning on trunk and branch-0.22,There is 1 findbugs warnings on trunk. See attached html file. This must be fixed or filtered out to get back to 0 warnings. The OK_FINDBUGS_WARNINGS property in src/test/test-patch.properties should also be set to 0 in the patch that fixes this issue.,security
RPC client gets stuck,"One of the dfs clients in our cluster stuck on waiting for a RPC result. However the IPC connection thread who is receiving the RPC result died on OOM error:  INFO >> Exception in thread ""IPC Client (47) connection to XX from root"" java.lang.OutOfMemoryError: Java heap space  INFO >> at java.util.Arrays.copyOfRange(Arrays.java:3209)  INFO >> at java.lang.String.<init>(String.java:216)  INFO >> at java.lang.StringBuffer.toString(StringBuffer.java:585)  INFO >> at java.net.URI.toString(URI.java:1907)  INFO >> at java.net.URI.<init>(URI.java:732)  INFO >> at org.apache.hadoop.fs.Path.initialize(Path.java:137)  INFO >> at org.apache.hadoop.fs.Path.<init>(Path.java:126)  INFO >> at org.apache.hadoop.fs.FileStatus.readFields(FileStatus.java:206)  INFO >> at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:237)  INFO >> at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:171)  INFO >> at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:219)  INFO >> at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)  INFO >> at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:531)  INFO >> at org.apache.hadoop.ipc.Client$Connection.run(Client.java:466)",ipc
Wrong description of Block-Compressed SequenceFile Format in SequenceFile's javadoc,"Here is the following description for Block-Compressed SequenceFile Format in SequenceFile's javadoc:     * <li>   * Record <i>Block</i>   *   <ul>   *     <li>Compressed key-lengths block-size</li>   *     <li>Compressed key-lengths block</li>   *     <li>Compressed keys block-size</li>   *     <li>Compressed keys block</li>   *     <li>Compressed value-lengths block-size</li>   *     <li>Compressed value-lengths block</li>   *     <li>Compressed values block-size</li>   *     <li>Compressed values block</li>   *   </ul>   * </li>   * <li>   * A sync-marker every few <code>100</code> bytes or so.   * </li>    This description misses ""Uncompressed record number in the block"". And ""A sync-marker every few <code>100</code> bytes or so"" is not the case for Block-Compressed SequenceFile Format. Correct description should be:     * <li>   * Record <i>Block</i>   *   <ul>   *     <li>Uncompressed record number in the block</li>   *     <li>Compressed key-lengths block-size</li>   *     <li>Compressed key-lengths block</li>   *     <li>Compressed keys block-size</li>   *     <li>Compressed keys block</li>   *     <li>Compressed value-lengths block-size</li>   *     <li>Compressed value-lengths block</li>   *     <li>Compressed values block-size</li>   *     <li>Compressed values block</li>   *   </ul>   * </li>   * <li>   * A sync-marker every block.   * </li>    ",io
TestReconfiguration should be junit v4,TestReconfiguration should be a junit v4 unit test. I'll also add some messages to the assertions.,conf
proxyuser host/group config properties don't work if user name as DOT in it,"If the user contains a DOT, ""foo.bar"", proxy user configuration fails to be read properly and it does not kick in.",security
hadoop-daemons.sh doesnt work in cygwin,"  if you run Hadoop in Pseudo-Distributed Mode in Windows under Cygwin, your Datanode will fail to start on execution of ""start-dsf.sh"". In the log there will be a ClassNotFoundException for org.apache.hadoop.util.PlatformName.    The problem seems to be in 'hadoop-daemons.sh' in that line:  exec ""$bin/slaves.sh"" --config $HADOOP_CONF_DIR cd ""$HADOOP_HOME"" \; ""$bin/hadoop-daemon.sh"" --config $HADOOP_CONF_DIR ""$@""    if you change it to:  ""$bin/slaves.sh"" --config $HADOOP_CONF_DIR cd ""$HADOOP_HOME"" ; ""$bin/hadoop-daemon.sh"" --config $HADOOP_CONF_DIR ""$@""    the problems on cygwin are gone!    ",scripts
misspelling of threshold in conf/log4j.properties,"In ""log4j.threshhold=ALL"", threshhold is a misspelling of threshold. So ""log4j.threshhold=ALL"" has no effect on the control of log4j logging.    ",conf
wrong FSNamesystem Audit logging setting in conf/log4j.properties,"""log4j.logger.org.apache.hadoop.fs.FSNamesystem.audit=WARN"" should be ""log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=WARN"".",conf
Update of commons logging libraries causes EventCounter to count logging events incorrectly,"Hadoop 0.20.2 uses commons logging 1.0.4. EventCounter works correctly with this version of commons logging. Hadoop 0.21.0 uses commons logging 1.1.1 which causes EventCounter to count logging events incorrectly. I have verified it with Hadoop 0.21.0. After start-up of hadoop, I checked jvmmetrics.log after several minutes. In every metrics record, ""logError=0, logFatal=0, logInfo=3, logWarn=0"" was shown. The following text is an example.    jvm.metrics: hostName=jingguolin, processName=DataNode, sessionId=, gcCount=3, gcTimeMillis=31, logError=0, logFatal=0, logInfo=3, logWarn=0, maxMemoryM=888.9375, memHeapCommittedM=38.0625, memHeapUsedM=3.6539612, memNonHeapCommittedM=18.25, memNonHeapUsedM=11.335686, threadsBlocked=0, threadsNew=0, threadsRunnable=8, threadsTerminated=0, threadsTimedWaiting=6, threadsWaiting=6    Then I stopped hadoop and replaced commons logging 1.1.1 with 1.0.4. After the re-start of hadoop, a lot of logging events showed up in jvmmetrics.log.    I have checked the source code of Log4JLogger for both 1.0.4 (http://svn.apache.org/viewvc/commons/proper/logging/tags/LOGGING_1_0_4/src/java/org/apache/commons/logging/impl/Log4JLogger.java?view=markup) and 1.1.1 (http://svn.apache.org/viewvc/commons/proper/logging/tags/commons-logging-1.1.1/src/java/org/apache/commons/logging/impl/Log4JLogger.java?view=markup). For 1.0.4, Level instances such as Level.INFO are used to construct LoggingEvent. But for 1.1.1, Priority instances such as Priority.INFO are used to construct LoggingEvent. So 1.1.1 version's event.getLevel() always returns Priority instances. EventCounter append method's ""=="" check always fails between a Level instance and a Priority instance. For ""logInfo=3"" metrics records produced by commons logging 1.1.1., I think that these 3 logging events are produced by log4j code directly instead of through commons logging API. The following code is EventCounter's append method.        public void append(LoggingEvent event) {          Level level = event.getLevel();          if (level == Level.INFO) {              counts.incr(INFO);          }          else if (level == Level.WARN) {              counts.incr(WARN);          }          else if (level == Level.ERROR) {              counts.incr(ERROR);          }          else if (level == Level.FATAL) {              counts.incr(FATAL);          }        }",metrics
IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,"{noformat}          throw new IOException( ""Premeture EOF from inputStream"");  {noformat}",util
"Remove ""unused"" warning in native code","The file:     src/native/src/org_apache_hadoop.h    declares the static function ""do_dlsym"" in the header as non-inline. Files including the header (e.g. for the THROW macro) receive a ""defined but unused"" warning during compilation.    This function should either a) be inlined or b) use GCC ""unused"" attribute.",native
A more elegant FileSystem#listCorruptFileBlocks API,"I'd like to change the newly added listCorruptFileBlocks signature to be:  {code}  /**  * Get all files with corrupt blocks under the given path  */  RemoteIterator<Path> listCorruptFileBlocks(Path src) throws IOException;  {code}  This new API does not expose ""cookie"" to user although underlying implementation may still need to invoke multiple RPCs to get the whole list.",fs
unprecise javadoc for CompressionCodec,"In CompressionCodec.java, there is the following code:      /**     * Create a stream decompressor that will read from the given input stream.     *      * @param in the stream to read compressed bytes from     * @return a stream to read uncompressed bytes from     * @throws IOException     */    CompressionInputStream createInputStream(InputStream in) throws IOException;    ""stream decompressor"" should be ""{@link CompressionInputStream}"".",io
Single Node Setup example command line error,"Example codes from: http://hadoop.apache.org/common/docs/r0.21.0/single_node_setup.html    Section  ""Standalone Operation""  the line ""bin/hadoop jar hadoop-*-examples.jar grep input output 'dfs[a-z.]+' "" can not run correctly, because the example jar in the directory is ""hadoop-mapred-examples-0.21.0.jar"", include a revision number.  the command should be "" bin/hadoop jar hadoop*examples*.jar grep input output 'dfs[a-z.]+'  ""    And in section ""Pseudo-Distributed Operation / Execution"", also refered a command using hadoop*exapmples.jar, it also should be changed.    ",documentation
ChecksumFileSystem should implement flush(),ChecksumFileSystem does not support flush() since it just uses OutputStream.flush() but OutputStream.flush() do nothing.,fs
Replace forrest with supported framework,"It's time to burn down the forrest.  Apache forrest, which is used to generate the documentation for all three subprojects, has not had a release in several years (0.8, the version we use was released April 18, 2007), and requires JDK5, which was EOL'ed in November 2009.  Since it doesn't seem likely Forrest will be developed any more, and JDK5 is not shipped with recent OSX versions, or included by default in most linux distros, we should look to find a new documentation system and convert the current docs to it.",documentation
JAAS configuration should delegate unknown application names to pre-existing configuration,"As reported here: https://issues.cloudera.org/browse/DISTRO-66 it is impossible to use secured Hadoop inside an application that relies on other JAAS configurations. This is because the static initializer of UserGroupInformation replaces the JAAS configuration, but we don't delegate unknown applications up to whatever Configuration was installed previously. The delegation technique seems to be used by JBoss's XMLLoginConfigImpl for example.",security
test-patch.sh has bad ps arg,None,build
Remove java5 dependencies from build,As the first short-term step let's remove JDK5 dependency from build(s),build
Make the 'docs' target work with JDK6,It's possible to make Forrest work with JDK6 by disabling sitemap and stylesheet validation  in the forrest.properties file. See FOR-984 and PIG-1508 for more details.,documentation
Splittable Gzip,Files compressed with the gzip codec are not splittable due to the nature of the codec.  This limits the options you have scaling out when reading large gzipped input files.    Given the fact that gunzipping a 1GiB file usually takes only 2 minutes I figured that for some use cases wasting some resources may result in a shorter job time under certain conditions.  So reading the entire input file from the start for each split (wasting resources!!) may lead to additional scalability.  ,io
Use Java's ServiceLoader to add default resources to Configuration,Currently each class with a main() method (in HDFS and MapReduce) calls Configuration.addDefaultResource() to add the names of resource files to load (e.g. see DataNode and NameNode which both add hdfs-default.xml and hdfs-site.xml). We could reduce the code duplication by allowing the use of [java.util.ServiceLoader|http://download.oracle.com/javase/6/docs/api/java/util/ServiceLoader.html] to do the initialization.,conf
Refactor IPC proxy creation to use Builder pattern,"The various RPC proxy creation methods are getting really messy, with 7 or 8 arguments which get tracked through a long chain of method calls. I'm interested in refactoring these to use a Builder pattern to make it easier to add new settings for RPC without a messy patch.",ipc
QA bot doesn't post to JIRA if build times out,"Currently there are some HDFS tests that time out and deadlock trying to exit. When this happens, the QA bot builds time out and doesn't post anything to the issue. See for example:  https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/55/console    We could either fix the QA bot somehow, or preferably, is there a JUnit runner we can use that will kill -9 the forked JVM if it deadlocks trying to exit?",test
Configuration.writeXML should not hold lock while outputting,Common side of HDFS-1542,conf
Allow SecureIO to be disabled for developer workstations,"In testing with secure Hadoop, the new requirement for native code is annoying on platforms like OSX where the native code can be tricky to get compiled and working. We should allow developers to disable this aspect of security by setting a special flag.","native,security"
Remove java5 dependencies from site's build,Java5 dependency needs to be removed from http://svn.apache.org/repos/asf/hadoop/site/build.xml,build
"Documentation uses urls with ""current""","The mapred and hdfs default links on the following page are broken. Looking into the cause, the links always point to the current docs so old generated docs have the wrong pointers (they should point to docs/<release> instead of docs/current). This is because cluster_setup.xml uses site.xml which defines references to the docs using http://hadoop.apache.org/core/docs/current.    http://hadoop.apache.org/common/docs/r0.20.2/cluster_setup.html    ",documentation
Retrying socket connection failure times can be made as configurable,"Retrying socket connection failure times are hard coded as 45 and it is giving the retryring message for 45 times as below.     2011-01-04 15:14:30,700 INFO ipc.Client (Client.java:handleConnectionFailure(487)) - Retrying connect to server: /10.18.52.124:50020. Already tried 1 time(s).    This can be made as configurable and also we can keep the default value as 45. If the user wants to decrease/increase,  they can add this configurable property otherwise it can continue with the default value.    common\src\java\org\apache\hadoop\ipc\Client.java:  -----------------------------------------------------------------------    private synchronized void setupConnection() throws IOException {        short ioFailures = 0;        short timeoutFailures = 0;        while (true) {          try {            this.socket = socketFactory.createSocket();            this.socket.setTcpNoDelay(tcpNoDelay);            // connection time out is 20s            NetUtils.connect(this.socket, remoteId.getAddress(), 20000);            if (rpcTimeout > 0) {              pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval            }            this.socket.setSoTimeout(pingInterval);            return;          } catch (SocketTimeoutException toe) {            /*             * The max number of retries is 45, which amounts to 20s*45 = 15             * minutes retries.             */            handleConnectionFailure(timeoutFailures++, 45, toe);          } catch (IOException ie) {            handleConnectionFailure(ioFailures++, maxRetries, ie);          }        }      }",conf
SequenceFile.createWriter ignores FileSystem parameter,The SequenceFile.createWriter methods that take a FileSystem ignore this parameter after HADOOP-6856. This is causing some MR tests to fail and is a breaking change when users pass unqualified paths to these calls.,io
Fix link resolution logic in hadoop-config.sh,"The link resolution logic in bin/hadoop-config.sh fails when when executed via a symlink, from the root directory.  We can replace this logic with cd -P and pwd -P, which should be portable across Linux, Solaris, BSD, and OSX. ",scripts
Possible resource leaks in hadoop core code,"It is always a good practice to close the IO streams in a finally block..     For example, look at the following piece of code in the Writer class of BloomMapFile     {code:title=BloomMapFile .java|borderStyle=solid}      public synchronized void close() throws IOException {        super.close();        DataOutputStream out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);        bloomFilter.write(out);        out.flush();        out.close();      }  {code}     If an exception occurs during fs.create or on any other line,  out.close() will not be executed..    The following can reduce the scope of resorce leaks..  {code:title=BloomMapFile .java|borderStyle=solid}      public synchronized void close() throws IOException {        super.close();        DataOutputStream out = null;        try{            out = fs.create(new Path(dir, BLOOM_FILE_NAME), true);            bloomFilter.write(out);            out.flush();        }finally{    IOUtils.closeStream(out);      }  {code}     ","fs/s3,io"
reloginFromKeytab() should happen even if TGT can't be found,"HADOOP-6965 introduced a getTGT() method and prevents reloginFromKeytab() from happening when TGT is not found. This results in the RPC layer not being able to refresh TGT after TGT expires. The reason is RPC layer only does relogin when the expired TGT is used and an exception is thrown. However, when that happens, the expired TGT will be removed from Subject. Therefore, getTGT() will return null and relogin will not be performed. We observed, for example, JT will not be able to re-connect to NN after TGT expires.",security
Consolidate startup shell scripts,"- Today we struggled ~1-2 hours to find out that the Cloudera init scripts have a bug in the shell code (bug filled by my colleague)  - Many projects that started at hadoop copied and adapted the shell startup code. So there's a lot of code duplication between hbase, zookeeper, hadoop and maybe others  - There already are some open issues regarding to shell code  - The shell code isn't properly tested (automaticaly) and will first probably fail at customer side    Would it make sense to build a shell library of most often used functionality for startup scripts? Is there already such a library somewhere?  This issue should collect thoughts in this area.",scripts
Servlets should default to text/plain,In trunk the servlets like /stacks and /metrics are returning text/html content-type instead of text/plain. Security wise it's much safer to default to text/plain and require servlets to explicitly set the content-type to text/html when required.,io
hadoop.css got lost during project split,"hadoop.css no longer exists in common or HDFS, so the web UIs look pretty ugly. The HTML still refers to this file, it's just gone.",build
java.library.path missing basedir,"My Hadoop installation is  having trouble loading the native code library. It appears from the log below that java.library.path is missing the basedir in its path. The libraries are built, and present in the directory shown below (relative to hadoop-common directory). Instead of seeing:     /build/native/Linux-amd64-64/lib    I would expect to see:     /path/to/hadoop-common/build/native/Linux-amd64-64/lib    I'm working in branch-0.22.    2011-01-10 17:09:27,695 DEBUG org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path  2011-01-10 17:09:27,695 DEBUG org.apache.hadoop.util.NativeCodeLoader: java.library.path=/build/native/Linux-amd64-64/lib  2011-01-10 17:09:27,695 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","build,native"
tasktracker property not set in conf/hadoop-env.sh,"For all cluster components, except TaskTracker the OPTS environment variable is set like this in hadoop-env.sh:  export HADOOP_<COMPONENT>_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_<COMPONENT>_OPTS""    The provided patch fixes this.",conf
distcp can copy blocks in parallel,"The minimum unit of work for a distcp task is a file. We have files that are greater than 1 TB with a block size of  1 GB. If we use distcp to copy these files, the tasks either take a long long long time or finally fails. A better way for distcp would be to copy all the source blocks in parallel, and then stich the blocks back to files at the destination via the HDFS Concat API (HDFS-222)",tools/distcp
Make RawLocalFileSystem more friendly to sub-classing,"This patch does 3 things that makes sub-classing RawLocalFileSystem easier.    First, it adds a constructor that allows a sub-class to avoid calling getInitialWorkingDirectory(). This is important because if a sub-class has an initially null uri (prior to initialize() being called), then getInitialWorkingDirectory() will cause a NullPointerException when it tries to work with the FS's uri.    Second, allows subclasses to modify the working directory.    The third thing this patch does is change loadPermissions to not pass the URI object to the File(URI) constructor, but rather pass the string representation of the path. This is important because URI's that are not using the ""file"" scheme will cause the File(URI) constructor to throw an exception.",fs
Build broken by HADOOP-6811,"The commit of HADOOP-6811 removed the ec2 contrib but didn't update build.xml, which references some of these files from the packaging targets. So, the hudson build is currently broken.","build,contrib/cloud"
UserGroupInformation.getCurrentUser() fails when called from non-Hadoop JAAS context,"If a Hadoop client is run from inside a container like Tomcat, and the current AccessControlContext has a Subject associated with it that is not created by Hadoop, then UserGroupInformation.getCurrentUser() will throw NoSuchElementException, since it assumes that any Subject will have a hadoop User principal.",security
"Remove ""fs.ramfs.impl"" field from core-deafult.xml","""fs.ramfs.impl"" used to be configuration parameter for InMemoryFileSystem, which was deprecated in 0.18 (HADOOP-3501) and removed in 0.21 (HADOOP-4648). Configuration should have been cleaned up at the time.",conf
Remove unnecessary DNS reverse lookups from RPC layer,"RPC connection authorization needs to verify client's Kerberos principal name matches what specified for the protocol. For service clients like DN's, their Kerberos principal names can be specified in the form of  ""datanode/_HOST@DOMAIN.COM"". To get the expected  client principal name, the server needs to substitute ""_HOST"" with the client's fully qualified domain name, which requires a reverse DNS lookup from client IP address. However, for connections from clients whose principal name are either unspecified or specified not using the ""_HOST"" convention, the substitution is not required and the reverse DNS lookup should be avoided. Currently the reverse DNS lookup is done for all clients, which could slow services like NN down, when local named cache is not available.","ipc,security"
[IPC] Improvement of lock mechanism in Listener and Reader thread,"In many client cocurrent access, single thread Listener will become bottleneck. Many client can't be served, and get connection time out.  To improve Listener capacity, we make 2 modification.  1.  Tuning ipc.server.listen.queue.size to a larger value to avoid client retry.  2. In currently implement, Listener will call registerChannel(), and finishAdd() in Reader, which will request Reader synchronized lock. Listener will cost too many time in waiting for this lock.    We have made test,     ./bin/hadoop org.apache.hadoop.hdfs.NNThroughputBenchmark  -op create -threads 10000 -files 10000    case 1 : Currently   can not pass. and report   hadoop-rd101.jx.baidu.com/10.65.25.166:59310. Already tried 0 time(s).    case 2 : tuning back log to 10240  average cost : 1285.72 ms    case 3 : tuning back log to 10240 , and improve lock mechanism in patch  average cost :  941.32 ms      performance in average cost will improve 26%    ",ipc
Re-organize hadoop subversion layout,As discussed on general@ at http://tinyurl.com/4q6lhxm,build
Ability to retrigger Hudson QA bot without re-uploading patch,"Currently to retrigger the test-patch bot you have to upload a new patch. This doesn't work well for a couple cases:  - sometimes a build fails due to issues on a build machine (eg all tests timed out) and you want to give it another go  - sometimes a test sits in ""patch available"" state for a couple weeks and it would be good to re-verify the patch before committing    Committers can retrigger from the Hudson UI, but we should have something that any contributor can use. I would suggest a magic string to put in a comment (eg ""@hudson.build"") which the script would watch for and trigger on",test
Implement chmod with JNI,MapReduce is currently using a race-prone workaround to approximate chmod() because forking chmod is too expensive. This race is causing build failures (and probably task failures too). We should implement chmod in the NativeIO library so we can have good performance (ie not fork) and still not be prone to races.,"io,native"
Several TFile tests failing when native libraries are present,"When running tests with native libraries present, TestTFileByteArrays and TestTFileJClassComparatorByteArrays fail on trunk. They don't seem to fail in 0.20 with native libraries.",io
Issue a warning when GenericOptionsParser libjars are not on local filesystem,In GenericOptionsParser#getLibJars() any jars that are not local filesystem paths are silently ignored. We should issue a warning for users.,"conf,filecache"
FsShell should dump all exceptions at DEBUG level,"Most of the FsShell commands catch exceptions and then just print out an error like ""foo: "" + e.getLocalizedMessage(). This is fine when the exception is ""user-facing"" (eg permissions errors) but in the case of a user hitting a bug you get a useless error message with no stack trace. For example, something ""chmod: null"" in the case of a NullPointerException bug.    It would help debug these cases for users and developers if we also logged the exception with full trace at DEBUG level.",fs
raise contrib junit test jvm memory size to 512mb,The streaming tests are failing with out of memory. Raise the memory limit to 512mb.,test
Move secondary namenode checkpoint configs from core-default.xml to hdfs-default.xml,"  The following configs are in core-default.xml, but are really read by the Secondary Namenode. These should be moved to hdfs-default.xml for consistency.  <property>  <name>fs.checkpoint.dir</name>  <value>${hadoop.tmp.dir}/dfs/namesecondary</value>  <description>Determines where on the local filesystem the DFS secondary  name node should store the temporary images to merge.  If this is a comma-delimited list of directories then the image is  replicated in all of the directories for redundancy.  </description>  </property>    <property>  <name>fs.checkpoint.edits.dir</name>  <value>${fs.checkpoint.dir}</value>  <description>Determines where on the local filesystem the DFS secondary  name node should store the temporary edits to merge.  If this is a comma-delimited list of directoires then teh edits is  replicated in all of the directoires for redundancy.  Default value is same as fs.checkpoint.dir  </description>  </property>    <property>  <name>fs.checkpoint.period</name>  <value>3600</value>  <description>The number of seconds between two periodic checkpoints.  </description>  </property>    <property>  <name>fs.checkpoint.size</name>  <value>67108864</value>  <description>The size of the current edit log (in bytes) that triggers  a periodic checkpoint even if the fs.checkpoint.period hasn't expired.  </description>  </property>    ",conf
add Kerberos HTTP SPNEGO authentication support to Hadoop JT/NN/DN/TT web-consoles,Currently the JT/NN/DN/TT web-consoles don't support any form of authentication.    Hadoop RPC API already supports Kerberos authentication.    Kerberos enables single sign-on.    Popular browsers (Firefox and Internet Explorer) have support for Kerberos HTTP SPNEGO.    Adding support for Kerberos HTTP SPNEGO to Hadoop web consoles would provide a unified authentication mechanism and single sign-on for Hadoop web UI and Hadoop RPC.,security
200 new Findbugs warnings,ant test-patch on an empty patch over hdfs trunk.  {noformat}       [exec] -1 overall.         [exec]        [exec]     +1 @author.  The patch does not contain any @author tags.       [exec]        [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.       [exec]                         Please justify why no new tests are needed for this patch.       [exec]                         Also please list what manual steps were performed to verify this patch.       [exec]        [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.       [exec]        [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.       [exec]        [exec]     -1 findbugs.  The patch appears to introduce 200 new Findbugs (version 1.3.9) warnings.       [exec]        [exec]     -1 release audit.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).       [exec]        [exec]     +1 system test framework.  The patch passed system test framework compile.  {noformat},test
Exceptions while serializing IPC call response are not handled well,"We had a situation where for some reason the serialization of an RPC call's response was throwing OOME. When this happens, the exception is not caught, and the call never gets a response - the client just hangs. Additionally, the OOME propagated all the way to the top of the IPC handler and caused the handler. Plus, the Handler upon exit only logged to stdout and not to the log4j logs.",ipc
Timed out shell commands leak Timer threads,"When a shell command times out, the TimerThread used to cause the timeout is leaked.",util
Bug in login error handling in org.apache.hadoop.fs.ftp.FTPFileSystem,"I was playing around with PMD, just to see what kind of messages it gives on hadoop.  I noticed a message about ""Dead code"" in org.apache.hadoop.fs.ftp.FTPFileSystem    Starting at about line 80:       String userAndPassword = uri.getUserInfo();     if (userAndPassword == null) {       userAndPassword = (conf.get(""fs.ftp.user."" + host, null) + "":"" + conf           .get(""fs.ftp.password."" + host, null));       if (userAndPassword == null) {         throw new IOException(""Invalid user/passsword specified"");       }     }    The last ""if"" block is the dead code as the string will always contain at least the text "":"" or ""null:null""    This means that the error handling fails to work as intended.  It will probably fail a bit later when really trying to login with a wrong uid/password.    P.S. Fix the silly typo ""passsword"" in the exception message too.",fs
Provide overwrite option (-overwrite/-f) in put and copyFromLocal command line options,"FileSystem has the API         *public void copyFromLocalFile(boolean delSrc, boolean overwrite, Path[] srcs, Path dst)*                                    This API provides overwrite option. But the mapping command line doesn't have this option. To maintain the consistency and better usage  the command line option also can support the overwrite option like to put the files forcefully. ( put [-f] <srcpath> <dstPath>) and also for copyFromLocal command line option.  ",fs
rmr command is not displaying any error message when a path contains wildcard characters and does not exist.,When we give invalid directory path then it will show error message on the console. But if we provide the wildcard expression in invalid directory path then it will not show any error message even there is no pattern match for that path.    linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin # ./hdfs dfs -rmr /test/test  rmr: cannot remove /test/test: No such file or directory.    *linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin # ./hdfs dfs -rmr /test* *  *linux-9j5v:/home/hadoop-hdfs-0.22.0-SNAPSHOT/bin #*  ,fs
"set() and toString Methods of the org.apache.hadoop.io.Text class does not include the root exception, in the wrapping RuntimeException."," In below code snippets, we can include e, instead of e.toString(), so that caller can get complete trace.    1)      /** Set to contain the contents of a string.     */    public void set(String string) {      try {        ByteBuffer bb = encode(string, true);        bytes = bb.array();        length = bb.limit();      }catch(CharacterCodingException e) {        throw new RuntimeException(""Should not have happened "",e.toString());      }    }   2)     public String toString() {      try {        return decode(bytes, 0, length);      } catch (CharacterCodingException e) {        throw new RuntimeException(""Should not have happened "",e.toString());      }    }  ",io
"CLONE to COMMON - HDFS-1445 Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file","The fix for HDFS-1445 ""Batch the calls in DataStorage to FileUtil.createHardLink(), so we call it once per directory instead of once per file"" requires coordinated change in COMMON and HDFS.  This is the COMMON portion, submitted here under a separate bug to activate the automated testing.    Warning: this patch to COMMON, by itself, will break HDFS.  It requires coordinated commit of the HDFS portion of the patch in HDFS-1445.",util
configure files that are generated as part of the released tarball need to have executable bit set,Currently the configure files that are packaged in a tarball are -rw-rw-r--,build
Commit 949660 broke FileUtil.copyMerge,Looking at http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.21/src/java/org/apache/hadoop/fs/FileUtil.java?r1=949659&r2=949660&  it seems this commit broke FileUtil.copyMerge by omitting NOT operator.  copyMerge only makes sense if srcDir is a directory.    Should be      if (!srcFS.getFileStatus(srcDir).isDirectory())         return false;  ,fs
Allow appending to existing SequenceFiles,None,io
hadoop fs -put and -copyFromLocal do not support globs in the source path,"I'd like to be able to use Hadoop globbing with the FsShell -put command, but it doesn't work:    {noformat}  $ ls  file1 file2  $ hadoop fs -put '*' .  put: File * does not exist.  {noformat}    This has probably gone unnoticed because your shell usually handles it, but a) I'd like to be able to call 'hadoop fs' programatically without a shell, b) it doesn't work in Pig or Grunt, where there is no shell helping you, and c) Hadoop globbing differs from shell globbing and it would be nice to be able to use Hadoop globbing consistently throughout Hadoop.",fs
Hive Hadoop20SShims depends on removed HadoopArchives,Compiling (Hive 0.6 + HIVE-1264) or Hive-trunk against 0.20.100 fails compilation.  /hive/shims/src/0.20S/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java depends on o.a.h.tools.HadoopArchives which was removed from 0.20.100.    HadoopArchives in turn depends on src/core/o.a.h/fs/HarFileSystem.java which was also removed from 0.20.100.,fs
setnetgrent in native code is not portable,"HADOOP-6864 uses the setnetgrent function in a way which is not compatible with BSD APIs, where the call returns void rather than int. This prevents the native libs from building on OSX, for example.",native
Topology script should have some kind of timeout,"We've seen an issue where a custom topology script was hanging for some external reason. This caused the NN to block during startup without any kind of clear indication what was going on.    We should consider adding a timeout for the topology script execution, or at least a timer which triggers WARN or ERROR level messages if it takes longer than a few seconds.",net
Create a module system for adding extensions to Hadoop,Currently adding extensions to Hadoop is difficult. I propose adding the concept of modules that can add jars and libraries to the system.,util
MapWritable violates contract of Map interface for equals() and hashCode(),"o.a.h.io.MapWritable implements the java.util.Map interface, however it does not define an implementation of the equals() or hashCode() methods; instead the default implementations in java.lang.Object are used.    This violates the contract of the Map interface which defines different behaviour for equals() and hashCode() than Object does. More information here: http://download.oracle.com/javase/6/docs/api/java/util/Map.html#equals(java.lang.Object)    The practical consequence is that MapWritables containing equal entries cannot be compared properly. We were bitten by this when trying to write an MRUnit test for a Mapper that outputs MapWritables; the MRUnit driver cannot test the equality of the expected and actual MapWritable objects.",io
Should set MALLOC_ARENA_MAX in hadoop-config.sh,"New versions of glibc present in RHEL6 include a new arena allocator design. In several clusters we've seen this new allocator cause huge amounts of virtual memory to be used, since when multiple threads perform allocations, they each get their own memory arena. On a 64-bit system, these arenas are 64M mappings, and the maximum number of arenas is 8 times the number of cores. We've observed a DN process using 14GB of vmem for only 300M of resident set. This causes all kinds of nasty issues for obvious reasons.    Setting MALLOC_ARENA_MAX to a low number will restrict the number of memory arenas and bound the virtual memory, with no noticeable downside in performance - we've been recommending MALLOC_ARENA_MAX=4. We should set this in hadoop-env.sh to avoid this issue as RHEL6 becomes more and more common.",scripts
Create build hosts configurations to prevent degradation of patch validation and snapshot build environment,"Build hosts are being re-jumped, in need for configuration updates, etc. It is hard to maintain the same configuration across a 10+ hosts. A specialized service such as Puppet can be used to maintain build machines software and configurations at a desired level.    Such configs should be checked into SCM system along with the of sources code.",build
"Reduce RPC packet size for homogeneous arrays, such as the array responses to listStatus() and getBlockLocations()","While commenting on HADOOP-6949, which proposes a big improvement in the RPC wire format for arrays of primitives, Konstantin Shvachko said:  ""Can/should we extend this to arrays of non-primitive types? This should benefit return types for calls like listStatus() and getBlockLocations() on a large directory.""    The improvement for primitive arrays is based on not type-labeling every element in the array, so the array in question must be strictly homogenous; it cannot have subtypes of the assignable type.  For instance, it could not be applied to heartbeat responses of DatanodeCommand[], whose array elements carry subtypes of DatanodeCommand, each of which must be type-labeled independently.  However, as Konstantin points out, it could really help lengthy response arrays for things like listStatus() and getBlockLocations().    I will attach a prototype implementation to this Jira, for discussion.  However, since it can't be automatically applied to all arrays passing through RPC, I'll just providing the wrapper type.  By using it, a caller is asserting that the array is strictly homogeneous in the above sense.  ",io
Federation: update Balancer documentation,Update Balancer documentation for the new balancing policy and CLI.,documentation
RPC server should log the client hostname when read exception happened,This makes find mismatched clients easier,ipc
Configurable initial buffersize for getGroupDetails(),"{code:title=trunk/src/native/src/org/apache/hadoop/security/getGroup.c|borderStyle=solid}  int getGroupDetails(gid_t group, char **grpBuf) {    struct group * grp = NULL;    size_t currBufferSize = sysconf(_SC_GETGR_R_SIZE_MAX);    if (currBufferSize < 1024) {      currBufferSize = 1024;    }    *grpBuf = NULL;     char *buf = (char*)malloc(sizeof(char) * currBufferSize);      if (!buf) {      return ENOMEM;    }    int error;    for (;;) {      error = getgrgid_r(group, (struct group*)buf,                         buf + sizeof(struct group),                         currBufferSize - sizeof(struct group), &grp);      if(error != ERANGE) {         break;      }      free(buf);      currBufferSize *= 2;      buf = malloc(sizeof(char) * currBufferSize);      if(!buf) {        return ENOMEM;      }  ...  {code}    For large groups, this implies at least 2 queries for the group (number of queries = math.ceil(math.log(response_size/1024, 2)))    In the case of a large cluster with central user/group databases (exposed via LDAP etc), this leads to unnecessary load on the central services. This can be alleviated to a large extent by changing the initial buffer size to a configurable parameter    --","native,security"
Remove unnecessary oro package from dependency management section,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",build
FsShell: call srcFs.listStatus(src) twice,in file ./src/java/org/apache/hadoop/fs/FsShell.java line 555  call method twice:  1. for init variable  2. for getting data    ,fs
"""java.net.SocketTimeoutException: 60000 millis timeout"" happens a lot","We don't have retries for the case where the secure SASL connection is getting created from the tasks. There is retry  for TCP connections, but once the TCP connection has been set up, communication at the RPC layer (and that includes  SASL handshake) happens without retries. So for example, a client's ""read"" can timeout.",ipc
"listLocatedStatus(path, filter) is not redefined in FilterFs","listLocatedStatus(path, filter) is not redefined in FilterFs. So if a job client uses a FilterFs to talk to NameNode, it does not trigger the bulk location optimization.",fs
Support UGI in FileContext API,The FileContext API needs to support UGI.,security
Support UGI in FileContext API,The FileContext API needs to support UGI.,security
SecureIO should not check owner on non-secure clusters that have no native support,"The SecureIOUtils.openForRead function currently uses a racy stat/open combo if security is disabled and the native libraries are not available. This ends up shelling out to ""ls -ld"" which is very very slow. We've seen this cause significant performance regressions on clusters that match this profile.    Since the racy permissions check doesn't buy us any security anyway, we should just fall back to a normal ""open"" without any stat() at all, if we can't use the native support to do it efficiently.","io,security"
"null is displayed in the console,if the src path is invalid while doing copyToLocal operation from commandLine","When we perform copyToLocal operations from commandLine and if src Path is invalid     srcFS.globStatus(srcpath) will return null. So, when we find the length of resulted value, it will *throw NullPointerException*.     Since we are handling generic exception , it will display null as the message.  ",fs
FileSystem should have an option to control the .crc file creations at Local.,"When we copy the files from DFS to local, it is creating the .crc files in local filesystem for the verification of checksum. When user dont want to do any check sum verifications, this files will not be useful.     Command line already has an option ignoreCrc, to control this.  So, we should have the similar option on FileSystem also..... like fs.ignoreCrc().   This should set the setVerifyChecksum to false and also should select the non CheckSumFileSystem as local fs.",fs
Add isEnabled() to Trash,"The moveToTrash method returns false in a number of cases.  It's not possible to discern if false means an error occurred. In particular, it's not possible to know if the trash is disabled vs. an error occurred.",fs
CodecPool should report which compressor it is using,Certain native compression libraries are overly verbose causing confusion while reading the task logs.  Let's actually say which compressor we got when we report it in the task logs.,native
Improve HDFS startup scripts,"Startup scripts should start namenodes, secondary namenodes and datanodes on hosts returned by getConfig (new feature of HDFS Federation).  This jira is Hdfs counterpart of HDFS-1703.  This patch makes changes to hadoop-config.sh and slaves.sh where they belong to COMMON project.  And HDFS-1703 modifies scripts, hdfs, start-hdfs.sh and stop-hdfs.sh where they go to HDFS project.",scripts
Improve CommandFormat,"CommandFormat currently takes an array and offset for parsing and returns a list of arguments.  It'd be much more convenient to have it process a list too.  It would also be nice to differentiate between too few and too many args instead of the generic ""Illegal number of arguments"".  Finally, CommandFormat is completely devoid of tests.",fs
use ThreadLocal for HardLink command buffers,"Referring to HADOOP-7133, Hairong observed that HardLink could be improved if it used ThreadLocal to implement thread-safe re-usable buffers.  We agreed to open it in a new bug so the original patch could proceed.",util
"remove stub implementation of HardLink from FileUtil, after fixing any remaining dependencies","This is the third part of a refactoring of HardLink:  1. HADOOP-7133 moved HardLink from an inner class of FileUtil to a stand-alone class, but left a stub class in FileUtil to prevent breaking clients in HDFS.  2. HDFS-1445 changes the clients in HDFS to point at the new implementation.  3. After HDFS-1445 is committed, this ticket is to clean up the code by removing the backward-compatibility stub.",util
Remove deprecated local.cache.size from core-default.xml,MAPREDUCE-2379 documents the new name of this parameter (mapreduce.tasktracker.cache.local.size) in mapred-default.xml where it belongs.,"documentation,filecache"
 BackUpNameNode is using 100% CPU and not accepting any requests.,"In our environment, after 3 days long run Backup NameNode is using 100% CPU and not accepting any calls.   *Thread dump*  ""IPC Server Responder"" daemon prio=10 tid=0x00007f86c41c6800 nid=0x3b2a runnable [0x00007f86ce579000]  java.lang.Thread.State: RUNNABLE  at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)  at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)  at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)  at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)    locked <0x00007f86d67e2a20> (a sun.nio.ch.Util$1)   locked <0x00007f86d67e2a08> (a java.util.Collections$UnmodifiableSet)   locked <0x00007f86d67e26a8> (a sun.nio.ch.EPollSelectorImpl)  at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)  at org.apache.hadoop.ipc.Server$Responder.run(Server.java:501)   Looks like we are running into similar issue like this Jetty one. http://jira.codehaus.org/browse/JETTY-937    ",ipc
Socket Leak in org.apache.hadoop.metrics.ganglia.GangliaContext,Init method is creating DatagramSocket. But this is not closed any where.   ,metrics
Add ability to enable 'debug' property in JAAS configuration,"Occasionally users have run into weird ""Unable to login"" messages. Unfortunately, JAAS obscures the underlying exception message in many cases because it thinks leaking the exception might be insecure in itself. Enabling the ""debug"" option in the JAAS configuration gets it to dump the underlying issue and makes troubleshooting this kind of issue easier.",security
Put metrics v1 back into the hadoop-20-security branch,The metrics v1 code was removed on the branch. It should be put back and deprecated.,metrics
fs -stat docs aren't updated to reflect the format features,"The html docs of the 'fs -stat' command (that is found listed in the File System Shell Guide), does not seem to have the formatting abilities of -stat explained (along with the options).    Like 'fs -help', the docs must also reflect the latest available features.    I shall attach a doc-fix patch shortly.    If anyone has other discrepancies to point out in the web version of the guide, please do so :)",documentation
Help message is wrong for touchz command.,Help message for touchz command is   -touchz <path>: Write a timestamp in yyyy-MM-dd HH:mm:ss format                  in a file at <path>. An error is returned if the file exists with non-zero length.     Actually current DFS behaviour is that it will not write any time stamp in created file. Just it is creating zero size file.    So better to change the help message to give exact meaning.  ,fs
Potential Resource leak in IOUtils.java,"{code:title=IOUtils.java|borderStyle=solid}    try {        copyBytes(in, out, buffSize);      } finally {        if(close) {          out.close();          in.close();        }      }  {code}   In the above code if any exception throws from the out.close() statement, in.close() statement will not execute and the input stream will not be closed.  ",io
RawLocalFileSystem.rename() should not try to do copy,"RawLocalFileSystem.rename() try to copy file if fails to call rename of java File. It's really confusing to do copy in a rename interface. For example rename(/a/b/c, /e/f/g) will invoke the copy if /e/f does not exist.",fs
Support for non-standard ssh port for slaves,"I was trying to add a slave that ran sshd in a non-standard port (eg. 2222 in stead of 22), when I noticed that there was no way to support another port through the configuration for a single node.    Supporting a different port for all the slaves is possible through the HADOOP_SSH_OPTS variable, but not for a single slave.",util
Crash due to reuse of checksum files,"copyFromLocalFile crashes if a cheksum file exists on the local filesystem and the checksum does not match the file content. This will for example crash ""hadoop -fs put ./foo ./foo"" with a non-descriptive error.    It is therefore not possible to do:    1. copyToLocalFile(hdfsFile, localFile)       // creates checksum file  2. modify localFile  3. copyFromLocalFile(localFile, hdfsFile)  // uses old checksum    Solution: do not reuse checksum files, or add a parameter  to copyFromLocalFile that specifies that checksum files should not be reused.",fs
Add Cassandra to list of Hadoop-related projects at Apache,"Cassandra provides native read/write support for java Hadoop jobs (with locality), Streaming, Pig, and Hive.",documentation
Folder Paths,"I tried to execute: ./bin/hadoop namenode -format in hadoop-0.21.0 inside a folder with path: ~/Desktop/my test/hadoop-0.21.0/  But i kept receiving the following error:    Exception in thread ""main"" java.lang.NoClassDefFoundError: test/hadoop-0/21/0/bin////logs  Caused by: java.lang.ClassNotFoundException: me.hadoop-0.21.0.bin....logs    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:307)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:248)    Tried to change the language just to test what would happen. So I got a same error with a folder name with spaces:    Exception in thread ""main"" java.lang.NoClassDefFoundError: ??????  Caused by: java.lang.ClassNotFoundException: ??????    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:307)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:248)    (You can notice the ???, might this be a java issue?)    I was able to run it only in folder names that didn't include spaces. Both english and greek.    I tried to trace this issue and I found that the problem is in the HADOOP_HOME/hdfs script. On the last line where: exec ""$JAVA"" $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS ""$@"" tries to be executed, variable HADOOP_OPTS has white spaces which produce the above errors.     A solution to this problem could be to include double quotes to each path when they are collected and call exec command with eval at the front.    I found a workaround to this issue by adding:    tmp=`echo ${HADOOP_OPTS// /\\\ }`  eval exec ""$JAVA"" $JAVA_HEAP_MAX $(echo ${tmp//\\ \-/\ \-}) "" "" $CLASS ""$@""    This code basically it added backslashes in front of each space in a path.  ",build
Chown command is not working from FSShell.,chown command is not invoking the setOwner on FileSystem.,fs
Security uses proprietary Sun APIs,"The security code uses the KrbException, Credentials, and PrincipalName classes from sun.security.krb5 and Krb5Util from sun.security.jgss.krb5. These may disappear in future Java releases. Also Hadoop does not compile using jdks that do not support them, for example with the following IBM JDK.    {noformat}  $ /home/eli/toolchain/java-x86_64-60/bin/java -version  java version ""1.6.0""  Java(TM) SE Runtime Environment (build pxa6460sr9fp1-20110208_03(SR9 FP1))  IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Linux amd64-64 jvmxa6460sr9-20110203_74623 (JIT enabled, AOT enabled)  J9VM - 20110203_074623  JIT  - r9_20101028_17488ifx3  GC   - 20101027_AA)  JCL  - 20110203_01  {noformat}  ",security
Reuse connection,"One of my recent RPC change introduced a regression. It makes the first RPC to server, getProtocolSignature, and following RPCs not sharing the same connection. If all clients are short lived, this regression would double the number of connections in the cluster.    The cause of the regression is that getProtocolSingature uses VersionProtocol to create a Connection object, and the following RPCs uses its own protocol name like ClientProtocol. Since protocol name is part of Connection object hashcode, this forces the RPC client to create a new Connection object, therefore forcing to create a new TCP/IP connection.",ipc
RPC clients must connect over a network interface corresponding to the host name in the client's kerberos principal key,"HADOOP-7104 introduced a change where RPC server matches client's hostname with the hostname specified in the client's Kerberos principal name. RPC client binds the socket to a random local address, which might not match the hostname specified in the principal name. This results authorization failure of the client at the server.",security
HADOOP-7202 broke TestDFSShell in HDFS,"The commit of HADOOP-7202 now requires that classes that extend {{FsCommand}} implement the {{void run(PathData)}} method. The {{Count}} class was changed to extend {{FsCommand}}, but renamed the {{run}} method and did not provide a replacement.",test
FileUtil.copyMerge implementation error,if (srcFS.getFileStatus(srcDir).isDirectory())     return false;    should be     if (!srcFS.getFileStatus(srcDir).isDirectory())       return false;,fs
"When Namenode network is unplugged, DFSClient operations waits for ever","When NN/DN is shutdown gracefully, the DFSClient operations which are waiting for a response from NN/DN, will throw exception & come out quickly    But when the NN/DN network is unplugged, the DFSClient operations which are waiting for a response from NN/DN, waits for ever.  ",ipc
Inconsistent behavior when passing a path with special characters as literals to some FsShell commands,"The following work:  hadoop dfs --put test^ing /tmp               hadoop dfs --ls /tmp                           The following do not:  hadoop dfs --ls /tmp/test^ing                hadoop dfs --get /tmp/test^ing test^ing     The first fails with ""ls: Cannot access /tmp/test^ing: No such file or directory.""   The second fails with ""get: null"".     It is possible to put a file with some special characters, such as ^ using the hadoop shell.  But once put one cannot ls, cat, or get the file due to the way some commands deal with file globbing.  Harsh J suggested on the mailing list that perhaps a flag that would turn off globbing could be implemented. Perhaps something like single quoting the file path on the command line to disable globbing would work as well.       As an example in the source for 0.20.2 the ^ character in particular wasn't escaped in in the output pattern in FileSystem.java @line 1050 in setRegex(String filePattern).:    ...      } else if (pCh == '[' && setOpen == 0) {            setOpen++;            hasPattern = true;          } else if (pCh == '^' && setOpen > 0) {          } else if (pCh == '-' && setOpen > 0) {            // Character set range            setRange = true;  ...    After looking in trunk, it seems to have been dealt with in later versions (refactored into GlobPattern.java)    ...   case '^': // ^ inside [...] can be unescaped            if (setOpen == 0) {              regex.append(BACKSLASH);            }            break;   case '!': //  ...      but even after pushing that back in 0.20.2 and testing it appears to resolve the issue for commands like ls, but not for get.  So perhaps there is more to be done for other commands?    ",fs
FileContext createFlag combinations during create are not clearly defined,"During file creation with FileContext, the expected behavior is not clearly defined for combination of createFlag EnumSet.  ",fs
DataNode.setNewStorageID pulls entropy from /dev/random,"DataNode.setNewStorageID uses SecureRandom.getInstance(""SHA1PRNG"") which always pulls fresh entropy.      It wouldn't be so bad if this were only the 120 bits needed by sha1, but the default impl of SecureRandom actually uses a BufferedInputStream around /dev/random and pulls 1024 bits of entropy for this one call.    If you are on a system without much entropy coming in, this call can block and block others.    Can we just change this to use ""new SecureRandom().nextInt(Integer.MAX_VALUE)"" instead?  ",fs
CLONE - DataNode.setNewStorageID pulls entropy from /dev/random,"DataNode.setNewStorageID uses SecureRandom.getInstance(""SHA1PRNG"") which always pulls fresh entropy.      It wouldn't be so bad if this were only the 120 bits needed by sha1, but the default impl of SecureRandom actually uses a BufferedInputStream around /dev/random and pulls 1024 bits of entropy for this one call.    If you are on a system without much entropy coming in, this call can block and block others.    Can we just change this to use ""new SecureRandom().nextInt(Integer.MAX_VALUE)"" instead?  ",fs
Remove protocol version check at proxy creation in Hadoop RPC.,"Currently when a proxy is created for a protocol, there is a round trip of messages to check the protocol version. The protocol version is not checked in any subsequent rpc which could be a problem if the server restarts with a new protocol version. This issue and also the additional round-trip at proxy creation can be avoided if we add the protocol version in every rpc, and server checks the protocol version for every call.",ipc
jar names are not compatible with 0.20.2,"The jars in 203 are named differently vs. Apache Hadoop 0.20.2.  I understand this was done to make the Maven people less cranky.  However, this breaks compatibility especially for streaming users. We need to make sure we have a release note or something significant so that users aren't taken by surprise.",documentation
Absolute path to kinit in auto-renewal thread,"In the auto-renewal thread for Kerberos credentials in {{UserGroupInformation}}, the path to {{kinit}} is defaulted to {{/usr/kerberos/bin/kinit}}. This is the default path to {{kinit}} on RHEL/CentOS for MIT krb5, but not on Debian/Ubuntu (and perhaps others OSes.)",security
Move -fs usage tests from hdfs into common,"The -fs usage tests are in hdfs which causes an unnecessary synchronization of a common & hdfs bug when changing the text.  The usages have no ties to hdfs, so they should be moved into common.",test
Fix synopsis for -count,"The synopsis for the count command is wrong.  1) missing a space in ""-count[-q]""  2) missing ellipsis for multiple path args",util
Fix javadoc warnings,The javadoc is currently generating 31 warnings.,documentation
Refactor FsShell's ls,Need to refactor ls to conform to new FsCommand class.,fs
Hudson auto test for HDFS has started throwing javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list,"Hudson automated testing has started failing with one javadoc warning message, consisting of  javadoc: warning - Error fetching URL: http://java.sun.com/javase/6/docs/api/package-list    This may be due to Oracle's decommissioning of the sun.com domain.  If one tries to access it manually, it is redirected to   http://download.oracle.com/javase/6/docs/api/package-list    So it looks like a build script needs to be updated.","build,test"
FsShell tail doesn't handle globs,The tail command doesn't bother trying to expand it's arguments which is inconsistent with other commands.,fs
Refactor FsShell's mkdir,Need to refactor tail to conform to new FsCommand class.,fs
Refactor FsShell's touchz,Need to refactor touchz to conform to new FsCommand class.,fs
Refactor FsShell's cat & text,Need to refactor cat & text to conform to new FsCommand class.,fs
"The variables in the configuration files are not replaced with values, when the job is submitted by Job client","We have a case where we wanted to create the Job names dynamically at run time.Since JobConf is an extension for the Configuration object, we thought we can make use of the variable substitution concept in configuration like below    Job job = new Job(conf, ""${mapred.user.name}"" + ""-job"" + new Random().nextInt());   job.setJarByClass(WordCount.class);   job.setMapperClass(TokenizerMapper.class);   job.setCombinerClass(IntSumReducer.class);   job.setReducerClass(IntSumReducer.class);   job.setOutputKeyClass(Text.class);   job.setOutputValueClass(IntWritable.class);   FileInputFormat.addInputPath(job, new Path(otherArgs[0]));   FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));   job.submit();       We set the required run time variables(in this case mapred.user.name) before calling the job submit. But on the Job tracker side the variables are not replaced correctly.  ",conf
fix typo of command 'hadoop fs -help tail',"Fix the typo of command 'hadoop fs -help tail'.    $ hadoop fs -help tail  -tail [-f] <file>:  Show the last 1KB of the file.     The -f option shows apended data as the file grows.     The ""apended data"" should be ""appended data"".","fs,test"
Documentation change for updated configuration keys,Common counterpart of HDFS-671.,documentation
Have a way to automatically update Eclipse .classpath file when new libs are added to the classpath through Ivy for 0.20-* based sources,Backport HADOOP-6407 into 0.20 based source trees,build
Refactor FsShell's chmod/chown/chgrp,Need to refactor permissions commands to conform to new FsCommand class.,fs
Refactor FsShell's setrep,Need to refactor setrep to conform to new FsCommand class.,fs
Refactor FsShell's getmerge,Need to refactor getmerge to conform to new FsCommand class.,fs
JUnit shows up as a compile time dependency,None,"build,conf,test"
Programmatically start  processes with JMX port open ,We propose a programmatic way to start processes with JMX enabled.  This is the counter part of HDFS-1874.,scripts
contrib modules should include build.properties from parent.,Current build.properties in the hadoop root directory is not included by the contrib modules.,build
update the tlp site with the bylaws,Include the bylaws in the tlp site.,documentation
Disable IPV6 for junit tests,"IPV6 addresses not handles currently in the common library methods. IPV6 can return address as ""0:0:0:0:0:0:port"". Some utility methods such as NetUtils#createSocketAddress(), NetUtils#normalizeHostName(), NetUtils#getHostNameOfIp() to name a few, do not handle IPV6 address and expect address to be of format host:port.    Until IPV6 is formally supported, I propose disabling IPV6 for junit tests to avoid problems seen in HDFS-1891.",test
Bump avro version to at least 1.4.1,Needed by mapreduce 2.0 avro support. Maybe we could jump to Avro 1.5. There is incompatible API changes from 1.3x to 1.4x (Utf8 to CharSequence in user facing APIs) not sure about 1.5x though.,io
Keep track of relative paths,"As part of the effort to standardize the display of paths, the PathData tracks the exact string used to create a path.  When obtaining a directory's contents, the relative nature of the original path should be preserved.",fs
Deprecate metrics v1,None,metrics
Refactor FsShell's rm/rmr/expunge,Refactor to conform to the FsCommand class.,fs
FileContext.getLocalFSFileContext() behavior needs to be fixed w.r.t tokens,"FileContext.getLocalFSFileContext() instantiates a FileContext object upon the first call to it, and for all subsequent calls returns back that instance (a static localFsSingleton object). With security turned on, this causes some hard-to-debug situations when that fileContext is used for doing HDFS operations. This is because the UserGroupInformation is stored when a FileContext is instantiated. If the process in question wishes to use different UserGroupInformation objects for different file system operations (where the corresponding FileContext objects are obtained via calls to FileContext.getLocalFSFileContext()), it doesn't work.","fs,security"
S3 Native should allow customizable file meta-data (headers),"The S3 Native FileSystem currently writes all files with a set of default headers:     * Content-Type: binary/octet-stream   * Content-Length: <computed from object size>   * Content-MD5: <computed from object data>    This is a good start, however many applications would benefit from the ability to customize (for example) the Content-Type and Expires headers for the file. Ideally the implementation should be abstract enough to customize all of the available S3 headers and provide a facility for other FileSystems to specify optional file metadata.",fs/s3
Server$Listener.getAddress(..) throws NullPointerException,"In [build #469|https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/469//testReport/org.apache.hadoop.hdfs.server.datanode/TestFiDataTransferProtocol2/pipeline_Fi_29/],  {noformat}  2011-05-09 23:21:35,528 ERROR datanode.DataNode (DataXceiver.java:run(133)) - 127.0.0.1:57149:DataXceiver  java.lang.NullPointerException    org.apache.hadoop.ipc.Server$Listener.getAddress(Server.java:518)    org.apache.hadoop.ipc.Server.getListenerAddress(Server.java:1662)    org.apache.hadoop.hdfs.server.datanode.DataNode.getIpcPort(DataNode.java:1461)    org.apache.hadoop.hdfs.server.datanode.DataNode.getDatanodeId(DataNode.java:2747)    org.apache.hadoop.hdfs.server.datanode.BlockReceiverAspects.ajc$after$org_apache_hadoop_hdfs_server_datanode_BlockReceiverAspects$9$725950a6(BlockReceiverAspects.aj:226)    org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close(BlockReceiver.java:230)    org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:157)    org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:173)    org.apache.hadoop.hdfs.server.datanode.DataXceiver.opWriteBlock(DataXceiver.java:408)    org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opWriteBlock(DataTransferProtocol.java:414)    org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:360)    org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:131)    java.lang.Thread.run(Thread.java:662)  {noformat}",ipc
Standardize error messages,"The FsShell commands have no standard format for the same error message.  For instance, here is a snippet of the variations of just one of many error messages:    cmd: $path: No such file or directory  cmd: cannot stat `$path': No such file or directory  cmd: Can not find listing for $path  cmd: Cannot access $path: No such file or directory.  cmd: No such file or directory `$path'  cmd: File does not exist: $path  cmd: File $path does not exist  ... etc ...    These need to be common.",fs
Remove unnecessary security related info logs,"Two info logs are printed when connection to RPC server is established, is not necessary. On a production cluster, these log lines made up of close to 50% of lines in the namenode log. I propose changing them into debug logs.      ","ipc,security"
Shell doesn't give specific details about why it can't create a file,"It would be a lot more useful to print an error message indicating why that you can't create the file - e.g. can't create a directory with the name of the file you're trying to write to.    Also, the log file isn't much better. You'll see exceptions that look like this:  2011-04-14 12:29:37,757 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user ___  org.apache.hadoop.util.Shell$ExitCodeException: id: ___: No such user  at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)  at org.apache.hadoop.util.Shell.run(Shell.java:182)  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375) at org.apache.hadoop.util.Shell.execCommand(Shell.java:461)  at org.apache.hadoop.util.Shell.execCommand(Shell.java:444) at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:66)  at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:43)  at org.apache.hadoop.security.Groups.getGroups(Groups.java:79)  at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1022)  at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:50)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4920)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:4903)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:1878)  at org.apache.hadoop.hdfs.server.namenode.NameNode.getFileInfo(NameNode.java:795)  at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1416)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1412)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1410) ",fs
CLONE - IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,"Same fix as for HADOOP-7057 for the Hadoop security branch    {noformat}          throw new IOException( ""Premeture EOF from inputStream"");  {noformat}",util
Refactor FsShell's stat,Refactor to conform to the FsCommand class.,fs
Hadoop native builds fail on ARM due to -m32,"The native build fails on machine targets where gcc does not support -m32. This is any target other than x86, SPARC, RS/6000, or PowerPC, such as ARM.    $ ant -Dcompile.native=true  ...       [exec] make  all-am       [exec] make[1]: Entering directory  `/home/trobinson/dev/hadoop-common/build/native/Linux-arm-32'       [exec] /bin/bash ./libtool  --tag=CC   --mode=compile gcc  -DHAVE_CONFIG_H -I. -I/home/trobinson/dev/hadoop-common/src/native  -I/usr/lib/jvm/java-6-openjdk/include  -I/usr/lib/jvm/java-6-openjdk/include/linux  -I/home/trobinson/dev/hadoop-common/src/native/src  -Isrc/org/apache/hadoop/io/compress/zlib  -Isrc/org/apache/hadoop/security -Isrc/org/apache/hadoop/io/nativeio/  -g -Wall -fPIC -O2 -m32 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF  .deps/ZlibCompressor.Tpo -c -o ZlibCompressor.lo `test -f  'src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c' || echo  '/home/trobinson/dev/hadoop-common/src/native/'`src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c       [exec] libtool: compile:  gcc -DHAVE_CONFIG_H -I.  -I/home/trobinson/dev/hadoop-common/src/native  -I/usr/lib/jvm/java-6-openjdk/include  -I/usr/lib/jvm/java-6-openjdk/include/linux  -I/home/trobinson/dev/hadoop-common/src/native/src  -Isrc/org/apache/hadoop/io/compress/zlib  -Isrc/org/apache/hadoop/security -Isrc/org/apache/hadoop/io/nativeio/  -g -Wall -fPIC -O2 -m32 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF  .deps/ZlibCompressor.Tpo -c  /home/trobinson/dev/hadoop-common/src/native/src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c   -fPIC -DPIC -o .libs/ZlibCompressor.o       [exec] make[1]: Leaving directory  `/home/trobinson/dev/hadoop-common/build/native/Linux-arm-32'       [exec] cc1: error: unrecognized command line option ""-m32""       [exec] make[1]: *** [ZlibCompressor.lo] Error 1       [exec] make: *** [all] Error 2  ",native
Add Eclipse launch tasks for the 0.20-security branch,"This is to add the eclipse launchers from HADOOP-5911 to the 0.20 security branch.    Eclipse has a notion of ""run configuration"", which encapsulates what's needed to run or debug an application. I use this quite a bit to start various Hadoop daemons in debug mode, with breakpoints set, to inspect state and what not.    This is simply configuration, so no tests are provided. After running ""ant eclipse"" and refreshing your project, you should see entries in the Run Configurations and Debug Configurations for launching the various hadoop daemons from within eclipse. There's a template for testing a specific test, and also templates to run all the tests, the job tracker, and a task tracker. It's likely that some parameters need to be further tweaked to have the same behavior as ""ant test"", but for most tests, this works.    This also requires a small change to build.xml for the eclipse classpath.",build
Automatic Hadoop cluster deployment for build validation,It'd be great to have a way of automatically deploying Hadoop cluster to a set of machine once all components are successfully built (in the form or tar or whatever). Deployed cluster then can be used to run a set of validation jobs to make sure that produced artifacts are viable. ,build
update site to include the 0.20.203.0 release,Update site to include the 0.20.203.0 release.,documentation
getRemoteIp could return null in cases where the call is ongoing but the ip went away.,"getRemoteIp gets the ip from socket instead of the stored ip in Connection object. Thus calls to this function could return null when a client disconnected, but the rpc call is still ongoing...",ipc
Include 32-bit and 64-bit native libraries in Jenkins tarball builds,"The job at https://builds.apache.org/hudson/view/G-L/view/Hadoop/job/Hadoop-22-Build/ is building tarballs, but they do not currently include both 32-bit and 64-bit native libraries. We should update/duplicate hadoop-nighly/hudsonBuildHadoopRelease.sh to support post-split builds.",build
Trash and shell's rm does not work for viewfs,None,viewfs
Refactor FsShell's test,Need to refactor to conform to FsCommand subclass.,fs
Refactor FsShell's du/dus/df,Need to refactor to conform to FsCommand subclass.,fs
Configuration deprecation mechanism doesn't work properly for GenericOptionsParser/Tools,"For example, you can't use -D options on the ""hadoop fs"" command line in order to specify the deprecated names of configuration options. The issue is that the ordering is:  - JVM starts  - GenericOptionsParser creates a Configuration object and calls set() for each of the options specified on command line  - DistributedFileSystem or other class eventually instantiates HdfsConfiguration which adds the deprecations  - Some class calls conf.get(""new key"") and sees the default instead of the version set on the command line",conf
FsShell.DelayedExceptionThrowing generates QA errors,The inner class o.a.h.fs.FsShell.DelayedExceptionThrowing is causing the QA bot to fail all patches due to a findbugs issue (the inner class should be static).    It's an abstract class that is not currently used. It looks as though it's old code that simply needs removing.,fs
ivy: test conf should not extend common conf,"Otherwise, the same jars will appear in both {{build/ivy/lib/Hadoop-Common/common/}} and {{build/ivy/lib/Hadoop-Common/test/}}.",build
Unit test failure in TestUserGroupInformation.testGetServerSideGroups,"Testsuite: org.apache.hadoop.security.TestUserGroupInformation  Tests run: 14, Failures: 1, Errors: 0, Time elapsed: 0.278 sec  ------------- Standard Output ---------------  trobinson:users guest git  ------------- ---------------- ---------------    Testcase: testGetServerSideGroups took 0.051 sec         FAILED  expected:<g[ues]t> but was:<g[i]t>  junit.framework.AssertionFailedError: expected:<g[ues]t> but was:<g[i]t>         at org.apache.hadoop.security.TestUserGroupInformation.testGetServerSideGroups(TestUserGroupInformation.java:94)    It seems like the test is assuming that the groups returned by UserGroupInformation.getGroupNames() are in the same order as those returned by executing `id -Gn`. getGroupNames() only documents that the primary group is first, and `man id` doesn't document any ordering, so it seems like the test needs to be reworked to remove that assumption.",security
Metrics 2 TestSinkQueue is racy,"The TestSinkQueue is racy (Thread.yield is not enough to guarantee other intended thread getting run), though it's the first time (from HADOOP-7289) I saw it manifested here.",metrics
FileUtil uses wrong stat command for FreeBSD,"I get next exception when try to use append:    2011-05-16 17:07:54,648 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.112.0.207:50010, storageID=DS-1047171559-  10.112.0.207-50010-1302796304164, infoPort=50075, ipcPort=50020):DataXceiver  java.io.IOException: Failed to get link count on file /var/data/hdfs/data/current/finalized/subdir26/subdir17/subdir55/blk_-1266943884751786595:   message=null; error=stat: illegal option -- c; exit value=1          at org.apache.hadoop.fs.FileUtil.createIOException(FileUtil.java:709)          at org.apache.hadoop.fs.FileUtil.access$000(FileUtil.java:42)          at org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:682)          at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.unlinkBlock(ReplicaInfo.java:215)          at org.apache.hadoop.hdfs.server.datanode.FSDataset.append(FSDataset.java:1116)    It seems that FreeBSD is treated like UNIX and so calls 'stat -c%h', while FreeBSD is much more like Mac (since they have same BSD roots):    $ stat --help  stat: illegal option -- -  usage: stat [-FlLnqrsx] [-f format] [-t timefmt] [file ...]    $ stat -f%l a_file  1  ",fs
The FsPermission(FsPermission) constructor does not use the sticky bit,"The FsPermission(FsPermission) constructor copies u, g, o from the supplied FsPermission object but ignores the sticky bit.  ",fs
Error in the documentation regarding Checkpoint/Backup Node,On http://hadoop.apache.org/common/docs/r0.20.203.0/hdfs_user_guide.html#Checkpoint+Node: the command bin/hdfs namenode -checkpoint required to launch the backup/checkpoint node does not exist.  I have removed this from the docs.,documentation
Add test utility for writing multi-threaded tests,"A lot of our tests spawn off multiple threads in order to check various synchronization issues, etc. It's often tedious to write these kinds of tests because you have to manually propagate exceptions back to the main thread, etc.    In HBase we have developed a testing utility which makes writing these kinds of tests much easier. I'd like to copy that utility into Hadoop so we can use it here as well.",test
FSOutputSummer do not flushBuffer on OutputStream.flush,"When working with ""file://"" FileSystem (e.g. in tests), ChecksumFileSystem is used.  I am calling next operations:  create  write  hflush  open  seek  read    This results in:    java.lang.IllegalStateException: java.io.IOException: Cannot seek after EOF    com.dbl.util.grid.apache.hdfs.FileSystemStorageHolder.get(FileSystemStorageHolder.java:52)    com.dbl.util.grid.apache.hdfs.FileSystemStorageTest.testRecursiveSave(FileSystemStorageTest.java:31)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)    org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)    org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)    org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)    org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)    org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)    org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)    org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)    org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)    org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)    org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)    org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)    org.junit.runner.JUnitCore.run(JUnitCore.java:130)    com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:97)    com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:192)    com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:60)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)  Caused by: java.io.IOException: Cannot seek after EOF    org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.seek(ChecksumFileSystem.java:296)    org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:42)    com.dbl.util.grid.apache.hdfs.FileSystemStorageHolder.get(FileSystemStorageHolder.java:49)   ... 26 more      It seems that FSOutputSummer does not implements flush OutputStream method - so hflush gets ignored.",fs
Configuration methods that return collections are inconsistent about mutability,"In particular, getTrimmedStringCollection seems to return an immutable collection, whereas getStringCollection returns a mutable one.    IMO we should always return mutable collections since these methods by definition are doing copies.",conf
webinterface.private.actions should not be in common,"The comment in -defaults says that webinterface.private.actions applies to both NN and JT. This is wrong. This option is only referenced via the JobTracker.     I propose to delete it here, and file a second issue in MAPREDUCE for renaming the option (deprecating the existing name.)",documentation
'docs' target incompatible with apache forrest 0.9,"The ant target ""cn-docs"" fails when using forrest 0.9.  I did confirm that the build is fine using 0.8, and that the docs for HDFS and MAPREDUCE work with 0.9.  I did not try using JDK5 due to HADOOP-7075.    Will attach the stacktrace to this ticket - and I would appreciate confirmation of the bug as today was my first day using forrest.    ",build
BackUpNameNode is using 100% CPU and not accepting any requests. ,"In our environment, Backup NameNode is using 100% CPU and not accepting any calls in 3days long run.  Thread dump   ""IPC Server Responder"" daemon prio=10 tid=0x00007f86c41c6800 nid=0x3b2a runnable [0x00007f86ce579000]  java.lang.Thread.State: RUNNABLE  at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)  at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)  at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)  at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)   locked <0x00007f86d67e2a20> (a sun.nio.ch.Util$1)   locked <0x00007f86d67e2a08> (a java.util.Collections$UnmodifiableSet)   locked <0x00007f86d67e26a8> (a sun.nio.ch.EPollSelectorImpl)  at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)  at org.apache.hadoop.ipc.Server$Responder.run(Server.java:501)   Looks like same issue occurred in jetty also. http://jira.codehaus.org/browse/JETTY-937                ",ipc
Eclipse project files are incomplete,"After a fresh checkout of hadoop-common I do 'ant compile eclipse'.  I open eclipse, set ANT_HOME and build the project.   At that point the following error appears:  {quote}  The type com.sun.javadoc.RootDoc cannot be resolved. It is indirectly referenced from required .class files ExcludePrivateAnnotationsJDiffDoclet.java /common/src/java/org/apache/hadoop/classification/tools line 1 Java Problem  {quote}    The solution is to add the ""tools.jar"" from the JDK to the buildpath/classpath.  This should be fixed in the build.xml.",build
Start metrics system even if config files are missing,"Per experience and discussion with HDFS-1922, it seems preferable to treat missing metrics config file as empty/default config, which is more compatible with metrics v1 behavior (the MBeans are always registered.)",metrics
Remove unused TaskLogAppender configurations from log4j.properties,"MAPREDUCE-2372 improved TaskLogAppender to no longer need as much ""wiring"" in log4j.properties. There are also some old properties in there that are no longer used (eg logsRetainHours and noKeepSplits).",conf
Port remaining metrics v1 from trunk to branch-0.20-security,HADOOP-7190 added metrics packages/classes. This is a port from trunk to make them actually work for the security branch.,metrics
core-default.xml lists configuration version as 0.21,"This key was added in HADOOP-6233, though appears unused. I suppose it's somewhat useful to try to diagnose if someone has old versions of core-default.xml on the classpath.    Either way it should probably be updated to say 0.22 in the branch and 0.23 in trunk.",conf
Remove all framework use of Configuration default args,The Hadoop code base should not use the form of Configuration#get*() that accepts a default.,conf
Add public javadocs to FSDataInputStream and FSDataOutputStream,"This is a method made public for testing.  In comments in HADOOP-7301 after commit, adding javadoc comments was requested.  This is a follow up jira to address it.",documentation
RPC.stopProxy doesn't actually close proxy,"Discovered while investigating HDFS-1965, it turns out that the reference-counting done in WritableRpcEngine.ClientCache doesn't map one-to-one with open TCP connections. This means that it's easy to accidentally leave TCP connections open longer than expected so long as the client has any other connections open at all.",ipc
MD5Hash factory should reset the digester it returns,"Currently the getDigest() method in MD5Hash does not reset the digester it returns. Since it's a thread-local, this means that a previous aborted usage of the same digester could leave some state around. For example, if the secondary namenode receives an IOException while transfering the image, and does another image transfer with the same thread, it will think it has received an invalid digest.",io
Coarse-grained dynamic configuration changes,"HADOOP-7001 introduced mechanism for performing dynamic configuration changes where reconfigureProperty()/reconfigurePropertyImpl() only notifies single property change.    Normally, components which use ReconfigurableBase would involve several related properties whose update should be done atomically.    This JIRA provides coarse-grained dynamic configuration changes with the following benefits:  1. consistency updating related properties dynamically  2. reduction of lock contention when multiple properties are changed in proximity",conf
The link to the documentation on the hadoop common page is broken,Currently the following link is broken for the documentation: http://hadoop.apache.org/common/docs/stable/,documentation
Add capability to resolve compression codec based on codec name,"When setting up a compression codec in an MR job the full class name of the codec must be used.    To ease usability, compression codecs should be resolved by their codec name (ie 'gzip', 'deflate', 'zlib', 'bzip2') instead their full codec class name.    Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name, it could simplify how HBase resolves loads the codecs.",io
Ganglia plugins for metrics v2,"Although, all metrics in metrics v2 are exposed via the standard JMX mechanisms, most users are using Ganglia to collect metrics.",metrics
hadoop command - do not accept class names starting with a hyphen,If this is committed I will look at patches for hdfs and mapred.    When teaching a good portion of the students in every single class execute:    {code}  $ hadoop -fs ls /  {code}    The -fs is passed directly to the JVM and the JVM fails to start:    {code}  $ ./bin/hadoop -fs ls /  Unrecognized option: -fs  Could not create the Java virtual machine.  {code}    Which is confusing and typically requires explanation. The attached patch improves that behavior:    {code}  $ ./bin/hadoop -fs ls /  Error: No command named `-fs' was found. Perhaps you meant `hadoop fs'  {code}    The only risk I can see is if someone is abusing the implementation of hadoop command doing something like so:    {code}  $ ./bin/hadoop -Xmx1g  org.apache.hadoop.util.RunJar  RunJar jarFile [mainClass] args...  {code}    The hadoop command does not appear to advertise allowing JVM options before the classname.,scripts
TestTrash.testTrashEmptier() infinite loops if run on a home directory with stuff in .Trash,"This bug was discovered while investigating HDFS-1967, intermittent failure of TestHDFSTrash, which calls TestTrash.  If the user id running the test has a ~/.Trash directory that already has 4 or more files in it, a loop in TestTrash.testTrashEmptier() will never terminate, because it is waiting for the count of objects to increase from zero to exactly three (and it creates one object right away).",test
FileSystem.listStatus() throws NullPointerException instead of IOException upon access permission failure,"Many processes that call listStatus() expect to handle IOException, but instead are getting runtime error NullPointerException, if the directory being scanned is visible but no-access to the running user id.  For example, if directory foo is drwxr-xr-x, and subdirectory foo/bar is drwx------, then trying to do listStatus(Path(foo/bar)) will cause a NullPointerException.",fs
"When a serializer class is missing, return null, not throw an NPE.","When you have a key/value class that's non Writable and you forget to attach io.serializers for the same, an NPE is thrown by the tasks with no information on why or what's missing and what led to it. I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones.",io
incomplete help message  is displayed for df -h option,"The help message for the command ""hdfs dfs -help df"" is displayed like this:  ""-df [<path> ...]:    Shows the capacity, free and used space of the filesystem.          If the filesystem has multiple partitions, and no path to a          particular partition is specified, then the status of the root          partitions will be shown.""  and the information about df -h option is missed,despite the fact that df -h option is implemented.    Therefore,the expected message should be displayed like this:  ""-df [-h] [<path> ...]:    Shows the capacity, free and used space of the filesystem.          If the filesystem has multiple partitions, and no path to a          particular partition is specified, then the status of the root          partitions will be shown.            -h   Formats the sizes of files in a human-readable fashion                 rather than a number of bytes.""",fs
The metrics source mbean implementation should return the attribute value instead of the object,The MetricsSourceAdapter#getAttribute in 0.20.203 is returning the attribute object instead of the value.,metrics
Make hadoop-daemon.sh to return 1 if daemon processes did not get started,Makes hadoop-daemon.sh to return 1 if daemon processes did not get started.,scripts
Deadlock in IPC,"Saw this during a run of TestIPC on 0.22 branch:        [junit] Java stack information for the threads listed above:      [junit] ===================================================      [junit] ""IPC Client (47) connection to /0:0:0:0:0:0:0:0:48853 from an unknown user"":      [junit]     at org.apache.hadoop.ipc.Client$ParallelResults.callComplete(Client.java:879)      [junit]     - waiting to lock <0x00000000f599ef88> (a org.apache.hadoop.ipc.Client$ParallelResults)      [junit]     at org.apache.hadoop.ipc.Client$ParallelCall.callComplete(Client.java:862)      [junit]     at org.apache.hadoop.ipc.Client$Call.setException(Client.java:185)      [junit]     - locked <0x00000000f59e2818> (a org.apache.hadoop.ipc.Client$ParallelCall)      [junit]     at org.apache.hadoop.ipc.Client$Connection.cleanupCalls(Client.java:843)      [junit]     at org.apache.hadoop.ipc.Client$Connection.close(Client.java:832)      [junit]     - locked <0x00000000f59d8a90> (a org.apache.hadoop.ipc.Client$Connection)      [junit]     at org.apache.hadoop.ipc.Client$Connection.run(Client.java:708)      [junit] ""Thread-242"":      [junit]     at org.apache.hadoop.ipc.Client$Connection.markClosed(Client.java:788)      [junit]     - waiting to lock <0x00000000f59d8a90> (a org.apache.hadoop.ipc.Client$Connection)      [junit]     at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:742)      [junit]     at org.apache.hadoop.ipc.Client.call(Client.java:1109)      [junit]     - locked <0x00000000f599ef88> (a org.apache.hadoop.ipc.Client$ParallelResults)      [junit]     at org.apache.hadoop.ipc.TestIPC$ParallelCaller.run(TestIPC.java:135)  ",ipc
Performance improvement in PureJavaCrc32,"I would like to propose a small patch to       org.apache.hadoop.util.PureJavaCrc32.update(byte[] b, int off, int len)    Currently the method stores the intermediate result back into the data member ""crc."" I noticed this method gets  inlined into DataChecksum.update() and that method appears as one of the hotter methods in a simple hprof profile collected while running terasort and gridmix.    If the code is modified to save the temporary result into a local and just once store the final result back into the data member, it results in slightly more efficient hotspot codegen.    I tested this change using the the ""org.apache.hadoop.util.TestPureJavaCrc32$PerformanceTest"" which is embedded in the existing unit test for this class, TestPureJavaCrc32 on a variety of linux x64 AMD and Intel multi-socket and multi-core systems I have available to test.    The patch removes several stores of the intermediate result to memory yielding a 0%-10% speedup in the ""org.apache.hadoop.util.TestPureJavaCrc32$PerformanceTest"" which is embedded in the existing unit test for this class, TestPureJavaCrc32.     If you use a debug hotspot JVM with -XX:+PrintOptoAssembly, you can see the intermediate stores such as:    414     movq    R9, [rsp + #24] # spill  419     movl    [R9 + #12 (8-bit)], RDX # int ! Field PureJavaCrc32.crc  41d     xorl    R10, RDX        # int    The patch results in just one final store of the fully computed value.  ","performance,util"
test-patch should check for hard tabs,"Our coding guidelines say that hard tabs are disallowed in the Hadoop code, but they sometimes sneak in (there are about 280 in the common codebase at the moment).    We should run a simple check for this in the test-patch process so it's harder for them to sneak in.","build,test"
Force entropy to come from non-true random for tests,"Passing the system property {{-Djava.security.egd=file:///dev/urandom}} forces the JVM to seed its PRNG from non-true random (/dev/urandom) instead of the true random (/dev/random). This makes the tests run faster, since without it they often hang waiting for entropy while Jetty is initializing.    We should turn this on for the test targets by default, so developers/hudson boxes don't have to make this change system-wide or use workarounds like rngtools.","build,test"
Annotate PureJavaCrc32 as a public API,The API of PureJavaCrc32 is stable.  It is incorrect to annotate it as private unstable.  ,util
Introduce a buffered checksum for avoiding frequently calls on Checksum.update(),"We found that PureJavaCRC32/CRC32.update() is the TOP 1 of the methods consuming CPU in a map side, and in reduce side, it cost a lots of CPU too.    IFileOutputStream would frequently call Checksum.update() during writing a record. It's very common a MR key/value less than 512 bytes. Checksum.update() would be called every time writing a key/value.     Test case: terasort 100MB.   Checksum.update() calls has be reduced from 4030348 to 28069.  This method is not a hotspot anymore.      ",util
incomplete help message is displayed for getmerge [addnl] option,"The help message for the command ""hdfs dfs -help getmerge"" is displayed like this:  ""-getmerge <src> <localdst> [addnl]: Get all the files in the directories that    match the source file pattern and merge and sort them to only    one file on local fs. <src> is kept.""  and the information about [addnl] option is missed,despite the fact that [addnl] option is implemented.    Therefore,the expected message should be displayed like this:  ""-getmerge <src> <localdst> [addnl]: Get all the files in the directories that    match the source file pattern and merge and sort them to only    one file on local fs. <src> is kept.      addnl   Optionally addnl can be set to enable adding a newline               character at the end of each file.""",fs
Fix option parsing in CommandFormat,"CommandFormat currently allows options in any location within the args.  This is not the intended behavior for FsShell commands.  Prior to the redesign, the commands used to expect option processing to stop at the first non-option.    CommandFormat was an existing class prior the redesign, but it was only used by ""count"" to find the -q flag.  All commands were converted to using this class, thus inherited the unintended behavior.",fs
backport HADOOP-7008 and HADOOP-7042 to branch-0.20-security,backport HADOOP-7008 and HADOOP-7042 to branch-0.20-security so that we can enable test-patch.sh to have a configured number of acceptable findbugs and javadoc warnings,test
globStatus doesn't grok groupings with a slash,"If a glob contains a grouping with a single item that contains a slash, ex. ""{a/b}"", then globStatus throws {{""Illegal file pattern: Unclosed group near index 2""}} -- regardless of whether the path exists.  However, if the glob set contains more than one item, ex. ""{a/b,c}"", then it throws a {{NullPointerException}} from {{FileSystem.java:1277}}.    {code}  1276: FileStatus[] files = globStatusInternal(new Path(filePattern), filter);  1277: for (FileStatus file : files) {  1278:   results.add(file);  1279: }  {code}    The method {{globStatusInternal}} can return null, so the iterator fails with the NPE.",fs
listStatus for local files throws NPE instead of permission denied,"Calling {{fs.listStatus}} on a local directory where the user does not have permissions generates a {{NullPointerException}}:    {noformat}  Exception in thread ""main"" java.lang.NullPointerException    org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1115)    org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1150)    org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:494)  {noformat}    FileSystem.java:  {code}  1111:  private void listStatus(ArrayList<FileStatus> results, Path f,  1112:      PathFilter filter) throws FileNotFoundException, IOException {  1113:    FileStatus listing[] = listStatus(f);  1114:  1115:    for (int i = 0; i < listing.length; i++) {  {code}",fs
Send back nicer error to clients using outdated IPC version,"When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump, the client currently just gets a non-useful error message like ""EOFException"".    Instead, the IPC server code can speak just enough of prior IPC protocols to send back a ""fatal"" message indicating the version mismatch.",ipc
IPC Wire Compatibility,None,ipc
Modify the option of FsShell getmerge from [addnl] to [-nl] for consistency,"The [addnl] option of FsShell getmerge should be either ""true"" or ""false"",but it is very hard to understand by users, especially  who`s never used this option before.   So,the [addnl] option should be changed to [-nl] for consistency.  ",fs
HADOOP-7121 accidentally disabled some tests,"When I converted TestIPC to JUnit 4, I missed a couple of tests towards the bottom of the file when adding the @Test annotation.","ipc,test"
Use ServiceLoader to discover compression codec classes,"By using a ServiceLoader users wouldn't have to add codec classes to io.compression.codecs for codecs that aren't shipped with Hadoop (e.g. LZO), since they would be automatically picked up from the classpath.","conf,io"
Contracts of LocalFileSystem and DistributedFileSystem should require FileSystem::listStatus throw IOException not return null upon access error,"In HADOOP-6201 and HDFS-538 it was agreed that FileSystem::listStatus should throw FileNotFoundException instead of returning null, when the target directory did not exist.    However, in LocalFileSystem implementation today, FileSystem::listStatus still may return null, when the target directory exists but does not grant read permission.  This causes NPE in many callers, for all the reasons cited in HADOOP-6201 and HDFS-538.  See HADOOP-7327 and its linked issues for examples.  ","fs,fs/s3"
Cleanup FsShell and prevent masking of RTE stacktraces,"{{FsShell}}'s top level exception handler catches and displays exceptions.  Unfortunately it displays only the first line of an exception, which means an unexpected {{RuntimeExceptions}} like {{NullPointerException}} only display ""{{cmd: NullPointerException}}"".  This user has no context to understand and/or accurately report the issue.    Found due to bugs such as {{HADOOP-7327}}.",fs
hadoop.io.compress.TestCodec#main() should exit with non-zero exit code if test failed,It's convenient to run something like  {noformat}  HADOOP_CLASSPATH=hadoop-test-0.20.2.jar bin/hadoop org.apache.hadoop.io.compress.TestCodec  -count 3 -codec fo  {noformat}  but the error code it returns isn't interesting.    1-line patch attached fixes that.,test
Improve log levels when exceptions caught in RPC handler,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level.    I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",ipc
FsShell does not preserve relative paths with globs,FsShell currently preserves relative paths that do not contain globs.  Unfortunately the method {{fs.globStatus()}} is fully qualifying all returned paths.  This is causing inconsistent display of paths.,fs
TestRawLocalFileSystemContract is needed,"FileSystemContractBaseTest is supposed to be run with each concrete FileSystem implementation to insure adherence to the ""contract"" for FileSystem behavior.  However, currently only HDFS and S3 do so.  RawLocalFileSystem, at least, needs to be added. ",fs
TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test,TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test. ,test
OS X puts Java headers in non-standard place,"On OS X, $JAVA_HOME doesn't have JNI headers.  Another check is needed to locate them.",native
getgrouplist() in getGroup.c is not portable,"getGroupIDList uses getgrouplist() to fetch the groups for a user.  Unfortunately, this routine is a BSD-specific call and is not present in most System V-based operating systems. ",native
Mechanism for providing version info of hadoop jars,"In 0.20.x, only one jar (hadoop-core-*.jar) really matters. The o.a.h.util.VersionInfo combined with saveVersion.sh script (generating a package level annotation) served us well. For 0.23+, due to the project split and modularization of mapreduce (currently in MR-279), a lot more essential hadoop jars are created. The potential of mixing up the jars is significantly increased as well. We need a simple way to list the version info (version, branch, source checksum etc.) for all the jars involved. This is essential for QE and tracking down various issues.    I propose that we use a VersionProvider interface (similar to the current VersionInfo util class) and ServiceLoader to enumerate the version providers in the jars.  {code}  public interface VersionProvider {    String getJar();     String getPackage();    String getVersion();    String getRevision();    String getBranch();    String getDate();    String getUrl();    String getSourceChecksum();  }  {code}",build
Optimize pread on ChecksumFileSystem,"Currently the implementation of positional read in ChecksumFileSystem is verify inefficient - it actually re-opens the underlying file and checksum file, then seeks and uses normal read. Instead, it can push down positional read directly to the underlying FS and verify checksum.",fs
Improve tarball distributions,"Hadoop release tarball contains both raw source and binary.  This leads users to use the release tarball as base for applying patches, to build custom Hadoop.  This is not the recommended method to develop hadoop because it leads to mixed development system where processed files and raw source are hard to separate.      To correct the problematic usage of the release tarball, the release build target should be defined as:    ""ant source"" generates source release tarball.  ""ant binary"" is binary release without source/javadoc jar files.  ""ant tar"" is a mirror of binary release with source/javadoc jar files.    Does this sound reasonable?",build
Remove ref of 20.3 release from branch-0.20 CHANGES.txt,CHANGES.txt on branch-0.20 claims there was a 0.20.3 release on 1/5. There has not been a 0.20.3 release.    {noformat}  Release 0.20.4 - Unreleased  ...  Release 0.20.3 - 2011-1-5  {noformat}    We should update this to indicate 0.20. is unreleased.,documentation
Don't add tools.jar to the classpath when running Hadoop,"bin/hadoop-config.sh (and bin/rcc) add lib/tools.jar from JAVA_HOME to the classpath. This has been there since the initial commit of bin/hadoop, but I don't think it's needed. *Executing* Hadoop does not depend on tools.jar (or other libraries only available in the JDK, not the JRE) so let's not automatically add it. Marking this as an incompatible change since a job could potentially have relied on Hadoop adding tools.jar to the CLASSPATH automatically (though such a job would not have run on a system that did not have JAVA_HOME point to a jdk). The build of course still requires a JDK.",scripts
Fix command name handling affecting DFSAdmin,"When an error occurs in the get/set quota commands in DFSAdmin, they are displaying the following:  setQuota: failed to get SetQuotaCommand.NAME    The {{Command}} class expects the {{NAME}} field to be accessible, but for DFSAdmin, it's not.",fs
hadoop dfs -ls : Do not expand directories (was HDFS-1475),"In a nutshell, ls needs the ability to list a directory but not its contents.  W/o -d, it is impossible to list the root directory's owner, permissions, etc.  See the original hdfs bug for details.",fs
Add ability to include Protobufs in ObjectWritable,"Per HDFS-2060, it would make it easier to piecemeal switch to protocol buffer based data structures in the wire protocol if we could intermix the two. The IPC framework currently provides the concept of ""engines"" for RPC, but that doesn't easily allow mixed types within the same framework for ease of transition.    I'd like to add the cases to ObjectWritable to be handle subclasses of {{Message}}, the superclass of codegenned protobufs.","io,ipc"
Add client failover functionality to o.a.h.io.(ipc|retry),Implementing client failover will likely require changes to {{o.a.h.io.ipc}} and/or {{o.a.h.io.retry}}. This JIRA is to track those changes.,"ha,ipc"
FindBugs OutOfMemoryError,"When running the findbugs target from Jenkins, I get an OutOfMemory error.  The ""effort"" in FindBugs is set to Max which ends up using a lot of memory to go through all the classes. The jvmargs passed to FindBugs is hardcoded to 512 MB max.    We can leave the default to 512M, as long as we pass this as an ant parameter which can be overwritten in individual cases through -D, or in the build.properties file (either basedir, or user's home directory).  ",build
hadoop 0.20.203.0 Eclipse Plugin does not work with Eclipse,"hadoop 0.20.203.0 Eclipse Plugin does not work with Eclipse,while adding DFS Locations,Advance parameters didin't hava a option hadoop.job.ugi",contrib/eclipse-plugin
HDFS needs to export protobuf library dependency in pom,"MR builds are failing since the HDFS protobuf patch went in, since they aren't picking up protobuf as a transitive dependency. I think we just need to add it to the HDFS pom template.",build
Use of TestingGroups by tests causes subsequent tests to fail,"As mentioned in HADOOP-6671, {{UserGroupInformation.createUserForTesting(...)}} manipulates static state which can cause test cases which are run after a call to this function to fail.",test
VersionInfo not generated properly in git after unsplit,"The version information generated during the build of common when running from git has revision and branch Unknown. I believe this started after the unsplit:    @HadoopVersionAnnotation(version=""0.22.0-SNAPSHOT"", revision=""Unknown"", branch=""Unknown"",                           user=""tgraves"", date=""Tue Jun 14 13:39:10 UTC 2011"", url=""file:///home/tgraves/git/hadoop-common/common"",                           srcChecksum=""0f78ea668971fe51e7ebf4f97f84eed2"")    The ./src/saveVersion.sh script which generates the package-info.java file with the version info looks for the presence of .git directory and that is now a level up instead of in the common directory.",build
"PathData saves original string value, inviting failure when CWD changes","PathData#string stores the pathstring originally used to construct the Path, and returns it from various methods, apparently in an attempt to improve the user experience for the shell.    However, the current working directory may change, and if so this string value becomes meaningless and/or incorrect in context.  ",fs
Chinese documentation can't be built with Forrest 0.9,"Running {{ant cn-docs}} with Forrest 0.8 will work. Running the same thing with Forrest 0.9 prints the following error:    {noformat}  Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/fop/messaging/MessageHandler    org.apache.cocoon.serialization.FOPSerializer.configure(FOPSerializer.java:122)    org.apache.avalon.framework.container.ContainerUtil.configure(ContainerUtil.java:201)    org.apache.avalon.excalibur.component.DefaultComponentFactory.newInstance(DefaultComponentFactory.java:289)    org.apache.avalon.excalibur.pool.InstrumentedResourceLimitingPool.newPoolable(InstrumentedResourceLimitingPool.java:655)    org.apache.avalon.excalibur.pool.InstrumentedResourceLimitingPool.get(InstrumentedResourceLimitingPool.java:371)    org.apache.avalon.excalibur.component.PoolableComponentHandler.doGet(PoolableComponentHandler.java:198)    org.apache.avalon.excalibur.component.ComponentHandler.get(ComponentHandler.java:381)    org.apache.avalon.excalibur.component.ExcaliburComponentSelector.select(ExcaliburComponentSelector.java:215)    org.apache.cocoon.components.ExtendedComponentSelector.select(ExtendedComponentSelector.java:268)    org.apache.cocoon.components.pipeline.AbstractProcessingPipeline.setSerializer(AbstractProcessingPipeline.java:311)    org.apache.cocoon.components.pipeline.impl.AbstractCachingProcessingPipeline.setSerializer(AbstractCachingProcessingPipeline.java:171)    org.apache.cocoon.components.treeprocessor.sitemap.SerializeNode.invoke(SerializeNode.java:120)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.SelectNode.invoke(SelectNode.java:103)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:47)    org.apache.cocoon.components.treeprocessor.sitemap.PreparableMatchNode.invoke(PreparableMatchNode.java:131)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.PipelineNode.invoke(PipelineNode.java:143)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.PipelinesNode.invoke(PipelinesNode.java:93)    org.apache.cocoon.components.treeprocessor.ConcreteTreeProcessor.process(ConcreteTreeProcessor.java:235)    org.apache.cocoon.components.treeprocessor.ConcreteTreeProcessor.process(ConcreteTreeProcessor.java:177)    org.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:254)    org.apache.cocoon.components.treeprocessor.sitemap.MountNode.invoke(MountNode.java:118)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.SelectNode.invoke(SelectNode.java:98)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.PipelineNode.invoke(PipelineNode.java:143)    org.apache.cocoon.components.treeprocessor.AbstractParentProcessingNode.invokeNodes(AbstractParentProcessingNode.java:69)    org.apache.cocoon.components.treeprocessor.sitemap.PipelinesNode.invoke(PipelinesNode.java:93)    org.apache.cocoon.components.treeprocessor.ConcreteTreeProcessor.process(ConcreteTreeProcessor.java:235)    org.apache.cocoon.components.treeprocessor.ConcreteTreeProcessor.process(ConcreteTreeProcessor.java:177)    org.apache.cocoon.components.treeprocessor.TreeProcessor.process(TreeProcessor.java:254)    org.apache.cocoon.Cocoon.process(Cocoon.java:699)    org.apache.cocoon.bean.CocoonWrapper.getPage(CocoonWrapper.java:514)    org.apache.cocoon.bean.CocoonBean.processTarget(CocoonBean.java:499)    org.apache.cocoon.bean.CocoonBean.process(CocoonBean.java:356)    org.apache.cocoon.Main.main(Main.java:321)  Caused by: java.lang.ClassNotFoundException: org.apache.fop.messaging.MessageHandler    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:307)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:248)   ... 38 more  {noformat}",documentation
Allow configurable timeouts when connecting to HDFS via java FileSystem API,"If the NameNode is not available (in, for example, a network partition event separating the client from the NameNode), and an attempt is made to connect, then the FileSystem api will *eventually* timeout and throw an error. However, that timeout is currently hardcoded to be 20 seconds to connect, with 45 retries, for a total of a 15 minute wait before failure. While in many circumstances this is fine, there are also many circumstances (such as booting a service) where both the connection timeout and the number of retries should be significantly less, so as not to harm availability of other services.    Investigating Client.java, I see that there are two fields in Connection: maxRetries and rpcTimeout. I propose either re-using those fields for initiating the connection as well; alternatively, using the already existing dfs.socket.timeout parameter to set the connection timeout on initialization, and potentially adding a new field such as dfs.connection.retries with a default of 45 to replicate current behaviors.",ipc
HdfsProxyTests fails when the -Dtest.build.dir and -Dbuild.test is set ,"HdfsProxyTests fails when the -Dtest.build.dir and -Dbuild.test is set a dir other than build dir    test-junit:       [copy] Copying 1 file to /home/y/var/builds/thread2/workspace/Cloud-Hadoop-0.20.1xx-Secondary/src/contrib/hdfsproxy/src/test/resources/proxy-config      [junit] Running org.apache.hadoop.hdfsproxy.TestHdfsProxy      [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec      [junit] Test org.apache.hadoop.hdfsproxy.TestHdfsProxy FAILED",build
Unit test TestPureJavaCRC32 warmup code warms up the not-so-important loop in PureJavaCRC32.update(),"When the warmup code sequence in TestPureJavaCRC32.java is executed, it sends size=len=2 and due to the value of 'trials' in for loop in doBench(), the crc.update() gets run > the compile threshold, thus providing the information that 'while 0<len<7' is a hot loop and 'while len>7' is a cold loop. This brings the MB/s number for len > 7 in PureJavaCRC32.update() way down (e.g. ~28.5% for size=len=65536).    The workaround would be to use size=len=>7 (so just having size=len=2101 ahead of size=len=2 will do the trick) in the warmup section. ",test
TestConfiguration doesn't clean up after itself,"{{testGetFile}} and {{testGetLocalPath}} both create directories a, b, and c in the working directory from where the tests are run. They should clean up after themselves.",test
libhadoop is all or nothing,"As a result of a ton of new code in libhadoop being added in 0.20.203/0.22, a lot of features that used to work no longer do reliably.  The most common problem is native compression, but other issues such as Mac OS X's group support broke as well.  The native code checks need to be refactored such that libhadoop.so should report what it supports rather than having the Java-side assume that if it loads, it is all supported.  This would allow us to stub routines until they've been vetted, removing the chances of such regressions appearing in the future.",native
"Add a ""I broke the Hadoop build"" poster","Some people have been known to check in code that does not work, and hence break the build. These people (and I may be one of them) deserve to have their contribution acknowledged with an ""I broke the build"" poster. I have a draft slide for this, using an upside down copy of the logo. It may be A4 paper, when US-letter is probably more appropriate. ",build
Add javadoc for SnappyCodec,HADOOP-7206 failed to include a javadoc for public methods.,io
TestTFileByteArrays is failing on Hudson,"This test has failed in the last 4 nightly builds, as seen here: https://builds.apache.org/job/Hadoop-Common-trunk/    I can't reproduce this failure on my machine, running the test either in isolation or as part of the full suite.",io
Create Jenkins build for Maven patch testing,We need an equivalent of https://builds.apache.org/job/PreCommit-HADOOP-Build for the Maven build. Until this is live it would be triggered manually and wouldn't post comments to JIRA.,build
HDFS package reference throws error,"I have referenced this package in my Java code. The project needs to be compiled using JDK 1.5 compiler only.    The Hadoop reference throws error from javac as below.     [javac] class file has wrong version 50.0, should be 49.0    Note: If compiled using Java 1.6 , it goes through.  ",build
Test that the topology script is always passed IP addresses,"Now that HADOOP-6682 has been fixed, Hadoop should always pass the topology script an IP address rather than a hostname. We should write a test that covers this (specifically that DNSToSwitchMapping#resolve is always passed IP addresses) so users can safely write topology scripts that don't handle hostnames.   ",test
Document topology script requirements,The topology script documentation is cluster_setup.xml is unclear. The topology script:  # Only needs to handle IP addresses (not hostnames)  # Needs to handle multiple arguments for caching to work effectively    We should check in an example script or include an example one in the docs.,documentation
ReflectionUtils.setConf would configure anything Configurable twice,"In  the setConf method of org.apache.hadoop.util.ReflectionUtils, any instance of Configurable would be configured twice.    In 0.21.0, KeyFieldBasedPartitioner implements the Configurable interface. When configured twice, it get two KeyDescription and gives out wrong partition number.       public static void setConf(Object theObject, Configuration conf) {      if (conf != null) {        if (theObject instanceof Configurable) {          ((Configurable) theObject).setConf(conf);        }        setJobConf(theObject, conf);      }    }  ",util
syntax error in smart-apply-patch.sh ,{noformat}       [exec]     Finished build.       [exec] hdfs/src/test/bin/smart-apply-patch.sh: line 60: syntax error in conditional expression: unexpected token `('    BUILD FAILED  hdfs/build.xml:1595: exec returned: 1  {noformat},scripts
IPC connection is orphaned with null 'out' member,"We had a situation a JT ended up in a state where a certain user could not submit a job, due to an NPE on the following line in {{sendParam}}:  {code}  synchronized (Connection.this.out) {  {code}  Looking at the code, my guess is that an RTE was thrown in setupIOstreams, which only catches IOE. This could leave the connection in a half-setup state which is never cleaned up and also cannot perform IPCs.",ipc
Add another IOUtils#copyBytes method,"Common side of HDFS-2110, adds a new IOUtils copy bytes method and cleanup.",util
Improve error message when moving to trash fails due to quota issue,-rm command doesn't suggest -skipTrash on failure.,fs
Test DiskChecker's functionality in identifying bad directories (Part 2 of testing DiskChecker),Add a test for the DiskChecker#checkDir method used in other projects (HDFS).,"test,util"
Native libs are not in platform-specific dirs in the tarball,"The SO file/links are copied into lib/native/, they should be copied to lib/native/$OS_ARCH",build
Make pre-commit checks run against the correct branch,The Hudson pre-commit tests are presently only capable of testing a patch against trunk. It'd be nice if this could be extended to automatically run against the correct branch.,test
IOUtils.copybytes will suppress the stream closure exceptions. ,"{code}    public static void copyBytes(InputStream in, OutputStream out, long count,        boolean close) throws IOException {      byte buf[] = new byte[4096];      long bytesRemaining = count;      int bytesRead;        try {        .............        .............      } finally {        if (close) {          closeStream(out);          closeStream(in);        }      }    }    {code}    Here if any exception in closing the stream, it will get suppressed here.    So, better to follow the stream closure pattern as HADOOP-7194.    ",io
update example code with the correct libraries,This example code should be updated to use the latest libraries for mapreduce:  http://hadoop.apache.org/common/docs/r0.20.203.0/mapred_tutorial.html#Example%3A+WordCount+v1.0  ,documentation
"Docs in core-default.xml still reference deprecated config ""topology.script.file.name""","HADOOP-6233 renamed the config ""{{topology.script.file.name}}"" to ""{{net.topology.script.file.name}}"" but missed a few spots in the docs of core-default.xml.","conf,documentation"
Support for pluggable Trash policies,It would be beneficial to make the Trash policy pluggable. One primary use-case for this is to archive files (in some remote store) when they get removed by Trash emptier.,fs
Add CRC32C as another DataChecksum implementation,"CRC32C is another checksum very similar to our existing CRC32, but with a different polynomial. The chief advantage of this other polynomial is that SSE4.2 includes hardware support for its calculation. HDFS-2080 is the umbrella JIRA which proposes using this new polynomial to save substantial amounts of CPU.","io,util"
Implement bulk checksum verification using efficient native code,"Once HADOOP-7444 is implemented (""bulk"" API for checksums), good performance gains can be had by implementing bulk checksum operations using JNI. This JIRA is to add checksum support to the native libraries. Of course if native libs are not available, it will still fall back to the pure-Java implementations.","native,util"
Implement CRC32C native code using SSE4.2 instructions,"Once HADOOP-7445 is implemented, we can get further performance improvements by implementing CRC32C using the hardware support available in SSE4.2. This support should be dynamically enabled based on CPU feature flags, and of course should be ifdeffed properly so that it doesn't break the build on architectures/platforms where it's not available.","native,performance"
Add a warning message for FsShell -getmerge when the src path is no a directory,"While the <src> specified for FsShell -getmerge is not a directory,the command does nothing and there's no any message explaining the issue.  Furthermore,the exitCode is zero.    $ hdfs dfs -getmerge /user/hadoop/testfile /work/tmp/testfile  $ echo $?  0  $ ls /work/tmp/testfile  ls: cannot access /work/tmp/testfile: No such file or directory      ",fs
Bump jetty to 6.1.26,"Bump the jetty version, as previous version has an issue that can cause it to hang at startup.    6.1.14 jetty is also tends to hung on heavy datanode loads.",build
Introduce HA Service Protocol Interface,This jira introduces a protocol interface to be implemented by services that provide HA functionality.,"ha,util"
Connection with RemoteException is not removed from cached HashTable and cause memory leak,"In a long running system like Oozie, we use hadoop client APIs, such as FileSystem.exists(), to check files exist on hdfs or not to kick off a user job. But in a production environment, user sometimes gives wrong or invalid format of file/directory paths. In that case, after server was up for couple days, we found around 80% of memory were taken away by hadoop ipc client connections. In one of the connections, there was a hashtable contains 200k entries. We cross-checked Hadoop code and found out that in org.apache.hadoop.ipc.Client.receiveResponse(), if state if fatal, the call object does not remove from the hashtable (calls) and keeps in the memory until system throws OutOfMemory error or crash. The code in question is here :    * org.apache.hadoop.ipc.Client.receiveResponse()   } else if (state == Status.FATAL.state) {            // Close the connection            markClosed(new RemoteException(WritableUtils.readString(in),                                            WritableUtils.readString(in)));   }",fs
Remove out-of-date Chinese language documentation,"The Chinese language documentation haven't been updated (other than copyright years and svn moves) since their original contribution several years ago.  Worse than no docs is out-of-date, wrong docs.  We should delete them from the source tree.",documentation
Namenode not get started! FSNamesystem initialization failed. java.io.FileNotFoundException,"2011-07-13 12:04:12,967 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.  java.io.FileNotFoundException: File does not exist: /opt/data/tmp/mapred/system/job_201107041958_0120/j^@^@^@^@^@^@          at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetPermission(FSDirectory.java:544)          at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:724)          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:992)          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:812)          at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:364)          at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:87)          at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:311)          at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:292)          at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:201)          at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:279)          at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:956)          at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:965)  2011-07-13 12:04:13,006 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.io.FileNotFoundException: File does not exist: /opt/data/tmp/mapred/system/job_201107041958_0120/j^@^@^@^@^@^@          at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetPermission(FSDirectory.java:544)          at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:724)          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:992)          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:812)          at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:364)          at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:87)          at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:311)          at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:292)          at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:201)          at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:279)          at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:956)          at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:965)      In the path /opt/data/tmp/mapred, ""system/"" folder itself is not available",fs
Jackson Dependency Not Declared in Hadoop POM,"(COMMENT: This bug still affects 0.20.205.0, four months after the bug was filed.  This causes total failure, and the fix is trivial for whoever manages the POM -- just add the missing dependency! --ben)    This issue was identified and the fix & workaround was documented at     https://issues.cloudera.org/browse/DISTRO-44    The issue affects use of Hadoop 0.20.203.0 from the Maven central repo. I built a job using that maven repo and ran it, resulting in this failure:    Exception in thread ""main"" java.lang.NoClassDefFoundError: org/codehaus/jackson/map/JsonMappingException    thinkbig.hadoop.inputformat.TestXmlInputFormat.run(TestXmlInputFormat.java:18)    thinkbig.hadoop.inputformat.TestXmlInputFormat.main(TestXmlInputFormat.java:23)  Caused by: java.lang.ClassNotFoundException: org.codehaus.jackson.map.JsonMappingException        ",build
Feature 'http://apache.org/xml/features/xinclude' is not recognized error in Hadoop 0.20.2,"I have build a library to manipulate Hbase tables and am calling these methods inside a complexe java project. I am using the compiled version of Hadoop 0.20.2 and hbase 0.90.3.  My project compiled without any problem, during the execution amd having the error to xInclude feature is not recognized. I have found suggestion to put a patch in configuration.java, but the issue am using the compiled version of hadoop. below is the stack of the error.    thanks for your help    Jul 15, 2011 10:10:11 AM org.apache.hadoop.conf.Configuration loadResource  SEVERE: error parsing conf file: javax.xml.parsers.ParserConfigurationException: Feature 'http://apache.org/xml/features/xinclude' is not recognized.  java.lang.RuntimeException: javax.xml.parsers.ParserConfigurationException: Feature 'http://apache.org/xml/features/xinclude' is not recognized.    org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1171)    org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1030)    org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)    org.apache.hadoop.conf.Configuration.get(Configuration.java:382)    org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:63)    org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:89)    org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:100)    org.b3mn.poem.Hbase_hadoop_data_access_methods.addRow(Hbase_hadoop_data_access_methods.java:46)    org.b3mn.poem.Representation.contentExists(Representation.java:108)    org.b3mn.poem.Representation.setSvg(Representation.java:239)    org.b3mn.poem.Identity.newModel(Identity.java:92)    org.b3mn.poem.handler.NewModelHandler.doPost(NewModelHandler.java:129)    org.b3mn.poem.Dispatcher.dispatch(Dispatcher.java:376)    org.b3mn.poem.Dispatcher.doPost(Dispatcher.java:409)    javax.servlet.http.HttpServlet.service(HttpServlet.java:637)    javax.servlet.http.HttpServlet.service(HttpServlet.java:717)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.b3mn.poem.security.filter.AuthenticationFilter.doFilter(AuthenticationFilter.java:156)    org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)    org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)    org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)    org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)    org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)    org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)    org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)    org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298)    org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859)    org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588)    org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)    java.lang.Thread.run(Thread.java:662)  Caused by: javax.xml.parsers.ParserConfigurationException: Feature 'http://apache.org/xml/features/xinclude' is not recognized.    org.apache.xerces.jaxp.DocumentBuilderFactoryImpl.newDocumentBuilder(Unknown Source)    org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1061)   ... 30 more",build
hadoop fs -stat '{glob}' gives null with combo of absolute and non-existent files,"I'm trying to {{hadoop fs -stat}} a list of HDFS files all at once, because doing them one at a time is slow.  stat doesn't accept multiple arguments, so I'm using a glob of the form '{file1,file2}' (quoted from the shell).  I've discovered this doesn't work for me because the glob expands non-existent files to nothing, and I get nothing back from stat.  It would be nice to be able to use stat for this, but perhaps that's more of a feature request.    However, in the process, I discovered that with relative pathnames, I get back the stats for the existing files.  With absolute filenames, I get back {{stat: null}}.        $ hadoop fs -touchz file1 file2  $ hadoop fs -stat '{file1,file2}'  2011-07-15 21:21:19  2011-07-15 21:21:19  $ hadoop fs -stat '{file1,file2,nonexistent}'  2011-07-15 21:21:19  2011-07-15 21:21:19  $ hadoop fs -stat '{/user/me/file1,/user/me/file2}'  2011-07-15 21:21:19  2011-07-15 21:21:19  $ hadoop fs -stat '{/user/me/file1,/user/me/file2,nonexistent}'  stat: null      Perhaps I'm doing something dumb, but it seems like stat should give the same results whether you use relative or absolute paths.",fs
A several tiny improvements for the LOG format,"There are several fields in the log that the space characters are missed.  For instance:  src/java/org/apache/hadoop/ipc/Client.java(248): LOG.debug(""The ping interval is"" + this.pingInterval + ""ms."");  src/java/org/apache/hadoop/fs/LocalDirAllocator.java(235):  LOG.warn( localDirs[i] + ""is not writable n"", de);  ","fs,ipc"
hadoop-core JAR contains a log4j.properties file,"the hadoop-core JAR in the distribution and in the Maven repositories has a log4j JAR. This can break the logging of any client programs which import that JAR to do things like DFSClient work. It should be stripped from future releases. This should not impact server-side deployments, as the properties file in conf/ should be picked up instead. ",build
add a standard handler for socket connection problems which improves diagnostics,"connection refused, connection timed out, no route to host, etc, are classic IOExceptions that can be raised in a lot of parts of the code. The standard JDK exceptions are useless for debugging as they   # don't include the destination (host, port) that can be used in diagnosing service dead/blocked problems  # don't include any source hostname that can be used to handle routing issues  # assume the reader understands the TCP stack.  It's obvious from the -user lists that a lot of people hit these problems and don't know how to fix them. Sometimes the source has been patched to insert the diagnostics, but it may be convenient to have a single method to translate some  {code}  SocketException processIOException(SocketException e, String destHost, int destPort) {    String localhost = getLocalHostname();    String details = ""From ""+ localhost +"" to ""+ desthost + "":""+destPort;    if (e instanceof ConnectException) {      return new ConnectException(details               + "" -- see http://wiki.apache.org/hadoop/ConnectionRefused --"" + e, e);    }    if (e instanceof UnknownHostException) {      return new UnknownHostException(details               + "" -- see http://wiki.apache.org/hadoop/UnknownHost --"" + e, e);    }    // + handlers for other common socket exceptions      //and a default that returns an unknown class unchanged    return e;  }      {code}    Testing: try to connect to an unknown host, a local port that isn't live, etc. It's hard to replicate all failures consistently. It may be simpler just to verify that if you pass in a specific exception, the string is expanded and the class is unchanged.    This code could then be patched in to places where IO takes place. Note that Http Components and HttpClient libs already add some destination details on some operation failures, with their own HttpException tree: it's simplest to leave these alone.  ",util
move up to Jackson 1.8.8,I see that hadoop-core still depends on Jackson 1.0.1 -but that project is now up to 1.8.2 in releases. Upgrading will make it easier for other Jackson-using apps that are more up to date to keep their classpath consistent.    The patch would be updating the ivy file to pull in the later version; no test,util
the saveVersion.sh script sometimes fails to extract SVN URL,"When using an SVN checkout of the source, sometime the {{svn info}} command outputs a 'Copied from URL: ###' line in addition to the 'URL: ###'.    This breaks the saveVersion.sh script that assume there is only one line in the output of {{svn info}} that contains the word URL.",build
RPC client should deal with the IP address changes,"The current RPC client implementation and the client-side callers assume that the hostname-address mappings of servers never change. The resolved address is stored in an immutable InetSocketAddress object above/outside RPC, and the reconnect logic in the RPC Connection implementation also trusts the resolved address that was passed down.    If the NN suffers a failure that requires migration, it may be started on a different node with a different IP address. In this case, even if the name-address mapping is updated in DNS, the cluster is stuck trying old address until the whole cluster is restarted.    The RPC client-side should detect this situation and exit or try to recover.    Updating ConnectionId within the Client implementation may get the system work for the moment, there always is a risk of the cached address:port become connectable again unintentionally. The real solution will be notifying upper layer of the address change so that they can re-resolve and retry or re-architecture the system as discussed in HDFS-34.     For 0.20 lines, some type of compromise may be acceptable. For example, raise a custom exception for some well-defined high-impact upper layer to do re-resolve/retry, while other will have to restart.  For TRUNK, the HA work will most likely determine what needs to be done.  So this Jira won't cover the solutions for TRUNK.  ",ipc
task-controller can drop last char from config file,It looks as though task-controller's configuration file reader assumes that the output of getline() always ends with \n\0.  This assumption does not appear to be safe.  See comments for more. ,security
Native builds (compile-core-native) fail on Mac OSX due to dependency on 'ldd' or 'objdump',"When trying to build the patch for HDFS-7432, which is a NativeIO implementation for chmod, it became clear that building the ""compile-core-native"" target doesn't work on Mac, apparently due to:  {{[exec] checking zlib.h presence... configure: error: Can't find either 'objdump' or 'ldd' to compute the dynamic library for '-lz'}}    These utility programs are not present on Mac.  One source suggests using {{otool -L}} instead.",build
0-byte files retained in the dfs while the FSshell -put is unsuccessful,"The process of putting file into dfs is approximately as follows:  1) create a file in the dfs  2) copy from one stream to the file  But the problem is that the file is still retained in the dfs when the process 2) is terminated abnormally with unexpected exceptions,such as there is no DataNode alive.",fs
HA: fencing mechanism,"In an HA cluster, when there are two NNs, the invariant that only one NN is active at a time has to be preserved in order to prevent ""split brain syndrome."" Thus, when a standby NN is transition to ""active"" state during a failover, it needs to somehow _fence_ the formerly active NN to ensure that it can no longer perform edits. This JIRA is to discuss and implement NN fencing.",ha
Add a vector graphics version of the Hadoop logo,"There is currently no high resolution or vector graphics version of the Hadoop logo. To correct this HP has got the existing PNG logo traced out to vector form, where it can be rendered at any scale. This JIRA issue proposes adding them to the source tree.    One question: where to put it? I propose common/src/docs/src/documentation/resources/images",documentation
Skip registering Configuration instances that don't load default values,"The Configuration constructor registers each instance in a WeakHashMap for reloading if a new default resource is added by way of Configuration.addDefaultResource(). Those configurations that don't declare that they load the defaults (an attribute that is only ever set in the constructor) could skip being registered.    This would deliver minor performance/memory improvements if anyone is creating many instances that don't read in default values. If this is not a common practise, it's not worth adding the fix (which would include making Configuration.loadDefaults final)",conf
Multiple Java installed confuses Hadoop Debian package installer,"When openjdk and sun java are both installed on the machine, Hadoop debian package can not determine correct Java to use.",build
Update HDFS dependency of Java for deb package,"Java dependency for Debian package is specified as open JDK, but it should depends on Sun version of Java.",build
"Add ""-h"" option for FSshell -ls ","Add ""-h"" for FSshell -ls to format the sizes of files in a human-readable format.  For instance:  $hdfs dfs -ls -h /user/hadoop/mylog  -rw-r--r--   1 root supergroup       1.7k 2011-07-26 20:54 /user/hadoop/mylog",fs
TestCLI tests are failing because of -h option in ls command.,None,test
DF should throw a more reasonable exception when mount cannot be determined,"Currently, when using the DF class to determine the mount corresponding to a given directory, it will throw the generic exception ""Expecting a line not the end of stream"" if it can't determine the mount (for example if the directory doesn't exist).    This error message should be improved in several ways:  # If the dir to check doesn't exist, we can see that before even execing df, and throw a better exception (or behave better by chopping path components until it exists)  # Rather than parsing the lines out of df's stdout, collect the whole output, and then parse. So, if df returns a non-zero exit code, we can avoid trying to parse the empty result  # If there's a success exit code, and we still can't parse it (eg incompatible OS), we should include the unparseable line in the exception message.",fs
hadoop command should respect HADOOP_OPTS when given a class name ,When using the hadoop command HADOOP_OPTS and HADOOP_CLIENT_OPTS options are not passeed through.  ,scripts
Add automated test for the RPC IP addr change detection and reconnect feature,Make sure HADOOP-7472 is forward ported to 0.23 by adding an automated test.  ,ipc
[HDFS-362] Provide ShortWritable class in hadoop.,"As part of HDFS-362, Provide the ShortWritable class.  ",io
Add -c option for FSshell -tail,"Add the ""-c"" option for FSshell -tail to allow users to specify the output bytes(currently,it's -1024 by default).  For instance:  $ hdfs dfs -tail -c -10 /user/hadoop/xiexs  or  $ hdfs dfs -tail -c+10 /user/hadoop/xiexs  ",fs
break Maven TAR & bintar profiles into just LAYOUT & TAR proper,"Currently the tar & bintar profile create the layout and create tarball.    For development it would be convenient to break them into layout and tar, thus not having to pay the overhead of TARing up.",build
Remove legacy TAR layout creation,"Currently the build creates 2 different tarball layouts.    One is the legacy one, the layout used until 0.22 (ant tar &  mvn package -Ptar)    The other is new new one, the layout used in trunk that mimics the Unix layout (ant binary & mvn package -Pbintar).    The legacy layout is of not use as all the scripts have been modified to work with the new layout only.    We should thus remove the legacy layout generation.    In addition we could rename the current 'bintar' to just 'tar'  ",build
Add method for doing a sanity check on hostnames in NetUtils,"As part of MAPREDUCE-2489, we need a method in NetUtils to do a sanity check on hostnames",util
Write a script to migrate patches to Maven layout,HADOOP-6671 changed the source directory layout. It would be useful to have a script to fix patches that were written with the old layout.,build
publish Hadoop Common artifacts (post HADOOP-6671) to Apache SNAPSHOTs repo,"A *distributionManagement* section must be added to the hadoop-project POM with the SNAPSHOTs section, then 'mvn deploy' will push the artifacts to it.",build
Client#getRemotePrincipal NPEs when given invalid dfs.*.name,"The following code in Client#getRemotePrincipal NPEs if security is enabled and dfs.https.address, dfs.secondary.http.address, dfs.secondary.https.address, or fs.default.name, has an invalid value (eg hdfs://foo.bar.com.foo.bar.com:1000). We should check address.checkAddress() for null (or check this earlier)  and give a more helpful error message.    {noformat}    return SecurityUtil.getServerPrincipal(conf.get(serverKey), address      .getAddress().getCanonicalHostName());  {noformat}  ","ipc,security"
hadoop-metrics.properties missing some Ganglia31 options ,"The ""jvm"", ""rpc"", and ""ugi"" sections of hadoop-metrics.properties should have Ganglia31 options like ""dfs"" and ""mapred""",metrics
EOFException in RPC stack should have a nicer error message,"Lots of user logs involve a user running mismatched versions, and for some reason or another, they get EOFException instead of a proper version mismatch exception. We should be able to catch this at appropriate points, and have a nicer exception message explaining that it's a possible version mismatch, or that they're trying to connect to the incorrect port.",ipc
hadoopcommon build version cant be set from the maven commandline,"pom.xml had to introduce hadoop.version property with the default value set to the snapshot version. If someone during build time want to override the version from maven command line they can do so by passing -Dhadoop.version="""". For ppl who doesnt want to change the default version can continue building. ",build
jvm metrics all use the same namespace,"Ganglia jvm metrics don't make sense because it's not clear which java process the metrics refer to. In fact, all hadoop java processes running on a node report their jvm metrics to the same namespace.    The metrics are exposed by the ""jvm"" context in JvmMetrics.java. This leads to confusing and nonsensical graphs in ganglia and maybe other monitoring tools.    One way to fix this is to make sure the process name is reported in the jvm context, making it clear which process is associated with the context, and separating out the jvm metrics per process.    This is marked as an ""incompatible change"" because the fix provided removes the JVM metrics and replaces it with process-specific metrics.",metrics
compiled nativelib is in wrong directory and it is not picked up by surefire setup,The location of the compiled native libraries differs from the one surefire plugin (run testcases) is configured to use.    This makes testcases using nativelibs to fail loading them.,build
Tokens should use original hostname provided instead of ip,Tokens currently store the ip:port of the remote server.  This precludes tokens from being used after a host's ip is changed.  Tokens should store the hostname used to make the RPC connection.  This will enable new processes to use their existing tokens.,security
Clicking 'Namenode Logs' link in name node web UI results in 404.,I've never actually seen this link lead to something that's not a 404 no matter what I do with my hadoop cluster.,metrics
Fix example mistake in WritableComparable javadocs,"From IRC, via uberj:    {code}  [9:58pm] uberj: http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/WritableComparable.html  [9:58pm] uberj: In the example it says ""int thatValue = ((IntWritable)o).value;""  [9:59pm] uberj: should 'o' be replaced with 'w'?  [9:59pm] uberj: int thatValue = ((IntWritable)w).value;  {code}    Attaching patch for s/w/o.",documentation
mvn-deploy target fails,"When executing mvn-deploy target, the build fails.  hadoop-common and hadoop-common-sources deploy, but the test jar does not.    property staging is not set and/or set to false, meaning when you try to deploy a snapshot build.    The error reads:  Invalid reference: 'hadoop.core.test'.  ",build
Build fails with ClassCastException when running both mvn-install and mvn-deploy targets,"Although this may not be a common use-case, the exception thrown is really confusing and does not clarify what the problem is.    The resulting error is: java.lang.ClassCastException: org.codehaus.plexus.DefaultPlexusContainer cannot be cast to org.codehaus.plexus.PlexusContainer    The error occurs because mvn-init target gets called twice.",build
test-patch reports the wrong number of javadoc warnings,None,build
hadoop common build fails creating docs,"post hadoop-6671 merge   executing the following command fails on creating docs   $MAVEN_HOME/bin/mvn clean verify checkstyle:checkstyle findbugs:findbugs -DskipTests -Pbintar -Psrc -Pnative -Pdocs    {noformat}  Main:      [mkdir] Created dir: /home/jenkins/jenkins-slave/workspace/Hadoop-Common-trunk-maven/trunk/hadoop-common/target/docs-src       [copy] Copying 33 files to /home/jenkins/jenkins-slave/workspace/Hadoop-Common-trunk-maven/trunk/hadoop-common/target/docs-src  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 1:33.807s  [INFO] Finished at: Fri Aug 05 08:50:43 UTC 2011  [INFO] Final Memory: 35M/462M  [INFO] ------------------------------------------------------------------------  [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (site) on project hadoop-common: An Ant BuildException has occured: Execute failed: java.io.IOException: Cannot run program ""/home/hudson/tools/forrest/latest/bin/forrest"" (in directory ""/home/jenkins/jenkins-slave/workspace/Hadoop-Common-trunk-maven/trunk/hadoop-common/target/docs-src""): java.io.IOException: error=2, No such file or directory -> [Help 1]  [ERROR]   [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.  [ERROR] Re-run Maven using the -X switch to enable full debug logging.  [ERROR]   [ERROR] For more information about the errors and possible solutions, please read the following articles:  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException  [INFO] Scanning for projects...  {noformat}",build
Unable to start HDFS cluster on trunk (after the common mavenisation),"This is what I do:    In common directory:  mvn package -Pbintar -Dmaven.test.skip.exec=true   cp target/hadoop-common-0.23.0-SNAPSHOT-bin.tar.gz ~/tmp/common/    In hdfs directory:  ant veryclean binary -Dresolvers=internal  cp build/hadoop-hdfs-0.23.0-SNAPSHOT-bin.tar.gz ~/tmp/hdfs/    Untar the tarballs, and start namenode as follows:  {quote}  export HADOOP_COMMON_HOME=~vinodkv/tmp/common/hadoop-common-0.23.0-SNAPSHOT-bin;  export HADOOP_CONF_DIR=~vinodkv/tmp/conf;  export HADOOP_HDFS_HOME=~vinodkv/tmp/hdfs/hadoop-hdfs-0.23.0-SNAPSHOT;  $HADOOP_COMMON_HOME/sbin/hadoop-daemon.sh start namenode  {quote}  The last one simply says ""Hadoop common not found."" and exits.",scripts
hadoop fs commands should support tar/gzip or an equivalent,"The ""hadoop fs"" subcommand should offer options for batching, unbatching, compressing, and uncompressing files on hdfs.  The equivalent of ""hadoop fs -tar"" or ""hadoop fs -gzip"".  These commands would greatly facilitate moving large data (especially in a large number of files) back and forth from hdfs.",fs
hadoop-main fails to deploy,Doing a Maven deployment hadoop-main (trunk/pom.xml) fails to deploy because it does not have the distribution management information.,build
bintar created tarball should use a common directory for prefix,"The binary tarball contains the directory structure like:    {noformat}  hadoop-common-0.23.0-SNAPSHOT-bin/bin                                   /etc/hadoop                                   /libexec                                   /sbin                                   /share/hadoop/common  {noformat}    It would be nice to rename the prefix directory to a common directory where it is common to all Hadoop stack software.  Therefore, user can untar hbase, hadoop, zookeeper, pig, hive all into the same location and run from the top level directory without manually renaming them to the same directory again.    By default the prefix directory can be /usr.  Hence, it could merge with the base OS.",build
Abandon support for expanding ${user.name} in configs,"In practice, I find that the presence of ${user.name} in configs is rarely helpful, and often causes issues.",conf
Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException,"Test org.apache.hadoop.fs.TestFilterFileSystem fails due to java.lang.NoSuchMethodException. Here is the error message:    -------------------------------------------------------------------------------  Test set: org.apache.hadoop.fs.TestFilterFileSystem  -------------------------------------------------------------------------------  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.232 sec <<< FAILURE!  testFilterFileSystem(org.apache.hadoop.fs.TestFilterFileSystem)  Time elapsed: 0.075 sec  <<< ERROR!  java.lang.NoSuchMethodException: org.apache.hadoop.fs.FilterFileSystem.copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean)    java.lang.Class.getDeclaredMethod(Class.java:1937)    org.apache.hadoop.fs.TestFilterFileSystem.testFilterFileSystem(TestFilterFileSystem.java:157)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    junit.framework.TestCase.runTest(TestCase.java:168)    junit.framework.TestCase.runBare(TestCase.java:134)    junit.framework.TestResult$1.protect(TestResult.java:110)    junit.framework.TestResult.runProtected(TestResult.java:128)    junit.framework.TestResult.run(TestResult.java:113)    junit.framework.TestCase.run(TestCase.java:124)    junit.framework.TestSuite.runTest(TestSuite.java:232)    junit.framework.TestSuite.run(TestSuite.java:227)    org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)    org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)    org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)    org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)    org.apache.maven.surefire.Surefire.run(Surefire.java:104)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)    org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)    This prevents a clean build.",test
Change RPC to allow multiple protocols including multiple versions of the same protocol,None,ipc
Make arguments to test-patch optional,"Currently you have to specify all the arguments to test-patch.sh, which makes it cumbersome to use. We should make all arguments except the patch file optional. ",scripts
Add TestPath tests for URI conversion and reserved characters  ,TestPath needs tests that cover URI conversion (eg places where Paths and URIs differ) and handling of URI reserved characters in paths. ,fs
Maven build fails in Windows,Maven does not run in window for the following reasons:    * Enforcer plugin restricts build to Unix  * Ant run snippets to create TAR are not cygwin friendly,build
Possible deadlock in metrics2,Lock cycle detected by jcarder between MetricsSystemImpl and DefaultMetricsSystem,metrics
"The landing pages for Apache Hadoop website has an incorrect spelling for the word ""Foundation"" in the Trademark statement","The landing pages for at least these URLs has an incorrect spelling for the word ""Foundation"" in the Trademark statement in the page footer:    http://hadoop.apache.org/  http://hadoop.apache.org/common/  http://hadoop.apache.org/hdfs/  http://hadoop.apache.org/mapreduce/    Needs someone with the necessary privilges to change the spelling from ""Foundataion"" to ""Foundation"".",documentation
Add servlet util methods for handling paths in requests ,Common side of HDFS-2235.,util
Make test-patch.sh ensure that dependent projects can compile,"Every once in a while a change in Common causes HDFS or M/R to not compile, or a change in HDFS causes M/R to not compile. With the project unsplit, it wouldn't be difficult to make test-patch.sh simply ensure that all the Hadoop sub-projects continue to compile for any given change.",test
Correct the dependency version regressions introduced in HADOOP-6671,"I just noticed the versions specified for dependencies have gone backward with HADOOP-6671.  To name a few,  * commons-logging  was 1.1.1, now 1.0.4  * commons-logging-api  was 1.1, now 1.0.4  * slf4 was 1.5.11, now 1.5.8    There might be more.",build
Add PowerMock for the development of better tests,"We already have Mockito, but PowerMock extends its capabilties so that we can mock constructors and static methods. I find that it is extremely difficult, if not impossible, to properly test some of the low-level features without this. ",build
Add dense update option for metrics2 file sink,"Currently, if File sink is enabled for MRAppMaster or Resourcemanager, it does not populate the file with all the available attributes. It would be useful for debugging and admin purpose to have all the metrics populated in the file.    For eg: MRAppMaster metrics currently logs value only for JobsRunning even though the total available job level metrics are JobsCompleted, JobsFailed, JobsKilled, JobsPreparing etc    ",metrics
Dependencies should be revisited,Some transitive dependencies seem that are not being used.    As a follow up to HADOOP-7934 and HADOOP-7935 we should do a purging of unused dependencies.,build
Add issuer field to delegation tokens,Tokens currently lack traceability to its issuer.  This complicates the ability to reliably renew tokens.  Tokens should have an optional issuer.,security
common -tests jar should not include properties and configs,"This is the cause of HDFS-2242. The -tests jar generated from the common build should only include the test classes, and not the test resources.","build,test"
Add -n option for FSshell -tail,"Add the ""-n"" option for FSshell -tail to allow users to specify the output lines.  ",fs
Fix the warning in writable classes.[ WritableComparable is a raw type. References to generic type WritableComparable<T> should be parameterized  ],WritableComparable is a raw type. References to generic type WritableComparable<T> should be parameterized.    Also address the same in example implementation in WritableComparable interface's javadoc.,io
ant binary target fails if native has not been built,"The ""binary"" target on branch-0.20-security fails with the following, it assumes the native dir exists.    BUILD FAILED  /home/eli/src/hadoop-branch-0.20-security/build.xml:1572: /home/eli/src/hadoop-branch-0.20-security/build/hadoop-0.20.206.0-SNAPSHOT/native not found.  ",build
Use JDK ServiceLoader mechanism to find FileSystem implementations,"Currently configuring FileSystem implementations must be done by declaring the FileSystem class in the Hadoop configuration files (core-default.xml, ...).    Using JDK ServiceLoader mechanism this configuration step can be avoided. Adding the JAR file with the additional FileSystem implementation would suffice.     This is similar to what is being proposed for compression codecs (HADOOP-7350).  ",fs
Need for Integrity Validation of RPC,"Some recent investigation of network packet corruption has shown a need for hadoop RPC integrity validation beyond assurances already provided by 802.3 link layer and TCP 16-bit CRC.    During an unusual occurrence on a 4k node cluster, we've seen as high as 4 TCP anomalies per second on a single node, sustained over an hour (14k per hour).   A TCP anomaly  would be an escaped link layer packet that resulted in a TCP CRC failure, TCP packet out of sequence  or TCP packet size error.    According to this paper[*]:  http://tinyurl.com/3aue72r  TCP's 16-bit CRC has an effective detection rate of 2^10.   1 in 1024 errors may escape detection, and in fact what originally alerted us to this issue was seeing failures due to bit-errors in hadoop traffic.  Extrapolating from that paper, one might expect 14 escaped packet errors per hour for that single node of a 4k cluster.  While the above error rate  was unusually high due to a broadband aggregate switch issue, hadoop not having an integrity check on RPC makes it problematic to discover, and limit any potential data damage due to  acting on a corrupt RPC message.        ------  [*] In case this jira outlives that tinyurl, the IEEE paper cited is:  ""Performance of Checksums and CRCs over Real Data"" by Jonathan Stone, Michael Greenwald, Craig Partridge, Jim Hughes.  ",ipc
LocalDirAllocator should incorporate LocalStorage,"The o.a.h.fs.LocalDirAllocator is not aware of o.a.h.m.t.LocalStorage (introduced in MAPREDUCE-2413) - it always considers the configured local dirs, not just the ones that happen to be good. Therefore if there's a disk failure then *every* call to get a local path will result in LocalDirAllocator#confChanged doing a disk check of *all* the configured local dirs. It seems like LocalStorage should be a private class to LocalAllocator so that all users of LocalDirAllocator benefit from the disk failure handling and all the various users of LocalDirAllocator don't have to be modified to handle disk failures. Note that LocalDirAllocator already handles faulty directories.",fs
FileUtil#fullyDelete doesn't throw IOE but lists it in the throws clause,"FileUtil#fullyDelete doesn't throw IOException so it shouldn't have IOException in its throws clause. Having it listed makes it easy to think you'll get an IOException eg trying to delete a non-existant file or on an IO error accessing the local file, but you don't.",fs
hadoop-common tries to find hadoop-assemblies:jar:0.23.0-SNAPSHOT in http://snapshots.repository.codehaus.org ,hadoop-common tries to find hadoop-assemblies:jar:0.23.0-SNAPSHOT in http://snapshots.repository.codehaus.org - shouldn't it be apache repo?,build
Use 'assembly' rather than '-Ptar' for creating the tarball,AFAIK assembly is the way to create artifacts - currently we use profiles (-Ptar or -Pbintar).     MapReduce uses assembly via MAPREDUCE-279.    I'd like to see a common way across the whole project. Thoughts?,build
Add a eclipse-generated files to .gitignore,"The .gitignore file in the hadoop-mapreduce directory specifically excludes .classpath, .settings, and .project files/dirs. We should move these excludes to the top level .gitignore so that Common and HDFS have these files excluded as well.",build
Add system tests instrumentation to the maven build,AOP is used for fault injection tests as well as for cluster system validation (aka Herriot). The latter has been missed from the scope of this work.,build
"hadoop-config.sh setup CLASSPATH, HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME incorrectly",HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME was set to HADOOP_PREFIX/share/hadoop/hdfs and HADOOP_PREFIX/share/hadoop/mapreduce.  This setup confuses the location of hdfs and mapred scripts.  Instead the script should look for hdfs and mapred script in HADOOP_PREFIX/bin.,scripts
TestSecureIOUtils and TestTFileSeqFileComparison are flaky,https://builds.apache.org/job/Hadoop-Common-trunk-Commit/764/,io
mvn eclipse:eclipse fails for hadoop-alredo,"[INFO] Resource directory's path matches an existing source directory. Resources will be merged with the source directory src/test/resources  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]   [INFO] Apache Hadoop Project POM ......................... SUCCESS [18.780s]  [INFO] Apache Hadoop Annotations ......................... SUCCESS [0.173s]  [INFO] Apache Hadoop Project Dist POM .................... SUCCESS [0.114s]  [INFO] Apache Hadoop Assemblies .......................... SUCCESS [0.145s]  [INFO] Apache Hadoop Alfredo ............................. FAILURE [0.296s]  [INFO] Apache Hadoop Common .............................. SKIPPED  [INFO] Apache Hadoop HDFS ................................ SKIPPED  [INFO] Apache Hadoop Main ................................ SKIPPED  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 42.627s  [INFO] Finished at: Mon Aug 22 15:41:47 PDT 2011  [INFO] Final Memory: 12M/265M  [INFO] ------------------------------------------------------------------------  [ERROR] Failed to execute goal org.apache.maven.plugins:maven-eclipse-plugin:2.8:eclipse (default-cli) on project hadoop-alfredo: Request to merge when 'filtering' is not identical. Original=resource src/test/resources: output=target/test-classes, include=[krb5.conf], exclude=[**/*.java], test=true, filtering=true, merging with=resource src/test/resources: output=target/test-classes, include=[], exclude=[krb5.conf|**/*.java], test=true, filtering=false -> [Help 1]  [ERROR]   ",build
SequenceFile should not print into stdout,"The following line in {{SequenceFile.Reader.initialize()}} should be removed:  {code}  System.out.println(""Setting end to "" + end);  {code}  ",io
Remove common start-all.sh,MAPREDUCE-2736 removes start-mapred.sh. We should either update the call to start-mapred to hadoop-yarn/bin/start-all.sh instead or just remove the script since it's deprecated.,scripts
hadoop-config.sh needs to be updated post MR2,hadoop-common/src/main/bin/hadoop-config.sh needs to be updated post MR2 (eg the layout of mapred home has changed).,scripts
hadoop-config.sh needs to be updated post mavenization,hadoop-common/src/main/bin/hadoop-config.sh needs to be updated post mavenization (eg it still refers to build/classes etc).,scripts
Ability to run the daemons from source trees,"It's very useful for developers to be able to run the daemons w/o building/deploying tarballs. This feature used to work, is now very out of date post RPM and maven related changes.",scripts
hadoop should log configuration reads,"For debugging, it would often be valuable to know which configuration options ever got read out of the Configuration into the rest of the program -- an unread option didn't cause a problem. This patch logs the first time each option is read.",conf
Improvement for FSshell -stat,"Add two optional formats for FSshell -stat, one is %G for group name of owner and the other is %U for user name.",fs
Support fully qualified paths as part of LocalDirAllocator,Contexts with configuration path strings using fully qualified paths (e.g. file:///tmp instead of /tmp) mistakenly creates a directory named 'file:' and sub-directories in the current local file system working directory.,fs
Fix findbugs warnings in Hadoop Auth (Alfredo),Found in HADOOP-7567: https://builds.apache.org/job/PreCommit-HADOOP-Build/65//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-alfredo.html,security
TT does not start due to backwards compatibility wrt. EventCounter,"Between metrics1 and mertrics2 EventCounter was moved from o.a.h.log to o.a.h.metrics.jvm.  On 0.20-security a wrapper marked with @Deprecated was added back to o.a.h.log for compatibility, the same wrapper exists on trunk, but no on 0.22.    Without it the TT will fail to start with a ClassNotFoundException.  Hive configuration also point to this class in the log4j.properties.  ",metrics
Rename package names from alfredo to auth,None,security
Truncate stack traces coming out of RPC,Currently stack traces logged and sent back as part of RPC responses have a lot of cruft on them from the internals of the IPC stack. These aren't particularly useful for users or developers - it would be nicer to truncate these portions of the trace.,ipc
Hadoop daemon does not clean up pid file upon normal shutdown,"hadoop-daemon.sh script created pid files on when namenode, jobtrackers, etc. deamons are started.  When the deamon is stopped in a regular manner through the stop script, it should clean up the pid file to indicate normal controlled exit.    Presence of a pid without running corresponding process would then indicated abnormal end of the process.",scripts
hadoop-auth module has 4 findbugs warnings,"Precommit builds are all assigning ""-1 findbugs"" due to the following four findbugs warning:  Found 4 Findbugs warnings (/home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/target/findbugsXml.xml)  Due to some issues with the current hudson setup, I'm not sure what the warnings are, but clear that's where they're coming from.","build,security"
Prefer mvn test -DskipTests over mvn compile in test-patch.sh,"I got a failure running test-patch with a clean .m2 directory.    To quote Alejandro:  {quote}  The reason for this failure is because of how Maven reactor/dependency  resolution works (IMO a bug).    Maven reactor/dependency resolution is smart enough to create the classpath  using the classes from all modules being built.    However, this smartness falls short just a bit. The dependencies are  resolved using the deepest maven phase used by current mvn invocation. If  you are doing 'mvn compile' you don't get to the test compile phase.  This  means that the TEST classes are not resolved from the build but from the  cache/repo.    The solution is to run 'mvn test -DskipTests' instead 'mvn compile'. This  will include the TEST classes from the build.  {quote}    So this is to replace mvn compile in test-patch.sh with mvn test -DskipTests",build
Mavenize streaming and MR examples,MR1 code is still available in MR2 for testing contribs.    While this is a temporary until contribs tests are ported to MR2.    As a follow up the contrib projects themselves should be mavenized.,build
JobHistory cleanup task causing JT going OOM,"It's failing because of Hadoop JobHistory cleanup task.  JobHistory cleanup task, tries to clean up all the done task from the JT_JOBHISTORY_COMPLETED_LOCATION(/home/y/logs/hadoop/history/done).  the directory  has around 3206925 entries, it's load all files info from directory which cause OOM exception.  ",util
AssertionError in TestHttpServer.testMaxThreads(),"TestHttpServer passed but there were AssertionError in the output.  {noformat}  11/08/30 03:35:56 INFO http.TestHttpServer: HTTP server started: http://localhost:52974/  Exception in thread ""pool-1-thread-61"" java.lang.AssertionError:     org.junit.Assert.fail(Assert.java:91)    org.junit.Assert.assertTrue(Assert.java:43)    org.junit.Assert.assertTrue(Assert.java:54)    org.apache.hadoop.http.TestHttpServer$1.run(TestHttpServer.java:164)    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)    java.lang.Thread.run(Thread.java:680)  {noformat}   ",test
Upgrade dependency to Avro 1.5.3,Avro 1.5.3 depends on Snappy-Java 1.5.3 which enables the use of its SO file from the java.library.path,build
Enable jsvc to work with Hadoop RPM package,"For secure Hadoop 0.20.2xx cluster, datanode can only run with 32 bit jvm because Hadoop only packages 32 bit jsvc.  The build process should download proper jsvc versions base on the build architecture.  In addition, the shell script should be enhanced to locate hadoop jar files in the proper location.",build
dfs -chmod does not work as expected,"The operator ""="" of chmod should have the top priority,i.e,the rest operators should be omitted if the first operator is ""="".  For example:  # hdfs dfs -ls /user/  dr--r--r--   - root supergroup          0 2011-08-31 19:42 /user/hadoop  # hdfs dfs -chmod =+w /user/hadoop  # hdfs dfs -ls /user/  d-w--w--w-   - root supergroup          0 2011-08-31 19:42 /user/hadoop    ",fs
smart-apply-patch.sh does not handle patching from a sub directory correctly.,"smart-apply-patch.sh does not apply valid patches from trunk, or from git like it was designed to do in some situations.",build
Improve hadoop setup conf script to setup secure Hadoop cluster,Setting up a secure Hadoop cluster requires a lot of manual setup.  The motivation of this jira is to provide setup scripts to automate setup secure Hadoop cluster.,scripts
Move common fs implementations to a hadoop-fs module,"Much of the hadoop-common dependencies is from the fs implementations. We have more fs implementations on the way (ceph, lafs etc). I propose that we move all the fs implementations to a hadoop-fs module under hadoop-common-project.",fs
bin-package target shouldn't fail if you aren't building the native libraries,"Currently the bin-package target fails if the native directory doesn't exist, but there are many cases where that combination makes sense.",build
Upgrade Jackson to version 1.7.1 to match the version required by Jersey,"As of 2 days ago, 13 tests started failing, all with errors in Avro-related tests.",test
Simplify the RPC proxy cleanup process,"The process to clean up an RPC proxy object is to call RPC.stopProxy, which looks up the RPCEngine previously associated with the interface which that proxy object provides and calls RPCEngine.stopProxy passing in the proxy object. Every concrete implementation of RPCEngine.stopProxy then looks up the invocation handler associated with the proxy object and calls close() on that invocation handler.    This process can be simplified by cutting out the steps of looking up the previously-registered RPCEngine, and instead just having RPC.stopProxy directly look up the invocation handler for the proxy object and call close() on it.",ipc
SnappyCodec check for Hadoop native lib is wrong,"Currently SnappyCodec is doing:    {code}    public static boolean isNativeSnappyLoaded(Configuration conf) {      return LoadSnappy.isLoaded() && conf.getBoolean(          CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_KEY,          CommonConfigurationKeys.IO_NATIVE_LIB_AVAILABLE_DEFAULT);    }  {code}    But the conf check is wrong as it defaults to true. Instead it should use *NativeCodeLoader.isNativeCodeLoaded()*",io
Debian package shows invalid hdfs user,"First time install debian package on Debian machine, there is a error message showing:    invalid hdfs user.  invalid mapred user.    Looks like the users are not created during the installation.  Not sure if this is EC2 related or debian related.  Investigating...",scripts
/etc/profile.d does not exist on Debian,"As part of post installation script, there is a symlink created in /etc/profile.d/hadoop-env.sh to source /etc/hadoop/hadoop-env.sh.  Therefore, users do not need to configure HADOOP_* environment.  Unfortunately, /etc/profile.d only exists in Ubuntu.  [Section 9.9 of the Debian Policy|http://www.debian.org/doc/debian-policy/ch-opersys.html#s9.9] states:    {quote}  A program must not depend on environment variables to get reasonable defaults. (That's because these environment variables would have to be set in a system-wide configuration file like /etc/profile, which is not supported by all shells.)    If a program usually depends on environment variables for its configuration, the program should be changed to fall back to a reasonable default configuration if these environment variables are not present. If this cannot be done easily (e.g., if the source code of a non-free program is not available), the program must be replaced by a small ""wrapper"" shell script which sets the environment variables if they are not already defined, and calls the original program.    Here is an example of a wrapper script for this purpose:    {noformat}       #!/bin/sh       BAR=${BAR:-/var/lib/fubar}       export BAR       exec /usr/lib/foo/foo ""$@""  {noformat}    Furthermore, as /etc/profile is a configuration file of the base-files package, other packages must not put any environment variables or other commands into that file.  {quote}    Hence the default environment setup should skip for Debian.  ",scripts
SequenceFile.Sorter creates local temp files on HDFS,"When using SequenceFile.Sorter to sort or merge sequence files that exist in HDFS, it attempts to create temp files in a directory structure specified by mapred.local.dir but on HDFS, not in the local file system. The problem code is in MergeQueue.merge(). Starting at line 2953:  {code}              Path outputFile =  lDirAlloc.getLocalPathForWrite(                                                  tmpFilename.toString(),                                                  approxOutputSize, conf);              LOG.debug(""writing intermediate results to "" + outputFile);              Writer writer = cloneFileAttributes(                                                  fs.makeQualified(segmentsToMerge.get(0).segmentPathName),                                                   fs.makeQualified(outputFile), null);  {code}  The outputFile here is a local path without a scheme, e.g. ""/mnt/mnt1/mapred/local"", specified by the mapred.local.dir property. If we are sorting files on HDFS, the fs object is a DistributedFileSystem. The call to fs.makeQualified(outputFile) appends the fs object's scheme to the local temp path returned by lDirAlloc, e.g. hdfs://mnt/mnt1/mapred/local. This directory is then created (if the proper permissions are available) on HDFS. If the HDFS permissions are not available, the sort/merge fails even though the directories exist locally.    The code should instead always use the local file system if retrieving a path from the mapred.local.dir property. The unit tests do not test this condition, they only test using the local file system for sort and merge.    ",io
Change test-patch to run tests for all nested modules,"HADOOP-7561 changed the behaviour of test-patch to run tests for changed modules, however this was assuming a flat structure. Given the nested maven hierarchy we should always run all the common tests for any common change, all the HDFS tests for any HDFS change, and all the MapReduce tests for any MapReduce change.    In addition, we should do a top-level build to test compilation after any change.",build
invalid common pom breaking HDFS build,"[WARNING] The POM for org.apache.hadoop:hadoop-common:jar:0.24.0-20110905.195220-41 is invalid, transitive dependencies (if any) will not be available: 5 problems were encountered while building the effective model for org.apache.hadoop:hadoop-common:0.24.0-SNAPSHOT  [ERROR] 'dependencies.dependency.version' for asm:asm:jar is missing. @   [ERROR] 'dependencies.dependency.version' for com.sun.jersey:jersey-core:jar is missing. @   [ERROR] 'dependencies.dependency.version' for com.sun.jersey:jersey-json:jar is missing. @   [ERROR] 'dependencies.dependency.version' for com.sun.jersey:jersey-server:jar is missing. @   [ERROR] 'dependencies.dependency.version' for org.apache.hadoop:hadoop-auth:jar is missing. @   ",build
Reloading configuration when using imputstream resources results in org.xml.sax.SAXParseException,"When using an inputstream as a resource for configuration, reloading this configuration will throw the following exception:    Exception in thread ""main"" java.lang.RuntimeException: org.xml.sax.SAXParseException: Premature end of file.    org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1576)    org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1445)    org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1381)    org.apache.hadoop.conf.Configuration.get(Configuration.java:569)  ...  Caused by: org.xml.sax.SAXParseException: Premature end of file.    com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:249)    com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)    javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:124)    org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1504)   ... 4 more    To reproduce see following testcode:      Configuration conf = new Configuration();      ByteArrayInputStream bais = new ByteArrayInputStream(""<configuration></configuration>"".getBytes());      conf.addResource(bais);      System.out.println(conf.get(""blah""));      conf.addResource(""core-site.xml""); //just add a named resource, doesn't matter which one      System.out.println(conf.get(""blah""));    Allowing inputstream resources is flexible, but in cases such as this in can lead to difficult to debug problems.    What do you think is the best solution? We could:  A) reset the inputstream after it is read instead of closing it (but what to do when the stream does not support marking?)  B) leave it up to the client (for example make sure you implement close() so that it resets the steam)  C) when reading the inputstream for the first time, cache or wrap the contents somehow so that is can be read multiple times (let's at least document it)  D) remove inputstream method altogether  e) something else?    For now I have attached a patch for solution A.",conf
Binary layout does not put share/hadoop/contrib/*.jar into the class path,"For contrib projects, contrib jar files are not included in HADOOP_CLASSPATH in the binary layout.  Several projects jar files should be copied to $HADOOP_PREFIX/share/hadoop/lib for binary deployment.  The interesting jar files to include in $HADOOP_PREFIX/share/hadoop/lib are: capacity-scheduler, thriftfs, fairscheduler.",scripts
Fix duplicate Jenkins integration JIRA comments and Maven deployments,After a change is committed the corresponding JIRA is updated with multiple comments saying the same thing. E.g.    https://issues.apache.org/jira/browse/HADOOP-7612?focusedCommentId=13100560&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13100560,build
Make Hadoop authentication filter display some content when authentication fails,"Currently if one enables the built-in SPNEGO authentication filter on Hadoop and authentication fails, a 401 is returned with a blank page. We should respond with some, potentially customizable, content which says why the failure occurred and what the user might need to do to fix it.",security
alfredo config should be in a file not readable by users,"[thxs ATM for point this one out]    Alfredo configuration currently is stored in the core-site.xml file, this file is readable by users (it must be as Configuration defaults must be loaded).    One of Alfredo config values is a secret which is used by all nodes to sign/verify the authentication cookie.    A user could get hold of this secret and forge authentication cookies for other users.    Because of this the Alfredo configuration, should be move to a user non-readable file.",security
S3FileSystem reports block-size as length of File if file length is less than a block,in S3FileSystem create a File with Block size as 67108864.  Write some data in file of size 2048 (less than 67108864)  Assert the block size of the file. the block size reported will be 2048 rather than 67108864.    This behavior is not inline with HDFS.,fs/s3
Set things up for a top level hadoop-tools module,See this thread: http://markmail.org/thread/cxtz3i6lvztfgfxn    We need to get things up and running for a top level hadoop-tools module. DistCpV2 will be the first resident of this new home. Things we need:   - The module itself and a top level pom with appropriate dependencies   - Integration with the patch builds for the new module   - Integration with the post-commit and nightly builds for the new module.,build
Allow overwrite of HADOOP_CLASSPATH and HADOOP_OPTS,"Quote email from Ashutosh Chauhan:    bq. There is a bug in hadoop-env.sh which prevents hcatalog server to start in secure settings. Instead of adding classpath, it overrides them. I was not able to verify where the bug belongs to, in HMS or in hadoop scripts. Looks like hadoop-env.sh is generated from hadoop-env.sh.template in installation process by HMS. Hand crafted patch follows:    bq. - export HADOOP_CLASSPATH=$f  bq. +export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:$f    bq. -export HADOOP_OPTS=""-Djava.net.preferIPv4Stack=true ""  bq. +export HADOOP_OPTS=""${HADOOP_OPTS} -Djava.net.preferIPv4Stack=true ""  ",scripts
Improve MetricsAsserts to give more understandable output on failure,"In developing a test case that uses MetricsAsserts, I had two issues:  1) the error output in the case that an assertion failed does not currently give any information as to the _actual_ value of the metric  2) there is no way to retrieve the metric variable (eg to assert that the sum of a metric over all DNs is equal to some value)    This JIRA is to improve this test class to fix the above issues.","metrics,test"
hadoop-metrics2.properties should have a property *.period set to a default value foe metrics,currently the hadoop-metrics2.properties file does not have a value set for *.period    This property is useful for metrics to determine when the property will refresh. We should set it to default of 60,conf
"In mapred-site.xml, stream.tmpdir is mapped to ${mapred.temp.dir} which is undeclared.","Streaming jobs seem to fail with the following exception:    {noformat}  Exception in thread ""main"" java.io.IOException: No such file or directory          at java.io.UnixFileSystem.createFileExclusively(Native Method)          at java.io.File.checkAndCreate(File.java:1704)          at java.io.File.createTempFile(File.java:1792)          at org.apache.hadoop.streaming.StreamJob.packageJobJar(StreamJob.java:603)          at org.apache.hadoop.streaming.StreamJob.setJobConf(StreamJob.java:798)          at org.apache.hadoop.streaming.StreamJob.run(StreamJob.java:117)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)          at org.apache.hadoop.streaming.HadoopStreaming.main(HadoopStreaming.java:32)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.util.RunJar.main(RunJar.java:156)  {noformat}    Eric pointed out that in RPM based installs, in /etc/hadoop/mapred-site.xml, stream.tmpdir is mapped to ${mapred.temp.dir}, but ${mapred.temp.dir} is not declared",conf
NPE in copyToLocal,[todd@c0309 hadoop-trunk-home]$ ./bin/hadoop fs -copyToLocal /hbase/.META./1028785192/ /tmp/meta/  copyToLocal: Fatal internal error  java.lang.NullPointerException          at org.apache.hadoop.fs.shell.PathData.getPathDataForChild(PathData.java:182)          at org.apache.hadoop.fs.shell.CommandWithDestination.processPaths(CommandWithDestination.java:115)          at org.apache.hadoop.fs.shell.Command.recursePath(Command.java:329)          at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:302)          at org.apache.hadoop.fs.shell.CommandWithDestination.processPaths(CommandWithDestination.java:116)          at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:272)          at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:255)          at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:239)          at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:105)          at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:185)          at org.apache.hadoop.fs.shell.Command.run(Command.java:149)  ,fs
log4j.properties should be added to the hadoop conf on deploy,currently the log4j properties are not present in the hadoop conf dir. We should add them so that log rotation happens appropriately and also define other logs that hadoop can generate for example the audit and the auth logs as well as the mapred summary logs etc.,conf
Cluster setup docs specify wrong owner for task-controller.cfg ,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,"documentation,security"
RetryInvocationHandler should release underlying resources on close,"It is often the case that RPC invocation handlers (e.g. {{o.a.h.ipc.WritableRpcEngine.Invoker}}) are wrapped in a {{RetryInvocationHandler}} instance to handle RPC retry logic. Since {{RetryInvocationHandler}} doesn't have any resources of its own, and is incapable of releasing the resources of the wrapped {{InvocationHandler}}, users of {{RetryInvocationHandler}} must keep around a reference to the underlying {{InvocationHandler}} only for the purpose of closing. For an example of this, see {{o.a.h.hdfs.DFSClient}}, in particular the member variables {{namenode}} and {{rpcNamenode}}.",ipc
Fair scheduler configuration file is not bundled in RPM,"205 build of tar is fine, but rpm failed with:    {noformat}        [rpm] Processing files: hadoop-0.20.205.0-1        [rpm] warning: File listed twice: /usr/libexec        [rpm] warning: File listed twice: /usr/libexec/hadoop-config.sh        [rpm] warning: File listed twice: /usr/libexec/jsvc.i386        [rpm] Checking for unpackaged file(s): /usr/lib/rpm/check-files /tmp/hadoop_package_build_hortonfo/BUILD        [rpm] error: Installed (but unpackaged) file(s) found:        [rpm]    /etc/hadoop/fair-scheduler.xml        [rpm]     File listed twice: /usr/libexec        [rpm]     File listed twice: /usr/libexec/hadoop-config.sh        [rpm]     File listed twice: /usr/libexec/jsvc.i386        [rpm]     Installed (but unpackaged) file(s) found:        [rpm]    /etc/hadoop/fair-scheduler.xml        [rpm]         [rpm]         [rpm] RPM build errors:    BUILD FAILED  /grid/0/dev/mfoley/hadoop-0.20-security-205/build.xml:1747: '/usr/bin/rpmbuild' failed with exit code 1  {noformat}",build
Add Apache License to template config files,Files in hadoop-common-project/hadoop-common/src/main/packages/templates/conf don't have Apache Software License in the header.,build
create hadoop-dist module where TAR stitching would happen,"Instead having a post build script that stitches common&hdfs&mmr, this should be done as part of the build when running 'mvn package -Pdist -Dtar'    ",build
Bump up the version of aspectj,"When the fault injection target is enabled, aspectj fails with the following message:  ""Can't parameterize a member of non-generic type:""    This is fixed by upgrading aspectj. I tested with 1.6.11 and it worked.  It will also apply to trunk, but I believe trunk has other problems.    ","build,test"
Fix the delegation token tests to use the new style renewers,"Currently, TestDelegationTokenRenewal and TestDelegationTokenFetcher use the old style renewal and fail.    ",security
HTTP auth tests requiring Kerberos infrastructure are not disabled on branch-0.20-security,"The back-port of HADOOP-7119 to branch-0.20-security included tests which require Kerberos infrastructure in order to run. In trunk and 0.23, these are disabled unless one enables the {{testKerberos}} maven profile. In branch-0.20-security, these tests are always run regardless, and so fail most of the time.    See this Jenkins build for an example: https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-0.20-security/26/",security
Make hadoop-common use same version of avro as HBase,"HBase depends on avro 1.5.3 whereas hadoop-common depends on 1.3.2.  When building HBase on top of hadoop, this should be consistent.","io,ipc"
Update bylaws to reflect PMC chair rotation,The PMC voted to rotate the chair annually. Let's update the bylaws accordingly.,documentation
Update date from 2009 to 2011,Build files contains year parameter that shows up in the UI.  Some of the documentation has 2009 copyrights on them.,"build,documentation"
TestMapredGroupMappingServiceRefresh and TestRefreshUserMappings  fail after HADOOP-7625,"TestMapredGroupMappingServiceRefresh and TestRefreshUserMappings  fail after HADOOP-7625.  The classpath has been changed, so they try to create the rsrc file in a jar and fail.  ","security,test"
Automated test for the tarballs,Currently we don't test the generated tarball so we don't automatically detect changes that break the artifact generation or bin scripts (eg HADOOP-7356). We should have a simple functional test run from Jenkins that covers this stuff.    I wrote some simple automation for 20x tarballs that starts a pseudo cluster with multiple DNs and TTs and runs basic HDFS commands and MR jobs (with and w/o the LinuxTaskController) that could be used as a starting point. Code lives here: https://github.com/elicollins/hadoop-dev/tree/pseudo    Eric also added a lot of cluster config generation in HADOOP-7599 that could be used as a starting point as well.    This should also support trunk (Yarn) of course.,test
Hadoop Record compiler generates Java files with erroneous byte-array lengths for fields trailing a 'ustring' field,"Hadoop Record compiler produces Java files from a DDL file. If a DDL file has a class that contains a 'ustring' field, then the generated 'compareRaw()' function for this record is erroneous in computing the length of remaining bytes after the logic of computing the buffer segment for a 'ustring' field.    Below is a line in a generated 'compareRaw()' function for a record class with a 'ustring' field :            s1+=i1; s2+=i2; l1-=i1; l1-=i2;  This line shoud be corrected by changing the last 'l1' to 'l2':            s1+=i1; s2+=i2; l1-=i1; l2-=i2;    To fix this bug, one should correct the 'genCompareBytes()' function in the 'JString.java' file of the package 'org.apache.hadoop.record.compiler' by changing the line below to the ensuing line. There is only one digit difference:          cb.append(""s1+=i1; s2+=i2; l1-=i1; l1-=i2;\n"");          cb.append(""s1+=i1; s2+=i2; l1-=i1; l2-=i2;\n"");    This bug is serious as it will always crash unserializing a record with a simple definition like the one below  class PairStringDouble {    ustring first;    double  second;  }  Unserializing a record of this class will throw an exception as the 'second' field does not have 8 bytes for a double value due to the erroneous length computation for the remaining buffer.    Both Hadoop 0.20 and 0.21 have this bug.",record
"Provide a mechanism for a client Hadoop configuration to 'poison' daemon startup; i.e., disallow daemon start up on a client config.","We've seen folks who have been given Hadoop configuration to act as a client accidentally type ""hadoop namenode"" and get things into a confused, or incorrect state.  Most recently, we've seen data corruption when users accidentally run extra secondary namenodes (https://issues.apache.org/jira/browse/HDFS-2305).    I'd like to propose that we introduce a configuration property, say, ""client.poison.servers"", which, if set, disables the Hadoop daemons (nn, snn, jt, tt, etc.) with a reasonable error message.  Hadoop administrators can hand out/install configs that are on machines intended to just be clients with a little less worry that they'll accidentally get run.  ","conf,scripts"
tarball doesn't include .eclipse.templates,The hadoop tarball doesn't include .eclipse.templates. This results in a failure to successfully run ant eclipse-files:    eclipse-files:    BUILD FAILED  /home/natty/Downloads/hadoop-0.20.2/build.xml:1606: /home/natty/Downloads/hadoop-0.20.2/.eclipse.templates not found.    ,build
Change maven setup to not run tests by default,"I find that I now type ""-DskipTests"" many, many times per day when working on Hadoop. We should change the default to not run tests except when explicitly given a test target.",build
fs -getmerge isn't guaranteed to work well over non-HDFS filesystems,"When you use {{fs -getmerge}} with HDFS, you are guaranteed file list sorting (part-00000, part-00001, onwards). When you use the same with other FSes we bundle, the ordering of listing is not guaranteed at all. This is cause of http://download.oracle.com/javase/6/docs/api/java/io/File.html#list() which we use internally for native file listing.    This should either be documented as a known issue on -getmerge help pages/mans, or a consistent ordering (similar to HDFS) must be applied atop the listing. I suspect the latter only makes it worthy for what we include - while other FSes out there still have to deal with this issue. Perhaps we need a recommendation doc note added to our API?",fs
"Maven generated .classpath doesnot includes ""target/generated-test-source/java"" as source directory.","I can see lot of compilation issues after setting up my development environment using ""mvn eclipse:eclipse"". All these compilation issues are resolved after adding ""target/generated-test-sources"" as a source folder to the common project.    When verified the ""pom.xml"", it's noticed that these are included under ""generate-test-sources"" phase. This seems to be a problem occurred because of incorrect understanding/usage of ""build-helper-maven-plugin"" in Common project.    All these compilation issues are resolved after changing the phase to ""generate-sources"".    I found similar issue here.  https://issues.sonatype.org/browse/MNGECLIPSE-2387",build
TestHDFSTrash failing on 22,Seems to have started failing recently in many commit builds as well as the last two nightly builds of 22:  https://builds.apache.org/hudson/job/Hadoop-Hdfs-22-branch/51/testReport/org.apache.hadoop.hdfs/TestHDFSTrash/testTrashEmptier/    https://issues.apache.org/jira/browse/HDFS-1967,test
o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. ,o.a.h.conf.Configuration complains of overriding final parameter even if the value with which its attempting to override is the same. ,conf
branch-0.20-security doesn't include SPNEGO settings in core-default.xml,Looks like back-port of HADOOP-7119 to branch-0.20-security missed the changes to {{core-default.xml}}.,security
branch-0.20-security doesn't include o.a.h.security.TestAuthenticationFilter,Looks like the back-port of HADOOP-7119 to branch-0.20-security missed {{o.a.h.security.TestAuthenticationFilter}}.,security
Add a NetUtils method that can tell if an InetAddress belongs to local host,"This utility will be useful for mechanism proposed in HDFS-2231 where based on matching configured address with local address, a node determines the related configuration parameters.",util
test-patch should not -1 for lack of tests just build changes,If all a patch changed was build scripts then there should be an exemption to the required test changes.,build
"Single tar for common, hdfs and mr ","On trunk, after mavenization, the mvn package target creates separate tar files for common and hdfs, and the mr tar is created separately. It will be useful for deployment to have a maven target that creates a single tar file for all of them.",build
ant eclipse fails with stock release ,"download a tarball of 0.20.204.0, untar and do ""ant eclipse"": it will fail with the following error message:    The fix seems simple - just pre-create .eclipse.templates directory.    eclipse:    [eclipse] There were no settings found.    [eclipse] Writing the project definition in the mode ""java"".    [eclipse] Writing the classpath definition.    BUILD FAILED  /private/tmp/hadoop-0.20.204.0/build.xml:2245: /private/tmp/hadoop-0.20.204.0/.eclipse.templates does not exist.",conf
"Nightly build+test should run with ""continue on error"" for automated testing after successful build","It appears that scripts for nightly build in Apache Jenkins will stop after unit testing if any unit tests fail.  Therefore, contribs, schedulers, and some other system-level automated tests don't ever run until the unit tests are clean.  This results in two-phase cleanup of broken builds, which wastes developers' time.  Please change them to run even in the presence of unit test errors, as long as the compile+packaging build successfully.    This jira does not relate to CI builds, which emphasize test-patch execution.",build
log4j.properties templates does not define mapred.jobsummary.logger,"In templates/conf/hadoop-env.sh, HADOOP_JOBTRACKER_OPTS is defined as -Dsecurity.audit.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dmapred.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}  However, in templates/conf/hadoop-env.sh, instead of mapred.jobsummary.logger, hadoop.mapreduce.jobsummary.logger is defined as follows:  hadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}  This is preventing collection of jobsummary logs.    We have to consistently use mapred.jobsummary.logger in the templates.  ",conf
log4j.properties is missing properties for security audit and hdfs audit should be changed to info,log4j.properties defines the security audit and hdfs audit files but is missing properties for security audit which causes security audit logs to not be present and also updates the hdfs audit to log at a WARN level. hdfs-audit logs should be at the INFO level so admin's/users can track when the namespace got the appropriate change.,conf
"taskTracker could not start because ""Failed to set permissions"" to ""ttprivate to 0700""","ERROR org.apache.hadoop.mapred.TaskTracker:Can not start task tracker because java.io.IOException:Failed to set permissions of path:/tmp/hadoop-cyg_server/mapred/local/ttprivate to 0700      at org.apache.hadoop.fs.RawLocalFileSystem.checkReturnValue(RawLocalFileSystem.java:525)      at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:499)      at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:318)      at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:183)      at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:635)      at org.apache.hadoop.mapred.TaskTracker.(TaskTracker.java:1328)      at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3430)    Since hadoop0.20.203 when the TaskTracker initialize, it checks the permission(TaskTracker Line 624) of (org.apache.hadoop.mapred.TaskTracker.TT_LOG_TMP_DIR,org.apache.hadoop.mapred.TaskTracker.TT_PRIVATE_DIR, org.apache.hadoop.mapred.TaskTracker.TT_PRIVATE_DIR).RawLocalFileSystem(http://svn.apache.org/viewvc/hadoop/common/tags/release-0.20.203.0/src/core/org/apache/hadoop/fs/RawLocalFileSystem.java?view=markup) call setPermission(Line 481) to deal with it, setPermission works fine on *nx, however,it dose not alway works on windows.    setPermission call setReadable of Java.io.File in the line 498, but according to the Table1 below provided by oracle,setReadable(false) will always return false on windows, the same as setExecutable(false).    http://java.sun.com/developer/technicalArticles/J2SE/Desktop/javase6/enhancements/    is it cause the task tracker ""Failed to set permissions"" to ""ttprivate to 0700""?  Hadoop 0.20.202 works fine in the same environment.     ",fs
jobhistory server and secondarynamenode should have init.d script,The current set of init.d scripts can start/stop process for:    namenode  datanode  jobtracker  tasktracker    It is missing init.d scripts for:    secondarynamenode  jobhistory,scripts
Issues with hadoop-common-project\hadoop-common\src\main\packages\hadoop-setup-conf.sh file ,"hadoop-common-project\hadoop-common\src\main\packages\hadoop-setup-conf.sh has following issues  1. check_permission does not work as expected if there are two folders with $NAME as part of their name inside $PARENT  e.g. /home/hadoop/conf, /home/hadoop/someconf,   The result of `ls -ln $PARENT | grep -w $NAME| awk '{print $3}'` is non zero..it is 0 0 and hence the following if check becomes true.  {code:xml}  if [ ""$OWNER"" != ""0"" ]; then  RESULT=1  break  fi   {code}    2. Spelling mistake  {code:xml}  HADDOP_DN_ADDR=""0.0.0.0:50010""  {code}  it should be     {code:xml}  HADOOP_DN_ADDR=""0.0.0.0:50010""  {code}    3. HADOOP_SNN_HOST is not set due to which the hdfs-site.xml contains following configuration  {code:xml}  <property>  <name>dfs.namenode.http-address</name>  <value>:50070</value>  <description>  The address and the base port where the dfs namenode web ui will listen on.  If the port is 0 then the server will start on a free port.  </description>  </property>  {code}      ",scripts
Process cannot exit when there is many RPC readers more that actual client,"I met a strange issue that process cannot exit when I run RPC test cases in my Eclipse.  Conditions:  1. Only one Server and one client(local)  2. I have set many readers(conf.setInt(""ipc.server.read.threadpool.size"", 5)), it great than client number.    After any test cases, the process cannot exit. I tested with several cases and found the root cause.    RPC serves socket with reader(transferring binary to Call), and even shutdown the thread pool. But all the free readers are blocked at ""readSelector.select()"" (they are useless by Listener)  Those threads and process cannot exit always.    It can be fixed by invoking corresponding selector for each reader.The same thing was done at 0.20 version.        ",ipc
fix RPC.Server#addProtocol to work in AvroRpcEngine,"HADOOP-7524 introduced a new way of passing protocols to RPC servers, but it was not implemented correctly by AvroRpcEngine in that issue.  This is required to fix HDFS-2298.",ipc
Calling toString() on an RPC proxy object backed by WritableRpcEngine fails,"Calling {{toString}} (or any method not declared as part of the implemented RPC interface) on a proxy object backed by {{WritableRpcEngine}} will fail. This is because the declaring class's ({{Object}}'s) {{versionID}} field is checked using reflection, which obviously doesn't exist in the case of {{Object}}.",ipc
RPC.stopProxy can throw unintended exception while logging error,"{{RPC.stopProxy}} includes the following lines in case of error:    {code}  LOG.error(""Could not get invocation handler "" + invocationHandler +            "" for proxy "" + proxy + "", or invocation handler is not closeable."");  {code}    Trouble is, the {{proxy}} object is usually backed by {{WritableRpcEngine}}, which will fail in the event {{toString}} is called on one of its proxy objects. See HADOOP-7694 for more details on that issue. Until that's addressed, we might as well change the log message in {{RPC.stopProxy}} to not call {{toString()}} on {{proxy}}.",ipc
Add a note regarding the debug-log stack trace printing done in Configuration class,"{code}     /** A new configuration where the behavior of reading from the default   * resources can be turned off.   *   * If the parameter {@code loadDefaults} is false, the new instance   * will not load resources from the default files.   * @param loadDefaults specifies whether to load from the default files   */   public Configuration(boolean loadDefaults) {    this.loadDefaults = loadDefaults;    if (LOG.isDebugEnabled()) {      LOG.debug(StringUtils.stringifyException(new IOException(""config()"")));    }    synchronized(Configuration.class) {      REGISTRY.put(this, null);    }    this.storeResource = false;   }  {code}    The LOG.debug line prints out an exceptionized-stacktrace, which seems to be confusing to some users/hackers who're reading sources, or are running hadoop in debug mode.    Perhaps we can add a comment to explain why an exception is being logged (for stack trace), or handle printing a point stacktrace in a more elegant way.",conf
Remove dependency on different version of slf4j in avro,"Avro upgrade led to a mixture of slf4j versions. Hadoop uses slf4j 1.5.11, and avro brings in 1.6.1",build
jsvc target fails on x86_64,"Recent changes to the build.xml determine with jsvc file to download based on the os.arch.  It maps various arch values to i386 or x86_64. However, it notably doesn't consider x86_64 to be x86_64.  The result is this the download fails because {{os-arch}} doesn't expand.    {code}  build.xml:2626: Can't get http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz  {code}    This breaks {{test-patch}}.",build
Test failures if user has kerberos tickets,"I haven't run all the tests, and I haven't tested on trunk, but TestSaslRPC is crashing if I have a kerberos ticket.  I have to issue kdestroy before running tests, lest java crashes:    {noformat}      [junit] Running org.apache.hadoop.ipc.TestSaslRPC      [junit] Invalid memory access of location 0x2e rip=0x1010ab900      [junit]       [junit] Running org.apache.hadoop.ipc.TestSaslRPC      [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec      [junit] Test org.apache.hadoop.ipc.TestSaslRPC FAILED (crashed)  {noformat}    I had hoped I could run unit tests with security enabled...",test
Hadoop debs seem to be including source code,"Size of files in the build shows that the rpm's (both 32 and 64 bit) are about the same size as the binary tarball (compressed), while the deb's (both 32 and 64 bit) are about the same size as the full tarball (compressed) including source -- 71MB for the former, 98MB for the latter.    Working hypothesis is that the debs are incorrectly incorporating the source code.",build
"Add a log4j back end that can push out JSON data, one per line","If we had a back end for Log4j that pushed out log events in single line JSON content, we'd have something that is fairly straightforward to machine parse. If: it may be harder to do than expected. Once working HADOOP-6244 could use it.",util
have the pre-commit checks run by hudson veto patches with System.out and System.err calls in them,"I don't know how to do this, but we ought to get checkstyle to reject System.out and System.err calls, to stop them unintentionally getting into code. We may still need them in some places (LogLevel, external entry points), but those main() methods could all delegate to our own log output method that provides a single place where messages get printed",build
"improve config generator to allow users to specify proxy user, turn append on or off, turn webhdfs on or off",None,conf
config generator does not update the properties file if on exists already,We are copying configs to the hadoop conf dir but are not using the -f option. This leads to conf file not getting replaced in case the file exists and thus the user never gets the new conf.,conf
hadoop-env.sh generated from templates has duplicate info,None,conf
Umbrella for usage of native calls to manage OS cache and readahead,"Especially in shared HBase/MR situations, management of the OS buffer cache is important. Currently, running a big MR job will evict all of HBase's hot data from cache, causing HBase performance to really suffer. However, caching of the MR input/output is rarely useful, since the datasets tend to be larger than cache and not re-read often enough that the cache is used. Having access to the native calls {{posix_fadvise}} and {{sync_data_range}} on platforms where they are supported would allow us to do a better job of managing this cache.","io,native,performance"
see log4j Error when running mr jobs and certain dfs calls,None,conf
Move handling of concurrent client fail-overs to RetryInvocationHandler,Currently every implementation of a {{FailoverProxyProvider}} will need to perform its own synchronization to ensure that multiple concurrent failed calls to a single client proxy object don't result in multiple client fail over events. It would be better to put this logic in {{RetryInvocationHandler.invoke}}.    This is based on feedback provided by Todd in [this comment|https://issues.apache.org/jira/browse/HDFS-1973?focusedCommentId=13119567&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119567].,ipc
improve the hadoop-setup-conf.sh to read in the hbase user and setup the configs,Read in the hbase user in the script to set the auth to local mapping appropriately in hadoop so hbase can write. If the value is not sent default to 'hbase',conf
fix test-patch so that Jenkins can accept patches to the hadoop-tools module.,"Basically, test-patch.sh needs some tinkering to recognize hadoop-tools-project along-side common/mapreduce/hdfs. It also needs changes to compile and run tests in hadoop-tools_projects on patch submission.",build
eclipse target does not build with 0.24.0,"I'm new to hadoop, java, and eclipse, so please forgive me if I jumble multiple issues together or mistake the symptoms of one problem for a separate issue.    Attempting to follow the build instructions from http://wiki.apache.org/hadoop/EclipseEnvironment, the following commands are to be executed:      1 - mvn test -DskipTests    2 - mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true    3 - cd hdfs; ant compile eclipse    4 - cd ../; cd mapreduce; ant compile eclipse    A - If ""mvn test -DskipTests"" is used for #1, #2 fails with ""[ERROR] Failed to execute goal on project hadoop-yarn-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-yarn-common:jar:0.24.0-SNAPSHOT:""    Per Luke Lu's suggestion, ""mvn install -DskipTests -P-cbuild"" instead of step #1 cleared up this issue.    B - For steps #3, and #4 there are no hdfs or mapreduce subdirectories. These appear to have been renamed ""hadoop-hdfs-project"" and ""hadoop-mapreduce-project"".    C - For step #3, if I then go to hadoop-hdfs-project instead and perform ""ant compile eclipse"" no build.xml file is found - ""Buildfile: build.xml does not exist!""    D - For step #4, if I go to hadoop-mapreduce-project and do ""ant compile eclipse"" a set of errors much like #A is produced:    > [ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::  > [ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::  > [ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::  > [ivy:resolve]           ::  > org.apache.hadoop#hadoop-yarn-server-common;0.24.0-SNAPSHOT: not found  > [ivy:resolve]           ::  > org.apache.hadoop#hadoop-mapreduce-client-core;0.24.0-SNAPSHOT: not found  > [ivy:resolve]           ::  > org.apache.hadoop#hadoop-yarn-common;0.24.0-SNAPSHOT: not found  > [ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::    E - If I ignore these issues and import the projects generated in step #2, I get a bunch of errors related to the lack of an M2_REPO definition. Adding this variable needs to be included in the build scripts or documentation in the wiki.    F - Once that is resolved, eclipse shows hundreds of errors and warnings starting with ""AvroRecord"" cannot be resolved to a type.    Thanks so much for your work on this, but it needs a little more effort in documentation and/or development before it is usable again.    Thanks.",build
fix some typos and tabs in CHANGES.TXT,This is a minor edit to CHANGES.txt; giving it a JIRA issue to have complete release notes (though I'm not going to add it to the CHANGES.txt file as that would be too recursive). There are a couple of tabs and mis-spelling of the word exception in the trunk CHANGES.TXT,build
hadoop-setup-conf.sh should be modified to enable task memory manager,The hadoop-setup-conf.sh should be modified such that task memory management is enabled.,conf
Send back valid HTTP response if user hits IPC port with HTTP GET,"Often, I've seen users get confused between the IPC ports and HTTP ports for a daemon. It would be easy for us to detect when an HTTP GET request hits an IPC port, and instead of sending back garbage, we can send back a valid HTTP response explaining their mistake.",ipc
Allow TestCLI to be run against a cluster,Use the same CLI test to test cluster bits (see HDFS-1762 for more info),test
Hadoop 0.20.2-4 Deb Install hangs on Ubuntu 11.04,"deb install hangs indefinitely, will not cancel.",build
hadoop java docs bad pointer to hdfs package,the following link     http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/hdfs/package-summary.html    leads to a 404    ,documentation
Mapreduce jobs are failing when JT has hadoop.security.token.service.use_ip=false and client has hadoop.security.token.service.use_ip=true,"I have added following property in core-site.xml of all the nodes in cluster and restarted    <property>  <name>hadoop.security.token.service.use_ip</name>  <value>false</value>  <description>desc</description>  <final></final>  </property>    Then ran a randomwriter, distcp jobs, they are all failing   $HADOOP_HOME/bin/hadoop --config $HADOOP_CONFIG_DIR jar $HADOOP_HOME/hadoop-examples.jar randomwriter -Dtest.randomwrite.bytes_per_map=256000 input_1318325953  Running 140 maps.  Job started: Tue Oct 11 09:48:09 UTC 2011  11/10/11 09:48:09 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 14 for <USERNAME> on <Namenode IP>:8020  11/10/11 09:48:09 INFO security.TokenCache: Got dt for  hdfs://<Namenode Hostname>/user/<USERNAME>/.staging/job_201110110946_0001;uri=<Namenode IP>:8020;t.service=<Namenode IP>:8020  11/10/11 09:48:09 INFO mapred.JobClient: Cleaning up the staging area  hdfs://<Namenode Hostname>/user/<USERNAME>/.staging/job_201110110946_0001  org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.io.IOException: Call to  <Namenode Hostname>/<Namenode IP>:8020 failed on local exception: java.io.IOException:  javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided  (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3943)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)  Caused by: java.io.IOException: Call to <Namenode Hostname>/<Namenode IP>:8020 failed on local exception:  java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid  credentials provided (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)          at org.apache.hadoop.ipc.Client.call(Client.java:1071)          at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)          at $Proxy7.getProtocolVersion(Unknown Source)          at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)          at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)          at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:118)          at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:222)          at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:187)          at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)          at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1328)          at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:65)          at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1346)          at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)          at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)          at org.apache.hadoop.mapred.JobInProgress$2.run(JobInProgress.java:401)          at org.apache.hadoop.mapred.JobInProgress$2.run(JobInProgress.java:399)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:399)          at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3941)          ... 11 more  Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No  valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:539)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:484)          at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:586)          at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)          at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)          at org.apache.hadoop.ipc.Client.call(Client.java:1046)          ... 31 more  Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials  provided (Mechanism level: Failed to find any Kerberos tgt)]          at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)          at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)          at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:381)          at org.apache.hadoop.ipc.Client$Connection.access$1100(Client.java:184)          at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:579)          at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:576)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:575)          ... 34 more  Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)          at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)          at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)          at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)          at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)          at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)          at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)          at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)          ... 43 more            at org.apache.hadoop.ipc.Client.call(Client.java:1066)          at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)          at org.apache.hadoop.mapred.$Proxy7.submitJob(Unknown Source)          at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:913)          at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:842)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:842)          at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:816)          at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1253)          at org.apache.hadoop.examples.RandomWriter.run(RandomWriter.java:272)          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)          at org.apache.hadoop.examples.RandomWriter.main(RandomWriter.java:283)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)          at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)          at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:64)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.util.RunJar.main(RunJar.java:156)        At the sametime Jobtracker log says  ===================================  2011-10-11 09:48:09,486 WARN org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server :  javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided  (Mechanism level: Failed to find any Kerberos tgt)]  2011-10-11 09:48:09,487 INFO org.apache.hadoop.ipc.Server: IPC Server handler 26 on 50300, call  submitJob(job_201110110946_0001, hdfs://<Namenode hostname> /user/<USER>/.staging/job_201110110946_0001,  org.apache.hadoop.security.Credentials@16381a53) from <client IP> :46859: error: java.io.IOException:  java.io.IOException: Call to <Namenode hostname> /<Namenode IP> :8020 failed on local exception:  java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid  credentials provided (Mechanism level: Failed to find any Kerberos tgt)]  java.io.IOException: java.io.IOException: Call to <Namenode hostname> /<Namenode IP> :8020 failed on local  exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No  valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3943)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)  Caused by: java.io.IOException: Call to <Namenode hostname> /<Namenode IP> :8020 failed on local exception:  java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid  credentials provided (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)          at org.apache.hadoop.ipc.Client.call(Client.java:1071)          at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)          at $Proxy7.getProtocolVersion(Unknown Source)          at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)          at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)          at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:118)          at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:222)          at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:187)          at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)          at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1328)          at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:65)          at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1346)          at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:244)          at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)          at org.apache.hadoop.mapred.JobInProgress$2.run(JobInProgress.java:401)          at org.apache.hadoop.mapred.JobInProgress$2.run(JobInProgress.java:399)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:399)          at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3941)          ... 11 more    Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No  valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]          at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:539)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:484)          at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:586)          at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)          at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)          at org.apache.hadoop.ipc.Client.call(Client.java:1046)          ... 31 more  Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials  provided (Mechanism level: Failed to find any Kerberos tgt)]          at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)          at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)          at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:381)          at org.apache.hadoop.ipc.Client$Connection.access$1100(Client.java:184)          at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:579)          at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:576)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)          at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:575)          ... 34 more  Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)          at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)          at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)          at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)          at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)          at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)          at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)          at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)          ... 43 more  ",security
metrics2 class names inconsistent between trunk and 20x,"The new metrics2 framework was backported into the 20x branch, but the class names differ between the two branches. So, if anyone were to build against the metrics2 API in 20x, it would break when they upgraded to 23. We should reconcile the two so that they are API-compatible.",metrics
Remove duplicate call of Path#normalizePath during initialization.,"Found during code reading on HADOOP-6490, there seems to be an unnecessary call of {{normalizePath(...)}} being made in the constructor {{Path(Path, Path)}}. Since {{initialize(...)}} normalizes its received path string already, its unnecessary to do it to the path parameter in the constructor's call of the same.",fs
normalize hadoop-mapreduce & hadoop-dist dist/tar build with common/hdfs,Normalize the build fo hadoop-mapreduce and hadoop-dist with hadoop-common and hadoop-hdfs making the -Pdist and -Dtar maven options to be consistent.    * -Pdist should create the layout  * -Dtar should create the TAR    ,build
Reconcile FileUtil and SecureIOUtils APIs between 20x and trunk,"The 0.20.20x has introduced various public APIs to these classes which aren't in trunk. For example, FileUtil.setPermission exists in 20x but not trunk. If people start to depend on these public APIs, they will break when they upgrade.",util
"security audit logger is not on by default, fix the log4j properties to enable the logger",None,conf
Maven related JIRAs to backport to 0.23,HADOOP-7624  HDFS-2294  MAPREDUCE-3014  HDFS-2322  HADOOP-7642  MAPREDUCE-3171  HADOOP-7737  MAPREDUCE-3177  MAPREDUCE-3003  HADOOP-7590  MAPREDUCE-3024  HADOOP-7538,build
Evolve metrics2 in 0.23 and trunk to allow coexistence with 0.20-security releases,"HADOOP-7734 expressed the desire to support downstream projects that depends on both 0.23+ and 0.20-security releases.    In order to achieve that we need to evolve the 0.23/trunk API a little to avoid name conflicts:    # Rename AbstractMetric to ImmutableMetric, which also address a common question sink implementer ask about thread safety.  # Rename MetricsRecord to ImmutableMetrics, ditto.  # Rename MetricsVisitor to MetricVisitor.    We better do it before 0.23 is released or, it will be an ""incompatible"" change.",metrics
Add Maven profile to create a full source tarball,Currently we are building binary distributions only.    We should also build a full source distribution from where Hadoop can be built.,build
Incorrect exit code for hadoop-core-test tests when exception thrown,Please see MAPREDUCE-3179 for a full description.,test
RunJar doesn't consider hadoop.tmp.dir argument,"When using RunJar class, there's not way to override default hadoop.tmp.dir property, because it's being read from default hadoop configuration before passed arguments are even parsed.",util
Add NetUtils call which provides more help in exception messages,"In setting up MR2, I accidentally had a bad configuration value specified for one of the IP configs. I was getting a NumberFormatException parsing this config, but no indication as to what config value was being fetched. This JIRA is to add an API to NetUtils.createSocketAddr which takes the configuration name, so that any exceptions thrown will point back to where the user needs to fix it.",util
rpm version is not being picked from the -Dversion option in 205,ran ant build -Dversion=0.20.205.1.... and the rpm generated is the following    hadoop-0.20.205.0-1.amd64.rpm    Where as the tar.gz has the correct value    hadoop-0.20.205.1.tar.gz    the same version string should be applied to tarball and rpms,build
Unable to start yarn services using the new hadoop-dist tar stitching module,"Using the maven target: $mvn package -Pdist -DskipTests  I am seeing the following error when trying to start the resourcemanager:  {code}  $ bin/yarn-daemon.sh start resourcemanager  starting resourcemanager, logging to /home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/../logs/yarn-ahmed-resourcemanager-ubuntu.out  /usr/lib/jvm/java-6-sun/bin/java -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/../logs -Dyarn.log.dir=/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/../logs -Dhadoop.log.file=yarn-ahmed-resourcemanager-ubuntu.log -Dyarn.log.file=yarn-ahmed-resourcemanager-ubuntu.log -Dyarn.home.dir=/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/.. -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -classpath /home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/conf/:~/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/conf/::/usr/lib/jvm/java-6-sun/lib/tools.jar:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/share/hadoop/common/*:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/share/hadoop/common/lib/*:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/share/hadoop/hdfs/*:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/share/hadoop/hdfs/lib/*:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/../modules/*:/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/bin/../lib/*:~/home/ahmed/apache/hadoop-common/hadoop-dist/target/hadoop-0.24.0-SNAPSHOT/conf//rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager  Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/resourcemanager/ResourceManager  Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:306)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:247)  Could not find the main class: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.  Program will exit.  {code}    Seems there are some missing jars in the modules directory in the dist:    {code}  $ ls -al modules/  total 476  drwxr-xr-x  2 ahmed ahmed   4096 2011-10-17 00:56 .  drwxr-xr-x 11 ahmed ahmed   4096 2011-10-17 00:58 ..  -rwxr-xr-x  1 ahmed ahmed 347941 2011-10-17 00:56 hadoop-mapreduce-client-app-0.24.0-SNAPSHOT.jar  -rwxr-xr-x  1 ahmed ahmed  79364 2011-10-17 00:56 hadoop-mapreduce-client-hs-0.24.0-SNAPSHOT.jar  -rwxr-xr-x  1 ahmed ahmed  30720 2011-10-17 00:56 hadoop-mapreduce-client-jobclient-0.24.0-SNAPSHOT.jar  -rwxr-xr-x  1 ahmed ahmed  15358 2011-10-17 00:56 hadoop-mapreduce-client-shuffle-0.24.0-SNAPSHOT.jar  {code}",build
Detect MapReduce PreCommit Trunk builds silently failing when running test-patch.sh,"MapReduce PreCommit build is silently failing only running a very small portion of tests. The build then errors out, yet +1 it given to the patch.    Last known Success build - 307 tests run and passed  https://builds.apache.org/view/G-L/view/Hadoop/job/PreCommit-MAPREDUCE-Build/990/testReport/    First known Error build - 69 tests run and passed  https://builds.apache.org/view/G-L/view/Hadoop/job/PreCommit-MAPREDUCE-Build/994/testReport/    Snippet from failed build log - Errors out and then +1 the patch  https://builds.apache.org/view/G-L/view/Hadoop/job/PreCommit-MAPREDUCE-Build/994/console    [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]   [INFO] hadoop-yarn-api ................................... SUCCESS [19.512s]  [INFO] hadoop-yarn-common ................................ FAILURE [13.835s]  [INFO] hadoop-yarn-server-common ......................... SKIPPED  [INFO] hadoop-yarn-server-nodemanager .................... SKIPPED  [INFO] hadoop-yarn-server-resourcemanager ................ SKIPPED  [INFO] hadoop-yarn-server-tests .......................... SKIPPED  [INFO] hadoop-yarn-server ................................ SKIPPED  [INFO] hadoop-yarn-applications-distributedshell ......... SKIPPED  [INFO] hadoop-yarn-applications .......................... SKIPPED  [INFO] hadoop-yarn-site .................................. SKIPPED  [INFO] hadoop-yarn ....................................... SKIPPED  [INFO] hadoop-mapreduce-client-core ...................... SKIPPED  [INFO] hadoop-mapreduce-client-common .................... SKIPPED  [INFO] hadoop-mapreduce-client-shuffle ................... SKIPPED  [INFO] hadoop-mapreduce-client-app ....................... SKIPPED  [INFO] hadoop-mapreduce-client-hs ........................ SKIPPED  [INFO] hadoop-mapreduce-client-jobclient ................. SKIPPED  [INFO] hadoop-mapreduce-client ........................... SKIPPED  [INFO] hadoop-mapreduce .................................. SKIPPED  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 33.784s  [INFO] Finished at: Wed Oct 12 12:03:22 UTC 2011  [INFO] Final Memory: 40M/630M  [INFO] ------------------------------------------------------------------------  [ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.2-beta-5:single (tar) on project hadoop-yarn-common: Failed to create assembly: Error adding file 'org.apache.hadoop:hadoop-yarn-api:jar:0.24.0-SNAPSHOT' to archive: /home/jenkins/jenkins-slave/workspace/PreCommit-MAPREDUCE-Build/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-api/target/classes isn't a file. -> [Help 1]  [ERROR]   [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.  [ERROR] Re-run Maven using the -X switch to enable full debug logging.  [ERROR]   [ERROR] For more information about the errors and possible solutions, please read the following articles:  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException  [ERROR]   [ERROR] After correcting the problems, you can resume the build with the command  [ERROR]   mvn <goals> -rf :hadoop-yarn-common      ======================================================================  ======================================================================      Running contrib tests.  ======================================================================  ======================================================================      /bin/kill -9 27543   kill: No such process  NOP      ======================================================================  ======================================================================      Checking the integrity of system test framework code.  ======================================================================  ======================================================================      /bin/kill -9 27548   kill: No such process  NOP          +1 overall.  Here are the results of testing the latest attachment     http://issues.apache.org/jira/secure/attachment/12498577/MR3166.patch    against trunk revision .        +1 @author.  The patch does not contain any @author tags.        +1 tests included.  The patch appears to include 3 new or modified tests.        +1 javadoc.  The javadoc tool did not generate any warning messages.        +1 javac.  The applied patch does not increase the total number of javac compiler warnings.        +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.        +1 release audit.  The applied patch does not increase the total number of release audit warnings.        +1 core tests.  The patch passed unit tests in .        +1 contrib tests.  The patch passed contrib unit tests.    Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/994//testReport/  Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/994//console    This message is automatically generated.",build
"Support fadvise and sync_data_range in NativeIO, add ReadaheadPool class",This JIRA adds JNI wrappers for sync_data_range and posix_fadvise. It also implements a ReadaheadPool class for future use from HDFS and MapReduce.,"io,native,performance"
Expose file descriptors from Hadoop-wrapped local FileSystems,"In HADOOP-7714, we determined that using fadvise inside of the MapReduce shuffle can yield very good performance improvements. But many parts of the shuffle are FileSystem-agnostic and thus operate on FSDataInputStreams and RawLocalFileSystems. This JIRA is to figure out how to allow RawLocalFileSystem to expose its FileDescriptor object without unnecessarily polluting the public APIs.","native,performance"
Make GlobFilter class public,Currently the GlobFilter class is package private.    As a generic filter it is quite useful (and I've found myself doing cut&paste of it a few times),fs
Ability to specify kinit location for setup scripts,"kinit location is in /usr/kerberos/bin in RHEL5, and switched to /usr/bin in RHEL6.  Hence, it may be better to support --kinit-location parameter for hadoop-create-user.sh, hadoop-setup-applications.sh, hadoop-setup-hdfs.sh and hadoop-validate-setup.sh.  ",scripts
Test file reference count is at least 3x actual value,patch file format example    diff --git hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java  index 26747cd..4ab14ad 100644  --- hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java  +++ hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/test/MapredTestDriver.java      method to count test file references will count 3 test files referenced instead of the more accurate 1 test file referenced     grep -c -i '/test'  ,build
BytesWritable / SequenceFile yields dummy linefeed at end as soon as content has one or more linefeeds.,"I create SequenceFiles which have BytesWritable as values.  I notice that if I store content which contains no linefeeds (""\n"") or one linefeed, in the value, the value can also be read out of the sequencefile properly.  However, as soon as I store input which contains two or more linefeeds (which is actually pretty much always the case), during the process of writing to the sequencefile and reading my data back, one *extra* linefeed is yielded at the end of the value, a linefeed which did not exist in the input.  So this effectively corrupts my data, although i could write a hacky workaround for it.  I have written a program that demonstrates the behavior, by showing what happens when writing 2 sequencefiles:  one that has a record which value contains one linefeeds.  another that has a record which value contains two linefeeds.  Upon reading, the latter value will contain 3 linefeeds.    Test file is : http://pastie.org/2728797",record
Improve performance of raw comparisons,Guava has a nice implementation of lexicographical byte-array comparison that uses sun.misc.Unsafe to compare unsigned byte arrays long-at-a-time. Their benchmarks show it as being 2x more CPU-efficient than the equivalent pure-Java implementation. We can easily integrate this into WritableComparator.compareBytes to improve CPU performance in the shuffle.,"io,performance,util"
Common side of MR-2736 (MR1 removal),Updates bin and config scripts per MR-2736.,scripts
HA: Administrative CLI to control HA daemons,"We'll need to have some way of controlling the HA nodes while they're live, probably by adding some more commands to dfsadmin.","ha,util"
Add top-level navigation to APT docs,We need navigation menus for the APT docs that have been written so far.,documentation
Debian package contain both system and tar ball layout,"When packaging is invoked as ""ant clean tar deb"".  The system creates both system layout and tarball layout in the same build directory.  Debian packaging target would pick up files for both layouts.  The end result of using produced debian package built this way, would end up README.txt LICENSE.txt, and jar files in /usr.",build
Document deprecated configuration properties,Many configuration properties are deprecated in 0.23. We should include a table in the documentation that maps between the old and new names.,documentation
PreCommit-HADOOP-Build is failing on hadoop-auth-examples,"Failure has been happening since October 13th.    Excerpt from https://builds.apache.org/job/PreCommit-HADOOP-Build/315/consoleText      [INFO]   [INFO] --- maven-war-plugin:2.1:war (default-war) @ hadoop-auth-examples ---  [INFO] Packaging webapp  [INFO] Assembling webapp [hadoop-auth-examples] in [/home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth-examples/target/hadoop-auth-examples-0.24.0-SNAPSHOT]  [INFO] Processing war project  [INFO] Copying webapp resources [/home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth-examples/src/main/webapp]  [INFO] ------------------------------------------------------------------------  [INFO] Reactor Summary:  [INFO]   [INFO] Apache Hadoop Annotations ......................... SUCCESS [2.261s]  [INFO] Apache Hadoop Auth ................................ SUCCESS [5.299s]  [INFO] Apache Hadoop Auth Examples ....................... FAILURE [0.878s]  [INFO] Apache Hadoop Common .............................. SKIPPED  [INFO] Apache Hadoop Common Project ...................... SKIPPED  [INFO] ------------------------------------------------------------------------  [INFO] BUILD FAILURE  [INFO] ------------------------------------------------------------------------  [INFO] Total time: 8.726s  [INFO] Finished at: Tue Oct 25 15:47:35 UTC 2011  [INFO] Final Memory: 24M/302M  [INFO] ------------------------------------------------------------------------  [ERROR] Failed to execute goal org.apache.maven.plugins:maven-war-plugin:2.1:war (default-war) on project hadoop-auth-examples: Failed to copy file for artifact [org.apache.hadoop:hadoop-auth:jar:0.24.0-SNAPSHOT:compile]: /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/target/classes (Is a directory) -> [Help 1]  [ERROR]   [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.  [ERROR] Re-run Maven using the -X switch to enable full debug logging.  [ERROR]   [ERROR] For more information about the errors and possible solutions, please read the following articles:  [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException  [ERROR]   [ERROR] After correcting the problems, you can resume the build with the command  [ERROR]   mvn <goals> -rf :hadoop-auth-examples",build
ViewFS getFileChecksum throws FileNotFoundException for files in /tmp and /user,"Thanks to Rohini Palaniswamy for discovering this bug. To quote  bq. When doing getFileChecksum for path /user/hadoopqa/somefile, it is trying to fetch checksum for /user/user/hadoopqa/somefile. If /tmp/file, it is trying /tmp/tmp/file. Works fine for other FS operations.",viewfs
Add support for protocol buffer based RPC engine,This jira adds support for protocol buffer RPC engine.,ipc
Make the Ipc-Header in a RPC-Payload an explicit header,None,ipc
Implement a base class for DNSToSwitchMapping implementations that can offer extra topology information,"HDFS-2492 has identified a need for DNSToSwitchMapping implementations to provide a bit more topology information (e.g. whether or not there are multiple switches). This could be done by writing an extended interface, querying its methods if present and coming up with a default action if there is no extended interface.     Alternatively, we have a base class that all the standard mappings implement, with a boolean isMultiRack() method; all the standard subclasses would extend this, as could any third party topology provider. The advantage of this approach is that it is easier to add new operations without going into a multi-interface mess.",util
Remove RecordIO,"HADOOP-6155 deprecated RecordIO in 0.21. We should remove it from trunk, as nothing anymore uses it and the tests are taking up resources.    We should attempt to remove record IO and also check for any references to it within the MR and HDFS projects. Meanwhile, Avro has come up as a fine replacement for it, and has been use inside Hadoop now for quite a while.",record
Aggregate project javadocs,"With 'mvn javadoc:javadoc' we now get docs spread over the maven modules. Is there a way to stich them all together?    Also, there are some differences in their generation: hadoop-auth and hadoop-yarn-* hadoop-mapreduce-* modules goto a top-level apidocs dir which isn't the case for hadoop-common and hadoop-hdfs - they go straight to target/site/api.",build
Add more symlink tests that cover intermediate links,This covers the tests for HDFS-2514.,fs
"Add equals, hashcode, toString to DataChecksum",Simple patch to add these functions to the DataChecksum interface. This is handy for the sake of HDFS-2130.,"io,util"
Make source tarball use conventional name.,"When building binary and source tarballs, I get the following artifacts:  Binary tarball: hadoop-0.23.0-SNAPSHOT.tar.gz   Source tarball: hadoop-dist-0.23.0-SNAPSHOT-src.tar.gz    Notice the ""-dist"" right between ""hadoop"" and the version in the source tarball name.  ",build
HA: Simple HealthMonitor class to watch an HAService,"This is a utility class which will be part of the FailoverController. The class starts a daemon thread which periodically monitors an HAService, calling its monitorHealth function. It then generates callbacks into another class when the health status changes (eg the RPC fails or the service returns a HealthCheckFailedException)",ha
hadoop-yarn-api - generate-sources build step waits for input,"At the step:  ""exec-maven-plugin:1.2:exec (generate-sources) @ hadoop-yarn-api ---""    the build will wait for user input and eventually fails after some timeouts.  It does that twice    ",build
Hadoop 23 does not assemble docs correctly,"When building with dist and docs, docs are supposed to be in: ""share/doc/hadoop/""  But there is only:  drwxrwxr-x bruno/bruno       0 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/  drwxrwxr-x bruno/bruno       0 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/  drwxrwxr-x bruno/bruno       0 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/hdfs/  -rw-r--r-- bruno/bruno  107334 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/hdfs/CHANGES.txt  -rw-r--r-- bruno/bruno     101 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/hdfs/NOTICE.txt  -rw-r--r-- bruno/bruno   13366 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/hdfs/LICENSE.txt  drwxrwxr-x bruno/bruno       0 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/common/  -rw-r--r-- bruno/bruno    1366 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/common/README.txt  -rw-r--r-- bruno/bruno  450982 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/common/CHANGES.txt  -rw-r--r-- bruno/bruno     101 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/common/NOTICE.txt  -rw-r--r-- bruno/bruno   13695 2011-10-31 18:23 hadoop-0.23.0-SNAPSHOT/share/doc/hadoop/common/LICENSE.txt      Although they do exist in their respective maven submodule target directory","build,documentation"
mvn clean package -Dsrc does not work,"""mvn clean package -Dsrc"" is supposed to create a source tarball in hadoop-dist/target/ but some interactions with the clean target will prevent this to happen.    The following would happen:  * -Dsrc makes maven create a source tarball in hadoop-dist/target/ for the root module as well as each submodule    1. The root module would first create a source tarball correctly in hadoop-dist/target/  2. Each submodule will also create a source tarball for their module in <MODULE>/hadoop-dist/target/. But not before executing the clean target and therefore deleting the main source tarball created at 1.",build
add -n option for FSshell cat,Add -n option for cat to display the files with line numbers.It's quite useful if you're reading big files.,fs
HADOOP-7773 introduced 7 new findbugs warnings,  Code Warning  Se com.google.protobuf.ByteString stored into non-transient field HadoopRpcProtos$HadoopRpcExceptionProto.exceptionName_  Se com.google.protobuf.ByteString stored into non-transient field HadoopRpcProtos$HadoopRpcExceptionProto.stackTrace_  Se Class org.apache.hadoop.ipc.protobuf.HadoopRpcProtos$HadoopRpcRequestProto defines non-transient non-serializable instance field request_  Se com.google.protobuf.ByteString stored into non-transient field HadoopRpcProtos$HadoopRpcRequestProto.methodName_  Se Class org.apache.hadoop.ipc.protobuf.HadoopRpcProtos$HadoopRpcResponseProto defines non-transient non-serializable instance field response_    Dodgy Warnings    Code Warning  UCF Useless control flow in org.apache.hadoop.ipc.protobuf.HadoopRpcProtos$HadoopRpcExceptionProto$Builder.maybeForceBuilderInitialization()  UCF Useless control flow in org.apache.hadoop.ipc.protobuf.HadoopRpcProtos$HadoopRpcRequestProto$Builder.maybeForceBuilderInitialization()  ,ipc
Fix the repository name to support pushing to the staging area of Nexus,"The repository name doesn't match the old one, leading to confusion.",build
Release artifacts need to be signed for Nexus,When I uploaded hadoop-0.23.0-rc1 artifacts to Nexus it complains that artifacts aren't signed. Hence I won't be able push the release jars after the vote.,build
Improvements to Doc: Hadoop MapReduce Next Generation - Cluster Setup,"- In section +Prerequisites+  -- Please add a link to download locaion of 0.23 release tar-ball.  - In section +Installation+  -- It would be good to have more details about where they should ""untar"" the image and what the directory structure should look like. I can provide my notes on the detailed steps I took to successfully install.  - In the section +Running Hadoop in Non-Secure Mode+  -- Can you add that templaces for the site and environment files can be found in {{./conf/}}, {{./share/hadoop/common/templates/conf/}}, and {{./share/hadoop/hdfs/templates/conf/}}  -- Also, it might be good to add a link to a sample configs that contain the bare minumum to start a cluster.  -- The +conf/hdfs-site.xml+ section references the {{dfs.datanode.data.dir}} property. Should we also include a reference to the {{dfs.datanode.data.dir.perm}} property as well?  -- The +Hadoop Rack Awareness+ section references {{topology.node.switch.mapping.impl}}, but I think this is deprecated. I think the new one is {{net.topology.node.switch.mapping.impl}}. Also, {{topology.script.file.name}} seems to be deprecated in favor of {{net.topology.script.file.name}}.  - In section +Operating the Hadoop Cluster+  -- The HDFS format command should have the {{-clusterid}} parameter:  --- {{$HADOOP_PREFIX_HOME/bin/hdfs namenode -format -clusterid <cluster_name>}}  -- The command to start the namenode is incorrect. It should either be:  1. {{$HADOOP_PREFIX_HOME/bin/hdfs --config $HADOOP_CONF_DIR  namenode &}} ## without the {{start}}  or  2. {{$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR start namenode}}  I prefer the second, since you can also use that to stop the daemon.  -- The same for the command to start the datanode  -- The same for the command to start the resourcemanager, historyserver, and nodemanagers, except that it should be {{yarn-daemon.sh}}  - Hadoop Shutdown:  -- If stopping the daemons is required, then {{hadoop-daemon.sh}} and {{yarn-daemon.sh}} should be used to both start and stop.    ",documentation
Could not find source jars in dist tarball,"Where the dist tarball is expanded:  [bruno@p8700 bigtop]$ find dist_tarball/ -name ""*-sources.jar""  <EMPTY>      From the build directory:  [bruno@p8700 hadoop-common]$ find . -name ""*-sources.jar""  hadoop-common-project/hadoop-auth/target/hadoop-auth-0.23.0-SNAPSHOT-sources.jar  hadoop-common-project/hadoop-common/target/hadoop-common-0.23.0-SNAPSHOT-test-sources.jar  hadoop-common-project/hadoop-common/target/hadoop-common-0.23.0-SNAPSHOT-sources.jar  hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-0.23.0-SNAPSHOT-sources.jar  hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-0.23.0-SNAPSHOT-test-sources.jar  ",build
HADOOP_PREFIX cannot be overriden,"hadoop-config.sh forces HADOOP_prefix to a specific value:  export HADOOP_PREFIX=`dirname ""$this""`/..    It would be nice to make this overridable.  ",build
The first put to a non-existing current directory doesn't work correctly,"Steps to reproduce:  - Format the dfs.  - Start it.  - Put a file without specifying the destination.    {noformat}  $ hadoop fs -ls  ls: `.': No such file or directory    $ hadoop fs -put /etc/passwd    $ hadoop fs -ls  Found 1 items  -rw-r--r--   1 kihwal supergroup       2076 2011-11-04 10:37 .._COPYING_  {noformat}    The namenode log:  {noformat}  - ugi=kihwal    ip=/127.0.0.1   cmd=create      src=/user/kihwal/.._COPYING_    dst=null         perm=kihwal:supergroup:rw-r--r--  - BLOCK* NameSystem.allocateBlock: /user/kihwal/.._COPYING_. BP-221429388-10.74.90.166-1320420960536  blk_1038813851536531761_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,  replicas=[ReplicaUnderConstruction[127.0.0.1:50010|RBW]]}  - BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to  blk_1038813851536531761_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1,  replicas=[ReplicaUnderConstruction[127.0.0.1:50010|RBW]]} size 0  - DIR* NameSystem.completeFile: file /user/kihwal/.._COPYING_ is closed by DFSClient_NONMAPREDUCE_809910865_1  - ugi=kihwal    ip=/127.0.0.1   cmd=rename      src=/user/kihwal/.._COPYING_    dst=/user/kihwal         perm=kihwal:supergroup:rwxr-xr-x  {noformat}     It ends up creating a wrong file. ",fs
enable hadoop config generator to set dfs.block.local-path-access.user to enable short circuit read,we have a new config that allows to select which user can have access for short circuit read. We should make that configurable through the config generator scripts.,conf
Hadoop distribution tarball bundle some libraries several times with different versions,It would be nice to only use only one version of each.  For instance (from the resulting dist tarball):    ./hadoop-0.23.0-SNAPSHOT/share/hadoop/common/lib/netty-3.2.4.Final.jar  ./hadoop-0.23.0-SNAPSHOT/lib/netty-3.2.3.Final.jar    ./hadoop-0.23.0-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-log4j12-1.5.11.jar  ./hadoop-0.23.0-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-api-1.5.11.jar  ./hadoop-0.23.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.5.11.jar  ./hadoop-0.23.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.5.11.jar  ./hadoop-0.23.0-SNAPSHOT/lib/slf4j-log4j12-1.6.1.jar  ./hadoop-0.23.0-SNAPSHOT/lib/slf4j-api-1.6.1.jar      ./hadoop-0.23.0-SNAPSHOT/share/hadoop/common/lib/stax-api-1.0.1.jar  ./hadoop-0.23.0-SNAPSHOT/share/hadoop/common/lib/stax-api-1.0-2.jar  ,build
Support binding to sub-interfaces,"Right now, with the {{DNS}} class, we can look up IPs of provided interface names ({{eth0}}, {{vm1}}, etc.). However, it would be useful if the I/F -> IP lookup also took a look at subinterfaces ({{eth0:1}}, etc.) and allowed binding to only a specified subinterface / virtual interface.    This should be fairly easy to add, by matching against all available interfaces' subinterfaces via Java.",util
Port token service changes from 205,Need to merge the 205 token bug fixes and the feature to enable hostname-based tokens.,"fs,security"
Backport HADOOP-5839 to 0.20-security - fixes to ec2 scripts to allow remote job submission,The fix for HADOOP-5839 was committed to 0.21 more than a year ago.  This bug is to backport the change (which is only 14 lines) to branch-0.20-security.  ===========  Original description:  i would very much like the option of submitting jobs from a workstation outside ec2 to a hadoop cluster in ec2. This has been explored here:    http://www.nabble.com/public-IP-for-datanode-on-EC2-tt19336240.html    the net result of this is that we can make this work (along with using a socks proxy) with a couple of changes in the ec2 scripts:  a) use public 'hostname' for fs.default.name setting (instead of the private hostname being used currently)  b) mark hadoop.rpc.socket.factory.class.default as final variable in the generated hadoop-site.xml (that applies to server side)    #a has no downside as far as i can tell since public hostnames resolve to internal/private IP addresses within ec2 (so traffic is optimally routed).,contrib/cloud
TestUserGroupInformation#testGetServerSideGroups test fails in chroot,It is common when running in chroot to have root's group vector preserved when running as your self.    For example    # Enter chroot  $ sudo chroot /myroot    # still root  $ whoami  root    # switch to user preserving root's group vector  $ sudo -u user -P -s    # root's groups  $ groups root  a b c    # user's real groups  $ groups user  d e f    # user's effective groups  $ groups  a b c d e f  -------------------------------  ,"security,test"
test-patch +1 patches that introduce javadoc and findbugs warnings in some cases,"test-patch.sh uses string comparisons of numbers to decide whether to +1 for javadoc and findbugs warnings.    decisions are made using the following construct    [[ $A > $B ]]     Brackets put the script into conditional expression mode     Operator definition for  conditional expression mode         string1 > string2                True if string1 sorts after string2 lexicographically in the current locale.    Examples  $ sh -c 'if [[ 99 > 100 ]]; then echo true; fi'  true    $ sh -c 'if [[ -99 > -10 ]]; then echo true; fi'  true    Arithmetic operations in conditional expressions are defined below         arg1 OP arg2                OP is one of -eq, -ne, -lt, -le, -gt, or -ge.  These arithmetic binary operators return true if arg1 is equal to, not equal to, less than, less than or equal to, greater  than,  or  greater  than  or equal to arg2, respectively.  Arg1 and arg2 may be positive or negative integers.    Alternatively arithmetic evaluation mode can be entered using double parenthesis ""(( .. ))""","build,test"
Map memory mb is being incorrectly set by hadoop-setup-conf.sh,"HADOOP-7728 enabled task memory management to be configurable in the hadoop-setup-conf.sh. However, the default value for mapred.job.map.memory.mb is being set incorrectly.",conf
RawLocalFileSystem.append() should give FSDataOutputStream with accurate .getPos(),When RawLocalFileSyste.append() is called it returns an FSDataOutputStream whose .getPos() returns 0.  getPos() should return position in the file where appends will start writing.    ,fs
DiskChecker#checkDir should fail if the directory is not executable,"DiskChecker#checkDir fails if a directory can't be created, read, or written but does not fail if the directory exists and is not executable. This causes subsequent code to think the directory is OK but later fail due to an inability to access the directory (eg see MAPREDUCE-2921). I propose checkDir fails if the directory is not executable. Looking at the uses, this should be fine, I think it was ignored because checkDir is often used to create directories and it creates executable directories.",util
http://wiki.apache.org/hadoop/EclipseEnvironment need to update with correct trunk Repository.,"Associate the Hadoop Trunk Repository  Select File > New > Other...     Then SVN > Repository Location wizard     Based on needs, use one of the following as the Root URL.   http://svn.apache.org/repos/asf/hadoop/common/trunk     http://svn.apache.org/repos/asf/hadoop/hdfs/trunk     http://svn.apache.org/repos/asf/hadoop/mapreduce/trunk     *need to update the latest folder structure as trunk URL*  hdfs and mapreduce may not be required here, because they already moved inside to common.",documentation
Make single node secure cluster setup documentation for 0.23,This JIRA is to track creation of documentation for the setup of a secure single node cluster.    ,documentation
"NativeIO.java flags and identifiers must be set correctly for each platform, not hardcoded to their Linux values","NativeIO.java flags and identifiers must be set correctly for each platform, not hardcoded to their Linux values.  Constants like O_CREAT, O_EXCL, etc. have different values on OS X and many other operating systems.",native
Hadoop wrapper script not picking up native libs correctly,"Originally discussed in https://mail-archives.apache.org/mod_mbox/hadoop-common-user/201111.mbox/%3C4EC3A3AE.7060402%40deri.org%3E    I'm testing out native lib support on our test amd64 test cluster   running 0.20.205 running the following    ./bin/hadoop jar hadoop-test-0.20.205.0.jar testsequencefile -seed 0   -count 1000 -compressType RECORD xxx -codec   org.apache.hadoop.io.compress.GzipCodec -check 2    it fails with    WARN util.NativeCodeLoader: Unable to load native-hadoop library for   your platform... using builtin-java classes where applicable    Looking at    bin/hadoop    it seems to successfully detect that the native libs are available (they   seem to come pre-compiled with 0.20.205 which is nice)       if [ -d ""${HADOOP_HOME}/lib/native"" ]; then       if [ ""x$JAVA_LIBRARY_PATH"" != ""x"" ]; then     JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}       else         JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}       fi     fi    and sets JAVA_LIBRARY_PATH to contain them.    Then in the following line, if ${HADOOP_HOME}/lib contains libhadoop.a   (which is seems to in the stock tar) then it proceeds to ignore the   native libs       if [ -e ""${HADOOP_PREFIX}/lib/libhadoop.a"" ]; then       JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/lib     fi    The libhadoop.a in ${HADOOP_HOME}/lib seems to be a copy of the lib/native/Linux-i386-32 going from the filesizes (and also noted by https://mail-archives.apache.org/mod_mbox/hadoop-common-user/201111.mbox/%3CCAOcnVr2AzUDnN0LFhmTqUmAyujYtvhfkmMm_J0R-bmxw2wU+9A@mail.gmail.com%3E)    hadoop@testhbase01:~$ ls -la hadoop/lib/libhadoop.*  -rw-r--r-- 1 hadoop hadoop 237244 Oct  7 08:20 hadoop/lib/libhadoop.a  -rw-r--r-- 1 hadoop hadoop    877 Oct  7 08:20 hadoop/lib/libhadoop.la  -rw-r--r-- 1 hadoop hadoop 160438 Oct  7 08:20 hadoop/lib/libhadoop.so  -rw-r--r-- 1 hadoop hadoop 160438 Oct  7 08:19 hadoop/lib/libhadoop.so.1.0.0  hadoop@testhbase01:~$ ls -la hadoop/lib/native/Linux-i386-32/  total 728  drwxr-xr-x 3 hadoop hadoop   4096 Nov 15 14:05 .  drwxr-xr-x 5 hadoop hadoop   4096 Oct  7 08:24 ..  -rw-r--r-- 1 hadoop hadoop 237244 Oct  7 08:20 libhadoop.a  -rw-r--r-- 1 hadoop hadoop    877 Oct  7 08:20 libhadoop.la  -rw-r--r-- 1 hadoop hadoop 160438 Oct  7 08:20 libhadoop.so  -rw-r--r-- 1 hadoop hadoop 160438 Oct  7 08:20 libhadoop.so.1  -rw-r--r-- 1 hadoop hadoop 160438 Oct  7 08:20 libhadoop.so.1.0.0    A possible solution includes removing libhadoop.a and friends from ${HADOOP_HOME}/lib and possibly also modifying the hadoop wrapper to remove        if [ -e ""${HADOOP_PREFIX}/lib/libhadoop.a"" ]; then       JAVA_LIBRARY_PATH=${HADOOP_PREFIX}/lib     fi    unless there is some other reason for this to exist.    This was also noted in a comment to HADOOP-6453",scripts
"{start,stop}-dfs.sh scripts are missing from tarball",They are missing from the bin directory. ,build
Delegation token manager should support token store abstraction,There are several issues with the interfaces in .20 that make it difficult to incorporate this as extension currently (see HIVE-2467). The plan is to port relevant changes to Hadoop so that future versions of the shim in Hive (or projects with similar needs) are easier to implement.  ,security
[DNS] Log a WARN when there's no matching interface and we are gonna use the default one.,"We do not currently log today when a requested interface is not found. Instead the code handles this by using the default interface instead, when the lookup result is null.    We should log a WARN when this is done, instead of letting it sneak by.",util
ConcurrentModificationException in getCurrentUser(),"I've seen the following exception from a job tracker log. Security/kerberos enabled.    {noformat}  2011-11-17 01:54:00,288 WARN org.apache.hadoop.mapred.CleanupQueue: Error deleting path  hdfs://namenode:1234/xxxx/xxxx/xxxx/job_xxxxxxxxx_xxxx  java.util.ConcurrentModificationException          at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:761)          at java.util.LinkedList$ListItr.next(LinkedList.java:696)          at javax.security.auth.Subject$SecureSet$1.next(Subject.java:1014)          at javax.security.auth.Subject$ClassSet$1.run(Subject.java:1345)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject$ClassSet.populateSet(Subject.java:1342)          at javax.security.auth.Subject$ClassSet.<init>(Subject.java:1317)          at javax.security.auth.Subject.getPrivateCredentials(Subject.java:731)          at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:379)          at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:398)          at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1445)          at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1346)          at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)          at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)          at org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext$1.run(CleanupQueue.java:78)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1064)          at org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext.deletePath(CleanupQueue.java:75)          at org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread.run(CleanupQueue.java:131)  {noformat}  ",security
Using mincore to understand the effect of fadvise in the Linux page cache,This Jira adds a way to log the percentage of pages of a file that were on the Linux page cache (when the file is being opened) with the purpose of measuring the effect of issuing posix_fadvise (POSIX_FADV_DONTNEED) calls.,"io,native,performance"
Inner classes of org.apache.hadoop.ipc.protobuf.HadoopRpcProtos generates findbugs warnings which results in -1 for findbugs,"findbugs reports the following medium priority warnings for some inner class in the generated class ./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/protobuf/HadoopRpcProtos.java:    * SE_BAD_FIELD_STORE: Non-serializable value stored into instance field of a serializable class  * SE_BAD_FIELD: Non-transient non-serializable instance field in serializable class  * UCF_USELESS_CONTROL_FLOW: Useless control flow    This can be fixed by adding the following findbugs exclude filter:    +    <Match>  +      <!-- protobuf generated code -->  +      <Class name=""~org\.apache\.hadoop\.ipc\.protobuf\.HadoopRpcProtos.*""/>  +    </Match>    which will exclude all inner classes of org.apache.hadoop.ipc.protobuf.HadoopRpcProtos",ipc
Support posix_fallocate in NativeIO and add call from DataNode code,None,"io,native,performance"
Support posix_fallocate in NativeIO and add call from DataNode code,None,"io,native,performance"
TestSaslRPC#testDigestAuthMethodHostBasedToken fails with hostname localhost.localdomain,TestSaslRPC#testDigestAuthMethodHostBasedToken fails on branch-1 on some hosts.    null expected:<localhost[]> but was:<localhost[.localdomain]>  junit.framework.ComparisonFailure: null expected:<localhost[]> but was:<localhost[.localdomain]>    null expected:<[localhost]> but was:<[eli-thinkpad]>  junit.framework.ComparisonFailure: null expected:<[localhost]> but was:<[eli-thinkpad]>  ,"ipc,test"
no NullAppender in the log4j config,"running sbin/start-dfs.sh gives me a telling off about no null appender -should one be in the log4j config file.    Full trace (failure expected, but full output not as expected)  {code}  ./start-dfs.sh   log4j:ERROR Could not find value for key log4j.appender.NullAppender  log4j:ERROR Could not instantiate appender named ""NullAppender"".  Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured.  Starting namenodes on []  cat: /Users/slo/Java/Hadoop/versions/hadoop-0.23.0/libexec/../etc/hadoop/slaves: No such file or directory  cat: /Users/slo/Java/Hadoop/versions/hadoop-0.23.0/libexec/../etc/hadoop/slaves: No such file or directory  Secondary namenodes are not configured.  Cannot start secondary namenodes.  {code}",conf
sbin/start-balancer doesnt,"you can't start the balance as it tries to call bin/hadoop-daemon.sh, which isn't there:  {code}  hadoop-0.23.0 slo$ sbin/start-balancer.sh   sbin/start-balancer.sh: line 25: /Users/slo/Java/Hadoop/versions/hadoop-0.23.0/libexec/../bin/hadoop-daemon.sh: No such file or directory  hadoop-0.23.0 slo$   {code}",scripts
hadoop shell commands should print usage if not given a class,"[root@bigtop-fedora-15 ~]# hdfs foobar  Exception in thread ""main"" java.lang.NoClassDefFoundError: foobar  Caused by: java.lang.ClassNotFoundException: foobar          at java.net.URLClassLoader$1.run(URLClassLoader.java:217)          at java.security.AccessController.doPrivileged(Native Method)          at java.net.URLClassLoader.findClass(URLClassLoader.java:205)          at java.lang.ClassLoader.loadClass(ClassLoader.java:321)          at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)          at java.lang.ClassLoader.loadClass(ClassLoader.java:266)  Could not find the main class: foobar. Program will exit.    Instead of loading any class, it would be nice to explain the command is not valid and to call print_usage()",scripts
Cleanup unnecessary exceptions thrown and unnecessary casts,Cleanup build warnings. It is the file in the hadoop-common subtree for hdfs-2564.,fs
hadoop-tools JARs are not part of the distro,"After mavenizing streaming, the hadoop-streaming JAR is not part of the final tar.    ",build
Run tests with non-secure random,Post-mavenization we lost the improvement made by HADOOP-7335 which set up a system property such that Random is seeded by {{urandom}}. This makes the tests run faster and prevents timeouts due to lack of entropy on the build boxes.,build
a compress tool for test compress/decompress locally use hadoop compress codecs,add a compress tool for test compress/decompress locally use hadoop compress codecs.    It can be used as follows:    compress a.txt to a.lzo:  hadoop jar test.jar org.apache.hadoop.io.compress.CodecTool org.apache.hadoop.io.compress.LzoCodec -c 1024 1024 a.txt a.lzo    decompress a.lzo to stdout:  hadoop jar test.jar org.apache.hadoop.io.compress.CodecTool org.apache.hadoop.io.compress.LzoCodec -x 1024 1024 a.lzo /dev/stdout  ,io
Link to downloads page from hadoop.apache.org,"It would be nice to have a new section on the main page that links to http://hadoop.apache.org/common/releases.html. Even better would be to have a tab on the top labeled ""Download""",documentation
download pages for mapred/hdfs don't match hadoop-common,"http://hadoop.apache.org/hdfs/releases.html  http://hadoop.apache.org/mapreduce/releases.html   don't match  http://hadoop.apache.org/common/releases.html    the common/releases.html page has a nice description of the current status (stable, alpha, beta) of each release, and without it users get confused.",documentation
Visiting /jmx on the daemon web interfaces may print unnecessary error in logs,"Logs that follow a {{/jmx}} servlet visit:    {code}  11/11/22 12:09:52 ERROR jmx.JMXJsonServlet: getting attribute UsageThreshold of java.lang:type=MemoryPool,name=Par Eden Space threw an exception  javax.management.RuntimeMBeanException: java.lang.UnsupportedOperationException: Usage threshold is not supported    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:856)  ...  {code}",metrics
Configuration.getClasses() never returns the default value.,Configuration.getClasses() never returns the default value.,conf
consolidate templates,"the hadoop-common project has templates for hdfs-site.xml and mapred-site.xml that are used by the config generator scripts.  The hadoop-{mapreduce,hdfs}-project's also have {mapred,hdfs}-site.xml templates, and the templates don't match. It would be good if these could be consolidated.",scripts
multiple javax security configurations cause conflicts,Both UGI and the SPNEGO KerberosAuthenticator set the global javax security configuration.  SPNEGO stomps on UGI's security config which leads to kerberos/SASL authentication errors.  ,security
UGI getCurrentUser is not synchronized,Sporadic {{ConcurrentModificationExceptions}} are originating from {{UGI.getCurrentUser}} when it needs to create a new instance.  The problem was specifically observed in a JT under heavy load when a post-job cleanup is accessing the UGI while a new job is being processed.,security
Improve DiskChecker javadocs,"The javadocs for DiskChecker#checkDir(File dir) trail off, look like they weren't completed, should be.     While checkDir(File) uses java File to check if a dir actually is writable, the version of checkDir that takes an FsPermission uses FsAction#implies which doesn't actually check if a dir is writable (eg it passes on a read-only file system). So switching from one version to the other can cause unexpected bugs. Let's call this out explicitly in the javadocs.",util
Improve DiskChecker javadocs,"The javadocs for DiskChecker#checkDir(File dir) trail off, look like they weren't completed, should be.     While checkDir(File) uses java File to check if a dir actually is writable, the version of checkDir that takes an FsPermission uses FsAction#implies which doesn't actually check if a dir is writable (eg it passes on a read-only file system). So switching from one version to the other can cause unexpected bugs. Let's call this out explicitly in the javadocs.",util
Encrypt hadoop files,"Hello everybody,      I have develop a simple compresor codec to encrypt the hadoop data with AES, It's an easy way to get security without use kerberos. (https://github.com/geisbruch/HadoopCryptoCompressor)    If you are ok I'd like to include this in some hadoop version.    Thanks   Gabriel",security
TestViewFsHdfs.testgetFileLinkStatus is failing an assert,Probably introduced by HADOOP-7783. I'll fix it.    {noformat}  java.lang.AssertionError    org.apache.hadoop.fs.FileContext.qualifySymlinkTarget(FileContext.java:1111)    org.apache.hadoop.fs.FileContext.access$000(FileContext.java:170)    org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1142)    org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1137)    org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2327)    org.apache.hadoop.fs.FileContext.getFileLinkStatus(FileContext.java:1137)    org.apache.hadoop.fs.FileContextTestHelper.checkFileLinkStatus(FileContextTestHelper.java:233)    org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testgetFileLinkStatus(ViewFsBaseTest.java:448)  {noformat},fs
tar file contains duplicate config files in etc/ and conf/,"When I extract the hadoop tar file download for 0.20.205.0 I notice that it contains a set of configuration files in conf/ and what seem to be a duplicate set in etc/hadoop.  There is no indication in the docs of why there are two copies of these files. In practice, the contents of conf/ seem to be used, not etc/hadoop.    I would suggest removing one of these or else clarifying their purpose.",build
"changes2html.pl should generate links to HADOOP, HDFS, and MAPREDUCE jiras",changes2html.pl correctly generates links to HADOOP jiras only. This hasn't been updated since projects split.,documentation
"Move the support for multiple protocols to lower layer so that Writable, PB and Avro can all use it",None,ipc
Building mvn site with Maven < 3.0.2 causes OOM errors,"If you try to run mvn site with Maven 3.0.0 (and possibly 3.0.1 - haven't actually tested that), you get hit with unavoidable OOM errors. Switching to Maven 3.0.2 or later fixes this. The enforcer should require 3.0.2 for builds.",build
"logging and gc JVM metrics should be provided as ""gauges""","JVM Metrics:    logWarn  logInfo  logError  logFatal  gcCount  gcTimeMillis    Are provided as ""counters"" only, meaning that they cumulate values over time rather than report real-time values. The code uses incrMetric() instead of setMetric(), for these metrics.    In tools like ganglia this leads to increasing graphs that aren't terribly useful: You can't tell by looking at a graph of these metrics whether or not garbage collection times are going up, how long individual gc events were, or when interesting log errors happened, because those events are overshadowed by trends when the metrics are reported as counters. Also, users are accustomed to thinking that a graph trending up indicates an operational issue, so these metrics cause interest and confusion among operators when they shouldn't.    I'm attaching a patch to JVM Metrics that adds the following metrics:    logWarnGauge  logInfoGauge  logErrorGauge  logFatalGauge  gcCountGauge  gcTimeMillisGauge    As well as a sample image of how those metrics look after running with this patch on a test cluster for a couple weeks.     J",metrics
Multiple SLF4J binding message in .out file for all daemons,"When I start the NameNode or DataNode using sbin/hadoop-daemon.sh, I get a variant of the following error on stdout:    {noformat}  SLF4J: Class path contains multiple SLF4J bindings.  SLF4J: Found binding in [jar:file:/Users/joecrow/Code/hadoop-0.23.0/share/hadoop/common/lib/slf4j-log4j12-1.5.11.jar!/org/slf4j/impl/StaticLoggerBinder.class]  SLF4J: Found binding in [jar:file:/Users/joecrow/Code/hadoop-0.23.0/share/hadoop/hdfs/lib/slf4j-log4j12-1.5.11.jar!/org/slf4j/impl/StaticLoggerBinder.class]  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.  {noformat}  ",scripts
"Hadoop native fails to compile when default linker option is -Wl,--as-needed","Recent releases of Ubuntu and Debian have switched to using --as-needed as default when linking binaries.    As a result the AC_COMPUTE_NEEDED_DSO fails to find the required DSO names during execution of configure resulting in a build failure.    Explicitly using ""-Wl,--no-as-needed"" in this macro when required resolves this issue.    See http://wiki.debian.org/ToolChain/DSOLinking for a few more details",native
HADOOP_HOME warning happens all of the time,"With HADOOP-7816, the check for HADOOP_HOME has moved after it is set by hadoop-config so that it always happens unless HADOOP_HOME_WARN_SUPPRESS is set in hadoop-env or the environment.",scripts
Build and publish indexed source code,"The HBase folks publish xref which produces pages like http://hbase.apache.org/xref/org/apache/hadoop/hbase/client/Delete.html. It's quite nice: it makes their code indexable by Google, and, since it understands Java, it's easy to move around between classes. Let's do this as well. Here's the maven plugin: http://maven.apache.org/plugins/maven-jxr-plugin/jxr-mojo.html  ",documentation
change Jersey version to 1.9 to go with MAPREDUCE-2863,increased version of Jersey to 1.9 in MAPREDUCE-2863.  Increase it in hadoop-project/pom.xml so that everyone is on one version.    ,build
native libs should be under lib/native/ dir,"Currently common and hdfs SO files end up under lib/ dir with all JARs, they should end up under lib/native.    In addition, the hadoop-config.sh script needs some cleanup when comes to native lib handling:    * it is using lib/native/${JAVA_PLATFORM} for the java.library.path, when it should use lib/native.  * it is looking for build/lib/native, this is from the old ant build, not applicable anymore.  * it is looking for the libhdfs.a and adding to the java.librar.path, this is not correct.  ",build
Add helper class to unwrap RemoteException from ServiceException thrown on protobuf based RPC,None,ipc
Allow access to BlockKey/DelegationKey encoded key for RPC over protobuf,"In order to support RPC over protobuf, the BlockKey needs to provide access to encoded key. The byte[] encoded key will be transported over protobuf as byte[], instead of SecretKey.",ipc
Regression HADOOP-7777 switch changes break HDFS tests when the isSingleSwitch() predicate is used,"This doesn't show up until you apply the HDFS-2492 patch, but the attempt to make the {{StaticMapping}} topology clever by deciding if it is single rack or multi rack based on its rack->node mapping breaks the HDFS {{TestBlocksWithNotEnoughRacks}} test. Why? Because the racks go in after the switch topology is cached by the {{BlockManager}}, which assumes the system is always single-switch.    Fix: default to assuming multi-switch; remove the intelligence, add a setter for anyone who really wants to simulate single-switch racks.     Test: verify that a newly created simple mapping is multi switch",util
The Single Node and Cluster Setup docs don't cover HDFS,The main docs page (http://hadoop.apache.org/common/docs/r0.23.0) only has HDFS docs for federation. Only MR2 is covered in the single node and cluster setup documentation.,documentation
"""webapps"" directory is available in ""HADOOP_HOME"" as well as in ""hadoop-core.jar""","The ""HADOOP_HOME"" (installation directory) and ""hadoop-core.jar"" contains the ""webapps"" folder. The one from ""hadoop-core.jar"" can be removed.",build
test-patch seems to fail when a patch goes across projects (common/hdfs/mapreduce) or touches hadoop-assemblies/hadoop-dist.,"Take for example HDFS-2178, the patch applies cleanly, but test-patch fails.",build
KerberosAuthenticatorHandler is not setting KerberosName name rules from configuration,"While the KerberosAuthenticatorHandler defines the name rules property, it does not set it in KerberosName.",security
TestFailoverProxy fails intermittently on trunk,"TestFailoverProxy can fail intermittently with the failures occurring in testConcurrentMethodFailures().  The test has a race condition where the two threads may be sequentially invoking the unreliable interface rather than concurrently.  Currently the proxy provider's getProxy() method contains the thread synchronization to enforce a concurrent invocation, but examining the source to RetryInvocationHandler.invoke() shows that the call to getProxy() during failover is too late to enforce a truly concurrent invocation.    For this particular test, one thread could race ahead and block on the CountDownLatch in getProxy() before the other thread even enters RetryInvocationHandler.invoke().  If that happens the second thread will cache the newly updated value for proxyProviderFailoverCount, since the failover has mostly been processed by the original thread.  Therefore the second thread ends up assuming no other thread is present, performs a failover, and the test fails because two failovers occurred instead of one.",test
"Change Record serialization *Vector, *Map, methods to collection interfaces","While attempting to throw a LinkedHashMap through  Record serialization (BinaryRecordOutput, etc), the type does not match TreeMap, and compilation fails.    I looked for any TreeMap/ArrayList specific method calls but couldn't find any.  BinaryRecordOutput calls .size() for both, which are methods of the interfaces Map/List respectively.",record
Redirect hadoop script's deprecation message to stderr,"$ hadoop dfs -ls  DEPRECATED: Use of this script to execute hdfs command is deprecated.  Instead use the hdfs command for it.  ...    If we're still letting the command run, I think we should redirect the deprecation message to stderr in case users have a script taking the output from stdout.  ",scripts
KerberosName method typo and log warning when rules are set,"The method hasRulesBeenSet() should be named haveRulesBeenSet()    if the rules setting is not done during UGI initialization because they have been already set, a warning should be logged. Along the following lines:      ""Not setting kerberos name mappings defined in hadoop.security.auth_to_local because name mappings are already set"")  ",security
"IPC logs too verbose after ""RpcKind"" introduction","Recently in trunk I started seeing the following log messages on every IPC connection:  11/12/07 15:56:49 INFO ipc.Server: rpcKind=RPC_WRITABLE, rpcRequestWrapperClass=class org.apache.hadoop.ipc.WritableRpcEngine$Invocation, rpcInvoker=org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker@320cf66b  which is probably more appropriately at DEBUG or TRACE level",ipc
Sort out tarball conf directories,"The conf directory situation in the tarball (generated by mvn pacakge -Dtar) is a mess. The top-level conf directory just contains mr2 conf, there are two other incomplete conf dirs:    {noformat}  hadoop-0.24.0-SNAPSHOT $ ls conf/  slaves  yarn-env.sh  yarn-site.xml  hadoop-0.24.0-SNAPSHOT $ find . -name conf  ./conf  ./share/hadoop/hdfs/templates/conf  ./share/hadoop/common/templates/conf  {noformat}    yet there are 4 hdfs-site.xml files:    {noformat}  hadoop-0.24.0-SNAPSHOT $ find . -name hdfs-site.xml  ./etc/hadoop/hdfs-site.xml  ./share/hadoop/hdfs/templates/conf/hdfs-site.xml  ./share/hadoop/hdfs/templates/hdfs-site.xml  ./share/hadoop/common/templates/conf/hdfs-site.xml  {noformat}    And it looks like ./share/hadoop/common/templates/conf contains the old MR1 style conf (eg mapred-site.xml).    We should generate a tarball with a single conf directory that just has common, hdfs and mr2 confs.",build
bin and sbin commands don't use  JAVA_HOME when run from the tarball ,"When running eg ./sbin/start-dfs.sh from a tarball the scripts complain JAVA_HOME is not set and could not be found even if the env var is set.    {noformat}  hadoop-0.24.0-SNAPSHOT $ echo $JAVA_HOME  /home/eli/toolchain/jdk1.6.0_24-x64  hadoop-0.24.0-SNAPSHOT $ ./sbin/start-dfs.sh   log4j:ERROR Could not find value for key log4j.appender.NullAppender  log4j:ERROR Could not instantiate appender named ""NullAppender"".  Starting namenodes on [localhost]  localhost: Error: JAVA_HOME is not set and could not be found.  {noformat}    I have to explicitly set this via hadoop-env.",build
HADOOP_LOG_DIR has to be set explicitly when running from the tarball,"When running bin and sbin commands from the tarball if HADOOP_LOG_DIR is not explicitly set in hadoop-env.sh it doesn't use HADOOP_HOME/logs by default like it used to, instead picks a wrong dir:    {noformat}  localhost: mkdir: cannot create directory `/eli': Permission denied  localhost: chown: cannot access `/eli/eli': No such file or directory  {noformat}    We should have it default to HADOOP_HOME/logs or at least fail with a message if the dir doesn't exist, the env var isn't set.",build
Automatically update doc versions,The docs version is hard coded.  It should be updated automatically.,"build,documentation"
"HA: if both NNs are in Standby mode, client needs to try failing back and forth several times with sleeps","For a manual failover, there may be an intermediate state for a non-trivial amount of time where both NNs are in standby mode. Currently, the failover proxy will immediately failover on receiving this exception from the first NN, and when it hits the same exception on the second NN, it immediately fails. It should probably fail back and forth nearly indefinitely if both NNs are in Standby mode.","ha,ipc"
ProtobufRPCEngine client side exception mechanism is not consistent with WritableRpcEngine,"In ProtobufRpcEngine the client side exceptions are wrapped in RpcClientException. The RpcClientException is used to create RemoteException which is set as cause in the thrown ServiceException. However WritableRpcEngine throws the client encountered exception as is. This difference in behavior causes, many tests to fail in the existing unit tests, which expect the client invoker() to thrown the exception as is. This jira makes the ProtobufRpcEngine behavior consistent with that of WritableRpcEngine.",ipc
Fix javadoc warnings in AuthenticationToken.java,Fix the following javadoc warning:  [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest  [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest  [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest  [WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/AuthenticationToken.java:33: warning - Tag @link: reference not found: HttpServletRequest  ,security
Generate proto java files as part of the build,currently the generated java files are precompiled and checked in into the source.,build
LocalDirAllocator confChanged() accesses conf.get() twice,"LocalDirAllocator.AllocatorPerContext.confChanged() accesses conf.get() twice unnecessarily. The 2 calls can give 2 different values, which can lead to issues because the first call's return value is saved in savedLocalDirs and is used for comparison in the next call to confChanged() method --- So the comparison is wrong.",fs
Have Configuration use Read/Write Locks,We can potentially improve performance by moving to read/write locks for configuration instead of the current synchronization. ,conf
hadoop artifacts do not contain 64 bit libhdfs native lib,the lidhfs native library is only being built for 32 bit and not for 64. We should add it to 64 bit as well.,build
check that protoc is avail and it is the right version in hadoop-main POM,"We should add a check for the protoc binary in the maven POM itself, so that its easier to stop compiling if it is not available (it is a required dep. for all major projects under hadoop today).",build
Port FileContext symlinks to FileSystem,"FileSystem isn't going away anytime soon (HADOOP-6446). It would be useful to implement HADOOP-6421 for FileSystem, this would allow interoperability between FileContext and FileSystem (eg currently a symlink created via FileContext is not readable via FileSystem), which will help people migrate to FileContext. The work is mostly moving the client-side link resolution code to a shared place and porting the tests or modifying them to be FC/FS agnostic.",fs
haoop-daemon.sh unconditionnally try to chown its log directory,"See:  ./hadoop-common-project/hadoop-common/src/main/bin/hadoop-daemon.sh:chown $HADOOP_IDENT_STRING $HADOOP_LOG_DIR    In some setups, hadoop daemon may have the rights to write in its log dir but not to chown/chmod it. ",scripts
Fix three javadoc warnings on branch-1,"Fix 3 javadoc warnings on branch-1:      [javadoc] /home/eli/src/hadoop-branch-1/src/core/org/apache/hadoop/io/Sequence  File.java:428: warning - @param argument ""progress"" is not a parameter name.      [javadoc] /home/eli/src/hadoop-branch-1/src/core/org/apache/hadoop/util/ChecksumUtil.java:32: warning - @param argument ""chunkOff"" is not a parameter name.      [javadoc] /home/eli/src/hadoop-branch-1/src/mapred/org/apache/hadoop/mapred/QueueAclsInfo.java:52: warning - @param argument ""queue"" is not a parameter name.",documentation
Implement a generic splittable signature-based compression format,"I propose to take the suggestion of PIG-42 extend it to   - add a more robust header such that false matches are vanishingly unlikely   - repeat initial bytes of the header for very fast split searching   - break down the stream into modest size chunks (~64k?) for rapid parallel encode and decode   - provide length information on the blocks in advance to make block decode possible in hardware    An optional extra header would be added to the gzip header, adding 36 bytes.    <sh> := <version><signature><uncompressedDataLength><compressedRecordLength>  <version> := 1 byte version field allowing us to later adjust the deader definition  <signature> := 23 byte signature of the form aaaaaaabcdefghijklmnopr where each letter represents a randomly generated byte  <uncompressedDataLength> := 32-bit length of the data compressed into this record  <compressedRecordLength> := 32-bit length of this record as compressed, including all headers, trailers    If multiple extra headers are present and the split header is not the first header, the initial implementation will not recognize the split.    Input streams would be broken down into blocks which are appended, much as BlockCompressorStream does. Non-split-aware decoders will ignore this header and decode the appended blocks without ever noticing the difference.    The signature has >= 132 bits of entropy which is sufficient for 80+ years of Moore's law before collisions become a significant concern.    The first 7 bytes are repeated for speed. When splitting, the signature search will look for the 32-bit value aaaa every 4 bytes until a hit is found, then the next 4 bytes identify the alignment of the header mod 4 to identify a potential header match, then the whole header is validated at that offset. So, there is a load, compare, branch, and increment per 4 bytes searched.    The existing gzip implementations do not provide access to the optional header fields (nor comment nor filename), so the entire gzip header will have to be reimplemented and compression will need to be done using the raw deflate options of the native library / built in deflater.    There will be some degradation when using splittable gzip:   - The gzip headers will each be 36 bytes larger. (4 byte extra header header, 32 byte extra header)   - There will be one gzip header per block.   - History will have to be reset with each block to allow starting from scratch at that offset resulting in some uncompressed bytes that would otherwise have been strings.    Issues to consider:   - Is the searching fast enough without the repeating 7 bytes in the signature?   - Should this be a patch to the existing gzip classes to add a switch, or should this be a whole new class?   - Which level does this belong at? CompressionStream? Compressor?   - Is it more advantageous to encode the signature into the less dense comment field?   - Optimum block size? Smaller splits faster and may conserve memory, larger provides slightly better compression ratio.    ",io
add configuration methods to handle human readable size values,"It's better to have a new configuration methods which handle human readable size values.  For example, see HDFS-1314.    ",conf
test-patch should run eclipse:eclipse to verify that it does not break again,Recently the eclipse:eclipse build was broken.  If we are going to document this on the wiki and have many developers use it we should verify that it always works.,build
Viewfs changes for MAPREDUCE-3529,"ViewFs.getDelegationTokens returns a list of tokens for the associated namenodes. Credentials serializes these tokens using the service name for the actual namenodes. Effectively, tokens are not cached for viewfs (some more details in MR 3529). Affects any job which uses the TokenCache in tasks along with viewfs (some Pig jobs).    Talk to Jitendra about this, some options  1. Change Credentials.getAllTokens to return the key, instead of just a token list (associate the viewfs canonical name with a token in credentials)  2. Have viewfs issue a fake token.  Both of these would allow for a single viewfs configuration only.  3. An additional API in FileSystem - something like getDelegationTokens(String renewer, Credentials credentials) - which would check the credentials object before making token requests to the actual namenode.  4. An additional API in FileSystem - getCanonicalServiceNames - similar to getDelegationTokens, which would return service names for the actual namenodes. TokenCache/Credentials can work using this list.  5. have getDelegationTokens check the current UGI - and fetch tokens only if they don't exist.    Have a quick patch for 3, along with associated MR changes.",viewfs
Fix bug in ProtoBufRpcEngine - ,The parent Jira moved the multiple protocol support to lower layer; it introduced a bug: the paramCLass parameter to  #server() constructor should be null so that it uses the registered rpc request deserializers.,ipc
duplicate declaration of hadoop-hdfs test-jar,"[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-common-project:pom:0.24.0-SNAPSHOT  [WARNING] 'dependencyManagement.dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdfs:test-jar -> duplicate declaration of version ${project.version} @ org.apache.hadoop:hadoop-project:0.24.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-project/pom.xml, line 140, column 19  ",build
compilation of protobuf files fails in windows/cygwin,HADOOP-7899 & HDFS-2511 introduced compilation of proto files as part of the build.    Such compilation is failing in windows/cygwin,build
[Doc] Remove hadoop.logfile.* properties.,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all.    These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.    {code}  137 <!--- logging properties -->  138   139 <property>  140   <name>hadoop.logfile.size</name>  141   <value>10000000</value>  142   <description>The max size of each log file</description>  143 </property>  144   145 <property>  146   <name>hadoop.logfile.count</name>  147   <value>10</value>  148   <description>The max number of log files</description>  149 </property>  {code}",documentation
Remove Avro RPC,"Please see the discussion in HDFS-2660 for more details. I have created a branch HADOOP-6659 to save the Avro work, if in the future some one wants to use the work that existed to add support for Avro RPC.",ipc
StandbyException should extend IOException,"In order to implement HDFS-2671 (HDFS methods should throw StandbyException when in wrong state), we'd need to either add ""throws StandbyException"" to all of the protocol APIs, or make StandbyException extend from IOException. The latter is much simpler and less probability of it being a breaking change.",ha
HA: Improve some logging for client IPC failovers and StandbyExceptions,"Simple tweaks to make the logging less verbose during failover scenarios:  - client should log a message before it sleeps for retry, including how long it plans to retry and whether it will failover  - server shouldn't log full stack trace when it throws StandbyExceptions, since they're a ""normal part of doing business""","ha,ipc"
Add ZK client for leader election,ZKClient needs to support the following capabilities:  # Ability to create a znode for co-ordinating leader election.  # Ability to monitor and receive call backs when active znode status changes.  # Ability to get information about the active node.,ha
?FailoverController for client-based configuration,Basic FailoverController to coordinate fail-over using a client-based config (ie fail-over from NameNode x to NameNode y). ,ha
Add interface and update CLI to query current state to HAServiceProtocol,Common side of HDFS-2679.,ha
"Test-patch should have maven.test.failure.ignore,maven.test.error.ignore to run all the tests even in case of failure/error.",This approach will help to know all the failures even if some testcase fails.,build
HA: Client failover policy is incorrectly trying to fail over all IOExceptions,"In the {{FailoverOnNetworkExceptionRetry}} implementation, we're returning FAILOVER_AND_RETRY for any IOException, so long as the call is idempotent. So, for example, {{getFileInfo}} on a file you don't have access to throws AccessControlException which incorrectly triggers a failover. We should not failover on RemoteExceptions unless they are StandbyException.","ha,ipc"
Port HADOOP-7070 to branch-1,"Without HADOOP-7070, it is impossible to use secured Hadoop inside an application that relies on other JAAS configurations. This is the case for at least a secure HBase which currently needs an external JAAS configuration for secure ZooKeeper access concurrent with secure access to HDFS.",security
Kerberos relogin interval in UserGroupInformation should be configurable,"Currently the check done in the *hasSufficientTimeElapsed()* method is hardcoded to 10 mins wait.    The wait time should be driven by configuration and its default value, for clients should be 1 min. ",security
o.a.h.ipc.WritableRpcEngine should have a way to force initialization,{{WritableRpcEngine}} currently relies on a static initializer to register itself with {{o.a.h.ipc.Server}} as a valid {{RpcKind}}. There should be a way to ensure this initialization has already occurred.,ipc
HA : Make client connection retries on socket time outs configurable.,None,"ha,ipc"
Normalize dependencies versions across all modules,Move all dependencies versions to the dependencyManagement section in the hadoop-project POM    Move all plugin versions to the dependencyManagement section in the hadoop-project POM,build
Fix the scope the dependencies across subprojects,"As a follow up of HADOOP-7934, we should fix the scope of dependencies, ie many deps meant for testing have compile scope and end up in the binary distribution (ie junit)",build
There's a Hoop README in the root dir of the tarball,The Hoop README.txt is now in the root dir of the tarball.    {noformat}  hadoop-trunk1 $ tar xvzf hadoop-dist/target/hadoop-0.24.0-SNAPSHOT.tar.gz  -C /tmp/  ..  hadoop-trunk1 $ head -n3 /tmp/hadoop-0.24.0-SNAPSHOT/README.txt   -----------------------------------------------------------------------------  HttpFS - Hadoop HDFS over HTTP  {noformat},build
Forward port SequenceFile#syncFs and friends from Hadoop 1.x,"HDFS-200 added a new public API SequenceFile#syncFs, we need to forward port this for compatibility. Looks like it might have introduced other APIs that need forward porting as well (eg LocaltedBlocks#setFileLength, and DataNode#getBlockInfo).",io
HA: the FailoverController should optionally fence the active during failover,"The FailoverController in HADOOP-7924 needs to be able to fence off the current active in case it fails to transition to standby (or the user requests it for sanity). This is needed even for manual failover (the CLI should use the configured fencing mechanism). The FC needs to access the HDFS-specific implementations HDFS-2179, could add a common fencing interface (or just shell out but we may not always want to do that).",ha
Improve Hadoop subcomponent integration in Hadoop 0.23,"h1. Introduction    For the rest of this proposal it is assumed that the current set  of Hadoop subcomponents is:   * hadoop-common   * hadoop-hdfs   * hadoop-yarn   * hadoop-mapreduce    It must be noted that this is an open ended list, though. For example,  implementations of additional frameworks on top of yarn (e.g. MPI) would  also be considered a subcomponent.    h1. Problem statement    Currently there's an unfortunate coupling and hard-coding present at the  level of launcher scripts, configuration scripts and Java implementation  code that prevents us from treating all subcomponents of Hadoop independently  of each other. In a lot of places it is assumed that bits and pieces  from individual subcomponents *must* be located at predefined places  and they can not be dynamically registered/discovered during the runtime.  This prevents a truly flexible deployment of Hadoop 0.23.     h1. Proposal    NOTE: this is NOT a proposal for redefining the layout from HADOOP-6255.   The goal here is to keep as much of that layout in place as possible,  while permitting different deployment layouts.    The aim of this proposal is to introduce the needed level of indirection and  flexibility in order to accommodate the current assumed layout of Hadoop tarball  deployments and all the other styles of deployments as well. To this end the  following set of environment variables needs to be uniformly used in all of  the subcomponent's launcher scripts, configuration scripts and Java code  (<SC> stands for a literal name of a subcomponent). These variables are  expected to be defined by <SC>-env.sh scripts and sourcing those files is  expected to have the desired effect of setting the environment up correctly.    # HADOOP_<SC>_HOME     ## root of the subtree in a filesystem where a subcomponent is expected to be installed      ## default value: $0/..    # HADOOP_<SC>_JARS      ## a subdirectory with all of the jar files comprising subcomponent's implementation      ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)    # HADOOP_<SC>_EXT_JARS     ## a subdirectory with all of the jar files needed for extended functionality of the subcomponent (nonessential for correct work of the basic functionality)     ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)/ext    # HADOOP_<SC>_NATIVE_LIBS     ## a subdirectory with all the native libraries that component requires     ## default value: $(HADOOP_<SC>_HOME)/share/hadoop/$(<SC>)/native    # HADOOP_<SC>_BIN     ## a subdirectory with all of the launcher scripts specific to the client side of the component     ## default value: $(HADOOP_<SC>_HOME)/bin    # HADOOP_<SC>_SBIN     ## a subdirectory with all of the launcher scripts specific to the server/system side of the component     ## default value: $(HADOOP_<SC>_HOME)/sbin    # HADOOP_<SC>_LIBEXEC     ## a subdirectory with all of the launcher scripts that are internal to the implementation and should *not* be invoked directly     ## default value: $(HADOOP_<SC>_HOME)/libexec    # HADOOP_<SC>_CONF     ## a subdirectory containing configuration files for a subcomponent     ## default value: $(HADOOP_<SC>_HOME)/conf    # HADOOP_<SC>_DATA     ## a subtree in the local filesystem for storing component's persistent state     ## default value: $(HADOOP_<SC>_HOME)/data    # HADOOP_<SC>_LOG     ## a subdirectory for subcomponents's log files to be stored     ## default value: $(HADOOP_<SC>_HOME)/log    # HADOOP_<SC>_RUN     ## a subdirectory with runtime system specific information     ## default value: $(HADOOP_<SC>_HOME)/run    # HADOOP_<SC>_TMP     ## a subdirectory with temprorary files     ## default value: $(HADOOP_<SC>_HOME)/tmp","build,conf,documentation,scripts"
method clear() in org.apache.hadoop.io.Text does not work,"LineReader reader = new LineReader(in, 4096);  ...    Text text = new Text();  while((reader.readLine(text)) > 0) {       ...       text.clear();  }  }    Even the clear() method is called each time, some bytes are still not filled as zero.  So, when reader.readLine(text) is called in a loop, some bytes are dirty which was from last call.",io
NoClassDefFoundError while running distcp/archive,"bin/hadoop distcp    {noformat}  Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCp  Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.DistCp          at java.net.URLClassLoader$1.run(URLClassLoader.java:202)          at java.security.AccessController.doPrivileged(Native Method)          at java.net.URLClassLoader.findClass(URLClassLoader.java:190)          at java.lang.ClassLoader.loadClass(ClassLoader.java:307)          at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)          at java.lang.ClassLoader.loadClass(ClassLoader.java:248)  Could not find the main class: org.apache.hadoop.tools.DistCp.  Program will exit.    {noformat}    Same is the case while running 'bin/hadoop archive'  ",build
DFS shell get/copy gives weird errors when permissions are wrong with directories,"Let /foo be a *directory* in HDFS (issue does not occur with files) and /bar be a local dir. Do something like:    {code}  $ chmod u-w /bar  $ hadoop -get /foo/myfile /bar  copyToLocal: Permission denied  # correctly tells me permission is denied  $ hadoop -get /foo /bar  copyToLocal: null             $ hadoop -get /foo/ /bar  copyToLocal: No such file or directory  {code}    I've been banging my head for a bit trying to figure out why hadoop thinks my directory doesn't exist, but it turns out the problem was just with my local permissions. The ""Permission denied"" error would've been a lot nicer to get.",fs
Would like to update development getting started pages on hadoop wiki but don't have permission,"I've created an account on the wiki but can't edit pages.    The wiki page http://wiki.apache.org/hadoop/EclipseEnvironment has some out of date information, for example     mvn test -DskipTests  mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true  cd ../; cd mapreduce; ant compile eclipse    should be    mvn install -DskipTests  mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true      'install' is needed instead of test in order to get the build artifacts in the local M2 repo for reference in the eclipse build path.  Also there isn't a 'mapreduce' directory anymore and the mvn eclipse:eclipse command craetes the necessary .project files under the 0.23 'hadoop-mapreduce-project' directory.    I'd also like to add a blurb about using eclipse with m2e/m2eclipse.  For a maven based project many devs would just import the root pom.xmls.  With the new release of the m2e plug-in, this doesn't work anymore as pretty much all targets are not supported by the new 'connector framework' - yes, it is a giant mess.  This means falling back to m2eclipse or just doing the eclipse generation as mention.    Adding a pointer to http://wiki.apache.org/hadoop/HowToContribute such that the requirements to install java, maven, and protoc compilers would be helpful.  The information on getting started from scratch seems a bit scattered and I'd like to help clean that up.    Please let me know how I can help to contribute in this area.",documentation
"Validate XMLs if a relevant tool is available, when using scripts","Given that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves (unless a tool that manages for you is used), it would be good to also validate the provided config XML (*-site.xml) files with a tool like {{xmllint}} or maybe Xerces somehow, when running a command or (at least) when starting up daemons.    We should use this only if a relevant tool is available, and optionally be silent if the env. requests.",scripts
Shell scripts created by hadoop-dist/pom.xml to build tar do not properly propagate failure,"The run() function, as defined in dist-layout-stitching.sh and dist-tar-stitching, created in hadoop-dist/pom.xml, does not properly propagate the error code of a failing command.  See the following:  {code}      ...      ""${@}""                 # call fails with non-zero exit code      if [ $? != 0 ]; then             echo                         echo ""Failed!""               echo                         exit $?            # $?=result of echo above, likely 0, thus exit with code 0      ...  {code}",build
Updated maxIdleTime default in the code to match core-default.xml,"HADOOP-2909 intended to set the server max idle time for a connection to twice the client value. (""The server-side max idle time should be greater than the client-side max idle time, for example, twice of the client-side max idle time."") This way when a server times out a connection it's due a crashed client and not an inactive client so we don't close client connections with outstanding requests (by setting 2x the client value on the server side the client should time out the connection first).    Looks like there was a typo in the patch and it set the default value to 1/5th the client value, instead of the intended 2x.    {noformat}  hadoop2 (pre-HADOOP-4687)$ git reset --hard 6fa4597e  hadoop2 (pre-HADOOP-4687)$ grep -r ipc.client.connection.maxidletime .   ./src/core/org/apache/hadoop/ipc/Client.java:      conf.getInt(""ipc.client.connection.maxidletime"", 10000); //10s  ./src/core/org/apache/hadoop/ipc/Server.java:    this.maxIdleTime = 2*conf.getInt(""ipc.client.connection.maxidletime"", 1000);  {noformat}",ipc
trunk test failure at org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs,I ran mvn test -Dtest=TestViewFileSystemHdfs on trunk and failed with the following error:    {code}  Failed tests:   testGetDelegationTokensWithCredentials(org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs): expected:<0> but was:<1>  {code}    This test was added in HADOOP-7933 .,viewfs
viewfs fails unless all mount points are available,"Obtaining a delegation token via viewfs will attempt to acquire tokens from all filesystems in the mount table.  All clients that obtain tokens, including job submissions, will fail if any of the mount points are unavailable -- even if paths in the unavailable mount will not be accessed.",viewfs
simplify common back to a single jar,The current build is too complex and we should move to a simple project for common.,build
Viewfs needs documentation,Currently the only documentation on how to use viewfs lives in the javadoc. Let's add some basic documentation under common (or perhaps federation since that's the context). ,viewfs
Debian Squeeze install of hadoop 1.0.0 .deb : 64 bit installs on 32 bit squeeze,dpkg -i hadoop_1.0.0-1_amd64.deb on a 32bit squeeze installs instead of complaining that about a 64 bit install on a 32 bit OS,scripts
hadoop 1.0.0 debian installer : hadoop-setup-conf.sh fails on chownsc,"install hadoop_1.0.0-1_i386.deb (""dpkg -i hadoop_1.0.0-1_i386.deb"") - OK  run hadoop-setup-conf.sh and accept defaults  script fails with three lines of ""chown: invalid user: `mr:hadoop'""  followed by configuration setup is completed run hadoop-setup-hdfs.sh  hadoop-setup-hdfs.sh fails with permission errors in the log and run directories",scripts
Remove duplicate definition of default config values,We define default configuration values in two places:  #1 The default.xml files (eg core-default.xml)  #2 The _DEFAULT defines in *Keys.java    This means the defaults used in the code may or may not be dead code based on whether the config is present in the default xml file. Would be good to define these in one place. Eg:  #1 Just have the defines in the code and figure out how to make those accessible as a loadable resource (eg could generate the default files from the defines in the KeysPublic* files)  #2 Remove one of the definitions entirely (possible to live w/o the default files?) or  #3 Remove the overlap between the code and default files,conf
HA: failover should error out if either host is not specified in the configs  ,Per HADOOP-7924 users should not be allowed to specify hosts on the command that are not listed as potential options in the config. This catches the case where someone does a failover X Y but there's a typo in X so we accidentally end up having two actives.,ha
Contradiction in Hadoop Documentation,"http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/util/Progressable.html The statement: ""This is especially important for operations which take an insignificant amount of time since, in-lieu of the reported progress, the framework has to assume that an error has occured and time-out the operation."" in the aforementioned URL directly contradicts http://hadoop.apache.org/common/docs/r0.20.2/mapred_tutorial.html#Reporter which states: ""where the application takes a significant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task.""    The two statements should be reconciled in order to remove confusion.",documentation
Apply audience and stability annotations to classes in Common for 1.x,Port HADOOP-6668 to branch-1.,documentation
Deadlock in class init.,"After HADOOP-7808, client-side commands hang occasionally. There are cyclic dependencies in NetUtils and SecurityUtil class initialization. Upon initial look at the stack trace, two threads deadlock when they hit the either of class init the same time.","security,util"
Support for protocol version and signature in PB,VersionedProtocol methods are currently not supported in PB.,ipc
Add back in the older -getmerge addnl param and add a test case for the new -nl param.,"This is to fix Eli's comments on the parent task:    bq. This breaks a public API, why not preserve it but add in the improved one and update the docs to refer to it? Also, the test for this (HDFS-2070) should be done before the change.",fs
Need generalized multi-token filesystem support,"Multi-token filesystem support and its interactions with the MR {{TokenCache}} is problematic.  The {{TokenCache}} tries to assume it has the knowledge to know if the tokens for a filesystem are available, which it can't possibly know for multi-token filesystems.  Filtered filesystems are also problematic, such as har on viewfs.  When mergeFs is implemented, it too will become a problem with the current implementation.  Currently {{FileSystem}} will leak tokens even when some tokens are already present.    The decision for token acquisition, and which tokens, should be pushed all the way down into the {{FileSystem}} level.  The {{TokenCache}} should be ignorant and simply request tokens from each {{FileSystem}}.","fs,security"
Errant println left in RPC.getHighestSupportedProtocol,"hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java: System.out.println(""Size of protoMap for "" + rpcKind + "" ="" + getProtocolImplMap(rpcKind).size());  ",ipc
"When copying a file out of HDFS, modifying it, and uploading it back into HDFS, the put fails due to a CRC mismatch","Performing an hdfs -get on a file, modifying the file, and using hdfs -put to place the file back into HDFS results in a checksum error.  It seems that the problem is a .crc file being generated locally from the -get command even though the -crc option was NOT specified.",fs
All HAServiceProtocol methods have to throw IOException since they are invoked over the network,HAServiceProtocol methods have to throw ioexception in addition to other exceptions since they are RPCs which are invoked over the network.,ha
HA: HAServiceProtocol exceptions need to be unwrapped before it can be used,Remote exceptions for all the interfaces in HAServiceProtocol class has to be unwrapped before it can be used. All the HAServiceProtocol interfaces throw exceptions of type RemoteException by default.,ha
DistributedFileSystem close has severe consequences,"The way {{FileSystem#close}} works is very problematic.  Since the {{FileSystems}} are cached, any {{close}} by any caller will cause problems for every other reference to it.  Will add more detail in the comments.",fs
TestViewFsTrash incorrectly determines the user's home directory,"HADOOP-7284 added a test called TestViewFsTrash which contains the following code to determine the user's home directory. It only works if the user's directory is one level deep, and breaks if the home directory is more than one level deep (eg user hudson, who's home dir might be /usr/lib/hudson instead of /home/hudson).    {code}      // create a link for home directory so that trash path works      // set up viewfs's home dir root to point to home dir root on target      // But home dir is different on linux, mac etc.      // Figure it out by calling home dir on target           String homeDir = fsTarget.getHomeDirectory().toUri().getPath();     int indexOf2ndSlash = homeDir.indexOf('/', 1);     String homeDirRoot = homeDir.substring(0, indexOf2ndSlash);     ConfigUtil.addLink(conf, homeDirRoot,         fsTarget.makeQualified(new Path(homeDirRoot)).toUri());      ConfigUtil.setHomeDirConf(conf, homeDirRoot);     Log.info(""Home dir base "" + homeDirRoot);  {code}    Seems like we should instead search from the end of the path for the last slash and use that as the base, ie ask the home directory for its parent.              ","fs,test"
Allow Hadoop clients and services to run in an OSGi container,"There's been past discussion on running Hadoop client and service code in OSGi. This JIRA issue exists to wrap up the needs and issues.     # client-side use of public Hadoop APIs would seem most important.  # service-side deployments could offer benefits. The non-standard Hadoop Java security configuration may interfere with this goal.  # testing would all be functional with dependencies on external services, to make things harder.",util
Native code: configure LDFLAGS and CXXFLAGS to fix the build on systems like Ubuntu 11.10,"I noticed that the build of Hadoop trunk (0.24) and the 1.0/0.20.20x branches fail on Ubuntu 11.10 when trying to include the native code in the build. The reason is that the default behavior of {{ld}} was changed in Ubuntu 11.10.    *Background*    From [Ubuntu 11.10 Release Notes|https://wiki.ubuntu.com/OneiricOcelot/ReleaseNotes#GCC_4.6_Toolchain]:    {code}      The compiler passes by default two additional flags to the linker:        [...snipp...]        -Wl,--as-needed with this option the linker will only add a DT_NEEDED tag      for a dynamic library mentioned on the command line if if the library is      actually used.  {code}    This was apparently planned to be changed already back in 11.04 but was eventually reverted in the final release. From [11.04 Toolchain Transition|https://wiki.ubuntu.com/NattyNarwhal/ToolchainTransition#Indirect_Linking_for_Shared_Libraries]:    {quote}  Also in Natty, ld runs with the {{\--as-needed}} option enabled by default.  This means that, in the example above, if no symbols from {{libwheel}} were needed by racetrack, then {{libwheel}} would not be linked even if it was explicitly included in the command-line compiler flags. NOTE: The ld {{\--as-needed}} default was reverted for the final natty release, and will be re-enabled in the o-series.  {quote}    I already run into the same issue with Hadoop-LZO (https://github.com/kevinweil/hadoop-lzo/issues/33).  See the link and the patch for more details.  For Hadoop, the problematic configure script is {{native/configure}}.    *How to reproduce*    There are two ways to reproduce, depending on the OS you have at hand.    1. Use a stock Ubuntu 11.10 box and run a build that also compiles the native libs:    {code}  # in the top level directory of the 'hadoop-common' repo,  # i.e. where the BUILDING.txt file resides    $ mvn -Pnative compile  {code}    2. If you do not have Ubuntu 11.10 at hand, simply add {{-Wl,\--as-needed}} explicitly to {{LDFLAGS}}.  This configures {{ld}} to work like Ubuntu 11.10's default behavior.      *Error message (for trunk/0.24)*    Running the above build command will produce the following output (I added {{-e -X}} switches to mvn).    {code}  [DEBUG] Executing: /bin/sh -l -c cd /home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native && make DESTDIR=/home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native/target install  [INFO] /bin/bash ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I/usr/lib/jvm/default-java/include -I/usr/lib/jvm/default-java/include/linux -I/home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native/src -I/home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native/javah -I/usr/local/include -g -Wall -fPIC -O2 -m64 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF .deps/ZlibCompressor.Tpo -c -o ZlibCompressor.lo `test -f 'src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c' || echo './'`src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c  [INFO] libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I/usr/lib/jvm/default-java/include -I/usr/lib/jvm/default-java/include/linux -I/home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native/src -I/home/mnoll/programming/git/hadoop/hadoop-common/hadoop-common-project/hadoop-common/target/native/javah -I/usr/local/include -g -Wall -fPIC -O2 -m64 -g -O2 -MT ZlibCompressor.lo -MD -MP -MF .deps/ZlibCompressor.Tpo -c src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c  -fPIC -DPIC -o .libs/ZlibCompressor.o  [INFO] src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c: In function 'Java_org_apache_hadoop_io_compress_zlib_ZlibCompressor_initIDs':  [INFO] src/org/apache/hadoop/io/compress/zlib/ZlibCompressor.c:71:41: error: expected expression before ',' token  [INFO] make: *** [ZlibCompressor.lo] Error 1  {code}      *How to fix*    The fix involves adding proper settings for {{LDFLAGS}} to the build config.  In trunk, this is {{hadoop-common-project/hadoop-common/pom.xml}}.  In branches 1.0 and 0.20.20x, this is {{build.xml}}.    Basically, the fix explicitly adds {{-Wl,\--no-as-needed}} to {{LDFLAGS}}.  Special care must be taken not to add this option when running on Mac OS as its version of ld does not support this option (and does not need it because by default it behaves as desired).",build
API Compatibility between 0.23 and 1.0 in org.apache.hadoop.io.compress.Decompressor,HADOOP-6835 introduced in org.apache.hadoop.io.compress.Decompressor the public int getRemaining() API. The forces custom decompressors to implement the new API in order to continue to be used.,io
Improve documentation for org.apache.hadoop.io.compress.Decompressor.getRemaining,None,io
UserGroupInformation fails to login if thread's context classloader can't load HadoopLoginModule,"In a few hard-to-reproduce situations, we've seen a problem where the UGI login call causes a failure to login exception with the following cause:    Caused by: javax.security.auth.login.LoginException: unable to find   LoginModule class: org.apache.hadoop.security.UserGroupInformation   $HadoopLoginModule    After a bunch of debugging, I determined that this happens when the login occurs in a thread whose Context ClassLoader has been set to null.",security
HA: failover should be able to pass args to fencers,"Currently fencing method args are passed in the config (eg ""sshfence(host1,8022)"" indicates to fence the service running on port 8022 on host1. The target service to fence should be determined by the failover (we fence the currently active service) not configured statically.",ha
Add hadoop --loglevel option to change log level,"It would be helpful if bin/hadoop had --loglevel option to change the log level. Currently users need to set an env variable or prefix the command (eg ""HADOOP_ROOT_LOGGER=DEBUG,console hadoop distcp"") which isn't very user friendly.",scripts
maven build should be super fast when there are no changes,"I use this command ""mvn -Pdist -P-cbuild -Dmaven.javadoc.skip -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",build
Support setting the run-as user in unsecure mode,"Some applications need to be able to perform actions (such as launch MR jobs) from map or reduce tasks. In earlier unsecure versions of hadoop (20.x), it was possible to do this by setting user.name in the configuration. But in 20.205 and 1.0, when running in unsecure mode, this does not work. (In secure mode, you can do this using the kerberos credentials).",security
HA: the FailoverController should check the standby is ready before failing over,"The FC in preFailoverChecks should check that the target service is ready for failover (eg in the case of HDFS, that it's not in SM) before failing over to it. We'll need to provide a force option as well since the standby may not actually be ready until the failover has been initiated (eg because becoming active kicks it to check the log and see newly allocated blocks).",ha
Hadoop ignores old-style config options for enabling compressed output,Hadoop seems to ignore the config options even though they are printed as deprecation warnings in the log: mapred.output.compress and  mapred.output.compression.codec    - settings that work on 0.20 but not on 0.23  mapred.output.compress=true  mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec    - settings that work on 0.23  mapreduce.output.fileoutputformat.compress=true  mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec    This breaks backwards compatibility and causes existing jobs to fail.    This was found to happen due to the JobSubmitter writing out the job.xml file with the old-style configs and can be fixed by handdling deprecation before the file is written out.  ,conf
"GenericOptionsParser ought to have better options parsing, and not pick only the options in the front","The ToolRunner provided GenericOptionsParser stops parsing known options when it encounters the first unknown option string in the String[] today.    Ideally we should have it be more intelligent and sift through the whole array to pick up all opts, and not just the opts that are at the front of the invoked CLI.",util
change location of the native libraries to lib instead of lib/native,None,"build,conf,documentation,scripts"
SequenceFile.createWriter(...createParent...) no longer works on existing file,"SequenceFile.createWriter no longer works on an existing file, because old version specified OVEWRITE by default and new version does not.  This breaks some HBase tests.    Tested against trunk.    Patch with test to follow.",io
CheckFileSystem does not correctly honor setVerifyChecksum,"Regardless of the verify checksum flag, {{ChecksumFileSystem#open}} will instantiate a {{ChecksumFSInputChecker}} instead of a normal stream.",fs
"""hadoop archive"" fails with ClassNotFoundException","Running ""hadoop archive"" from a command prompt results in this error:    Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/tools/HadoopArchives  Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.HadoopArchives    java.net.URLClassLoader$1.run(URLClassLoader.java:202)    java.security.AccessController.doPrivileged(Native Method)    java.net.URLClassLoader.findClass(URLClassLoader.java:190)    java.lang.ClassLoader.loadClass(ClassLoader.java:306)    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)    java.lang.ClassLoader.loadClass(ClassLoader.java:247)  Could not find the main class: org.apache.hadoop.tools.HadoopArchives.  Program will exit.      The hadoop front-end script expects the TOOL_PATH environment variable to be set, but nothing provides a default value for it if it is not set.  Since $TOOL_PATH expands to nothing, the hadoop-archives jar under share/hadoop/tools/lib isn't found, and we end up with a ClassNotFound exception.",scripts
ChecksumFileSystem's rename doesn't correctly handle checksum files,"Rename will move the src file and its crc *if present* to the destination.  If the src file has no crc, but the destination already exists with a crc, then src will be associated with the old file's crc.  Subsequent access to the file will fail with checksum errors.",fs
TestFSInputChecker is failing in trunk.,Trunk build number 939 failed with TestFSInputChecker.  https://builds.apache.org/job/Hadoop-Hdfs-trunk/939/    junit.framework.AssertionFailedError: expected:<10> but was:<0>    junit.framework.Assert.fail(Assert.java:47)    junit.framework.Assert.failNotEquals(Assert.java:283)    junit.framework.Assert.assertEquals(Assert.java:64)    junit.framework.Assert.assertEquals(Assert.java:130)    junit.framework.Assert.assertEquals(Assert.java:136)    org.apache.hadoop.hdfs.TestFSInputChecker.checkSkip(TestFSInputChecker.java:194)    org.apache.hadoop.hdfs.TestFSInputChecker.testChecker(TestFSInputChecker.java:224)  ,fs
Make SplitCompressionInputStream an interface instead of an abstract class,"To be splittable, a codec must extend SplittableCompressionCodec which has a function returning a SplitCompressionInputStream.    SplitCompressionInputStream is an abstract class which extends CompressionInputStream, the lowest level compression stream class.    So, no codec that wants to be splittable can reuse any code from DecompressorStream or BlockDecompressorStream.    You either have to duplicate that code, or not be splittable.    SplitCompressionInputStream adds just a few very thin functions. Can we make this an interface rather than an abstract class to allow splittable decompression streams to extend DecompressorStream, BlockDecompressorStream, or whatever else we should scheme up in the future?    To my knowledge, this would impact only the BZip2 codec. None of the other implement this form of splittability yet.    LineRecordReader looks only at whether the codec is an instance of SplittableCompressionCodec, and then calls the appropriate version of createInputStream. This would not change, so the application code should not have to change, just BZip and SplitCompressionInputStream.",io
Multiple SLF4J binding message in .out file for all daemons,{code:xml}  SLF4J: Class path contains multiple SLF4J bindings.  SLF4J: Found binding in [jar:file:/home/hadoop/install/hadoop-0.23.1-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]  SLF4J: Found binding in [jar:file:/home/hadoop/install/hadoop-0.23.1-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]  SLF4J: Found binding in [jar:file:/home/hadoop/install/hadoop-0.23.1-SNAPSHOT/share/hadoop/mapreduce/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]  SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.  {code},build
HA: use substitution token for fencing argument,"Per HADOOP-7983 currently the fencer always passes the target host:port to fence as the first argument to the fence script, it would be better to use a substitution token. That is to say, the user would configure ""myfence.sh $TARGETHOST foo bar"" and Hadoop would substitute the target. This would allow use of pre-existing scripts that might have a different ordering of arguments without a wrapper.",ha
Document how to change the default loglevels,"The hadoop.root.logger defined in log4j.properties is only used by applications. The scripts (bin and daemons) always set it to HADOOP_ROOT_LOGGER or ""INFO,console"" so the installed log4j.properties appears to be ignored. The wiki doesn't cover this:  http://wiki.apache.org/hadoop/HowToConfigure    We should update the cluster setup docs to indicate how the loglevel is configured and also add a comment in log4j.properties that the hadoop.root.logger defined there is not used by Hadoop itself, that HADOOP_ROOT_LOGGER must be set. This is relevant for commands and daemons, though the latter also has daemonlog (http://hadoop.apache.org/common/docs/current/commands_manual.html#daemonlog).  ",documentation
Create hadoop-client and hadoop-minicluster artifacts for downstream projects ,"Using Hadoop from projects like Pig/Hive/Sqoop/Flume/Oozie or any in-house system that interacts with Hadoop is quite challenging for the following reasons:    * *Different versions of Hadoop produce different artifacts:* Before Hadoop 0.23 there was a single artifact hadoop-core, starting with Hadoop 0.23 there are several (common, hdfs, mapred*, yarn*)    * *There are no 'client' artifacts:* Current artifacts include all JARs needed to run the services, thus bringing into clients several JARs that are not used for job submission/monitoring (servlet, jsp, tomcat, jersey, etc.)    * *Doing testing on the client side is also quite challenging as more artifacts have to be included than the dependencies define:* for example, the history-server artifact has to be explicitly included. If using Hadoop 1 artifacts, jersey-server has to be explicitly included.    * *3rd party dependencies change in Hadoop from version to version:* This makes things complicated for projects that have to deal with multiple versions of Hadoop as their exclusions list become a huge mix & match of artifacts from different Hadoop versions and it may be break things when a particular version of Hadoop requires a dependency that other version of Hadoop does not require.    Because of this it would be quite convenient to have the following 'aggregator' artifacts:    * *org.apache.hadoop:hadoop-client* : it includes all required JARs to use Hadoop client APIs (excluding all JARs that are not needed for it)  * *org.apache.hadoop:hadoop-minicluster* : it includes all required JARs to run Hadoop Mini Clusters    These aggregator artifacts would be created for current branches under development (trunk, 0.22, 0.23, 1.0) and for released versions that are still in use.    For branches under development, these artifacts would be generated as part of the build.    For released versions we would have a a special branch used only as vehicle for publishing the corresponding 'aggregator' artifacts.  ",build
hadoop-config.sh spews error message when HADOOP_HOME_WARN_SUPPRESS is set to true and HADOOP_HOME is present,Running hadoop daemon commands when HADOOP_HOME_WARN_SUPPRESS is set to true and HADOOP_HOME is present produces:  {noformat}    [: 76: true: unexpected operator  {noformat},scripts
hadoop-daemon.sh and yarn-daemon.sh are trying to mkdir and chow log/pid dirs which can fail,"Here's what I see when using Hadoop in Bigtop:    {noformat}  $ sudo /sbin/service hadoop-hdfs-namenode start  Starting Hadoop namenode daemon (hadoop-namenode): chown: changing ownership of `/var/log/hadoop': Operation not permitted  starting namenode, logging to /var/log/hadoop/hadoop-hdfs-namenode-centos5.out  {noformat}    This is a cosmetic issue, but it would be nice to fix it.",scripts
ViewFileSystem does not honor setVerifyChecksum,{{ViewFileSystem#setVerifyChecksum}} is a no-op.  It should call {{setVerifyChecksum}} on the mount points.,fs
"ViewFileSystem does not correctly implement getDefaultBlockSize, getDefaultReplication, getContentSummary",{{ViewFileSystem}} incorrectly returns the {{FileSystem}} default values for {{getDefaultBlockSize()}} and {{getDefaultReplication()}}.  This causes files to be created with incorrect values.  The problem is that the current apis are insufficient for viewfs because the defaults depend on the underlying mount point.  These methods need counterparts that accept a {{Path}} so viewfs can resolve the mount point for a path.,fs
ChRootFileSystem should extend FilterFileSystem,"{{ChRootFileSystem}} simply extends {{FileSystem}}, and attempts to delegate some methods to the underlying mount point.  It is essentially the same as {{FilterFileSystem}} but it mangles the paths to include the chroot path.  Unfortunately {{ChRootFileSystem}} is not delegating some methods that should be delegated.  Changing the inheritance will prevent a copy-n-paste of code for HADOOP-8013 and HADOOP-8014 into both {{ChRootFileSystem}} and {{FilterFileSystem}}.",fs
Configure hadoop-main pom to get rid of M2E plugin execution not covered,Last M2Eclipse plugin (maven plugin for eclipse) shows nasty errors when importing the hadoop maven modules (read more on http://wiki.eclipse.org/M2E_plugin_execution_not_covered).    The solution is to configure the build section of the pom with a org.eclipse.m2e:lifecycle-mapping plugin.    This configuration has no influence on the Maven build itself.  ,build
Reduce the allowed Javadoc warnings from 13 to 11,"OK_JAVADOC_WARNINGS is set too high in hadoop-common-project/dev-support/test-patch.properties    {noformat}  $ cd hadoop-common-project/  $ mvn clean test javadoc:javadoc -DskipTests -Pdocs -DHadoopPatchProcess > ~/patchJavadocWarnings.txt.hadoop-trunk 2>&1  $ grep '\[WARNING\]' ~/patchJavadocWarnings.txt.hadoop-trunk | awk '/Javadoc Warnings/,EOF' | grep warning | awk 'BEGIN {total = 0} {total += 1} END {print total}'  11  {noformat}    {noformat}  $ cat dev-support/test-patch.properties  OK_RELEASEAUDIT_WARNINGS=0  OK_FINDBUGS_WARNINGS=0  OK_JAVADOC_WARNINGS=13  {noformat}    This will allow in 2 new javadoc warnings and still +1 the build",build
Hadoop ignores old-style config options for enabling compressed output when passed on from PIG,Hadoop seems to ignore the config options even though they are printed as deprecation warnings in the log: mapred.output.compress and  mapred.output.compression.codec    - settings that work on 0.20 but not on 0.23  mapred.output.compress=true  mapred.output.compression.codec=org.apache.hadoop.io.compress.BZip2Codec    - settings that work on 0.23  mapreduce.output.fileoutputformat.compress=true  mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec    This breaks backwards compatibility and causes existing jobs to fail.    This is different from HADOOP-7993 in that that one fixes any jobs run from the CLI but PIG jobs are still facing this issue.,conf
Deprecate checkTGTAndReloginFromKeytab(),"checkTGTAndReloginFromKeytab() does a small check and then calls reloginFromKeytab() which has been updated to do the same check. checkTGTAndReloginFromKeytab() is redundant, and should be deprecated if it is publicly visible, or just removed if it is not publicly visible.",security
Add unset() method to Configuration,"HADOOP-7001 introduced the *Configuration.unset(String)* method.    MAPREDUCE-3727 requires that method in order to be back-ported.    This is required to fix an issue manifested when running MR/Hive/Sqoop jobs from Oozie, details are in MAPREDUCE-3727.  ",conf
Debugging Hadoop daemons in Eclipse / Netbeans debugger,To debug Hadoop daemons for prototyping; I discovered the use of Eclipse and Netbeans for the purpose.  To perform this import the source code of Hadoop as a Java Project (you will have to do some refactoring to make sure the imports and packages are correct)  instead of an ant project or importing an existing project.  Please make sure the apache-commons libraries are in classpath and you should now be able to launch the particular daemon from their respective packages inside the debugger.  I find it particularly useful to use the eclipse and netbeans standard debugger to perform any code increments as its fairly simple to perform stacktrace and point to the exact code error.  ,test
Mbeans and MBeanUtil are almost duplicated in code.,Looks Mbeans.java and MBeanUtil.java are duplicated and has the similar set of apis.   Not sure it was intentional to keep that.,metrics
org.apache.hadoop.io.nativeio.NativeIO.posixFadviseIfPossible does not handle EINVAL,"When Hadoop's directories reside on tmpfs in Debian Wheezy (and possibly all Linux 3.1 distros) in an installation that is using the native libraries fadvise returns EINVAL when trying to run a MapReduce job.  Since EINVAL isn't handled all MapReduce jobs report ""Map output lost, rescheduling: getMapOutput"".    A full stack trace for this issue looks like this:    [exec] 12/02/03 09:50:58 INFO mapred.JobClient: Task Id : attempt_201202030949_0001_m_000000_0, Status : FAILED  [exec] Map output lost, rescheduling: getMapOutput(attempt_201202030949_0001_m_000000_0,0) failed :  [exec] EINVAL: Invalid argument  [exec] at org.apache.hadoop.io.nativeio.NativeIO.posix_fadvise(Native Method)  [exec] at org.apache.hadoop.io.nativeio.NativeIO.posixFadviseIfPossible(NativeIO.java:177)  [exec] at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:4026)  [exec] at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)  [exec] at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)  [exec] at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)  [exec] at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)  [exec] at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:829)  [exec] at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)  [exec] at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)  [exec] at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)    Some logic will need to be implemented to handle EINVAL to properly support all file systems.",native
Configuration class can throw exceptions in quiet mode; quiet mode usage unclear,"hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java          if (doc == null && root == null) {          if (quiet)            return;          throw new RuntimeException(name + "" not found"");        }      Unclear why a diagnostic mode would throw an exception, while normally off it would not.    It's also not clear why 'quiet' is being passed around as well. Why not use logging levels and logging configuration and remove 'quiet' mode altogether?",conf
Configuration class fails to find embedded .jar resources; should use URL.openStream(),"While running a hadoop client within RHQ (monitoring software) using its classloader, I see this:    2012-02-07 09:15:25,313 INFO  [ResourceContainer.invoker.daemon-2] (org.apache.hadoop.conf.Configuration)- parsing jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml  2012-02-07 09:15:25,318 ERROR [InventoryManager.discovery-1] (rhq.core.pc.inventory.InventoryManager)- Failed to start component for Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com] from synchronized merge.  org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].  Caused by: java.lang.RuntimeException: core-site.xml not found    org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)    org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)    org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)    org.apache.hadoop.conf.Configuration.set(Configuration.java:438)    This is because the URL    jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml    cannot be found by DocumentBuilder (doesn't understand it). (Note: the logs are for an old version of Configuration class, but the new version has the same code.)    The solution is to obtain the resource stream directly from the URL object itself.    That is to say:    {code}           URL url = getResource((String)name);  -        if (url != null) {  -          if (!quiet) {  -            LOG.info(""parsing "" + url);  -          }  -          doc = builder.parse(url.toString());  -        }  +        doc = builder.parse(url.openStream());  {code}    Note: I have a full patch pending approval at Apple for this change, including some cleanup.",conf
mvn site:stage-deploy should be able to use the scp protocol to stage documents,mvn site:stage-deploy should be able to use the scp protocol to stage documents.    http://maven.apache.org/plugins/maven-site-plugin/examples/adding-deploy-protocol.html shows what we need to add to the pom.xml,"build,documentation"
HADOOP_JAVA_PLATFORM_OPS is no longer respected,"HADOOP-6284 introduced HADOOP_JAVA_PLATFORM_OPS and it's in branch-1, however it's not in trunk or 22, 23. It's referenced in hadoop-env.sh (commented out) but not actually used anywhere, not sure when it was removed from bin/hadoop. Perhaps the intention was to just use HADOOP_OPTS?",conf
Add comment/docs to enable assertions in hadoop-env.sh,"For testing it's useful to enable jvm assertions.   Let's add a commented out line to hadoop-env.sh that sets the ""-enableassertions"" and ""-enablesystemassertions"" jvm options to make it easier to run ""debug"" builds. Per HADOOP-8033 perhaps this should be HADOOP_JAVA_PLATFORM_OPTS.  ","conf,scripts"
Hadoop Maven site is inefficient and runs phases redundantly,"The Hadoop 0.23 Maven build has some major inefficiencies, most notably in site and javadoc. As a result of redundant plugin executions and certain plugin executions being inherited when they really shouldn't be, builds take a lot longer than they should.",build
TestViewFsTrash assumes the user's home directory is 2 levels deep,"Looks like HADOOP-7974 didn't fix the issue. Still get ""Path /var already exists as dir; cannot create link here Stacktrace"" when running on jenkins with home dir /var/lib/jenkins.    {noformat}  org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here    org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)    org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)    org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:165)  {noformat}","fs,test"
"Binary tarball does not preserve platform info for native builds, and RPMs fail to provide needed symlinks for libhadoop.so","The source tarball uses ""package"" ant target, which includes both sets of native builds (32 and 64 bit libraries), under subdirectories that are named for the supported platform, so you can tell what they are.    The binary tarball uses the ""bin-package"" ant target, which projects both sets of native builds into a single directory, stripping out the platform names from the directory paths.  Since the native built libraries have identical names, only one of each survives the process.  Afterward, there is no way to know whether they are intended for 32 or 64 bit environments.    It seems to be done this way as a step toward building the rpm and deb artifacts.  But the rpms and debs are self-identifying as to the platform they were built for, and contain only one set of libs each, while the binary tarball isn't.  The binary tarball should have the same platform-specific subdirectories that the full tarball does; but this means that the rpm and deb builds have to be more careful about include/exclude specs for what goes into those artifacts.  ",build
HA: Add 'ipc.client.connect.max.retries.on.timeouts' entry in core-default.xml file.,"HADOOP-7932 added the new property 'ipc.client.connect.max.retries.on.timeouts', which needs to be added in core-default.xml with documentation.","conf,ha"
mvn site:stage-deploy should not have broken links.,The stage-deployed site has a lot of broken links / missing pages. We should fix that.,"build,documentation"
Add symlink support to FileSystem,"HADOOP-6421 added symbolic links to FileContext. Resolving symlinks is done on the client-side, and therefore requires client support. An HDFS symlink (created by FileContext) when accessed by FileSystem will result in an unhandled UnresolvedLinkException. Because not all users will migrate from FileSystem to FileContext in lock step, and we want users of FileSystem to be able to access all paths created by FileContext, we need to support symlink resolution in FileSystem as well, to facilitate migration to FileContext.",fs
HA: log a warning when a failover is first attempted ,"Currently we always warn for each client operation made to a NN we've failedover to:    {noformat}  hadoop-0.24.0-SNAPSHOT $ ./bin/hdfs dfs -lsr /  12/02/08 17:43:04 WARN retry.RetryInvocationHandler: Exception while invoking getFileInfo of class  org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB after 0 fail over attempts. Trying to fail over immediately.  {noformat}    I'm going to remove this warning in HDFS-2922 since we shouldn't warn every time a client performs an operation, eg could be weeks after the failover. But we should eg log a warning when the client first does a failover so it shows up eg in the MR and HBase logs.",ha
KerberosAuthenticationFilter and friends have some problems,KerberosAuthenticationFilter and friends have three killer usability issues and bugs:    1. Documentation is misleading/wrong.  2. Shared secret stored in a world readable file.  3. Lacks support for _HOST macro  ,security
Unify ProtocolMetaInterface and ProtocolTranslator interface,"In the HA branch we have a ""ProtocolTranslator"" interface which we had to add, and it seems to have the same purpose as the ""ProtocolMetaInterface"" interface which got added in trunk in the meantime. We should unify the two.","ha,ipc"
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs does not handle many files well,"We were tryong to use MultipleOutputs to write one file per key. This produced the error:    exception:  org.apache.hadoop.ipc.RemoteException: java.io.IOException: File  /user/me/part6/_temporary/  _attempt_201202071305_0017_r_000000_2/2011-11-18-22-  attempt_201202071305_0017_r_000000_2-r-00000  could only be replicated to 0 nodes, instead of 1      at  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:  1520)      at  org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:  665)      at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)      at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:  25)      at java.lang.reflect.Method.invoke(Method.java:597)      at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)      at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434)      at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430)      at java.security.AccessController.doPrivileged(Native Method)      at javax.security.auth.Subject.doAs(Subject.java:396)      at  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:  1157)     When the nr. of files processed increased over 20 on a single developer system.     The solution proved to be to close each RecordWriter when the reducer was finished with a key, something that required that we extended the multiple outputs to fetch the recordwriter - not a good solution.     ",io
CachedDNSToSwitchMapping caches negative results forever,"This is very minor, just worth filing in JIRA unless someone wants to rethink topology caching for a dynamic world.    # The CachedDNSToSwitchMapping caches the results from all relayed DNS queries.  # The DNS script mapper returns the default rack for all unknown entries (or when the script fails)  # The Cache stores this in its map and never re-resolves it.    As a result, if a node is added to a live cluster that the existing script cannot resolve, then it won't get assigned to a rack unless the script is updated before the rack map is resolved.     This isn't usually that important, it just means ""update your scripts before adding new racks"". Perhaps there should be a page on that activity, ""runbook and checklist for adding new servers and racks"".    Where it would matter if anyone started playing with dynamic topologies, but in that situation the cached mapping itself would become the liability, as it assumes that servers never switch switches in a live system: the topology is static for existing nodes. ",util
Allow merging of Credentials,"There is an existing {{Credentials#addAll}} which combines two {{Credentials}}, but it overwrites all existing {{Credentials}}.  There should be a {{Credentials#mergeAll}} that will not overwrite.  This will facility the cleanup of code in {{TokenCache}} denoted with:  {code}  //TODO: Need to come up with a better place to put  //this block of code to do with reading the file  {code}    The token cache basically needs to merge the contents of a binary credentials file when it fails to find a token.  Performing the merge within {{Credentials}} is cleaner, and will break the cross-component dependency whereby the {{TokenCache}} currently has to have intimate knowledge of how the {{FileSystem}} will key tokens in the cache.",util
Add entry point/script to do preflight checking of script resolution,"The script supplied by the ops team could have many problems  # not run  # have the wrong permissions  # fail to parse/resolve some or all hosts  # not be the string set in the configuration file.    Diagnostics could be improved here with an entry point that  # reads in the configuration file  # instantiates whatever mapper is declared  # attempts to resolve all hosts passed in as arguments, logging success and failures  # prints out a topology map at the end  A key goal would be to recognise and report a complete failure to report a mapping as per MAPREDUCE-50, so let people identify problems in their script before updating the cluster    Test plan  -Java entry point: invoke it with arguments, get the output as a string and scan.  -Shell script. Include whatever post-install script tests are being written.",util
Deadlock in metrics,"The metrics serving thread and the periodic snapshot thread can deadlock.  It happened a few times on one of namenodes we have. When it happens RPC works but the web ui and hftp stop working. I haven't look at the trunk too closely, but it might happen there too.",metrics
HttpFS documentation it is not wired to the generated site,The left navigation bar should have a link to httpfs documentation.,documentation
Hadoop Metrics2 should emit Float.MAX_VALUE (instead of Double.MAX_VALUE) to avoid making Ganglia's gmetad core,"Ganglia's gmetad converts the doubles emitted by Hadoop's Metrics2 system to strings, and the buffer it uses is 256 bytes wide.    When the SampleStat.MinMax class (in org.apache.hadoop.metrics2.util) emits its default min value (currently initialized to Double.MAX_VALUE), it ends up causing a buffer overflow in gmetad, which causes it to core, effectively rendering Ganglia useless (for some, the core is continuous; for others who are more fortunate, it's only a one-time Hadoop-startup-time thing).    The fix needed to Ganglia is simple - the buffer needs to be bumped up to be 512 bytes wide, and all will be well - but instead of requiring a minimum version of Ganglia to work with Hadoop's Metrics2 system, it might be more prudent to just use Float.MAX_VALUE.    An additional problem caused in librrd (which Ganglia uses beneath-the-covers) by the use of Double.MIN_VALUE (which functions as the default max value) is an underflow when librrd runs the received strings through libc's strtod(), but the librrd code is good enough to check for this, and only emits a warning - moving to Float.MIN_VALUE fixes that as well.",metrics
"""har://hdfs-/foo"" is not a valid URI",None,fs
NPE with FilterFileSystem,"While running Hive tests, I'm seeing the following exception with 0.23.1,  {noformat}  ava.lang.NullPointerException          at org.apache.hadoop.fs.FileSystem.getDefaultBlockSize(FileSystem.java:1901)          at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:447)          at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)          at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)          at org.apache.hadoop.fs.ProxyFileSystem.getFileStatus(ProxyFileSystem.java:247)          at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:351)          at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1165)          at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:390)          at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:242)          at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:232)  {noformat}    Did not see this with 0.23.0, though.",fs
Distribution tar.gz does not contain etc/hadoop/core-site.xml,"A dist built from trunk (0.24.0-SNAPSHOT) does not contain a core-site.xml in $HADOOP_HOME/etc/hadoop/ folder.    $HADOOP_HOME/sbin/start-dfs.sh without that folder gives an exception  Exception in thread ""main"" java.lang.IllegalArgumentException: URI has an authority component   at java.io.File.<init>(File.java:368)   at org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageDirectory(NNStorage.java:310)   at org.apache.hadoop.hdfs.server.namenode.FSEditLog.init(FSEditLog.java:178)  ...    Manually creating $HADOOP_HOME/etc/hadoop/core-site.xml solves this problem and hadoop starts fine.    ",build
Configuration doesn't pass empty string values to tasks,"If I assign an *empty string* as a value to a property in a JobConf 'job' while I'm preparing it to run, the Configuration does store that value.  I can retrieve it later while in the same process and the value is maintained.    However, if then call JobClient.runJob(job), the Configuration that is received by the Map and Reduce tasks doesn't contain the property, and calling JobConf.get with that property name returns null (instead of an empty string).  Futher, if I inspect the job's configuration via Hadoop's web interface, the property isn't present.    It seems as if whatever serialization mechanism that is used to transmit the Configuration from the job client to the tasks discards properties with """" value.  ",conf
hadoop-setup-conf.sh not working because of some extra spaces.,"hadoop-setup-conf.sh is not working because of some extra spaces at the end of line, while configuring the getopt.",scripts
Configure Jenkins to run test from root trunk,"The current Jenkins configuration does a 'cd hadoop-common-project' before invoking $MAVEN_HOME/bin/mvn test -Pclover..., this is why we only have a partial report (only for hadoop-common). See for example https://builds.apache.org/job/Hadoop-Common-trunk/315/testReport/    If we had the complete report, anyone could compare its local tests to the Jenkins ones (supposed to be the reference).    If we do this, there can be more Jenkins mails on the lists (unstability, errors...), but I find the advantages worth the price.    Thx,  Eric  ",build
Add javadoc to InterfaceAudience and InterfaceStability,InterfaceAudience and InterfaceStability javadoc is incomplete. The details from HADOOP-5073.,documentation
Add a capability to discover and set checksum types per file.,"After the improved CRC32C checksum feature became default, some of use cases involving data movement are no longer supported.  For example, when running DistCp to copy from a file stored with the CRC32 checksum to a new cluster with the CRC32C set to default checksum, the final data integrity check fails because of mismatch in checksums.","fs,util"
Back port trunk metrics2 changes to 1.x branch,"For hysterical raisins, metrics2 code in hadoop 1.x branch and trunk diverged. It looks like HBase needs to support both 1.x and 0.23+ for a while, hence the more urgent need to clean up the situation.",metrics
Remove unnecessary dependency on w3c.org in document processing,"Our builds fail occasionally due to validation failures caused by unavailability of w3.org. In fact, w3.org has been fed up with this kind of requests, blacklisting/throttling have been in place for some time.    The ones I've seen are coming from maven pdf plugin v.1.1 used in hadoop-tools/hadoop-distcp.  The input validation has been made configurable in MPDF-39, which requires DOXIA-392 (fixed since doxia-1.1.3). Unfortunately, the maven pdf plugin 1.2 has been cooking for long time and not been released yet. May be we could force it to use a later version of doxia?",build
distcp should have an option to compress data while copying.,We would like compress the data while transferring from our source system to target system. One way to do this is to write a map/reduce job to compress that after/before being transferred. This looks inefficient.   Since distcp already reading writing data it would be better if it can accomplish while doing this.     Flip side of this is that distcp -update option can not check file size before copying data. It can only check for the existence of file.     So I propose if -compress option is given then file size is not checked.    Also when we copy file appropriate extension needs to be added to file depending on compression type.    ,fs
The full docs build intermittently fails,See for example:    https://builds.apache.org/job/Hadoop-Hdfs-trunk/954/  https://builds.apache.org/job/Hadoop-Common-trunk/317/,build
Errors building hadoop-gpl-compression with Hadoop 1.0.0 and HBase 0.92.0,"We were able to successfully configure and run HBase 0.92.0 on top of Hadoop 1.0.0. But when we were following the directions stated on http://wiki.apache.org/hadoop/UsingLzoCompression on enabling LZO compression in HBase we are getting errors building hadoop-gpl-compression with the hadoop-core-1.0.0.jar. These errors seem to be a result of different classes being deprecated.     When we use an older version of hadoop core jar such as hadoop-core-0.20.2.jar, we are able to build the hadoop-gpl-compression-0.1.0-dev.jar and native folder with no errors. Also after copying those files to the HBase library LZO functions without any errors even with HBase being on top of Hadoop 1.0.0.    Is there an LZO build compatible with Hadoop 1.0.0?",build
HA: void methods can swallow exceptions when going through failover path,"While running through scale testing, we saw an issue where clients were getting LeaseExpiredExceptions. We eventually tracked it down to the fact that some {{create}} calls had timed out (having been sent to a NN just before it crashed) but the resulting exception was swallowed by the retry policy code paths.","ha,ipc"
Enable TCP_NODELAY by default for IPC,"I think we should switch the default for the IPC client and server NODELAY options to true. As wikipedia says:  {quote}  In general, since Nagle's algorithm is only a defense against careless applications, it will not benefit a carefully written application that takes proper care of buffering; the algorithm has either no effect, or negative effect on the application.  {quote}  Since our IPC layer is well contained and does its own buffering, we shouldn't be careless.",ipc
Add standalone benchmark of protobuf IPC,"To be more comfortable with the switch to protobuf IPC, I'd like to contribute a standalone benchmark which can start any number of client threads and server threads.","benchmarks,ipc"
Avoid an extra packet in client code when nagling is disabled,"Currently, if you disable TCP_NODELAY in the IPC client, you get an extra packet for each call which contains the call's length. This is unnecessary. Instead, we can just reserve the 4 bytes in the buffer up front, then go back to fill it in before pushing the call to the wire.",ipc
bin/hadoop leaks pids when running a non-detached datanode via jsvc,See: https://issues.cloudera.org/browse/DISTRO-53  ,scripts
Add a test for the jmx metrics serving,"From HADOOP-8050,  bq. ...people can run something like jcarder with the unit tests and detect potential deadlocks.",metrics
Small bug in hadoop error message for unknown commands,"The hadoop fs command should be more user friendly if the user forgets the dash before the command. Also, this should say ""cat"" rather than ""at"".    {noformat}  hadoop-0.24.0-SNAPSHOT $ ./bin/hadoop fs cat  at: Unknown command  {noformat}",scripts
Lower native-hadoop library log from info to debug ,"The following log shows up in stderr all commands. We've already got a warning if the native library can't be loaded, don't need to log this every time at info level.    {noformat}  [eli@centos6 ~]$ hadoop fs -cat /user/eli/foo  12/02/12 20:10:20 INFO util.NativeCodeLoader: Loaded the native-hadoop library  {noformat}  ",native
hadoop (1.x) ant build fetches ivy JAR every time,"the <get ..> task does a timestamp check, as ivy JAR is final release it should use the skip if already downloaded check.",build
HA: fencing method should be able to be configured on a per-NN or per-NS basis,"Currently, the fencing method configuration is global. Given that different nameservices may use different underlying storage mechanisms or different types of PDUs, it would be preferable to allow the fencing method configuration to be scoped by namenode or nameservice.",ha
Add capability to turn on security in unit tests.,"We should be able to start a kdc server for unit tests, so that security could be turned on. This will greatly improve the coverage of unit tests.",test
Proposal for enhancements to Hadoop for Windows Server and Windows Azure development and runtime environments,"This JIRA is intended to capture discussion around proposed work to enhance Apache Hadoop to run well on Windows.  Apache Hadoop has worked on Microsoft Windows since its inception, but Windows support has never been a priority. Currently Windows works as a development and testing platform for Hadoop, but Hadoop is not natively integrated, full-featured or performance and scalability tuned for Windows Server or Windows Azure.  We would like to change this and engage in a dialog with the broader community on the architectural design points for making Windows (enterprise and cloud) an excellent runtime and deployment environment for Hadoop.       The Isotope team at Microsoft (names below) has developed an Apache Hadoop 1.0 patch set that addresses these performance, integration and feature gaps, allowing Apache Hadoop to be used with Azure and Windows Server without recourse to virtualization technologies such as Cygwin. We have significant interest in the deployment of Hadoop across many multi-tenant, PaaS and IaaS environments - which bring their own unique requirements.     Microsoft has recently completed a CCLA with Apache and would like to contribute these enhancements back to the Apache Hadoop community.    In the interest of improving Apache Hadoop so that it runs more smoothly on all platforms, including Windows, we propose first contributing this work to the Apache community by attaching it to this JIRA.  From there we would like to work with the community to refine the patch set until it is ready to be merged into the Apache trunk.    Your feedback solicited,     Alexander Stojanovic  Min Wei  David Lao  Lengning Liu  David Zhang  Asad Khan",native
"hadoop-common/src/main/native does not have a directory which name is ""m4""","There is a such line in configure.ac    AC_CONFIG_MACRO_DIR([m4])    But there doesn't have such a directory, so  if the version of automake is too high, autoreconf will fail.    Here is a minor patch for it:  ===================================================================  --- hadoop-common-project/hadoop-common/src/main/native/configure.ac    (revision 1244865)  +++ hadoop-common-project/hadoop-common/src/main/native/configure.ac    (working copy)  @@ -37,7 +37,7 @@   AC_INIT(src/org_apache_hadoop.h)   AC_CONFIG_SRCDIR([src/org_apache_hadoop.h])   AC_CONFIG_AUX_DIR([config])  -AC_CONFIG_MACRO_DIR([m4])  +#AC_CONFIG_MACRO_DIR([m4])   AC_CONFIG_HEADER([config.h])   AC_SYS_LARGEFILE   AC_GNU_SOURCE  ",native
HOD Should Be Less Restrictive Over Options Passed To Resource Manager,"At present, HOD requires any extra options passed to the resource manager to be of the form <option>:<sub-option>=<value>    This is quite restrictive. Considering Torque and qsub, this effectively limits the options to a subset of the resource constraints, variable lists and additional attributes. Potentially desirable attributes that cannot (to the best of my knowledge) be expressed:   - Mailing options. Given than it may take some time for a long-running HOD job to be scheduled, it would be useful to exploit the options (-m and -M) for notifying users   - Other resource constraints, such as particular nodes or number of processors per node. The format required by qsub(http://www.clusterresources.com/torquedocs/2.1jobsubmission.shtml#nodeExamples) is incompatible with the format required by HOD. In practice, it would probably be desirable to have these options specified via the command line as these may change per HOD job    I imagine some thought would be needed to balance flexibility whist not breaking the options HOD sets by default.",contrib/hod
add hadoop-client and hadoop-minicluster to the dependency-management section,This will allow other Hadoop sub-projects to use those artifacts without having to specify the version.,build
javadoc generation for some modules is not done under target/,After running a  clean build/dist in some modules an 'api/' directory shows up at module root level.    The javadoc plugin should be configured to work under 'target/',build
Protobuf RPC engine can be optimized to not do copying for the RPC request/response,None,ipc
Add RPC metrics to ProtobufRpcEngine,ProtobufRpcEngine is missing the RPC metrics compared to WritableRpcEngine. It is important information for monitoring the cluster and understanding the server performance.  ,"ipc,metrics"
"KerberosName silently sets defaultRealm to """" if the Kerberos config is not found, it should log a WARN",None,security
User-group mapping cache incorrectly does negative caching on transient failures,"We've seen a case where some getGroups() calls fail when the ldap server or the network is having transient failures. Looking at the code, the shell-based and the JNI-based implementations swallow exceptions and return an empty or partial list. The caller, Groups#getGroups() adds this likely empty list into the mapping cache for the user. This will function as negative caching until the cache expires. I don't think we want negative caching here, but even if we do, it should be intelligent enough to distinguish transient failures from ENOENT. The log message in the jni-based impl also needs an improvement. It should print what exception it encountered instead of just saying one happened.",security
cannot submit job from Eclipse plugin running on Windows,"Built eclipse plugin for 1.0.0 on Windows and using it to submit a job, I got the following error:    12/02/17 17:14:24 ERROR security.UserGroupInformation: PriviledgedActionException as:weilis cause:java.io.IOException: Failed to set permissions of path: \tmp\hadoop-weilis\mapred\staging\weilis753422487\.staging to 0700  Exception in thread ""main"" java.io.IOException: Failed to set permissions of path: \tmp\hadoop-weilis\mapred\staging\weilis753422487\.staging to 0700    org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:682)    org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:655)    org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)    org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:344)    org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:189)    org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116)    org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)    org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)    java.security.AccessController.doPrivileged(Native Method)    javax.security.auth.Subject.doAs(Subject.java:396)    org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)    org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)    org.apache.hadoop.mapreduce.Job.submit(Job.java:465)    org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:495)      The job staging area local on Windows system and I don't think there is a way to set a local file's permission to 0700 anyway. Please have a look.",contrib/eclipse-plugin
Add a topology mapper that reads hostname to rack mappings from a Java properties file,"Getting mapping scripts right is surprisingly hard -and if its wrong in production bad things happen. It would be good to have something simpler for beginners -and one that is trivial to generate by a machine based on infrastructure data.    I propose adding an alternative mapper, one driven by a java property file     # the specific topology mapper must be identified for loading   # it uses another key to identify the property file to load. This is checked for on startup -if missing, fail.   # one property, perhaps ""default-rack"" identifies the default rack mapping for any host not in the list   # every other entry lists a hostname to rack mapping   # hostname mapping is done on the first entry in the FQDN, to be less brittle to domain resolution.    Example  {code}  default-rack=/rack1  host1=/rack1  host2=/rack1  host3=/rack2  host4=/rack2  {code}    Implementation   * add a new mapper that builds a concurrent hash map   * read in every entry in the specified property file, add it to the map   * when queried, extract the hostname (i.e. everything before any ""."")   * match that in the hash table, return if found   * if not found: return the default rack    Feature creep would be to poll this file for changes at a (specified) frequency, and pick up the changes when they occur. This would require removing the caching topology mapper that wraps all others in the NN and RM.  ",util
Hadoop DataNode cannot start up in Pseudo-Distributed mode using start-all.sh if it is run as root,"Hadoop DataNode cannot start-up if you run start-all.sh (or start-dfs.sh) as root.    There is a HADOOP_OPTS setting in bin/hadoop file in case EUID equals 0 :    ...  elif [ ""$COMMAND"" = ""datanode"" ] ; then    CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'    if [[ $EUID -eq 0 ]]; then      HADOOP_OPTS=""$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS""    else      HADOOP_OPTS=""$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS""    fi  elif ...    Since -jvm is not a recognized option by Sun HotSpot Java JVM (64-bit), an error message is generated:    Unrecognized option: -jvm  Could not create the Java virtual machine.    If you replace -jvm server option with -server, DataNode will start up.",scripts
HadoopRpcRequestProto should not be serialize twice,    @Override      public void write(DataOutput out) throws IOException {        out.writeInt(message.toByteArray().length);        out.write(message.toByteArray());      }    The code is not effective.,ipc
Make maven-eclipse-plugin use the spring project nature,"If I want to have multiple versions of Apache Hadoop loaded into my Eclipse IDE today (or any other IDE maybe), I'm supposed to do the following when generating eclipse files, such that the version name is appended to the project name and thereby resolves conflict in project names when I import another version in:    {{mvn -Declipse.addVersionToProjectName=true eclipse:eclipse}}    But this does not work presently due to a lack of configuration in Apache Hadoop, which https://jira.codehaus.org/browse/MECLIPSE-702 demands. The problem being that though the project names are indeed named with version suffixes, the ""related project"" name it carries for dependencies do not carry the same suffix and therefore you have a broken import of projects errors everywhere about 'dependent project <regularname> not found'.    The fix is as Carlo details on https://jira.codehaus.org/browse/MECLIPSE-702 and it works perfectly. I'll attach a patch adding in the same configuration for Apache Hadoop so that the above mechanism is then possible.",build
add single point where System.exit() is called for better handling in containers,"with plans for OSGI integration afoot in HADOOP-7977, Hadoop needs unified place where System.exit() calls. When one runs any bit of Hadoop in a containers the container will block those exits with a security manager and convert the calls into security exceptions. A single exit method would enable such exceptions to be logged, and conceivably handled slightly more gracefully (e.g. tell the daemon to die).    ",util
TestRPCCallBenchmark failing w/ port in use -handling badly,"I'm seeing TestRPCCallBenchmark fail with port in use, which is probably related to some other test (race condition on shutdown?), but which isn't being handled that well in the test itself -although the log shows the binding exception, the test is failing on a connection timeout",ipc
KerberosAuthenticatorHandler should use _HOST replacement to resolve principal name,Currently the exact Kerberos principal name has to be set in the configuration of each node.    KerberosAuthenticatorHandler should do similar logic as the RPC ports to support HTTP/_HOST@REALM,security
Access Control support for Non-secure deployment of Hadoop on Windows,None,native
General Util  Changes for Hadoop for Windows,None,native
Hadoop-bin commands for windows,None,native
FilterFileSystem should delegate initialize rather than use super,"The FilterFileSystem javadoc says ""The class FilterFileSystem itself simply overrides all methods of FileSystem with versions that pass all requests to the contained file system."" initialize(URI, Configuration) is part of the FileSystem API, and before HADOOP-8013 it simply called fs.initialize(). After HADOOP-8013 it calls super.initialize() rather than delegating. This breaks a number of the Hive tests. Hive's ProxyLocalFileSystem would be able to work around this problem by calling setConf, but unfortunately FilterFileSystem does not override that method. We need to make FilterFileSystem call fs.initialize (instead of using super) and also override setConf.",fs
move method getHostPortString() from NameNode to NetUtils,This issue is the counter part of the issue HDFS-3003.,util
TestViewFsTrash occasionally fails,{noformat}  junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>    junit.framework.Assert.fail(Assert.java:47)    junit.framework.Assert.failNotEquals(Assert.java:283)    junit.framework.Assert.assertEquals(Assert.java:64)    junit.framework.Assert.assertEquals(Assert.java:195)    org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)    org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)   ...  {noformat}  There are quite a few TestViewFsTrash failures recently.  E.g. [build #624 for trunk|https://builds.apache.org/job/PreCommit-HADOOP-Build/624//testReport/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/] and [build #2 for 0.23-PB|https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-0.23-PB-Build/2/testReport/junit/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/].  ,fs
cannot see userlogs from the web,"After upgraded from Hadoop 0.20.2 to Hadoop 1.0.0, the userlogs cannot be viewed anymore from the web. The root cause is that the userlogs of the tasks are actually simlinks at level of dir with name pattern like attempt_201202241358_0001_m_000000_0. According to Jetty's documentation, simlinks are not defaultly rederable http://docs.codehaus.org/display/JETTY/How+to+enable+serving+aliased+files. Please check.",util
Add new test class to org.apache.hadoop.ipc.Client,Test class of org.apache.hadoop.ipc.Client dose not exists.    ,ipc
"Correction to BUILDING.txt: HDFS needs ProtocolBuffer, too (not just MapReduce)","Currently BUILDING.txt states:     {quote}    ProtocolBuffer 2.4.1+ (for MapReduce)  {quote}    But HDFS needs ProtocolBuffer too:     {code}  hadoop-common/hadoop-hdfs-project$ find . -name ""*.proto"" | wc -l        11  {code}    ",documentation
HA: RetriableCommand is using RetryPolicy incorrectly after HADOOP-7896,"HADOOP-7896 (on the HA branch) refactored RetryAction from an enum to a class, and also moved the act of sleeping to delay retries from the RetryPolicy implementations into RetryInvocationHandler. RetriableCommand, in the rewritten distcp tool, uses RetryPolicy and associated classes from o.a.h.io.retry. When MAPREDUCE-2765 was merged into the HA branch, RetriableCommand wasn't adjusted accordingly to make use of the new structure of the o.a.h.io.retry classes.    It's probably generally not kosher for RetriableCommand to be using the RetryPolicy classes at all, since they're not really intended to be used except by RetryInvocationHandler. But, regardless, this JIRA aims to make distcp's use of the o.a.h.io.retry classes functional again.",util
Upgrade test build to Surefire 2.12,"Surefire 2.9, which we're using currently, has a few annoying bugs. In particular, if a test exits with a non-zero exit code, it doesn't report the test as failed.","build,test"
Print the stack trace of InstanceAlreadyExistsException in trace level,"There are many InstanceAlreadyExistsException stack traces in the unit tests output like below.  {noformat}  javax.management.InstanceAlreadyExistsException: Hadoop:service=NameNode,name=NameNodeInfo    com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)    com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)   ...  {noformat}",metrics
Fix javac warnings in TestAuthenticationFilter,There are 43 javac warnings in TestAuthenticationFilter.,test
Active Directory Group Mapping Service,Planning on building a group mapping service that will go and talk directly to an Active Directory setup to get group memberships,security
Build should fail when compilation .proto files fail,Build continues even if the .proto file compilation fails. The build should stop and flag an error when generating java files from .proto files fail.,build
hadoop-project invalid pom warnings prevent transitive dependency resolution,"This issue is present for at least maven 2.2.1    [DEBUG]   Artifact resolved  [WARNING] POM for 'org.apache.hadoop:hadoop-common:pom:0.23.1:compile' is invalid.    Its dependencies (if any) will NOT be available to the current build.  [DEBUG] Reason: Failed to validate POM for project org.apache.hadoop:hadoop-common at Artifact [org.apache.hadoop:hadoop-common:pom:0.23.1:compile]  [DEBUG]  Validation Errors:  [DEBUG] For dependency Dependency {groupId=jdk.tools, artifactId=jdk.tools, version=1.6, type=jar}: system-scoped dependency must specify an absolute path systemPath.  [DEBUG]    [DEBUG]   org.apache.hadoop:hadoop-common:jar:0.23.1:compile (selected for compile)  ",build
Remove the deprecated Syncable.sync() method,The Syncable.sync() was deprecated in 0.21.  We should remove it.,fs
make hadoop-client set of curated jars available in a distribution tarball,One thing that the original patch for HADOOP-8082 didn't address is the need for those curated jars to be visible in the final tarball.,build
FTPFileSystem is broken when a file is modified on 02/29,Due to NET-188 the FTPClient from commons-net that is used by hadoop FTPFileSystem is broken.    The solution is to upgrade the commons-net version (that has been fixed 4 years ago).    We are currently running hadoop 0.20 with commons-net-2.0 without trouble and it fixed our issue with FTPClient.,fs
Upgrade commons-net,Hadoop is using commons-net 1.4.1 that contains critical bugs fixed many years ago. We definitely need to upgrade to at least 2.0,fs
ViewFileSystemTestSetup setupForViewFileSystem is erring when the user's home directory is somewhere other than /home (eg. /User) etc.,All TestFSMainOperationsLocalFileSystem tests (99 in all) fail saying:  {noformat}  java.io.FileNotFoundException: /home    org.apache.hadoop.fs.viewfs.InodeTree.resolve(InodeTree.java:403)    org.apache.hadoop.fs.viewfs.ViewFileSystem.mkdirs(ViewFileSystem.java:373)    org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1684)    org.apache.hadoop.fs.FSMainOperationsBaseTest.setUp(FSMainOperationsBaseTest.java:90)    org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:42)  {noformat},"fs,test"
FileContext.mkdir(..) causes a loss of setgid permission of directory created,"When FileContext.mkdir(..) is used to create a directory inside a parent directory (which has the setgid permission), the created directory loses the setgid permission.     We have noticed that in ContainerLocalizer.java:  {code}    private static void initDirs(Configuration conf, String user, String appId,        FileContext lfs, List<Path> localDirs) throws IOException {         .         .        // $x/usercache/$user/appcache/$appId/output        lfs.mkdir(new Path(appBase, OUTPUTDIR), null, false);         .         .    }  {code}","fs,util"
upgrade commons-daemon.version in hadoop-project/pom.xml from 1.0.3 to 1.0.5 to avoid jsvc / libcap.so linking problem,"On several recent Linux distributions (tested Ubuntu 9.04, RHEL 6.2, Amazon Linux AMI release 2011.09), there is no libcap.so.1. With commons-daemon 1.0.3, the current version selected by {{hadoop-project/pom.xml}}, this causes a linking error:    {code}  $ wget http://archive.apache.org/dist/commons/daemon/binaries/1.0.3/linux/commons-daemon-1.0.3-bin-linux-x86_64.tar.gz  $ tar xfz commons-daemon-1.0.3-bin-linux-x86_64.tar.gz  $ ldd jsvc    linux-vdso.so.1 =>  (0x00007ffff43cc000)   libdl.so.2 => /lib64/libdl.so.2 (0x00007f9cbbeee000)   libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f9cbbcd2000)   libcap.so.1 => not found   libc.so.6 => /lib64/libc.so.6 (0x00007f9cbb930000)   /lib64/ld-linux-x86-64.so.2 (0x00007f9cbc0fb000)  {code}  The same problem also occurs with 1.0.4, but 1.0.5 doesn't have the problem:    {code}  $ wget http://archive.apache.org/dist/commons/daemon/binaries/1.0.5/linux/commons-daemon-1.0.5-bin-linux-x86_64.tar.gz  $ tar xfz commons-daemon-1.0.5-bin-linux-x86_64.tar.gz   $ ldd jsvc    linux-vdso.so.1 =>  (0x00007fff157ff000)   libdl.so.2 => /lib64/libdl.so.2 (0x00007f9e14778000)   libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f9e1455c000)   libcap.so.2 => /lib64/libcap.so.2 (0x00007f9e14357000)   libc.so.6 => /lib64/libc.so.6 (0x00007f9e13fb6000)   /lib64/ld-linux-x86-64.so.2 (0x00007f9e14985000)   libattr.so.1 => /lib64/libattr.so.1 (0x00007f9e13db2000)  {code}  ","build,native"
DNS claims to return a hostname but returns a PTR record in some cases,"Per Shrijeet on HBASE-4109:    {quote}  If you are using an interface anything other than 'default' (literally that keyword) DNS.java's getDefaultHost will return a string which will have a trailing period at the end. It seems javadoc of reverseDns in DNS.java (see below) is conflicting with what that function is actually doing.     It is returning a PTR record while claims it returns a hostname. The PTR record always has period at the end , RFC: http://irbs.net/bog-4.9.5/bog47.html    We make call to DNS.getDefaultHost at more than one places and treat that as actual hostname.    Quoting HRegionServer for example    String machineName = DNS.getDefaultHost(conf.get(          ""hbase.regionserver.dns.interface"", ""default""), conf.get(          ""hbase.regionserver.dns.nameserver"", ""default""));    We may want to sanitize the string returned from DNS class. Or better we can take a path of overhauling the way we do DNS name matching all over.  {quote}    While HBase has worked around the issue, we should fix the methods that aren't doing what they've intended.    1. We fix the method. This may be an 'incompatible change'. But I do not know who outside of us uses DNS classes.  2. We fix HDFS's DN at the calling end, cause that is affected by the trailing period in its reporting back to the NN as well (Just affects NN->DN weblinks, non critical).    For 2, we can close this and open a HDFS JIRA.    Thoughts?",util
Add ByteBufferReadable interface to FSDataInputStream,"To prepare for HDFS-2834, it's useful to add an interface to FSDataInputStream (and others inside hdfs) that adds a read(ByteBuffer...) method as follows:    {code}    /**     * Reads up to buf.remaining() bytes into buf. Callers should use     * buf.limit(..) to control the size of the desired read.     *      * After the call, buf.position() should be unchanged, and therefore any data     * can be immediately read from buf.     *      * @param buf     * @return - the number of bytes available to read from buf     * @throws IOException     */    public int read(ByteBuffer buf) throws IOException;  {code}",fs
Enhance hadoop to use a newer version (0.8.1) of the jets3t library,"Hadoop is built against, and includes, an older version of the Jets3t library - version 0.6.1.  The current version of the Jets3t library(as of March 2012) is 0.8.1. This new version includes many improvements such as support for ""Requester-Pays"" buckets.    Since hadoop includes a copy of the version 0.6.1 jets3t library, and since this version ends up early in the CLASSPATH, any Map Reduce application that wants to use the jets3t library ends up getting version 0.6.1 of the jets3t library. The MR application fails, usually with an error stating that the method signature of some method in the Jets3t library does not match.    It would be useful to enhance Jets3tNativeFileSystemStore.java to use the API published by the 0.8.1 version of the jets3t library.  ",fs/s3
Site side links for commands manual (MAPREDUCE-3497),None,documentation
Path does not allow metachars to be escaped,"Path converts ""\"" into ""/"", probably for windows support?  This means it's impossible for the user to escape metachars in a path name.  Glob expansion can have deadly results.    Here are the most egregious examples. A user accidentally creates a path like ""/user/me/*/file"".  Now they want to remove it.  {noformat}""hadoop fs -rmr -skipTrash '/user/me/\*'"" becomes...  ""hadoop fs -rmr -skipTrash /user/me/*""{noformat}  * User/Admin: Nuked their home directory or any given directory    {noformat}""hadoop fs -rmr -skipTrash '\*'"" becomes...  ""hadoop fs -rmr -skipTrash /*""{noformat}  * User:  Deleted _everything_ they have access to on the cluster  * Admin: *Nukes the entire cluster*    Note: FsShell is shown for illustrative purposes, however the problem is in the Path object, not FsShell.  ",fs
Add method to init krb5 cipher suites,We have duplicated the code in a lot of places to set https.cipherSuites. We should put a utility method in SecurityUtil for it.,security
Update versions from 0.23.2 to 0.23.3,Some versions in the 0.23 branch are still 0.23.2.,build
pseudoSortByDistance in NetworkTopology doesn't work properly if no local node and first node is local rack node,"pseudoSortByDistance in NetworkTopology.java should sort nodes according to its distance with reader as local node, local rack node, ...   But if there is no local node with reader in nodes and the first node is local rack node with reader, then it will put a random node at position 0.",io
Automate testing of LdapGroupsMapping against ApacheDS,"HADOOP-8078 introduced an ApacheDS system to the automated tests, and the LdapGroupsMapping could benefit from automated testing against that DS instance","security,test"
FsShell commands cannot be interrupted,"Control-c is apparently generating {{InterruptedIOExceptions}}.  The path processing loop traps {{IOException}}, displays the error, and moves to the next path.  This means that recursive operations cannot be aborted.",fs
test-patch should run tests with -fn to avoid masking test failures,"If there are known failures, test-patch will bail out as soon as it sees them.  This causes the precommit builds to potentially not find real issues with a patch, because the tests that would fail might come after a known failure.  We should add -fn to just the mvn test command in test-patch to get the full list of failures.",test
Zero-copy ByteBuffer-based compressor / decompressor API,"Per Todd Lipcon's comment in HDFS-2834, ""    Whenever a native decompression codec is being used, ... we generally have the following copies:      1) Socket -> DirectByteBuffer (in SocketChannel implementation)    2) DirectByteBuffer -> byte[] (in SocketInputStream)    3) byte[] -> Native buffer (set up for decompression)    4*) decompression to a different native buffer (not really a copy - decompression necessarily rewrites)    5) native buffer -> byte[]      with the proposed improvement we can hopefully eliminate #2,#3 for all applications, and #2,#3,and #5 for libhdfs.  ""    The interfaces in the attached patch attempt to address:   A - Compression and decompression based on ByteBuffers (HDFS-2834)   B - Zero-copy compression and decompression (HDFS-3051)   C - Provide the caller a way to know how the max space required to hold compressed output.  ","io,performance"
cap space usage of default log4j rolling policy ,"I've seen several critical production issues because logs are not automatically removed after some time and accumulate. Changes to Hadoop's default log4j file appender would help with this.    I recommend we move to an appender which:    1) caps the max file size (configurable)  2) caps the max number of files to keep (configurable)  3) uses rolling file appender rather than DRFA, see the warning here:  http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/DailyRollingFileAppender.html  Specifically: ""DailyRollingFileAppender has been observed to exhibit synchronization issues and data loss.""    We'd lose (based on the default log4j configuration) the daily rolling aspect, however increase reliability.  ",conf
Error handling in snappy decompressor throws invalid exceptions,"SnappyDecompressor.c has the following code in a few places:  {code}      THROW(env, ""Ljava/lang/InternalError"", ""Could not decompress data. Buffer length is too small."");  {code}  this is incorrect, though, since the THROW macro doesn't need the ""L"" before the class name. This results in a ClassNotFoundException for Ljava.lang.InternalError being thrown, instead of the intended exception.","io,native"
Expand public APIs for security library classes,"Currently projects like Hive and HBase use UserGroupInformation and SecurityUtil methods. Both of these classes are marked LimitedPrivate(HDFS,MR) but should probably be marked more generally public.",security
DNS#getIPs shouldn't silently return the local host IP for bogus interface names,"DNS#getIPs silently returns the local host IP for bogus interface names. In this case let's throw an UnknownHostException. This is technically an incompatbile change. I suspect the current behavior was origininally introduced so the interface name ""default"" works w/o explicitly checking for it. It may also be used in cases where someone is using a shared config file and an option like ""dfs.datanode.dns.interface"" or ""hbase.master.dns.interface"" and eg interface ""eth3"" that some hosts don't have, though I think silently ignorning this is the wrong behavior (those hosts should be configured to use a different interface).",conf
Use enum for shell exit codes,"FsShell currently returns -1 (syntax error), 0 (success), 1 (error), 130 (hit by interrupt).  Uma has suggested that an enum would be a cleaner approach to tracking the exits.  DFSAdmin will also require changes to support this improvement since it currently subclasses FsShell.",fs
Remove ability to specify a custom nameserver for an interface in DNS.java,"A long time ago HADOOP-497 introduced the option to specify a specific nameserver to use when looking up the hostname of an interface specified in the config (eg by {{dfs.datanode.dns.interface}}). This feature is not in MR2, and I dont't think it makes sense for HDFS since HDFS stopped advertising DNs by hostname a long time ago (HADOOP-985). Also per HADOOP-8134 and HBASE-4109, I don't think this feature works, as it returns the PTR record verbatim instead of the hostname (eg PTR record may have a trailing ""."", "".lan"" etc that may not resolve).    How about we remove this feature from trunk and 23?   We can deprecate in 1.0, though I doubt anyone is using it.  This is an incompatible change. DNS is annotated LimitedPrivate and Unstable but HBase uses it so will file need to file a jira there (they use this via HBASE-1279 and HBASE-1279).",conf
haadmin should have configurable timeouts for failover commands,"The HAAdmin failover could should time out reasonably aggressively and go onto the fencing strategies if it's dealing with a mostly dead active namenode.  Currently it uses what's probably the default, which is to say no timeout whatsoever.    {code}    /**     * Return a proxy to the specified target service.     */    protected HAServiceProtocol getProtocol(String serviceId)        throws IOException {      String serviceAddr = getServiceAddr(serviceId);      InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);      return (HAServiceProtocol)RPC.getProxy(            HAServiceProtocol.class, HAServiceProtocol.versionID,            addr, getConf());    }  {code}",ha
"HardLink.getLinkCount() is getting stuck in eclipse ( Cygwin) for long file names, due to MS-Dos style Path.","HardLink.getLinkCount() is getting stuck in cygwin for long file names, due to MS-DOS style path.",util
Support for Azure Storage,None,native
Hadoop 1.0 has duplicate and conflicting conf directories,"In branch-1 and friends conf/* differs quite a bit from src/packages/templates/conf*. Eg log4j.properties is quite different. I don't think the package artifact needs different configuration settings, and HADOOP-6255 should not have have duplicated all of these files, let's share the files or at the very least make the files match up.",conf
Improve ActiveStandbyElector to provide hooks for fencing old active,"When a new node becomes active in an HA setup, it may sometimes have to take fencing actions against the node that was formerly active. This JIRA extends the ActiveStandbyElector which adds an extra non-ephemeral node into the ZK directory, which acts as a second copy of the active node's information. Then, if the active loses its ZK session, the next active to be elected may easily locate the unfenced node to take the appropriate actions.",ha
SshFenceByTcpPort uses netcat incorrectly,"SshFencyByTcpPort currently assumes that the NN is listening on localhost.  Typical setups have the namenode listening just on the hostname of the namenode, which would lead ""nc -z"" to not catch it.    Here's an example in which the NN is running, listening on 8020, but doesn't respond to ""localhost 8020"".  {noformat}  [root@xxx ~]# lsof -P -p 5286 | grep -i listen  java    5286 root  110u  IPv4            1772357              TCP xxx:8020 (LISTEN)  java    5286 root  121u  IPv4            1772397              TCP xxx:50070 (LISTEN)  [root@xxx ~]# nc -z localhost 8020  [root@xxx ~]# nc -z xxx 8020  Connection to xxx 8020 port [tcp/intu-ec-svcdisc] succeeded!  {noformat}    Here's the likely offending code:  {code}          LOG.info(              ""Indeterminate response from trying to kill service. "" +              ""Verifying whether it is running using nc..."");          rc = execCommand(session, ""nc -z localhost 8020"");  {code}    Naively, we could rely on netcat to the correct hostname (since the NN ought to be listening on the hostname it's configured as), or just to use fuser.  Fuser catches ports independently of what IPs they're bound to:    {noformat}  [root@xxx ~]# fuser 1234/tcp  1234/tcp:             6766  6768  [root@xxx ~]# jobs  [1]-  Running                 nc -l localhost 1234 &  [2]+  Running                 nc -l rhel56-18.ent.cloudera.com 1234 &  [root@xxx ~]# sudo lsof -P | grep -i LISTEN | grep -i 1234  nc         6766      root    3u     IPv4            2563626                 TCP localhost:1234 (LISTEN)  nc         6768      root    3u     IPv4            2563671                 TCP xxx:1234 (LISTEN)  {noformat}",ha
Handle paths using back slash as path separator for windows only,Please see the description in HADOOP-8139. Using escape character back slash as path separator could cause accidental deletion of data. This jira for now supports back slash only for windows. Eventually HADOOP-8139 will remove the support for back slash based paths.,fs
Code contribution of GlusterFS implementation of Hadoop FileSystem Interface,"GlusterFS is a software-only, highly available, scalable, centrally managed storage pool for public and private cloud environments. GlusterFS has been integrated with Hadoop using Hadoop's FileSystem interface.    This ticket is filed so as to get our code/libs to be included with Hadoop.",fs
Remove JDK 1.5 dependency from building forrest docs,Currently Hadoop requires both JDK 1.6 and JDK 1.5. JDK 1.5 is a requirement of Forrest. It is easy to remove the latter requirement by turning off forrest.validate.sitemap and forrest.validate.skins.stylesheets.,documentation
Configuration deprecation logic breaks backwards compatibility,"The deprecated Configuration logic works as follows:    For a dK deprecated key in favor of nK:    * on set(dK, V), it stores (nK,V)  * on get(dK) it does a reverseLookup of dK to nK and looks for get(nK)    While this works fine for single set/get operations, the iterator() method that returns an iterator of all config key/values, returns only the new keys.    This breaks applications that did a set(dK, V) and expect, when iterating over the configuration to find (dK, V).  ",conf
empty-string owners or groups causes {{MissingFormatWidthException}} in o.a.h.fs.shell.Ls.ProcessPath(),"In {{adjustColumnWidths()}}, we set the member variable {{lineFormat}}, which is used by {{ProcessPath()}} to print directory entries. Owners and groups are formatted using the formatting conversion {{%-Xs}}, where X is the max length of the owner or group. However, when trying this with an S3 URL, I found that the owner and group were empty (""""). This caused X to be 0, which means that the formatting conversion is set to {{%-0s}}. This caused a {{MissingFormatWidthException}} to be thrown when the formatting string was used in {{ProcessPath()}}.     Formatting conversions are described here:     http://docs.oracle.com/javase/1.6.0/docs/api/java/util/Formatter.html#intFlags    The specific exception thrown (a subtype of {{IllegalFormatException}}) is described here:    http://docs.oracle.com/javase/1.6.0/docs/api/java/util/MissingFormatWidthException.html  ",fs
DistCp fails when invoked by Oozie,"When DistCp is invoked through a proxy-user (e.g. through Oozie), the delegation-token-store isn't picked up by DistCp correctly. One sees failures such as:    ERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp  operation:   java.lang.SecurityException: Intercepted System.exit(-999)      at  org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)      at java.lang.Runtime.exit(Runtime.java:88)      at java.lang.System.exit(System.java:904)      at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)      at  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)      at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)      at java.lang.reflect.Method.invoke(Method.java:597)      at  org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)      at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)      at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)      at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)      at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)      at java.security.AccessController.doPrivileged(Native Method)      at javax.security.auth.Subject.doAs(Subject.java:396)      at  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)      at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)    Looking over the DistCp code, one sees that HADOOP_TOKEN_FILE_LOCATION isn't being copied to mapreduce.job.credentials.binary, in the job-conf. I'll post a patch for this shortly.",security
javadoc generation fails with java.lang.OutOfMemoryError: Java heap space,"building the docs (mvn package -Pdocs -Dtar -DskipTests) on branch-0.23 results in a javadoc java.lang.OutOfMemoryError: Java heap space. Note this seems to only happen when building with 32 bit java, 64 bit works fine.",build
LdapGroupsMapping should support a configurable search limit,"LDAP servers can be configured with search limits, which would cause the LdapGroupsMapping class to throw an exception if it returned more results than the search limit allowed. If a user belonged to a very large number of groups, or the search limit was set to a fairly low number, this could result in some undesirable behavior.    The LdapGroupsMapping should be augmented to page results, if necessary.",security
Configuration no longer sets all keys in a deprecated key list.,"I did not look at the patch for HADOOP-8167 previously, but I did in response to a recent test failure. The patch appears to have changed the following code (I am just paraphrasing the code)    {code}  if(!deprecated(key)) {    set(key, value);  } else {    for(String newKey: depricatedKeyMap.get(key)) {      set(newKey, value);    }  }  {code}    to be     {code}  set(key, value);  if(depricatedKeyMap.contains(key)) {     set(deprecatedKeyMap.get(key)[0], value);  } else if(reverseKeyMap.contains(key)) {     set(reverseKeyMap.get(key), value);  }  {code}    If a key is deprecated and is mapped to more then one new key value only the first one in the list will be set, where as previously all of them would be set.",conf
FsShell needs to handle quoted metachars,"Commands that access paths with quoted metachars work correctly, however commands that create paths with quoted metachars leave the \ in the path.  Ex.   {noformat}  fs -mkdir ""foo\*bar"" will create ""foo\*bar"" instead of ""foo*bar"".  {noformat}",fs
Remove confusing comment in Path#isAbsolute(),"The method is checking for absolute path correctly. When scheme and authority are present, the path is absolute. So the following comment needs to be removed.  {noformat}    /**     * There is some ambiguity here. An absolute path is a slash     * relative name without a scheme or an authority.     * So either this method was incorrectly named or its     * implementation is incorrect. This method returns true     * even if there is a scheme and authority.     */  {noformat}  ",fs
Add mkdir -p flag,"Users require the ability to create a directory iff it doesn't exist.  Currently, running FsShell's mkdir twice will result in an already exists error.  The unix mkdir -p flag won't fail if the directory already exists.  Per parent jira, pig project would also like this feature.    (Eventually mkdir should probably also stop creating all intermediate dirs unless -p is given, but that's an incompatible change)",fs
Disambiguate the destination of FsShell copies,"The copy commands currently do not provide a way to disambiguate the expected destination of a copy.  Ex.  {{fs -put myfile path/mydir}}    If ""mydir"" is an existing directory, then the copy produces {{path/mydir/file}}.  If ""mydir"" does not exist at all, the copy produces {{path/mydir}}.  The file has unexpectedly been renamed to what was expected to be a directory!  It's standard unix shell behavior, but it lacks a special trait.    Unix allows a user to disambiguate their intent by allowing {{path/mydir/}} or {{path/mydir/.}} to mean ""mydir"" is *always* be treated as a directory.  If the copy succeeds, it will always be called {{path/mydir/myfile}}.",fs
MBeans shouldn't try to register when it fails to create MBeanName,"{code:title=MBeans.java|borderStyle=solid}    static public ObjectName register(String serviceName, String nameName,                                      Object theMbean) {      final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();      ObjectName name = getMBeanName(serviceName, nameName);      try {        mbs.registerMBean(theMbean, name);        LOG.debug(""Registered ""+ name);        return name;      } catch (InstanceAlreadyExistsException iaee) {        if (LOG.isTraceEnabled()) {          LOG.trace(""Failed to register MBean \""""+ name + ""\"""", iaee);        } else {          LOG.warn(""Failed to register MBean \""""+ name              + ""\"": Instance already exists."");        }      } catch (Exception e) {        LOG.warn(""Failed to register MBean \""""+ name + ""\"""", e);      }      return null;    }      static private ObjectName getMBeanName(String serviceName, String nameName) {      ObjectName name = null;      String nameStr = ""Hadoop:service=""+ serviceName +"",name=""+ nameName;      try {        name = DefaultMetricsSystem.newMBeanName(nameStr);      } catch (Exception e) {        LOG.warn(""Error creating MBean object name: ""+ nameStr, e);      }      return name;    }  {code}    In getMBeanName() if DefaultMetricsSystem.newMBeanName(nameStr); fails with some reason like mbean already exists, getMBeanName() logs the exception and returns null and  mbs.registerMBean(theMbean, name) in register() tries to register with null and throws exception with the message 'Failed to register MBean ""null""'.",metrics
"Error Queue ""default"" does not exist when using : capacity scheduler","I use capacity scheduler when i run job has error :  Queue ""default"" does not exist when using  1.i have been add     <property>    <name>mapred.jobtracker.taskScheduler</name>    <value>org.apache.hadoop.mapred.CapacityTaskScheduler</value>   </property>     <property>    <name>mapred.queue.names</name>    <value>queue1,queue1,queue3</value>   </property>  2.copy hadoop-capacity-scheduler-*.jar into contrib/capacity-scheduler    thanks",conf
risk of NPE in CopyCommands processArguments(),"My IDE is warning me that the {{is.close()}} method will NPE if the {{is = src.fs.open(src.path);}} call raises an exception, which could happen if the source path could not be opened. There should be an if (is!=null) wrapper",fs
hadoop-setup-single-node.sh from DEB fails with chown permission error on Ubuntu 11.10,"Installed Hadoop 1.0.1 from amd64 DEB package.  Executed: sudo /usr/sbin/hadoop-setup-single-node.sh    It invokes /usr/sbin/hadoop-daemon.sh where in line 98    chown $HADOOP_IDENT_STRING $HADOOP_LOG_DIR    attempts to execute 'chown root /var/log/hadoop/root' as user hduser, resulting in      * Starting Apache Hadoop Name Node server hadoop-namenode                                                                                                       chown: changing ownership of `/var/log/hadoop/root': Operation not permitted    and     * Starting Apache Hadoop Data Node server hadoop-datanode                                                                                                       chown: changing ownership of `/var/log/hadoop/root': Operation not permitted    As a result, setup fails.",build
"Stop using ""mapred.used.genericoptionsparser"" to avoid unnecessary warnings","Its about time we stopped the following from appearing in 0.23/trunk:    {code}  12/03/19 20:53:51 WARN conf.Configuration: mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used  {code}",util
ProtoBuf RPC engine does not need it own reply packet - it can use the IPC layer reply packet.,None,ipc
Update namenode -format documentation and add -nonInteractive and -force,documentation changes related to HDFS-3094,documentation
LdapGroupsMapping shouldn't throw away IOException,"When extracting a password from a file during the config setup, the LdapGroupsMapping throws a RuntimeException, but doesn't bubble up the IOException that caused it",security
Eclipse plugin fails to access remote cluster,Eclipse plugin fails to access remote file system.,contrib/eclipse-plugin
"Refactor FailoverController/HAAdmin code to add an abstract class for ""target"" services","In working at HADOOP-8077, HDFS-3084, and HDFS-3072, I ran into various difficulties which are an artifact of the current design. A few of these:  - the service name is ""resolved"" from the logical name (eg ns1.nn1) to an IP address at the outer layer of DFSHAAdmin  -- this means it's difficult to provide the logical name ""ns1.nn1"" to fence scripts (HDFS-3084)  -- this means it's difficult to configure fencing method per-namespace (since the FailoverController doesn't know what the namespace is) (HADOOP-8077)  - the configuration for HA HDFS is weirdly split between core-site and hdfs-site, even though most users see this as an HDFS feature. For example, users expect to configure NN fencing configurations in hdfs-site, and expect the keys to have a dfs.* prefix  - proxies are constructed at the outer layer of the admin commands. This means it's impossible for the inner layers (eg FailoverController.failover) to re-construct proxies with different timeouts (HDFS-3072)    The proposed refactor is to add a new interface (tentatively named HAServiceTarget) which refers to target for one of the admin commands. An instance of this class is responsible for creating proxies, creating fencers, mapping back to a logical name, etc. The HDFS implementation of this class can then provide different results based on the particular nameservice, can use HDFS-specific configuration prefixes, etc. Using this class as the argument for fencing methods also makes the API more evolvable in the future, since we can add new getters to HAServiceTarget (whereas the current InetSocketAddress is quite limiting)",ha
viewfs: quota command does not report remaining quotas,The space and namesapce quotas and remaining are not reported.,viewfs
Backport FileContext to branch-1,"It's hard to get users to migrate to FileContext because they (rightly!) want their software to work against Hadoop 1.x which only has FileSystem. Backporting FileContext to branch-1 would allow people to migrate to FileContext and still work against both Hadoop 1.x and 2.x (or whatever we call it). It probably isn't that much work since FileContext is mostly net new code with lots of tests. It's a pain to support the same code in two places, but we already have to do that with FileSystem, and that's the cost of introducing a new API instead of improving FileSystem in place.",fs
avoid linker's stripping of dead code from interfering with configure's library name resolution,"The configure script generated by hadoop-common/hadoop-common-project/hadoop-common/src/main/native/configure.ac uses the AC_COMPUTE_NEEDED_DSO m4 macro to generate a small probe program in C which it then compiles and links, and then scans the resultant binary to find the names of certain libraries: currently this is used for Zlib and Snappy.    I was unable to compile with -Pnative on my Ubuntu Linux install because configure could not find libz and libsnappy. This turned out to be because the linker is removing the dependencies on libz and libsnappy at link-time because the libraries in question are not used in the simple probe code generated by the AC_COMPUTE_NEEDED_DSO m4 macro.    So my fix is modify the AC_COMPUTE_NEEDED_DSO to take another parameter that copies the given argument's text into the C program. Then, in the call to AC_COMPUTE_NEEDED_DSO, we can use this additional parameter, to include, in the generated C code, an actual library function call for each library. This prevents the linker from removing the linkage to the desired libraries.    My gcc and ldd version information are given below:    {code}  eugene@latitude:~/hadoop-common$ gcc --version  gcc (Ubuntu/Linaro 4.6.3-1ubuntu3) 4.6.3  Copyright (C) 2011 Free Software Foundation, Inc.  This is free software; see the source for copying conditions.  There is NO  warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.    eugene@latitude:~/hadoop-common$ ld --version  GNU ld (GNU Binutils for Ubuntu) 2.22  Copyright 2011 Free Software Foundation, Inc.  This program is free software; you may redistribute it under the terms of  the GNU General Public License version 3 or (at your option) a later version.  This program has absolutely no warranty.    {code}",native
Configuration logs WARNs on every use of a deprecated key,"The logic to do print a warning only once per deprecated key does not work:    {code}  2012-03-21 22:32:58,121  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name  ....  2012-03-21 22:32:58,123  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS  ...  2012-03-21 22:32:58,130  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address  2012-03-21 22:32:58,351  WARN Configuration:345 - fs.default.name is deprecated. Instead, use fs.defaultFS  ...  2012-03-21 22:32:58,843  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name  2012-03-21 22:32:58,844  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address  2012-03-21 22:32:58,844  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS  {code}",conf
Support multiple network interfaces,"Hadoop does not currently utilize multiple network interfaces, which is a common user request, and important in enterprise environments. This jira covers a proposal for enhancements to Hadoop so it better utilizes multiple network interfaces. The primary motivation being improved performance, performance isolation, resource utilization and fault tolerance. The attached design doc covers the high-level use cases, requirements, a proposal for trunk/0.23, discussion on related features, and a proposal for Hadoop 1.x that covers a subset of the functionality of the trunk/0.23 proposal.","io,performance"
Remove HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS ,The HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS env variables are no longer in trunk/23 since there's no MR1 implementation and the tests don't use them. This makes the patch for HADOOP-8149 easier.,conf
create the configure script for native compilation as part of the build,configure script is checked into svn and its not regenerated during build.   Ideally configure scritp should not be checked into svn and instead should be generated during build using autoreconf.  ,build
stopproxy() is not closing the proxies correctly,"I was running testbackupnode and noticed that NNprotocol proxy was not being closed. Talked with Suresh and he observed that most of the protocols do not implement ProtocolTranslator and hence the logic in stopproxy() does not work. Instead, since all of them are closeable, Suresh suggested that closeable property should be used at close.  ",ipc
Potential design improvements for ActiveStandbyElector API,"Bikas suggested some improvements to the API for ActiveStandbyElector in HADOOP-8163:  {quote}    I have a feeling that putting the fencing concept into the elector is diluting the distinctness between the elector and the failover controller. In my mind, the elector is a distributed leader election library that signals candidates about being made leader or standby. In the ideal world, where the HA service behaves perfectly and does not execute any instruction unless it is a leader, we only need the elector. But the world is not ideal and we can have errant leader who need to be fenced etc. Here is where the Failover controller comes in. It manages the HA service by using the elector to do distributed leader selection and get those notifications passed onto the HAservice. In addition is guards service sanity by making sure that the signal is passed only when it is safe to do so.   How about this slightly different alternative flow. Elector gets leader lock. For all intents and purposes it is the new leader. It passes the signal to the failover controller with the breadcrumb of the last leader.  appClient->becomeActive(breadcrumb);  the failoverController now has to ensure that all previous master are fenced before making its service the master. the breadcrumb is an optimization that lets it know that such an operation may not be necessary. If it is necessary, then it performs fencing. If fencing is successful, it calls.  elector->becameActive() or elector->transitionedToActive() at which point the elector can overwrite the breadcrumb with its own info. I havent thought through if this should be called before or after a successful call to HAService->transitionToActive() but my gut feeling is for the former.  This keeps the notion of fencing inside the controller instead of being in both the elector and the controller.    Secondly, we are performing blocking calls on the ZKClient callback that happens on the ZK threads. It is advisable to not block ZK client threads for long. The create and delete methods might be ok but I would try to move the fencing operation and transitioning to active operations away from the ZK thread. i.e. when the FailoverController is notified about becoming master, it returns the call and then processes fencing/transitioning on some other thread/threadpool. The above flow allows for this.  {quote}  This JIRA is to further discuss/implement these suggestions.","auto-failover,ha"
Common portion of ZK-based failover controller,"This JIRA is for the Common (generic) portion of HDFS-2185. It can't run on its own, but this JIRA will include unit tests using mock/dummy services.",ha
createproxy() in TestHealthMonitor is throwing NPE,"Looking at the test log output, createproxy in testhealthmonitor is triggering NPE resulting null proxy. This creates other test failures.    2012-03-24 22:16:11,591 FATAL ha.HealthMonitor (HealthMonitor.java:uncaughtException(268)) - Health monitor failed  java.lang.NullPointerException          at org.apache.hadoop.ha.TestHealthMonitor$1.createProxy(TestHealthMonitor.java:75)          at org.apache.hadoop.ha.HealthMonitor.tryConnect(HealthMonitor.java:171)          at org.apache.hadoop.ha.HealthMonitor.loopUntilConnected(HealthMonitor.java:158)          at org.apache.hadoop.ha.HealthMonitor.access$500(HealthMonitor.java:52)          at org.apache.hadoop.ha.HealthMonitor$MonitorDaemon.run(HealthMonitor.java:278)  ",ha
Disallow self failover,"It is currently possible for users to make a standby NameNode failover to itself and become active. We shouldn't allow this to happen in case operators mistype and miss the fact that there are now two active NNs.    {noformat}  bash-4.1$ hdfs haadmin -ns ha-nn-uri -failover nn2 nn2  Failover from nn2 to nn2 successful  {noformat}    After the failover above, nn2 will be active.",ha
Common side of HDFS-3148,"Common side of HDFS-3148, add necessary DNS and NetUtils methods. Test coverage is in the HDFS-3148 patch. ","io,performance"
Update commons-net version to 3.1,HADOOP-8210 requires the commons-net version be upgraded. Let's bump it to the latest stable version. The only other user is FtpFs.,"io,performance"
make hadoop script recognize a full set of deprecated commands,"bin/hadoop launcher script does a nice job of recognizing deprecated usage and vectoring users towards the proper command line tools (hdfs, mapred). It would be nice if we can take care of the following deprecated commands that don't get the same special treatment:    {noformat}    oiv                  apply the offline fsimage viewer to an fsimage    dfsgroups            get the groups which users belong to on the Name Node    mrgroups             get the groups which users belong to on the Job Tracker    mradmin              run a Map-Reduce admin client    jobtracker           run the MapReduce job Tracker node    tasktracker          run a MapReduce task Tracker node  {noformat}    Here's what I propos to do with them:    # oiv        -- issue DEPRECATED warning and run hdfs oiv    # dfsgroups  -- issue DEPRECATED warning and run hdfs groups    # mrgroups   -- issue DEPRECATED warning and run mapred groups    # mradmin    -- issue DEPRECATED warning and run yarn rmadmin    # jobtracker -- issue DEPRECATED warning and do nothing    # tasktracker-- issue DEPRECATED warning and do nothing    Thoughts?",scripts
Security support for ZK Failover controller,"To keep the initial patches manageable, kerberos security is not currently supported in the ZKFC implementation. This JIRA is to support the following important pieces for security:  - integrate with ZK authentication (kerberos or password-based)  - allow the user to configure ACLs for the relevant znodes  - add keytab configuration and login to the ZKFC daemons  - ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons","auto-failover,ha"
address log4j.properties inconsistencies btw main and template dirs,"DRFAAUDIT is in ./hadoop-common-project/hadoop-common/src/main/packages/templates/conf/log4j.properties but not in ./hadoop-common-project/hadoop-common/src/main/conf/log4j.properties (although it's used in hadoop-env.sh)    lesser issues but I noticed:    I see MRAUDIT commented out of main, but it's uncommented in template. (the conversion pattern also differs).     afaict the other appenders (JSA, MRAUDIT, TLA, etc...) are all still being used and should not be removed.    Should we be renaming some of these? JSA is being used by the history server for example.",conf
Edge case split-brain race in ZK-based auto-failover,"As discussed in HADOOP-8206, the current design for automatic failover has the following race:  - ZKFC1 gets active lock  - ZKFC1 is about to send transitionToActive() and machine freezes (eg GC pause + swapping)  - ZKFC1 loses its ZK lock, ZKFC2 gets ZK lock  - ZKFC2 calls transitionToStandby on NN1, and transitions NN2 to active  - ZKFC1 wakes up from pause, calls transitionToActive(), now we have a bad situation    This is rare, since it requires ZKFC1 to freeze longer than its ZK session timeout, but worth fixing, since the results can be disastrous.","auto-failover,ha"
RPC.closeProxy shouldn't throw error when closing a mock,"HADOOP-8202 changed the behavior of RPC.stopProxy() to throw an exception if called on an object which doesn't implement Closeable. Unfortunately, we use mock objects in many test cases, and those mocks don't implement Closeable. This is causing TestZKFailoverController to fail in trunk, for example.","ipc,test"
ZKFailoverController doesn't handle failure to become active correctly,"The ZKFC doesn't properly handle the case where the monitored service fails to become active. Currently, it catches the exception and logs a warning, but then continues on, after calling quitElection(). This causes a NPE when it later tries to use the same zkClient instance while handling that same request. There is a test case, but the test case doesn't ensure that the node that had the failure is later able to recover properly.","auto-failover,ha"
bin/hadoop should allow callers to set jsvc pidfile even when not-detached,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances    jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced.    if [ ""$_HADOOP_DAEMON_DETACHED"" = ""true"" ]; then        _JSVC_FLAGS=""-pidfile $_HADOOP_DAEMON_PIDFILE                    -errfile &1                    -outfile $_HADOOP_DAEMON_OUT""      else        # Even though we are trying to run a non-detached datanode,        # jsvc will not write to stdout/stderr, so we have to pipe        # it and tail the logfile.        log_path=/tmp/jsvc_${COMMAND}.$$        _JSVC_FLAGS=""-nodetach                     -errfile &1                     -outfile $log_path""        echo Non-detached jsvc output piping to: $log_path        touch $log_path        tail -f $log_path &      fi    And the relevant argument is '-pidfile' (http://linux.die.net/man/1/jsvc).  ",scripts
Initial patch for branch-1-win,None,native
Don't hardcode hdfs.audit.logger in the scripts,"The HADOOP_*OPTS defined for HDFS in hadoop-env.sh hard-code the hdfs.audit.logger (is explicitly set via ""-Dhdfs.audit.logger=INFO,RFAAUDIT"") so it's not overridable. Let's allow someone to override it as we do the other parameters by introducing HADOOP_AUDIT_LOGGER.",conf
a few native libraries are missing from a full build and binary tarball,"Hadoop 1.0.X ships the following native libraries in its tarball:    {noformat}  hadoop-1.0.1/c++/Linux-amd64-64/lib/libhadooppipes.a  hadoop-1.0.1/c++/Linux-amd64-64/lib/libhadooputils.a  hadoop-1.0.1/c++/Linux-i386-32/lib/libhadooppipes.a  hadoop-1.0.1/c++/Linux-i386-32/lib/libhadooputils.a  hadoop-1.0.1/lib/native/Linux-amd64-64/libhadoop.a  hadoop-1.0.1/lib/native/Linux-i386-32/libhadoop.a  hadoop-1.0.1/c++/Linux-amd64-64/lib/libhdfs.so  hadoop-1.0.1/c++/Linux-i386-32/lib/libhdfs.so  hadoop-1.0.1/lib/native/Linux-amd64-64/libhadoop.so  hadoop-1.0.1/lib/native/Linux-i386-32/libhadoop.so  {noformat}    While Hadoop 0.23.1 ships a subset:    {noformat}  ./lib/native/libhadoop.a  ./lib/native/libhdfs.a  ./lib/native/libhadoop.so  ./lib/native/libhdfs.so  {noformat}    The question is: do we have any reason not to support/publish the following libraries:  libhadooppipes.a, libhadooputils.a ?    And we also used to have librecordio that now seems to be missing from both.    If this is not intentional I can provide a patch to re-enable these bits.",build
Auto HA: Refactor tests and add stress tests,"It's important that the ZKFailoverController be robust and not contain race conditions, etc. One strategy to find potential races is to add stress tests which exercise the code as fast as possible. This JIRA is to implement some test cases of this style.","auto-failover,ha,test"
Make topologies easier to set up and debug,Topology scripts are a source of problems as they   # are site-specific.  # hard to get right.  # can have adverse consequences on cluster operation when they go wrong.  This issue is to group up the features needed to make it easier for ops people to get their scripts up and running.,util
Provide a command line entry point to view/test topology options,"Add a new command line entry point ""topo"" with commands for preflight checking of a clusters topology setup.     The initial operations would be to list the implementation class of the mapper, and attempt to load it, resolve a set of supplied hostnames, then dump the topology map after the resolution process.    Target audience:   # ops teams trying to get a new/changed script working before deploying it on a cluster.  # someone trying to write their first script.    Resolve and list the rack mappings of the given host  {code}  hadoop topo test [host1] [host2] ...   {code}    This would load the hostnames from a given file, resolve all of them and list the results:  {code}  hadoop topo testfile filename  {code}     This version is intended for the ops team who have a list of hostnames, IP addresses.     * Rather than just list them, the ops team may want to mandate that there were no /default-rack mappings found, as that is invariably a sign that the script isn't handling a hostname properly.  * No attempt to be clever and do IP address resolution, FQDN to hostname mapping, etc.      ",util
Enable user group mappings on Windows,The initial patch submitted is missing mapping of users to groups. A number of test failures are related to this. The jira tracks adding this support.,native
Support file permissions and ownership on Windows for RawLocalFileSystem,The initial patch submitted on HADOOP-8223 does not have ability to set file permissions on Windows. This is causing tests to fail. Jira tracks adding support to enable file permissions and ownership changes on Windows,native
Extend MD5MD5CRC32FileChecksum to show the actual checksum type being used,"In order to support HADOOP-8060, MD5MD5CRC32FileChecksum needs to be extended to carry the information on the actual checksum type being used. The interoperability between the extended version and branch-1 should be guaranteed when Filesystem.getFileChecksum() is called over hftp, webhdfs or httpfs.",fs
Allow users to specify a checksum type on create(),"Per discussion in HADOOP-8060, a way for users to specify a checksum type on create() is needed. The way FileSystem cache works makes it impossible to use dfs.checksum.type to achieve this. Also checksum-related API is at Filesystem-level, so we prefer something at that level, not hdfs-specific one.  Current proposal is to use CreatFlag.  ",fs
Building package fails under Windows,"After applying the patch in [MAPREDUCE-3540|https://issues.apache.org/jira/browse/MAPREDUCE-3540] the build fails if you do ""mvn clean package"" under Windows, in hadoop-hdfs-project and hadoop-dist.",build
Security support broken in CLI (manual) failover controller,"Some recent refactoring accidentally caused the proxies in some places to get created with a default Configuration, instead of using the Configuration set up by the DFSHAAdmin tool. This causes the HAServiceProtocol to be missing the configuration which specifies the NN principle -- and thus breaks the CLI HAAdmin tool in secure setups.","ha,security"
Fix flakiness in TestZKFailoverController,"When I loop TestZKFailoverController, I occasionally see two types of failures:  1) the ZK JMXEnv issue (ZOOKEEPER-1438)  2) TestZKFailoverController.testZooKeeperFailure fails with a timeout    This JIRA is for fixes for these issues.","auto-failover,ha"
Auto-HA: automatically scope znode by nameservice ID,"Talking to some folks who work on operations/deployment, they pointed out that it would make sense to automatically include the nameservice ID in the base znode used for automatic failover. For example, even though the ""root znode"" is ""/hadoop-ha"", we should put the znodes for a nameservice ""my-ha-cluster"" within ""/hadoop-ha/my-ha-cluster"". This allows federated setups to work with no additional configuration.","auto-failover,ha"
invalid hadoop-auth cookies should trigger authentication if info is avail before returning HTTP 401,"WebHdfs gives out cookies. But when the client passes them back, it'd sometimes reject them and return a HTTP 401 instead. (""Sometimes"" as in after a restart.) The interesting thing is that if the client doesn't pass the cookie back, WebHdfs will be totally happy.    The correct behaviour should be to ignore the cookie if it looks invalid, and attempt to proceed with the request handling.    I haven't tried HttpFs to see whether it handles restart better.    Reproducing it with curl:  {noformat}  ####################################################  ## Initial curl. Storing cookie to file.  ####################################################    [root@vbox2 ~]# curl -c /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'  HTTP/1.1 200 OK  Content-Type: application/json  Expires: Thu, 01-Jan-1970 00:00:00 GMT  Set-Cookie: hadoop.auth=""u=bcwalrus&p=bcwalrus&t=simple&e=1333614686366&s=z2w5xpFlufnnEoOHxVRiXqxwtqM="";Path=/  Content-Length: 597  Server: Jetty(6.1.26)    {""FileStatuses"":{""FileStatus"":[  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}  ]}}    ####################################################  ## Another curl. Using the cookie jar.  ####################################################    [root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'  HTTP/1.1 200 OK  Content-Type: application/json  Content-Length: 597  Server: Jetty(6.1.26)    {""FileStatuses"":{""FileStatus"":[  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}  ]}}    ####################################################  ## Restart NN.  ####################################################    [root@vbox2 ~]# /etc/init.d/hadoop-hdfs-namenode restartStopping Hadoop namenode:                                  [  OK  ]  stopping namenode  Starting Hadoop namenode:                                  [  OK  ]  starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-vbox2.out    ####################################################  ## Curl using cookie jar gives error.  ####################################################    [root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'  HTTP/1.1 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signature  Content-Type: text/html; charset=iso-8859-1  Set-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMT  Cache-Control: must-revalidate,no-cache,no-store  Content-Length: 1520  Server: Jetty(6.1.26)    <html>  <head>  <meta http-equiv=""Content-Type"" content=""text/html; charset=ISO-8859-1""/>  <title>Error 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</title>  </head>  <body><h2>HTTP ERROR 401</h2>  <p>Problem accessing /webhdfs/v1/. Reason:  <pre>    org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>                                                  ...    ####################################################  ## Curl without cookie jar is ok.  ####################################################    [root@vbox2 ~]# curl -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'  HTTP/1.1 200 OK  Content-Type: application/json  Expires: Thu, 01-Jan-1970 00:00:00 GMT  Set-Cookie: hadoop.auth=""u=bcwalrus&p=bcwalrus&t=simple&e=1333614995947&s=IXSvPIDbNrqmZryivGeoey6Kjwo="";Path=/  Content-Length: 597  Server: Jetty(6.1.26)    {""FileStatuses"":{""FileStatus"":[  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577906198,""owner"":""mapred"",""pathSuffix"":""tmp"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333577511848,""owner"":""hdfs"",""pathSuffix"":""user"",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""},  {""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1333428745116,""owner"":""mapred"",""pathSuffix"":""var"",""permission"":""755"",""replication"":0,""type"":""DIRECTORY""}  ]}}  {noformat}",security
"Auto-HA: add a config to enable auto-HA, which disables manual FC","Currently, if automatic failover is set up and running, and the user uses the ""haadmin -failover"" command, he or she can end up putting the system in an inconsistent state, where the state in ZK disagrees with the actual state of the world. To fix this, we should add a config flag which is used to enable auto-HA. When this flag is set, we should disallow use of the haadmin command to initiate failovers. We should refuse to run ZKFCs when the flag is not set. Of course, this flag should be scoped by nameservice.","auto-failover,ha"
Investigate uses of FileUtil and functional correctness based on current use cases,The current Windows patch replaces symlink with copy. This jira tracks understanding the implications of this change and others like it on expected functionality.,native
SecurityUtil.fetchServiceTicket broken after HADOOP-6941,"HADOOP-6941 replaced direct references to some classes with reflective access so as to support other JDKs. Unfortunately there was a mistake in the name of the Krb5Util class, which broke fetchServiceTicket. This manifests itself as the inability to run checkpoints or other krb5-SSL HTTP-based transfers:    java.lang.ClassNotFoundException: sun.security.jgss.krb5",security
Log message is same for two different conditions in BlockTokenSecretManager#checkAccess(),"{code}  if (!id.getBlockPoolId().equals(block.getBlockPoolId())) {        throw new InvalidToken(""Block token with "" + id.toString()            + "" doesn't apply to block "" + block);      }      if (id.getBlockId() != block.getBlockId()) {        throw new InvalidToken(""Block token with "" + id.toString()            + "" doesn't apply to block "" + block);      }  {code}  ",fs
Compilation error in ViewFileSystem.java,"ViewFileSystem.java fails to compile in branch-2 with these errors:    {noformat}  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[477,14] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[482,14] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[487,14] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[497,16] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[508,16] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[801,16] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem.InternalDirOfViewFs  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[806,16] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem.InternalDirOfViewFs  [ERROR] /hadoop/src/branch-2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java:[811,16] cannot find symbol  [ERROR] symbol  : class NotInMountpointException  [ERROR] location: class org.apache.hadoop.fs.viewfs.ViewFileSystem.InternalDirOfViewFs  {noformat}  ",build
Auto-HA: TestZKFailoverControllerStress occasionally fails with Mockito error,"Once in a while I've seen the following in TestZKFailoverControllerStress:     Unfinished stubbing detected here: -> at org.apache.hadoop.ha.TestZKFailoverControllerStress.testRandomHealthAndDisconnects(TestZKFailoverControllerStress.java:118)  E.g. thenReturn() may be missing....    This is because we set up the mock answers _after_ starting the ZKFCs. So if the ZKFC calls the mock object while it's in the middle of the setup, this exception occurs.","auto-failover,test"
Add interfaces for compression codecs to use direct byte buffers,"Currently, the codec interface only provides input/output functions based on byte arrays. Given that most of the codecs are implemented in native code, this necessitates two extra copies - one to copy the input data to a direct buffer, and one to copy the output data back to a byte array. We should add interfaces to Decompressor/Compressor that can work directly with direct byte buffers to avoid these copies.","io,native,performance"
Auto-HA: Replace ClientBaseWithFixes with our own modified copy of the class,"The class ClientBaseWithFixes is an attempt to add some workaround code to avoid spurious failures due to ZOOKEEPER-1438. But, even after making those workarounds, I've seen a few Jenkins failures due to that issue. Until ZK fixes this issue, I'd like to just copy the test infrastructure into our own code, and remove the problematic JMXEnv verifications.","auto-failover,test"
Auto-HA: Replace ClientBaseWithFixes with our own modified copy of the class,"The class ClientBaseWithFixes is an attempt to add some workaround code to avoid spurious failures due to ZOOKEEPER-1438. But, even after making those workarounds, I've seen a few Jenkins failures due to that issue. Until ZK fixes this issue, I'd like to just copy the test infrastructure into our own code, and remove the problematic JMXEnv verifications.","auto-failover,test"
Har file system doesn't deal with FS URIs with a host but no port,"If you try to run an MR job with a Hadoop Archive as the input, but the URI you give it has no port specified (e.g. ""hdfs://simon"") the job will fail with an error like the following:    {noformat}  java.io.IOException: Incomplete HDFS URI, no host: hdfs://simon:-1/user/atm/input.har/input  {noformat}",fs
"Between mapper and reducer, Hadoop inserts spaces into my string","In the mapper i send as key a number, and as value another number which has more than one digit, but i send them as Text objects. In my reducer all the values for a key have spaces between every digit of a value. I can't do my task because of this problem.     I don't use combiners or something else. ",io
Stringification of IPC calls not useful,"Since the Protobufification of Hadoop, the log messages on IPC exceptions on the server side now read like:    12/04/09 16:04:06 INFO ipc.Server: IPC Server handler 9 on 8021, call org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWritable@7087e9bf from 127.0.0.1:47989: error: org.apache.hadoop.ipc.StandbyException: Operation category READ is not supported in state standby    The call should instead stringify the method name and the request protobuf (perhaps abbreviated if it is longer than a few hundred chars)",ipc
A few pom.xml across Hadoop project may fail XML validation,In a few pom files there are embedded ant commands which contains '>' - redirection. This makes XML file invalid and this POM file can not be deployed into validating Maven repository managers such as Artifactory.,build
Fix some javadoc warnings on branch-1,"There are some javadoc warnings on branch-1, let's fix them.",documentation
hadoop-daemon.sh stop action should return 0 for an already stopped service ,"The following bit of code from hadoop-daemon.sh is not LSB compliant, since  according to http://refspecs.linuxbase.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html  a stop action on an already stopped service should return 0.    {noformat}   (stop)        if [ -f $pid ]; then        if kill -0 `cat $pid` > /dev/null 2>&1; then          echo stopping $command          kill `cat $pid`        else          echo no $command to stop          exit 1        fi      else        echo no $command to stop        exit 1      fi      ;;  {noformat}",scripts
BytesWritable length problem,"I tried to create my own Writable which contains a BytesWritable.    In my conctructor, I tried to create an empty BytesWritable :  BytesWritable key = new BytesWritable();    Next, in my readFields, I did :  key.readFields(in); LOG.debug(Bytes.toString(key.getBytes()));    The key contains much more bytes than I had wrote.  In fact, if my BytesWritable contains 100 bytes, I thing that the readFields() of BytesWritable call :  * setSize(0) (which seems useless since the values in the old range are preserved and any new values are undefined). * setSize(100) which extends the bytes array (by setCapacity) to 1.5 * the size (so 150) without initalizing it  * readFully(bytes, 0, 100) which fill the bytes array from '0' to '100' offsets.    And when I call getBytes() on it, the bytes array of 150 bytes is returned without any control.    That seems possible that the same problem happens in other conditions, when we increase ths bytes array size.",io
Update url for commons daemon ppc64 binary tarball,The following error message was seen while attempting to build branch-1 on PowerPC.    [get] Error opening connection java.io.FileNotFoundException: http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz        [get] Error opening connection java.io.FileNotFoundException: http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz        [get] Error opening connection java.io.FileNotFoundException: http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz        [get] Can't get http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz to /home/hadoop/branch-1/build/jsvc.ppc64/jsvc.ppc64.tar.gz    BUILD FAILED  /home/hadoop/branch-1_040612/build.xml:1606: The following error occurred while executing this line:  /home/hadoop/branch-1_040612/build.xml:2804: Can't get http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-${os-arch}.tar.gz to /home/hadoop/branch-1/build/jsvc.ppc64/jsvc.ppc64.tar.gz    There is no commons-daemon-1.0.2-bin-linux-ppc64.tar.gz available at the above URL.  ,build
Auto-HA: add config for java options to pass to zkfc daemon,"Currently the zkfc daemon is started without any ability to specify java options for it. We should be add a flag so heap size, etc can be specified.",scripts
Auto-HA: add basic HTTP interface to ZKFC,"Currently the ZKFC exposes no interfaces at all. It would be useful to add a very basic web interface, which shows its current status. This would also expose the usual /jmx, /conf, etc servlets for easier debugging and monitoring.",ha
Make sure components declare correct set of dependencies,"As mentioned by Scott Carey in https://issues.apache.org/jira/browse/MAPREDUCE-3378?focusedCommentId=13173437&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13173437, we need to make sure that components are declaring the correct set of dependencies. In current trunk there are errors of omission and commission (as reported by 'mvn dependency:analyze'):  * ""Used undeclared dependencies"" - these are dependencies that are being met transitively. They should be added explicitly as ""compile"" or ""provided"" scope.  * ""Unused declared dependencies"" - these are dependencies that are not needed for compilation, although they may be needed at runtime. They certainly should not be ""compile"" scope - they should either be removed or marked as ""runtime"" or ""test"" scope.  ",build
Auto-HA: Allow manual failover to be invoked from zkfc.,"HADOOP-8247 introduces a configure flag to prevent potential status inconsistency between zkfc and namenode, by making auto and manual failover mutually exclusive.    However, as described in 2.7.2 section of design doc at HDFS-2185, we should allow manual and auto failover co-exist, by:  - adding some rpc interfaces at zkfc  - manual failover shall be triggered by haadmin, and handled by zkfc if auto failover is enabled.   ","auto-failover,ha"
 Move VersionUtil/TestVersionUtil and GenericTestUtils from HDFS into Common.," We need to use VersionUtil in MAPREDUCE-4150. Moving VersionUtil/TestVersionUtil from HDFS into common will help this, especially since test-patch doesn't seem to deal well with cross-sub-project patches.","test,util"
start-all.sh refers incorrectly start-dfs.sh existence for starting start-yarn.sh,None,scripts
Allow tests to control token service value,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,test
"clover integration broken, also mapreduce poms are pulling in clover as a dependency",companion to MAPREDUCE-4141 - clover is broken on trunk.,build
Use ProtoBuf for RpcPayLoadHeader,None,ipc
Simplify getting a socket address from conf,"{{NetUtils.createSocketAddr(addr, port, confKey}} will throw an exception with a descriptive message of a malformed conf value.  A corresponding {{conf#getSocketAddr}} would make it easier to use, and ensure that {{NetUtils}} is used to parse the address.",conf
etc/hadoop is missing hadoop-env.sh,"The etc/hadoop directory in the tarball is missing hadoop-env.sh. It should be copied over like the other files in share/hadoop/common/templates/conf. Noticed templates/conf also contains mapred-site.xml and taskcontroller.cfg, we should remove those while we're at it.",conf
Hadoop Common project build failure in cygwin ,None,build
"Remove two remaining references to ""hadoop.native.lib"" oldprop",The following two test files still carry the old prop name:    {code}  # modified:   hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodec.java  # modified:   hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/file/tfile/TestTFileSeqFileComparison.java  {code}    This JIRA is to merely fix those up to use the new io.native.lib.enabled prop.,test
The native library's Makefile.am doesn't include JNI path,"When compiling on centos 6, I get the following error when compiling the native library:    {code}   [exec] /usr/bin/ld: cannot find -ljvm  {code}    The problem is simply that the Makefile.am libhadoop_la_LDFLAGS doesn't include AM_LDFLAGS.",build
IPC Connection becomes unusable even if server address was temporarilly unresolvable,"This is same as HADOOP-7428, but was observed on 1.x data nodes. This can happen more frequently after HADOOP-7472, which allows IPC Connection to re-resolve the name. HADOOP-7428 needs to be back-ported.",ipc
UTF8 class does not properly decode Unicode characters outside the basic multilingual plane,"this the log information  of the  exception  from the SecondaryNameNode:   2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for   non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????  ??????????tor.qzone.qq.com/keypart-00174          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)          at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)          at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)          at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)          at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)          at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)          at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)          at java.lang.Thread.run(Thread.java:619)    this is the log information  about the file from namenode:  2012-03-28 00:32:26,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss ip=/10.131.16.34 cmd=create src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 dst=null perm=boss:boss:rw-r--r--  2012-03-28 00:37:42,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174. blk_2751836614265659170_184668759  2012-03-28 00:37:42,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 is closed by DFSClient_attempt_201203271849_0016_r_000174_0  2012-03-28 00:37:50,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss ip=/10.131.16.34 cmd=rename src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 dst=/user/boss/pgv/fission/task16/split/  @?            tor.qzone.qq.com/keypart-00174 perm=boss:boss:rw-r--r--      after check the code that save FSImage,I found there are a problem that maybe a bug of HDFS Code,I past below:  -------------this is the saveFSImage method  in  FSImage.java, I make some mark at the problem code------------    /**     * Save the contents of the FS image to the file.     */    void saveFSImage(File newFile) throws IOException {      FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();      FSDirectory fsDir = fsNamesys.dir;      long startTime = FSNamesystem.now();      //      // Write out data      //      DataOutputStream out = new DataOutputStream(                                                  new BufferedOutputStream(                                                                           new FileOutputStream(newFile)));      try {        .........              // save the rest of the nodes        saveImage(strbuf, 0, fsDir.rootDir, out);------------------problem        fsNamesys.saveFilesUnderConstruction(out);------------------problem  detail is below        strbuf = null;      } finally {        out.close();      }        LOG.info(""Image file of size "" + newFile.length() + "" saved in ""           + (FSNamesystem.now() - startTime)/1000 + "" seconds."");    }     /**     * Save file tree image starting from the given root.     * This is a recursive procedure, which first saves all children of     * a current directory and then moves inside the sub-directories.     */    private static void saveImage(ByteBuffer parentPrefix,                                  int prefixLength,                                  INodeDirectory current,                                  DataOutputStream out) throws IOException {      int newPrefixLength = prefixLength;      if (current.getChildrenRaw() == null)        return;      for(INode child : current.getChildren()) {        // print all children first        parentPrefix.position(prefixLength);        parentPrefix.put(PATH_SEPARATOR).put(child.getLocalNameBytes());------------------problem        saveINode2Image(parentPrefix, child, out);      }     ..........    }       // Helper function that writes an INodeUnderConstruction    // into the input stream    //    static void writeINodeUnderConstruction(DataOutputStream out,                                             INodeFileUnderConstruction cons,                                             String path)                                              throws IOException {      writeString(path, out);------------------problem      ..........    }        static private final UTF8 U_STR = new UTF8();    static void writeString(String str, DataOutputStream out) throws IOException {      U_STR.set(str);      U_STR.write(out);------------------problem     }      /**     * Converts a string to a byte array using UTF8 encoding.     */    static byte[] string2Bytes(String str) {      try {        return str.getBytes(""UTF8"");------------------problem       } catch(UnsupportedEncodingException e) {        assert false : ""UTF8 encoding is not supported "";      }      return null;    }  ------------------------------------------below is the explain------------------------  in  saveImage method:  child.getLocalNameBytes(),the  bytes use the method of str.getBytes(""UTF8"");    but in writeINodeUnderConstruction, the bytes user the method of Class  UTF8 to get the bytes.    I make a test use our messy code file name , found the the two bytes arrsy are not equal. so I both use the class UTF8,then the problem desappare.    I think this is a bug of HDFS or UTF8.",io
Writable javadocs don't carry default constructor,The Writable API docs have a custom writable example but doesn't carry a default constructor in it. Apparently a default constructor is required and hence the example ought to carry it for benefit of the reader/paster.,documentation
ViewFs merge mounts ,A merge mount is a single mount represented by the union of two namespaces. See the viewfs docs (HADOOP-7426) and [ViewFs javadoc|http://hadoop.apache.org/common/docs/r0.23.0/api/org/apache/hadoop/fs/viewfs/ViewFs.html] for details.,viewfs
ViewFs doesn't work with a slash mount point,"We currently assume [a typical viewfs client configuration|https://issues.apache.org/jira/secure/attachment/12507504/viewfs_TypicalMountTable.png] is a set of non-overlapping mounts. This means every time you want to add a new top-level directory you need to update the client-side mountable config. If users could specify a slash mount, and then add additional mounts as necessary they could add a new top-level directory without updating all client configs (as long as the new top-level directory was being created on the NN the slash mount points to). This could be achieved by HADOOP-8298 (merge mounts, since we're effectively merging all new mount points with slash) or having the notion of a ""default NN"" for a mount table.",viewfs
Common (hadoop-tools) side of MAPREDUCE-4172,Patches on MAPREDUCE-4172 (for MR-relevant projects) that requires to run off of Hadoop Common project for Hadoop QA.    One sub-task per hadoop-tools submodule will be added here for reviews.,build
Clean up hadoop-rumen,Clean up a bunch of existing javac warnings in hadoop-rumen module.  ,build
Clean up hadoop-streaming,Clean up a bunch of existing javac warnings in hadoop-streaming module.  ,build
DNSToSwitchMapping should add interface to resolve individual host besides a list of host,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> resolve(List<String> names). But the two major caller: RackResolver.resolve() and DatanodeManager.resolveNetworkLocation() are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,io
distcp over viewfs is broken,This is similar to MAPREDUCE-4133. distcp over viewfs is broken because getDefaultReplication/BlockSize are being requested with no arguments.,viewfs
ZKFC: improve error message when ZK is not running,"Currently if you start the ZKFC without starting ZK, you get an ugly stack trace. We should improve the error message and give it a unique exit code.","auto-failover,ha"
The task-controller is not packaged in the tarball,"Ant in some situations, puts artifacts such as task-controller into the build/hadoop-*/ directory before the ""package"" target deletes it to start over.",build
Support cross-project Jenkins builds,This issue is to change test-patch to run only the tests for modules that have changed and then run from the top-level. See discussion at http://mail-archives.aurora.apache.org/mod_mbox/hadoop-common-dev/201204.mbox/%3CCAF-WD4TvKwYpuuQ9ibxv4UZ8B2BEhXnpfKb5mq3D-pwVKSHOzA@mail.gmail.com%3E.,build
Pseudo & Kerberos AuthenticationHandler should use getType() to create token,"Currently they use the constant TYPE, this means that if AuthenticationHandler are subclassed a new type cannot be used.    Instead they should use the getType() method.",security
FileContext#checkPath should handle URIs with no port,"AbstractFileSystem#checkPath is used to verify that a given path is for the same file system as represented by the AbstractFileSystem instance.    The original intent of the code was to allow for no port to be provided in the checked path, if the default port was being used by the AbstractFileSystem instance. However, before performing port handling, AFS#checkPath compares the full URI authorities for equality. Since the URI authority includes the port, the port handling code is never reached, and thus valid paths may be erroneously considered invalid.",fs
testpatch.sh should provide a simpler way to see which warnings changed,"test-patch.sh reports that a specific number of warnings has changed but it does not provide an easy way to see which ones have changed.  For at least the javac warnings we should be able to provide a diff of the warnings in addition to the total count, because we capture the full compile log both before and after applying the patch.    For javadoc warnings it would be nice to be able to provide a filtered list of the warnings based off of the files that were modified in the patch.",scripts
Add the ability for MetricsRegistry to remove metrics,Currently HBase is exposing metrics about a region through org.apache.hadoop.metrics.  As regions are moved around or deleted we will want to remove the metrics.,metrics
HttpServer#hasAdminAccess should return false if authorization is enabled but user is not authenticated,"If the user is not authenticated (request.getRemoteUser() returns NULL) or there is not authentication filter configured (thus returning also NULL), hasAdminAccess should return false. Note that a filter could allow anonymous access, thus the first case.  ",security
Support SASL-authenticated ZooKeeper in ActiveStandbyElector,"Currently, if you try to use SASL-authenticated ZK with the ActiveStandbyElector, you run into a couple issues:  1) We hit ZOOKEEPER-1437 - we need to wait until we see SaslAuthenticated before we can make any requests  2) We currently throw a fatalError when we see the SaslAuthenticated callback on the connection watcher    We need to wait for ZK-1437 upstream, and then upgrade to the fixed version for #1. For #2 we just need to add a case there and ignore it.","auto-failover,ha"
Audit logging should be disabled by default,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent).    Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",conf
Update maven-assembly-plugin to 2.3 - fix build on FreeBSD,There is bug in hadoop-assembly plugin which makes builds fail on FreeBSD because its chmod do not understand nonstgandard linux parameters. Unless you do mvn clean before every build it fails with:    [INFO] --- maven-assembly-plugin:2.2.1:single (dist) @ hadoop-common ---  [WARNING] The following patterns were never triggered in this artifact exclusion filter:  o  'org.apache.ant:*:jar'  o  'jdiff:jdiff:jar'    [INFO] Copying files to /usr/local/jboss/.jenkins/jobs/Hadoop-0.23/workspace/hadoop-common-project/hadoop-common/target/hadoop-common-0.23.3-SNAPSHOT  [WARNING] -------------------------------  [WARNING] Standard error:  [WARNING] -------------------------------  [WARNING]   [WARNING] -------------------------------  [WARNING] Standard output:  [WARNING] -------------------------------  [WARNING] chmod: /usr/local/jboss/.jenkins/jobs/Hadoop-0.23/workspace/hadoop-common-project/hadoop-common/target/hadoop-common-0.23.3-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-0.23.3-SNAPSHOT.jar: Inappropriate file type or format    [WARNING] -------------------------------  mojoFailed org.apache.maven.plugins:maven-assembly-plugin:2.2.1(dist)  projectFailed org.apache.hadoop:hadoop-common:0.23.3-SNAPSHOT  sessionEnded,build
Test Issue,None,benchmarks
FileSystem#checkPath and AbstractFileSystem#checkPath should share code,"Per the discussion on HADOOP-8310, these two methods can be refactored to share some code.",fs
Revert HADOOP-7940 and improve javadocs and test for Text.clear(),"Per [~jdonofrio]'s comments on HADOOP-7940, we should revert it as it has caused a performance regression (for scenarios where Text is reused, popular in MR).    The clear() works as intended, as the API also offers a current length API.",io
Add a ShutdownHookManager to be used by different components instead of the JVM shutdownhook,"FileSystem adds a JVM shutdown hook when a filesystem instance is cached.    MRAppMaster also uses a JVM shutdown hook, among other things, the MRAppMaster JVM shutdown hook is used to ensure state are written to HDFS.    This creates a race condition because each JVM shutdown hook is a separate thread and if there are multiple JVM shutdown hooks there is not assurance of order of execution, they could even run in parallel.      ",fs
test-patch can leak processes in some cases,"test-patch.sh can leak processes in some cases.  These leaked processes can cause subsequent tests to fail because they are holding resources, like ports that the others may need to execute correctly.",test
Duplicate FileSystem Statistics object for 'file' scheme,"Because of a change in HADOOP-8013, there are duplicate Statistics objects in FileSystem's statistics table: one for LocalFileSystem and one for RawLocalFileSystem. This causes MapReduce local file system counters to be incorrect some of the time. ",fs
Build fails with Java 7,"I am seeing the following message running IBM Java 7 running branch-1.0 code.  compile:  [echo] contrib: gridmix  [javac] Compiling 31 source files to /home/hadoop/branch-1.0_0427/build/contrib/gridmix/classes  [javac] /home/hadoop/branch-1.0_0427/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:396: error: type argument ? extends T is not within bounds of type-variable E  [javac] private <T> String getEnumValues(Enum<? extends T>[] e) {  [javac] ^  [javac] where T,E are type-variables:  [javac] T extends Object declared in method <T>getEnumValues(Enum<? extends T>[])  [javac] E extends Enum<E> declared in class Enum  [javac] /home/hadoop/branch-1.0_0427/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/Gridmix.java:399: error: type argument ? extends T is not within bounds of type-variable E  [javac] for (Enum<? extends T> v : e) {  [javac] ^  [javac] where T,E are type-variables:  [javac] T extends Object declared in method <T>getEnumValues(Enum<? extends T>[])  [javac] E extends Enum<E> declared in class Enum  [javac] Note: Some input files use unchecked or unsafe operations.  [javac] Note: Recompile with -Xlint:unchecked for details.  [javac] 2 errors    BUILD FAILED  /home/hadoop/branch-1.0_0427/build.xml:703: The following error occurred while executing this line:  /home/hadoop/branch-1.0_0427/src/contrib/build.xml:30: The following error occurred while executing this line:  /home/hadoop/branch-1.0_0427/src/contrib/build-contrib.xml:185: Compile failed; see the compiler error output for details.  ",build
TestSequenceFile.testCreateUsesFsArg() is broken,It seems HADOOP-8305 broke TestSequenceFile.testCreateUsesFsArg(). Fix the tests if the test is broken or the source.,test
Created patch that adds oracle support to DBInputFormat and solves a splitting duplication problem introduced with my last patch.,"This patch mainly resolves an overlap of records when splitting tasks in DBInputFormat, thereby removing duplication of records processed. Tested on 1.0.0 and 1.0.2",io
src/contrib/fuse-dfs build fails on non-Sun JVM environments,"src/contrib/fuse-dfs build failure when building in IBM Java 6 environment. The message on the console when the build aborts is:   [exec] /usr/bin/ld: cannot find -ljvm       [exec] collect2: ld returned 1 exit status       [exec] make[1]: *** [fuse_dfs] Error 1       [exec] make[1]: Leaving directory `/home/hadoop/branch-1.0_0427/src/contrib/fuse-dfs/src'       [exec] make: *** [all-recursive] Error 1    The reason this seems to be happening is because of the last line in src/contrib/fuse-dfs/src/Makefile.am    AM_LDFLAGS= -L$(HADOOP_HOME)/build/libhdfs -lhdfs -L$(FUSE_HOME)/lib -lfuse -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm    For hadoop to build on IBM Java, this last line should read as follows since this is where the libjvm library resides    AM_LDFLAGS= -L$(HADOOP_HOME)/build/libhdfs -lhdfs -L$(FUSE_HOME)/lib -lfuse -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/j9vm -ljvm    IMO, Changes like the following will need to be made to src/contrib/fuse-dfs/configure.ac (?) to include changes similar to that in src/native/ to check for the appropriate JVM and configure the appropriate path for ljvm.    dnl Check for '-ljvm'  JNI_LDFLAGS=""""  if test $JAVA_HOME != """"  then    JNI_LDFLAGS=""-L$JAVA_HOME/jre/lib/$OS_ARCH/server""    JVMSOPATH=`find $JAVA_HOME/jre/ -name libjvm.so | head -n 1`    JNI_LDFLAGS=""$JNI_LDFLAGS -L`dirname $JVMSOPATH`""  fi  ldflags_bak=$LDFLAGS  LDFLAGS=""$LDFLAGS $JNI_LDFLAGS""  AC_CHECK_LIB([jvm], [JNI_GetCreatedJavaVMs])  LDFLAGS=$ldflags_bak  AC_SUBST([JNI_LDFLAGS])    # Checks for header files.  dnl Check for Ansi C headers  AC_HEADER_STDC    dnl Check for other standard C headers  AC_CHECK_HEADERS([stdio.h stddef.h], [], AC_MSG_ERROR(Some system headers not found... please ensure their presence on your platform.))    dnl Check for JNI headers  JNI_CPPFLAGS=""""  if test $JAVA_HOME != """"  then    for dir in `find $JAVA_HOME/include -follow -type d`    do      JNI_CPPFLAGS=""$JNI_CPPFLAGS -I$dir""    done  fi                   ",build
Improve Configuration's address handling,There's a {{Configuration#getSocketAddr}} but no symmetrical {{setSocketAddr}}.  An {{updateSocketAddr}} would also be very handy for yarn's updating of wildcard addresses in the config.,util
LocalFileSystem Does not seek to the correct location when Checksumming is off.,"Hbase was seeing an issue when trying to read data from a local filesystem instance with setVerifyChecksum(false).  On debugging into it, the seek on the file was seeking to the checksum block index, but since checksumming was off that was the incorrect location.",fs
Can't renew or cancel HDFS delegation tokens over secure RPC,The fetchdt tool is failing for secure deployments when given --renew or --cancel on tokens fetched using RPC. (The tokens fetched over HTTP can be renewed and canceled fine.),security
jenkins complaining about 16 javadoc warnings ,See any of the mapreduce/hadoop jenkins reports recently and they all complain about 16 javadoc warnings.          -1 javadoc.  The javadoc tool appears to have generated 16 warning messages.    Which really means there are 24 since there are 8 that are supposed to be OK.,build
SNAPSHOT build versions should compare as less than their eventual final release,"We recently added a utility function to compare two version strings, based on splitting on '.'s and comparing each component. However, it considers a version like 2.0.0-SNAPSHOT as being greater than 2.0.0. This isn't right, since SNAPSHOT builds come before the final release.",util
HDFS command fails with exception following merge of HADOOP-8325,"We are seeing most hdfs commands in our nightly acceptance tests fail with an exception as shown below. This started with a few hours of the merge of HADOOP-8325 on 4/30/2012    hdfs --config conf/hadoop/ dfs -ls dirname  ls: `dirname': No such file or directory  12/05/01 16:57:52 WARN util.ShutdownHookManager: ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot remove a shutdownHook  java.lang.IllegalStateException: Shutdown in progress, cannot remove a shutdownHook    org.apache.hadoop.util.ShutdownHookManager.removeShutdownHook(ShutdownHookManager.java:166)    org.apache.hadoop.fs.FileSystem$Cache.remove(FileSystem.java:2202)    org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2231)    org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2251)    org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)  ",fs
Allow configuration of authorization for JmxJsonServlet and MetricsServlet,"When using authorization for the daemons' web server, it would be useful to specifically control the authorization requirements for accessing /jmx and /metrics.  Currently, they require administrative access.  This JIRA would propose that whether or not they are available to administrators only or to all users be controlled by ""hadoop.instrumentation.requires.administrator"" (or similar).  The default would be that administrator access is required.",util
Improve test-patch to make it easier to find javadoc warnings,"Often I have to spend a lot of time digging through logs to find javadoc warnings as the result of a test-patch. Similar to the improvement made in HADOOP-8339, we should do the following:  - test-patch should only run javadoc on modules that have changed  - the exclusions ""OK_JAVADOC"" should be per-project rather than cross-project  - rather than just have a number, we should check in the actual list of warnings to ignore and then fuzzy-match the patch warnings against the exclude list.","build,test"
HttpServer adds SPNEGO filter mapping but does not register the SPNEGO filter,"It seems the mapping was added to fullfil HDFS requirements, where the SPNEGO filter is registered.    The registration o the SPNEGO filter should be done at common level instead to it is avail for all components using HttpServer if security is ON.  ",security
Changes to support Kerberos with non Sun JVM (HADOOP-6941) broke SPNEGO,"before HADOOP-6941 hadoop-auth testcases with Kerberos ON pass, *mvn test -PtestKerberos*    after HADOOP-6941 the tests fail with the error below.    Doing some IDE debugging I've found out that the changes in HADOOP-6941 are making the JVM Kerberos libraries to append an extra element to the kerberos principal of the server (on the client side when creating the token) so *HTTP/localhost* ends up being *HTTP/localhost/localhost*. Then, when contacting the KDC to get the granting ticket, the server principal is unknown.    {code}  testAuthenticationPost(org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator)  Time elapsed: 0.053 sec  <<< ERROR!  org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)    org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:236)    org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:142)    org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:217)    org.apache.hadoop.security.authentication.client.AuthenticatorTestCase._testAuthentication(AuthenticatorTestCase.java:124)    org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$2.call(TestKerberosAuthenticator.java:77)    org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator$2.call(TestKerberosAuthenticator.java:74)    org.apache.hadoop.security.authentication.KerberosTestUtils$1.run(KerberosTestUtils.java:111)    java.security.AccessController.doPrivileged(Native Method)    javax.security.auth.Subject.doAs(Subject.java:396)    org.apache.hadoop.security.authentication.KerberosTestUtils.doAs(KerberosTestUtils.java:108)    org.apache.hadoop.security.authentication.KerberosTestUtils.doAsClient(KerberosTestUtils.java:124)    org.apache.hadoop.security.authentication.client.TestKerberosAuthenticator.testAuthenticationPost(TestKerberosAuthenticator.java:74)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    junit.framework.TestCase.runTest(TestCase.java:168)    junit.framework.TestCase.runBare(TestCase.java:134)    junit.framework.TestResult$1.protect(TestResult.java:110)    junit.framework.TestResult.runProtected(TestResult.java:128)    junit.framework.TestResult.run(TestResult.java:113)    junit.framework.TestCase.run(TestCase.java:124)    junit.framework.TestSuite.runTest(TestSuite.java:243)    junit.framework.TestSuite.run(TestSuite.java:238)    org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)    org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)    org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)    org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)    java.lang.reflect.Method.invoke(Method.java:597)    org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)    org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)    org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)    org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)    org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)  Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - UNKNOWN_SERVER)    sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:663)    sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:230)    sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)    org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:215)    org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:191)    java.security.AccessController.doPrivileged(Native Method)    javax.security.auth.Subject.doAs(Subject.java:396)    org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:191)   ... 36 more  Caused by: KrbException: Server not found in Kerberos database (7) - UNKNOWN_SERVER    sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:64)    sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:185)    sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:294)    sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:106)    sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:575)    sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:594)   ... 43 more  Caused by: KrbException: Identifier doesn't match expected value (906)    sun.security.krb5.internal.KDCRep.init(KDCRep.java:133)    sun.security.krb5.internal.TGSRep.init(TGSRep.java:58)    sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:53)    sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:46)  {code}",security
Hadoop Common logs misspell 'successful',"'successfull' is a misspelling of 'successful.'  Trivial patch attached.  The constants are private, and there doesn't seem to be any serialized form of these comments except in log files, so this shouldn't have compatibility issues.",security
Server$Listener.getAddress(..) may throw NullPointerException,"[Build #2365|https://builds.apache.org/job/PreCommit-HDFS-Build/2365//testReport/org.apache.hadoop.hdfs/TestHFlush/testHFlushInterrupted/]:  {noformat}  Exception in thread ""DataXceiver for client /127.0.0.1:35472 [Waiting for operation #2]"" java.lang.NullPointerException    org.apache.hadoop.ipc.Server$Listener.getAddress(Server.java:669)    org.apache.hadoop.ipc.Server.getListenerAddress(Server.java:1988)    org.apache.hadoop.hdfs.server.datanode.DataNode.getIpcPort(DataNode.java:882)    org.apache.hadoop.hdfs.server.datanode.DataNode.getDisplayName(DataNode.java:863)    org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:177)    java.lang.Thread.run(Thread.java:662)  {noformat}",ipc
ViewFS doesn't work when the root of a file system is mounted,Viewing files under a ViewFS mount which mounts the root of a file system shows trimmed paths. Trying to perform operations on files or directories under the root-mounted file system doesn't work. More info in the first comment of this JIRA.,viewfs
Improve NetUtils.getInputStream to return a stream which has a tunable timeout,"Currently, NetUtils.getInputStream will set the timeout on the new stream based on the socket's configured timeout at the time of construction. After that, the timeout cannot be changed. This causes a problem for cases like HDFS-3357. One approach used in some places in the code is to construct new streams when the timeout has to be changed, but this can cause bugs given that the streams are often wrapped by BufferedInputStreams.",util
hadoop-daemon.sh and yarn-daemon.sh can be misleading on stop,"The way that stop actions is implemented is a simple SIGTERM sent to the JVM. There's a time delay between when the action is called and when the process actually exists. This can be misleading to the callers of the *-daemon.sh scripts since they expect stop action to return when process is actually stopped.    I suggest we augment the stop action with a time-delay check for the process status and a SIGKILL once the delay has expired.    I understand that sending SIGKILL is a measure of last resort and is generally frowned upon among init.d script writers, but the excuse we have for Hadoop is that it is engineered to be a fault tolerant system and thus there's not danger of putting system into an incontinent state by a violent SIGKILL. Of course, the time delay will be long enough to make SIGKILL event a rare condition.    Finally, there's always an option of an exponential back-off type of solution if we decide that SIGKILL timeout is short.",scripts
test-patch findbugs may fail if a dependent module is changed,"This can happen when code in a dependent module is changed, but the change isn't picked up. E.g.     https://issues.apache.org/jira/browse/MAPREDUCE-4163?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13266867#comment-13266867    We can fix this by running 'mvn install -DskipTests -Dmaven.javadoc.skip=true' first.",build
SPNEGO filter throws/logs exception when authentication fails,"if the auth-token is NULL means the authenticator has not authenticated the request and it has already issue an UNAUTHORIZED response, there is no need to throw an exception and then immediately catch it and log it. The 'else throw' can be removed.",security
FileSystem service loading mechanism should print the FileSystem impl it is failing to load,"If by mistake somebody adds a FileSystem implementation to the service definition that is not serviceloader friendly (it does not override the getScheme() method) or adds to the classpath an older one defined in the service definition, the exception thrown should print out the FileSystem class failing to load.",fs
Restore security in Hadoop 0.22 branch,This is to track changes for restoring security in 0.22 branch.,security
Config-related WARN for dfs.web.ugi can be avoided.,"{code}  2012-05-04 11:55:13,367 WARN org.apache.hadoop.http.lib.StaticUserWebFilter: dfs.web.ugi should not be used. Instead, use hadoop.http.staticuser.user.  {code}    Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",conf
Clear up javadoc warnings in hadoop-common-project,"Javadocs added in HADOOP-8172 has introduced two new javadoc warnings. Should be easy to fix these (just missing #s for method refs).    {code}  [WARNING] Javadoc Warnings  [WARNING] /Users/harshchouraria/Work/code/apache/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:334: warning - Tag @link: missing '#': ""addDeprecation(String key, String newKey)""  [WARNING] /Users/harshchouraria/Work/code/apache/hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java:285: warning - Tag @link: missing '#': ""addDeprecation(String key, String newKey,  [WARNING] String customMessage)""  {code}",conf
empty-configuration.xml fails xml validation,/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/empty-configuration.xml    <?xml> declaration cant follow comment,test
Improve exception message when Configuration.set() is called with a null key or value,"Currently, calling Configuration.set(...) with a null value results in a NullPointerException within Properties.setProperty. We should check for null key/value and throw a better exception.",conf
Native build failure: javah: class file for org.apache.hadoop.classification.InterfaceAudience not found,"[INFO] --- native-maven-plugin:1.0-alpha-7:javah (default) @ hadoop-common ---  [INFO] /bin/sh -c cd /build/hadoop-common/hadoop-common-project/hadoop-common && /usr/lib/jvm/jdk1.7.0_02/bin/javah -d /build/hadoop-common/hadoop-common-project/hadoop-common/target/native/javah -classpath <...> org.apache.hadoop.io.compress.zlib.ZlibDecompressor org.apache.hadoop.security.JniBasedUnixGroupsMapping org.apache.hadoop.io.nativeio.NativeIO org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping org.apache.hadoop.io.compress.snappy.SnappyCompressor org.apache.hadoop.io.compress.snappy.SnappyDecompressor org.apache.hadoop.io.compress.lz4.Lz4Compressor org.apache.hadoop.io.compress.lz4.Lz4Decompressor org.apache.hadoop.util.NativeCrc32  Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate': class file for org.apache.hadoop.classification.InterfaceAudience not found  Cannot find annotation method 'value()' in type 'org.apache.hadoop.classification.InterfaceAudience.LimitedPrivate'  Error: cannot access org.apache.hadoop.classification.InterfaceStability    class file for org.apache.hadoop.classification.InterfaceStability not found    The fix for me was to changing the scope of hadoop-annotations from  ""provided"" to ""compile"" in pom.xml:       <dependency>       <groupId>org.apache.hadoop</groupId>       <artifactId>hadoop-annotations</artifactId>       <scope>compile</scope>     </dependency>    For some reason, it was the only dependency with scope ""provided"".",native
[Fsshell] Remove bin/hadoop reference from GenericOptionsParser default help text,Scenario:  ----------  Execute any fsshell command with invalid options    Like ./hdfs haadmin -transitionToActive...    Here it is logging as following..    bin/hadoop command [genericOptions] [commandOptions]...        Expected: Here help message is misleading to user saying that bin/hadoop that is not actually user ran    it's better to log bin/hdfs..Anyway hadoop is deprecated..    ,scripts
Hadoop 1.0.1 release - DFS rollback issues,See the next comment for details.,fs
normalizeHostName() in NetUtils is not working properly in resolving a hostname start with numeric character,"A valid host name can start with numeric value (You can refer RFC952, RFC1123 or http://www.zytrax.com/books/dns/apa/names.html), so it is possible in a production environment, user name their hadoop nodes as: 1hosta, 2hostb, etc. But normalizeHostName() will recognise this hostname as IP address and return directly rather than resolving the real IP address. These nodes will be failed to get correct network topology if topology script/TableMapping only contains their IPs (without hostname).","io,util"
Port RPC.getServerAddress to 0.23,"{{RPC.getServerAddress}} was introduced in trunk/2.0 as part of larger HA changes.  0.23 does not have HA, but this non-HA specific method is needed.",ipc
test-patch should stop immediately once it has found compilation errors,It does not makes sense to run findbugs or javadoc check if the program does not compile.  It was the behavior previously if I remember correctly.,build
Substitute _HOST with hostname  for HTTP principals ,"SPNEGO based Web Authentication uses HTTP/fqdn@REALM as the kerberos principal for each host.  Since it is difficult to modify the config for each host, a substitution feature where _HOST gets replaced by fqdn is implemented.   The task is to provide similar feature for the kerberos principals used for SPNEGO principals",security
TestKerberosAuthenticator fails,TestKerberosAuthenticator fails when executed on Jenkins    These failures are for  tests supposed to be ignored. (annotated with @Ignore) . But Jenkins executes and reports failures for these tests. ,security
hadoop script doesn't work if 'cd' prints to stdout (default behavior in Ubuntu),"if the 'hadoop' script is run as 'bin/hadoop' on a distro where the 'cd' command prints to stdout, the script will fail due to this line: 'bin=`cd ""$bin""; pwd`'    Workaround: execute from the bin/ directory as './hadoop'    Fix: change that line to 'bin=`cd ""$bin"" > /dev/null; pwd`'",scripts
Documentation - build hadoop for IBM PowerLinux (ppc64) enablement,"I have built hadoop for IBM PowerLinux, based on branch-1 source, and I would like to contribute to the Wiki a guide on how to do so. I'm adding here a version for revision and contribution, on MoinMoin wiki format.    A few notes about it and some Jira's related to that:    * https://issues.apache.org/jira/browse/DAEMON-249  This is need to compile jsvc binary for power. The latest version of commons-daemon (1.0.10) is not building. There is also a need to apply a patch from fedora project : http://pkgs.fedoraproject.org/gitweb/?p=apache-commons-daemon.git;a=blob_plain;f=apache-commons-daemon-ppc64-configure.patch;hb=53035f61b1bd51026377d5bcbc58d55f7f6e711d     * https://issues.apache.org/jira/browse/HADOOP-8333  The patch to compile fuse-dfs is not ready yet, but there is a workaround which consists of changing the value of AM_LDFLAGS in Makefile.am file.    * https://issues.apache.org/jira/browse/HDFS-3265  Fixed and committed    ",build
MetricsDynamicMBeanBase throws IllegalArgumentException for empty attribute list,MetricsDynamicMBeanBase.java      @Override    public AttributeList getAttributes(String[] attributeNames) {      if (attributeNames == null || attributeNames.length == 0)        throw new IllegalArgumentException();       The second clause isn't necessary.    There's some code used by management software (RHQ) that occasionally will query with an empty list.     Here is the stack trace.    Caused by: java.lang.IllegalArgumentException          at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttributes(MetricsDynamicMBeanBase.java:173)          at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:726)          at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:665)          at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1407)          at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)          at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)          at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)          at javax.management.remote.rmi.RMIConnectionImpl.getAttributes(RMIConnectionImpl.java:636)          at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)          at sun.rmi.transport.Transport$1.run(Transport.java:159)          at java.security.AccessController.doPrivileged(Native Method)          at sun.rmi.transport.Transport.serviceCall(Transport.java:155)          at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)          at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)          at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)          ... 3 more  ,metrics
TestFileSystemCanonicalization fails with JDK7,Failed tests:   testShortAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization):  expected:<myfs://host.a.b:123> but was:<myfs://host:123>   testPartialAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization):  expected:<myfs://host.a.b:123> but was:<myfs://host.a:123>    Passes on same machine with JDK 1.6.0_32.  ,test
Hadoop-auth should use log4j,"Per HADOOP-8086 hadoop-auth uses slf4j, don't see why it shouldn't use log4j to be consistent with the rest of Hadoop.",conf
"hadoop-config.sh missing variable exports, causes Yarn jobs to fail with ClassNotFoundException MRAppMaster","If you start a pseudo distributed yarn using ""start-yarn.sh"" you need to specify exports for HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, YARN_HOME, YARN_CONF_DIR, and HADOOP_MAPRED_HOME in hadoop-env.sh (or elsewhere), otherwise the spawned node manager will be missing these in it's environment. This is due to start-yarn using yarn-daemons. With this fix it's possible to start yarn (etc...) with only HADOOP_CONF_DIR specified in the environment. Took some time to track down this failure, so seems worthwhile to fix.",scripts
Add flag in RPC requests indicating when a call is a retry,"For idempotent operations, the IPC client transparently retries calls. For operations which aren't inherently idempotent, we often have to use some tricky logic to make them idempotent -- see HDFS-3031 for example. It would be nice if the RPC request had a flag indicating that the client was making a retry. Then, in the server side logic, we can add sanity checks that, when the logic indicates a call is an idempotent retry, the RPC call agrees.    One example where this is useful is the close() RPC. We can make it idempotent by saying that close() on an already-closed file should succeed. But, it's really an error to call close() twice outside the context of retries. Having this property set on the call would allow us to enable the ""double close is OK"" semantics only for retries.",ipc
Text shell command unnecessarily demands that a SequenceFile's key class be WritableComparable,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a sequence-file-header loaded key class to be a subclass of WritableComparable.    The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either.    We should relax the check and simply just check for ""Writable"", not ""WritableComparable"".",util
"DataStreamer, OutOfMemoryError, unable to create new native thread","We're trying to write about 1 few billion records, via ""Avro"". When we got this error, that's unrelated to our code:    10725984 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:58:43.290 # Written: 521000000 records  Exception in thread ""DataStreamer for file /Streams/Cubed/Stuff/objGame/aRandomGame/objType/aRandomType/2012/05/11/20/29/Shard.avro block blk_3254486396346586049_75838"" java.lang.OutOfMemoryError: unable to create new native thread          at java.lang.Thread.start0(Native Method)          at java.lang.Thread.start(Thread.java:657)          at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:612)          at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)          at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)          at org.apache.hadoop.ipc.Client.call(Client.java:1046)          at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)          at $Proxy8.getProtocolVersion(Unknown Source)          at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)          at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:160)          at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3117)          at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2586)          at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2790)  10746169 [Main] INFO net.gameloft.RnD.Hadoop.App - ## At: 2:59:03.474 # Written: 522000000 records  Exception in thread ""ResponseProcessor for block blk_4201760269657070412_73948"" java.lang.OutOfMemoryError          at sun.misc.Unsafe.allocateMemory(Native Method)          at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:117)          at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:305)          at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:75)          at sun.nio.ch.IOUtil.read(IOUtil.java:223)          at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)          at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:55)          at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)          at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)          at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)          at java.io.DataInputStream.readFully(DataInputStream.java:195)          at java.io.DataInputStream.readLong(DataInputStream.java:416)          at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)          at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2964)  #  # There is insufficient memory for the Java Runtime Environment to continue.  # Native memory allocation (malloc) failed to allocate 32 bytes for intptr_t in /build/buildd/openjdk-6-6b23~pre11/build/openjdk/hotspot/src/share/vm/runtime/deoptimization.cpp  [thread 1587264368 also had an error]  [thread 1111309168 also had an error]  [thread 1820371824 also had an error]  [thread 1343454064 also had an error]  [thread 1345444720 also had an error]  # An error report file with more information is saved as:  # [thread 1345444720 also had an error]  [thread -1091290256 also had an error]  [thread 678165360 also had an error]  [thread 678497136 also had an error]  [thread 675511152 also had an error]  [thread 1385937776 also had an error]  [thread 911969136 also had an error]  [thread -1086207120 also had an error]  [thread -1088251024 also had an error]  [thread -1088914576 also had an error]  [thread -1086870672 also had an error]  [thread 441797488 also had an error][thread 445778800 also had an error]    [thread 440400752 also had an error]  [thread 444119920 also had an error][thread 1151298416 also had an error]    [thread 443124592 also had an error]  [thread 1152625520 also had an error]  [thread 913628016 also had an error]  [thread -1095345296 also had an error][thread 1390799728 also had an error]    [thread 443788144 also had an error]  [thread 676506480 also had an error]  [thread 1630595952 also had an error]  pure virtual method called  terminate called without an active exception  pure virtual method called  Aborted    It seems to be a memory leak. We were opening 5 - 10 buffers to different paths when writing and closing them. We've tested that those buffers do not overrun. And they don't. But watching the application continue writing, we saw that over a period of 5 to 6 hours, it kept constantly increasing in memory, not by the average of 8MB buffer that we've set, but my small values. I'm reading the code and it seems there's a memory leak somewhere, in the way Hadoop does buffer allocation. While we specifically close the buffers if the count of open buffers is above 5 (meaning 5 * 8MB per buffer) this bug still happens.    Can it be fixed? As you can see from the strack trace, it writes a ""fan-out"" path of the type you see in the strack trace. We've let it execute till about 500M records, when this error blew. It's a blocker as these writers need to be production-grade ready, while they're not due to this native buffer allocation that when executing large amounts of writes, seems to generate a memory leak.    ",io
NPE thrown when IPC layer gets an EOF reading a response,"When making a call on an IPC connection where the other end has shut down, I see the following exception:  Caused by: java.lang.NullPointerException          at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:852)          at org.apache.hadoop.ipc.Client$Connection.run(Client.java:781)  from the lines:  {code}          RpcResponseHeaderProto response =               RpcResponseHeaderProto.parseDelimitedFrom(in);          int callId = response.getCallId();  {code}  This is because parseDelimitedFrom() returns null in the case that the next thing to be read on the stream is an EOF.",ipc
Remove JDK5 dependency from Hadoop 1.0+ line,This issues has been fixed in Hadoop starting from 0.21 (see HDFS-1552).  I propose to make the same fix for 1.0 line and get rid of JDK5 dependency all together.,build
"All commands warn ""Kerberos krb5 configuration not found"" when security is not enabled","Post HADOOP-8086 I get ""Kerberos krb5 configuration not found, setting default realm to empty"" warnings when running Hadoop commands even though I don't have kerb enabled.",security
Add support for generating pdf clover report in 1.1 release,Add support for generating clover PDF report.,build
bump up POMs version to 2.0.1-SNAPSHOT,None,build
ZKFC tests leak ZK instances,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.","auto-failover,test"
CompressionCodecFactory.CODEC_PROVIDERS iteration is thread-unsafe,"CompressionCodecFactory defines CODEC_PROVIDERS as:  {code}    private static final ServiceLoader<CompressionCodec> CODEC_PROVIDERS =      ServiceLoader.load(CompressionCodec.class);  {code}  but this is a lazy collection which is thread-unsafe to iterate. We either need to synchronize when we iterate over it, or we need to materialize it during class-loading time by copying to a non-lazy collection",io
MR doesn't work with a non-default ViewFS mount table and security enabled,"With security enabled, if one sets up a ViewFS mount table using the default mount table name, everything works as expected. However, if you try to create a ViewFS mount table with a non-default name, you'll end up getting an error like the following (in this case ""vfs-cluster"" was the name of the mount table) when running an MR job:    {noformat}  java.lang.IllegalArgumentException: java.net.UnknownHostException: vfs-cluster  {noformat}",viewfs
Fix TestCommandLineJobSubmission and TestGenericOptionsParser to work for windows,"There are multiple places in prod and test code where Windows paths are not handled properly. From a high level this could be summarized with:  1. Windows paths are not necessarily valid DFS paths (while Unix paths are)  2. Windows paths are not necessarily valid URIs (while Unix paths are)    #1 causes a number of tests to fail because they implicitly assume that local paths are valid DFS paths (by extracting the DFS test path from for example ""test.build.data"" property)    #2 causes issues when URIs are directly created on path strings passed in by the user","fs,test,util"
SPNEGO filter should have better error messages when not fully configured,"I upgraded to a build which includes SPNEGO, but neglected to configure  dfs.web.authentication.kerberos.principal. This resulted in the following error:    12/05/18 14:46:20 INFO server.KerberosAuthenticationHandler: Login using keytab //home/todd/confs/conf.pseudo.security//hdfs.keytab, for principal ${dfs.web.authentication.kerberos.principal}  12/05/18 14:46:20 WARN mortbay.log: failed SpnegoFilter: javax.servlet.ServletException: javax.security.auth.login.LoginException: Unable to obtain password from user    Instead, it should give an error that the principal needs to be configured. Even better would be if we could default to HTTP/_HOST@<default realm>",security
"TestStorageDirecotyFailure, TestTaskLogsTruncater, TestWebHdfsUrl and TestSecurityUtil fail on Windows",Jira tracking failures from the summary.,util
"TestModTime, TestDelegationToken and TestAuthenticationToken fail intermittently on Windows",Jira tracking failures from the summary.,test
Address problems related to localhost resolving to 127.0.0.1 on Windows,Localhost resolves to 127.0.0.1 on Windows and that causes the following tests to fail:   - TestHarFileSystem   - TestCLI   - TestSaslRPC    This Jira tracks fixing these tests and other possible places that have similar issue.,"fs,test"
getDouble() and setDouble() in org.apache.hadoop.conf.Configuration,"In the org.apache.hadoop.conf.Configuration class, methods exist to set Integers, Longs, Booleans, Floats and Strings, but methods for Doubles are absent. Are they not there for a reason or should they be added? In the latter case, the attached patch contains the missing functions.",conf
test-patch fails to apply the patch,"The patch needs to be applied from the root of the repository. The when test-patch is run frm common, hdfs or mapreduce subdirectories, the test-patch fails in applying the patch .    The error is shown below    [exec] ======================================================================       [exec]     Applying patch.       [exec] ======================================================================       [exec] ======================================================================       [exec]        [exec]        [exec] can't find file to patch at input line 5       [exec] Perhaps you used the wrong -p or --strip option?       [exec] The text leading up to this was:       [exec] --------------------------       [exec] |diff --git mapreduce/build.xml mapreduce/build.xml       [exec] |index ff73822..6a24ff2 100644       [exec] |--- mapreduce/build.xml       [exec] |+++ mapreduce/build.xml       [exec] --------------------------       [exec] File to patch:        [exec] Skip this patch? [y]   ",build
Fix UGI for IBM JDK running on Windows,The login module and user principal classes are different for 32 and 64-bit Windows in IBM J9 JDK 6 SR10. Hadoop 1.0.3 does not run on either because it uses the 32 bit login module and the 64-bit user principal class.,security
GzipCodec NPE upon reset with IBM JDK,"The GzipCodec will NPE upon reset after finish when the native zlib codec is not loaded. When the native zlib is loaded the codec creates a CompressorOutputStream that doesn't have the problem, otherwise, the GZipCodec uses GZIPOutputStream which is extended to provide the resetState method. Since IBM JDK 6 SR9 FP2 including the current JDK 6 SR10, GZIPOutputStream#finish will release the underlying deflater, which causes NPE upon reset. This seems to be an IBM JDK quirk as Sun JDK and OpenJDK doesn't have this issue.",io
Deprecate FileSystem#getDefault* and getServerDefault methods that don't take a Path argument ,"The javadocs for FileSystem#getDefaultBlockSize and FileSystem#getDefaultReplication claim that ""The given path will be used to locate the actual filesystem"" however they both ignore the path.",fs
MapFile.Reader.get() crashes jvm or throws EOFException on Snappy or LZO block-compressed data,"I am using Cloudera distribution cdh3u1.    When trying to check native codecs for better decompression  performance such as Snappy or LZO, I ran into issues with random  access using MapFile.Reader.get(key, value) method.  First call of MapFile.Reader.get() works but a second call fails.    Also  I am getting different exceptions depending on number of entries  in a map file.  With LzoCodec and 10 record file, jvm gets aborted.    At the same time the DefaultCodec works fine for all cases, as well as  record compression for the native codecs.    I created a simple test program (attached) that creates map files  locally with sizes of 10 and 100 records for three codecs: Default,  Snappy, and LZO.  (The test requires corresponding native library available)    The summary of problems are given below:    Map Size: 100  Compression: RECORD  ==================  DefaultCodec:  OK  SnappyCodec: OK  LzoCodec: OK    Map Size: 10  Compression: RECORD  ==================  DefaultCodec:  OK  SnappyCodec: OK  LzoCodec: OK    Map Size: 100  Compression: BLOCK  ================  DefaultCodec:  OK    SnappyCodec: java.io.EOFException  at  org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:114)    LzoCodec: java.io.EOFException at  org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData(BlockDecompressorStream.java:114)    Map Size: 10  Compression: BLOCK  ==================  DefaultCodec:  OK    SnappyCodec: java.lang.NoClassDefFoundError: Ljava/lang/InternalError  at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native  Method)    LzoCodec:  #  # A fatal error has been detected by the Java Runtime Environment:  #  #  SIGSEGV (0xb) at pc=0x00002b068ffcbc00, pid=6385, tid=47304763508496  #  # JRE version: 6.0_21-b07  # Java VM: Java HotSpot(TM) 64-Bit Server VM (17.0-b17 mixed mode linux-amd64 )  # Problematic frame:  # C  [liblzo2.so.2+0x13c00]  lzo1x_decompress+0x1a0  #      ",io
Upgrade commons-math version to 2.2,"From commons math 2.2 release note: ""This is primarily a maintenance release, but it also includes new features and enhancements. Users of version 2.1 are encouraged to upgrade to 2.2, as this release includes some important bug fixes."" Some downstream projects also need some new features in 2.2. Until we have a clear user container story, upgrading the dependency in Hadoop core is the most painless solution.",metrics
API to get info for deprecated key,"The current version of Hadoop deprecates many keys but, takes care of adding the new keys to the configuration accordingly.  For the end user only logs are there which indicates the key is deprecated and the message also suggests the new key to be used.    I was thinking; there should be an API/Utility which could probably provide the new key information when called with old key.  Using this API the user can judge in runtime to use the old key or new key.  Currently the Configuration provides an API only to check whether a key is deprecated or not but, doesn't provides a way to get the corresponding new key.  ",conf
"Convert Forrest docs to APT, incremental",Some of the forrest docs content in src/docs/src/documentation/content/xdocs has not yet been converted to APT and moved to src/site/apt. Let's convert the forrest docs that haven't been converted yet to new APT content in hadoop-common/src/site/apt (and link the new content into hadoop-project/src/site/apt/index.apt.vm) and remove all forrest dependencies.,documentation
Fix the tests FSMainOperationsBaseTest.java and F ileContextMainOperationsBaseTest.java to avoid potential test failure,"The above mentioned tests have a path filter to filter file names that contain either the string ""x"" or ""X"".  The filter contains the following if statement:  {code}  if(file.getName().contains(""x"") || file.toString().contains(""X""))  {code}  It should be corrected to:  {code}  if(file.getName().contains(""x"") || file.getName().contains(""X""))  {code}    Note that toString() returns the full path name.  These tests will fail when a directory name contains the string ""X"" and the base file name does not.  This was the case when we ran the tests on our system.","fs,test"
